---
title: 大数据平台框架常用参数优化
id: 2019-12-19 22:54:12
date: 2019-12-19 22:54:12
updated: 2019-12-19 22:54:12
categories:
tags:
keywords:
---

2019121901
大数据技术知识
Hadoop,HBase,Elasticsearch,HDFS,MapReduce,Spark,Storm,Kafka,Zookeeper


本文记录大数据平台框架的一些常用参数，这些参数基本是我见过的或者实际使用过的，我会列出参数的含义以及使用效果，具有一定的参考意义。当然，根据实际的场景不同，参数值并不能随便设置为一样，必须要考虑到实际的情况，否则可能没有效果，或者具有反作用。

<!-- more -->


# Hadoop


## HDFS

待整理

## MapReduce


- `map` 并发大小：`mapreduce.job.running.map.limit`，可以设置大点，50、100随便
- `map` 内存大小：`mapreduce.map.memory.mb`，单位为 `MB`，一般 `4GB` 够用
- `reduce` 启动延迟：`mapred.reduce.slowstart.completed.maps`，表示 `reduce` 在 `map` 执行到什么程度可以启动，例如设置为 `1.0` 表示等待 `map` 全部完成后才能执行 `reduce`
- `reduce` 内存大小：`mapreduce.reduce.memory.mb`，单位为 `MB`，要根据实际情况设置，一般 `4GB` 够用
- `reduce` 虚拟内存：`yarn.nodemanager.vmem-pmem-ratio`，一般2-5即可
- `reduce` 并发大小：`mapreduce.job.running.reduce.limit`，一般5-10个够用【根据业务场景、机器资源而定】


# HBase


待整理


# Elasticsearch


`allocate` 表示分片的复制分配；`relocate` 表示分片再次进行 `allocate`；`Recovery` 表示将一个索引的未分配 `shard` `allocate` 到一个结点的过程，在快照恢复、更改索引副本数量、结点故障、结点启动时发生。

如果设置索引副本数为1，同一个索引的主分片、副本分片不会被分配在同一个节点上面，这才能保证数据高可用，挂了一个节点也没关系。

- 磁盘空间使用占比上限：`cluster.routing.allocation.disk.watermark.high`，默认为90%，表示如果当前节点的磁盘使用占比超过这个值，则分片【针对所有类型的分片：主分片、副本分片】会被自动 `relocate ` 到其它节点，并且任何分片都不会 `allocate` 到当前节点【此外，对于新创建的 `primary` 分片也是如此，尽管不是 `allocate` 动作，除非整个 `Elasticsearch` 集群只有一个节点】
- 磁盘空间使用占比下限：`cluster.routing.allocation.disk.watermark.low`，默认为85%，表示如果当前节点的磁盘使用占比超过这个值，则分片【新创建的 `primary` 分片、从来没有进行过 `allocate` 的分片除外】不会被 `allocate` 到当前节点
- 索引的分片副本数：`number_of_replicas`，一般设置为1，表示总共有2份数据
- 每个节点分配的分片个数：`total_shards_per_node`，一般设置为2，一个节点只分配2个分片，分别为主分片、副本分片
- 索引的分片个数：`number_of_shards`，当索引数据很大时，一般设置为节点个数【例如 `索引数据大小/50GB` 大于节点个数，例如10个节点，索引大小 `800GB`，此时按照官方建议应该设置16个分片，但是分片过多也不好，就可以设置10个分片，每个分片大小 `80GB`】，再配合**分片副本数为1**、**每个节点分配的分片个数为2**，就可以确保分片分配在所有的节点上面，并且每个节点上有2个分片，分别为主分片、副本分片
- 数据刷新时间：`refresh_interval`，表示数据写入后等待多久可以被搜索到，默认值 `1s`，每次索引的 `refresh` 会产生一个新的 `lucene` 段，这会导致频繁的合并行为，如果业务需求对实时性要求没那么高，可以将此参数调大，例如调整为 `60s`，会大大降低 `cpu` 的使用率
- 索引的分片大小，官方建议是每个分片大小在 `30GB` 到 `50GB` 不要超过 `50GB`，所以当索引的数据很大时，就要考虑增加分片的数量
- 设置 `terms` 最大个数：`index.max_terms_count`，默认最大个数为65535，可以根据集群情况降低，例如设置为10000，为了集群稳定，一般不需要设置那么大
- 设置 `Boolean Query` 的子语句数量：`indices.query.bool.max_clause_count`，默认为1024，不建议增大这个值，也可以根据集群情况适当减小
- 查看热点线程：`http://your_ip:your_port/_nodes/your_node_name/hot_threads`，可以判断热点线程是 `search`，`bulk`，还是 `merge`，从而进一步分析是查询还是写入导致负载过高
- 数据目录：`path.data: /path/to/data`，多个目录使用逗号分隔，里面存放数据文件
- 日志目录：`path.logs: /path/to/logs`，里面存放的是节点的日志、慢查询日志、慢索引日志
- 家目录：`path.home: /path/to/home`，`elasticsearch` 的家目录，里面有插件、`lib`、配置文件等
- 插件目录：`path.plugins: /path/to/plugins`，插件目录，里面存放的是插件，例如：分词器
- 设置慢获取时间边界：`index.search.slowlog.threshold.fetch.warn: 30s`，超过这个时间的信息会被记录在日志文件中，`path.logs` 参数指定的目录中 `cluster-name_index_fetch_slowlog.log` 文件
- 设置慢查询时间边界：`index.search.slowlog.threshold.query.warn: 60s`，超过这个时间的信息会被记录在日志文件中，`path.logs` 参数指定的目录中 `cluster-name_index_search_slowlog.log` 文件
- 设置慢索引时间边界：`index.search.slowlog.threshold.index.warn: 60s`，超过这个时间的信息会被记录在日志文件中，`path.logs` 参数指定的目录中 `cluster-name_index_indexing_slowlog.log` 文件

script 脚本设置
rebalance 数据平衡
discovery配置
indices.breaker配置，熔断器，防止oom


# Spark


- 序列化方式：`spark.serializer`，可以选择：`org.apache.spark.serializer.KryoSerializer`
- `executor` 附加参数：`spark.executor.extraJavaOptions`，例如可以添加：`-Dxx=yy`【如果仅仅在 `driver` 端设置，`executor` 是不会有的】
- `driver` 附加参数：`spark.driver.extraJavaOptions`，例如可以添加：`-Dxx=yy`
- 日志配置文件设置，`Spark` 使用的是 `log4j`，默认在 `Spark` 安装目录的 `conf` 下面，如果想要增加 `log4j` 相关配置，更改 `driver` 机器上面的 `log4j.properties` 配置文件是无效的，必须把所有的 `executor` 节点上的配置文件全部更新。如果没有权限，也可以自己上传配置文件，然后需要在 `executor` 附加参数中指定：`-Dlog4j.configuration=file:/path/to/file`，启动 `Spark` 任务时还需要使用 `--files` 指定配置文件名称，多个用逗号分隔，用来上传配置文件到 `Spark` 节点
- 开启允许多 `SparkContext` 存在：`spark.driver.allowMultipleContexts`，设置为 `true` 即可，在使用多个 `SparkContext` 时，需要先停用当前活跃的，使用 `stop` 方法【在 `Spark v2.0` 以及以上版本，已经取消了这个限制】

## 集群参数

`yarn` 集群：

- 待定

`standalone` 集群

- 临时目录：`SPARK_LOCAL_DIRS`，用来存放 `Spark` 任务运行过程中的临时数据，例如内存不足时把数据缓存到磁盘，就会有数据写入这个目录，当然，在启动 `Spark` 任务时也可以单独指定，但是最好还是设置在集群上面，可以在 `spark-env.sh` 脚本中设置，键值对的形式，例如：`SPARK_LOCAL_DIRS=/your_path/spark/local`。需要注意的是，启动 `Excutor` 的用户必须有这个目录的写权限，并且保证这个目录的磁盘空间足够使用，否则在 `Spark` 任务中会出现异常：`java.io.IOException: Failed to create local dir in xx`，进而导致 `Task` 失败
- `Work` 目录：`SPARK_WORKER_DIR`，用来存放 `Work` 的信息，设置方式同上面的 `SPARK_LOCAL_DIRS`


# Storm


- `StormUI nimbus` 内容传输大小限制：`nimbus.thrift.max_buffer_size: 1048576`，取值的单位是字节，默认为`1048576`，如果 `nimbus` 汇报的内容过多，超过这个值，则在 `StormUI` 上面无法查看 `Topology Summary` 信息，会报错：`Internal Server Error org.apache.thrift7.transport.TTransportException: Frame size (3052134) larger than max length (1048576)`
- 执行实例 `worker` 对应的端口号：`supervisor.slots.ports:`，可以设置多个，和 `CPU` 的核数一致，或者稍小，提高机器资源的使用率
- `worker` 的 `JVM` 参数：`WORKER_GC_OPTS`，取值参考：`-Xms1G -Xmx5G -XX:+UseG1GC`，根据集群机器的资源多少而定，`G1` 是一种垃圾回收器
- `supervisor` 的 `JVM` 参数：`SUPERVISOR_GC_OPTS`，取值参考：`-Xms1G -Xmx5G -XX:+UseG1GC`，根据集群机器的资源多少而定，`G1` 是一种垃圾回收器
- `Storm UI` 的服务端口：`ui.port`，可以使用浏览器打开网页查看 `Topology` 详细信息
- `ZooKeeper` 服务器列表：`storm.zookeeper.servers`
- `ZooKeeper` 连接端口：`storm.zookeeper.port`
- `ZooKeeper` 中 `Storm` 的根目录位置：`storm.zookeeper.root`，用来存放 `Storm` 集群元信息
- 客户端连接 `ZooKeeper` 超时时间：`storm.zookeeper.session.timeout`
- `Storm` 使用的本地文件系统目录：`storm.local.dir`，注意此目录必须存在并且 `Storm` 进程有权限可读写
- `Storm` 集群运行模式：`storm.cluster.mode`，取值可选：`distributed`、`local`


# Kafka


注意，`Kafka` 的不同版本参数名、参数值会有变化，特别是 `v0.9.x` 之后，与之前的低版本差异很大，例如数据游标的参数可以参考我的另外一篇博文：[记录一个 Kafka 错误：OffsetOutOfRangeException](https://www.playpi.org/2017060101.html) ，`Kafka` 官网参见：[Kafka-v0.9.0.x-configuration](https://kafka.apache.org/090/documentation.html#consumerconfigs) 。

配置优化都是修改 `server.properties` 文件中参数值。

- `JVM` 参数：`KAFKA_HEAP_OPTS`，取值参考：`-Xmx2G`
- 文件存放位置：`log.dirs`，多个使用逗号分隔，注意所有的 `log` 级别需要设置为 `INFO`
- 单个 `Topic` 的文件保留策略：`log.retention.hours=72`【数据保留72小时，超过时旧数据被删除】，`log.retention.bytes=1073741824`【数据保留1GB，超过时旧数据被删除】
- 数据文件刷盘策略：`log.flush.interval.messages=10000`【每当 `producer` 写入10000条消息时，刷数据到磁盘】，`log.flush.interval.ms=1000`【每间隔1秒钟时间，刷数据到磁盘】
- `Topic` 的分区数量：`num.partitions=8`
- 启动 `Fetch` 线程给副本同步数据传输大小限制：`replica.fetch.max.bytes=10485760`，要比 `message.max.bytes` 大
- `message.max.bytes=10485700`，这个参数决定了 `broker` 能够接收到的最大消息的大小，要比 `max.request.size` 大
- `max.request.size=10480000`，这个参数决定了 `producer` 生产消息的大小
- `fetch.max.bytes=10485760`，这个参数决定了 `consumer` 消费消息的大小，要比 `message.max.bytes` 大
- `broker` 处理消息的最大线程数：`num.network.threads=17`，一般 `num.network.threads` 主要处理网络 `IO`，读写缓冲区数据，基本没有 `IO` 等待，配置线程数量为 `CPU` 核数加1
- `broker` 处理磁盘 `IO` 的线程数：`num.io.threads=32`，`num.io.threads` 主要进行磁盘 `IO` 操作，高峰期可能有些 `IO` 等待，因此配置需要大些，配置线程数量为 `CPU` 核数2倍，最大不超过3倍
- 强制新建一个 `segment`的时间：`log.roll.hour=72`
- 是否允许自动创建 `Topic`：`auto.create.topics.enable=true`，如果设置为 `false`，则代码无法创建，需要通过 `kafka` 的命令创建 `Topic`

留意参数取值大小的限制：`fetch.max.bytes` 大于 `message.max.bytes` 大于 `max.request.size`，`replica.fetch.max.bytes` 大于 `message.max.bytes` 大于 `max.request.size`。

外网集群可以访问
数据游标


# Zookeeper


待整理

