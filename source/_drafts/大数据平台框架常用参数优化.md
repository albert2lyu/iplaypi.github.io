---
title: 大数据平台框架常用参数优化
id: 2019-12-19 22:54:12
date: 2019-12-19 22:54:12
updated: 2019-12-19 22:54:12
categories:
tags:
keywords:
---

2019121901
大数据技术知识
Hadoop,HBase,Elasticsearch,HDFS,MapReduce,Spark,Storm,Kafka,Zookeeper


本文记录大数据平台框架的一些常用参数，这些参数基本是我见过的或者实际使用过的，我会列出参数的含义以及使用效果，具有一定的参考意义。当然，根据实际的场景不同，参数值并不能随便设置为一样，必须要考虑到实际的情况，否则可能没有效果，或者具有反作用。

<!-- more -->


# Hadoop


## HDFS

待整理

## MapReduce


- `map` 并发大小：`mapreduce.job.running.map.limit`，可以设置大点，50、100随便
- `map` 内存大小：`mapreduce.map.memory.mb`，单位为 `MB`，一般 `4GB` 够用
- `reduce` 启动延迟：`mapred.reduce.slowstart.completed.maps`，表示 `reduce` 在 `map` 执行到什么程度可以启动，例如设置为 `1.0` 表示等待 `map` 全部完成后才能执行 `reduce`
- `reduce` 内存大小：`mapreduce.reduce.memory.mb`，单位为 `MB`，要根据实际情况设置，一般 `4GB` 够用
- `reduce` 虚拟内存：`yarn.nodemanager.vmem-pmem-ratio`，一般2-5即可
- `reduce` 并发大小：`mapreduce.job.running.reduce.limit`，一般5-10个够用【根据业务场景、机器资源而定】


# HBase


待整理


# Elasticsearch


1、刷新频率
2、每个节点只分配一个分片
3、备份数
4、每个索引大小
5、磁盘使用占比


# Spark


- 序列化方式：`spark.serializer`，可以选择：`org.apache.spark.serializer.KryoSerializer`
- `executor` 附加参数：`spark.executor.extraJavaOptions`，例如可以添加：`-Dxx=yy`【如果仅仅在 `driver` 端设置，`executor` 是不会有的】
- `driver` 附加参数：`spark.driver.extraJavaOptions`，例如可以添加：`-Dxx=yy`
- 日志配置文件设置，`Spark` 使用的是 `log4j`，默认在 `Spark` 安装目录的 `conf` 下面，如果想要增加 `log4j` 相关配置，更改 `driver` 机器上面的 `log4j.properties` 配置文件是无效的，必须把所有的 `executor` 节点上的配置文件全部更新。如果没有权限，也可以自己上传配置文件，然后需要在 `executor` 附加参数中指定：`-Dlog4j.configuration=file:/path/to/file`，启动 `Spark` 任务时还需要使用 `--files` 指定配置文件名称，多个用逗号分隔，用来上传配置文件到 `Spark` 节点。


# Storm


待整理


# Kafka


注意，`Kafka` 的不同版本参数名、参数值会有变化，特别是 `v0.9.x` 之后，与之前的低版本差异很大，例如数据游标的参数可以参考我的另外一篇博文：[记录一个 Kafka 错误：OffsetOutOfRangeException](https://www.playpi.org/2017060101.html) ，`Kafka` 官网参见：[Kafka-v0.9.0.x-configuration](https://kafka.apache.org/090/documentation.html#consumerconfigs) 。


消息保存天数
外网集群可以访问
数据游标


# Zookeeper


待整理

