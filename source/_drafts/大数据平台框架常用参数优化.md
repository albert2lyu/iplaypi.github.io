---
title: 大数据平台框架常用参数优化
id: 2019-12-19 22:54:12
date: 2019-12-19 22:54:12
updated: 2019-12-19 22:54:12
categories:
tags:
keywords:
---

2019121901
大数据技术知识
Hadoop,HBase,Elasticsearch,HDFS,MapReduce,Spark,Storm,Kafka,Zookeeper


本文记录大数据平台框架的一些常用参数，这些参数基本是我见过的或者实际使用过的，我会列出参数的含义以及使用效果，具有一定的参考意义。当然，根据实际的场景不同，参数值并不能随便设置为一样，必须要考虑到实际的情况，否则可能没有效果，或者具有反作用。

<!-- more -->


# Hadoop


## HDFS

待整理

## MapReduce


- `map` 并发大小：`mapreduce.job.running.map.limit`，可以设置大点，50、100随便
- `map` 内存大小：`mapreduce.map.memory.mb`，单位为 `MB`，一般 `4GB` 够用
- `reduce` 启动延迟：`mapred.reduce.slowstart.completed.maps`，表示 `reduce` 在 `map` 执行到什么程度可以启动，例如设置为 `1.0` 表示等待 `map` 全部完成后才能执行 `reduce`
- `reduce` 内存大小：`mapreduce.reduce.memory.mb`，单位为 `MB`，要根据实际情况设置，一般 `4GB` 够用
- `reduce` 虚拟内存：`yarn.nodemanager.vmem-pmem-ratio`，一般2-5即可
- `reduce` 并发大小：`mapreduce.job.running.reduce.limit`，一般5-10个够用【根据业务场景、机器资源而定】


# HBase


待整理


# Elasticsearch


`allocate` 表示分片的复制分配；`relocate` 表示分片再次进行 `allocate`；`Recovery` 表示将一个索引的未分配 `shard` `allocate` 到一个结点的过程，在快照恢复、更改索引副本数量、结点故障、结点启动时发生。

如果设置索引副本数为1，同一个索引的主分片、副本分片不会被分配在同一个节点上面，这才能保证数据高可用，挂了一个节点也没关系。

- 磁盘空间使用占比上限：`cluster.routing.allocation.disk.watermark.high`，默认为90%，表示如果当前节点的磁盘使用占比超过这个值，则分片【针对所有类型的分片：主分片、副本分片】会被自动 `relocate ` 到其它节点，并且任何分片都不会 `allocate` 到当前节点【此外，对于新创建的 `primary` 分片也是如此，尽管不是 `allocate` 动作，除非整个 `Elasticsearch` 集群只有一个节点】
- 磁盘空间使用占比下限：`cluster.routing.allocation.disk.watermark.low`，默认为85%，表示如果当前节点的磁盘使用占比超过这个值，则分片【新创建的 `primary` 分片、从来没有进行过 `allocate` 的分片除外】不会被 `allocate` 到当前节点
- 索引的分片副本数：`number_of_replicas`，一般设置为1，表示总共有2份数据
- 每个节点分配的分片个数：`total_shards_per_node`，一般设置为2，一个节点只分配2个分片，分别为主分片、副本分片
- 索引的分片个数：`number_of_shards`，当索引数据很大时，一般设置为节点个数【例如 `索引数据大小/50GB` 大于节点个数，例如10个节点，索引大小 `800GB`，此时按照官方建议应该设置16个分片，但是分片过多也不好，就可以设置10个分片，每个分片大小 `80GB`】，再配合**分片副本数为1**、**每个节点分配的分片个数为2**，就可以确保分片分配在所有的节点上面，并且每个节点上有2个分片，分别为主分片、副本分片
- 数据刷新时间：`refresh_interval`，表示数据写入后等待多久可以被搜索到，默认值 `1s`，每次索引的 `refresh` 会产生一个新的 `lucene` 段，这会导致频繁的合并行为，如果业务需求对实时性要求没那么高，可以将此参数调大，例如调整为 `60s`，会大大降低 `cpu` 的使用率
- 索引的分片大小，官方建议是每个分片大小在 `30GB` 到 `50GB` 不要超过 `50GB`，所以当索引的数据很大时，就要考虑增加分片的数量


# Spark


- 序列化方式：`spark.serializer`，可以选择：`org.apache.spark.serializer.KryoSerializer`
- `executor` 附加参数：`spark.executor.extraJavaOptions`，例如可以添加：`-Dxx=yy`【如果仅仅在 `driver` 端设置，`executor` 是不会有的】
- `driver` 附加参数：`spark.driver.extraJavaOptions`，例如可以添加：`-Dxx=yy`
- 日志配置文件设置，`Spark` 使用的是 `log4j`，默认在 `Spark` 安装目录的 `conf` 下面，如果想要增加 `log4j` 相关配置，更改 `driver` 机器上面的 `log4j.properties` 配置文件是无效的，必须把所有的 `executor` 节点上的配置文件全部更新。如果没有权限，也可以自己上传配置文件，然后需要在 `executor` 附加参数中指定：`-Dlog4j.configuration=file:/path/to/file`，启动 `Spark` 任务时还需要使用 `--files` 指定配置文件名称，多个用逗号分隔，用来上传配置文件到 `Spark` 节点。


# Storm


待整理


# Kafka


注意，`Kafka` 的不同版本参数名、参数值会有变化，特别是 `v0.9.x` 之后，与之前的低版本差异很大，例如数据游标的参数可以参考我的另外一篇博文：[记录一个 Kafka 错误：OffsetOutOfRangeException](https://www.playpi.org/2017060101.html) ，`Kafka` 官网参见：[Kafka-v0.9.0.x-configuration](https://kafka.apache.org/090/documentation.html#consumerconfigs) 。


消息保存天数
外网集群可以访问
数据游标


# Zookeeper


待整理

