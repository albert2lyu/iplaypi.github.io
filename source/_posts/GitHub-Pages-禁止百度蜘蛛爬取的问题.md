---
title: GitHub Pages 禁止百度蜘蛛爬取的问题
id: 2019010501
date: 2019-01-05 00:42:49
updated: 2019-01-06 00:42:49
categories: 建站
tags: [建站,GitHub Pages,SEO,百度蜘蛛,Baiduspider]
keywords: GitHub Pages,SEO,百度蜘蛛,Baiduspider
---


最近才发现我的静态博客站点，大部分的网页没被百度收录，除了少量的网页是我自动提交（主动推动、自动推送）的，或者手动提交的，其它的网页都不被收录（网页全部是利用自动提交的 sitemap 方式提交的，一个都没收录）。我查看百度的站长工具后台，发现通过 sitemap 方式提交链接这种方式不可行，因为百度蜘蛛采集链接信息之前需要访问 baidusitemap.xml 文件，而这个文件是在 GitHub Pages 里面的，但是GitHub Pages 是禁止百度蜘蛛爬取的，所以百度蜘蛛在获取 baidusitemap.xml 文件这一步骤就被禁止了，GitHub Pages 返回403错误（在 http 协议中表示禁止访问），因此抓取失败（哪怕获取到 baidusitemap.xml 文件也不行，因为后续需要采集的静态网页全部是放在 GitHub Pages 中的，全部都会被禁止）。本文就详细描述这种现象，以及寻找可行的解决方案。


<!-- more -->


# 问题出现


首先在百度站长工具后台看到 baidusitemap.xml 抓取失败，查看具体原因是抓取失败（http 状态码 403）。

抓取失败
图

抓取失败原因概述
图

根据抓取失败原因，我还以为是文件不存在，或者根据链接打不开（链接是：[https://www.playpi.org/baidusitemap.xml](https://www.playpi.org/baidusitemap.xml) ），我使用浏览器和 curl 命令都尝试过了，链接没有问题，可以正常打开。然后根据 403 错误发现是拒绝访问，那就有可能是百度爬虫的问题了（被 GitHub Pages 禁止爬取了）。

使用浏览器打开
图

使用命令行打开（如下使用 curl  命令）

````bash
curl https://www.playpi.org/baidusitemap.xml
````

图

于是接下来，我就给官方提交了反馈，官方只是回复我说是链接问题。

提交反馈
图

反馈回复
图

前面我已经证明了链接没问题，那我就要猜想是百度蜘蛛爬虫的问题了，于是按照官方回复的建议，使用诊断工具看看是否可行。

诊断工具测试多次都失败
图

失败原因仍旧是拒绝访问（http 403状态码）
图

我又接着查看文档，发现拒绝访问的原因之一就是托管服务供应商阻止百度 Spider 访问我的网站。

文档说明截取片段
图

接下里我又查找了资料，并尝试给 GitHub 的技术支持发送邮件询问，得到了确认的答复，GitHub 已经禁止了百度蜘蛛爬虫的访问。

邮件回复
图




# 解决方案


本来这个问题是很好解决的（更换静态博客存储的主机即可，例如各种项目托管服务：码市、gitcafe、七牛云等，或者自己购买一台云主机），但是我不能抛弃 GitHub，于是问题变得复杂了。




# 问题总结








