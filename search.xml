<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[记录一次关于 log4j 的 ClassNotFoundException 异常]]></title>
    <url>%2F2019073001.html</url>
    <content type="text"><![CDATA[本来一个正常的 Java 项目，某一次运行的时候发现了一个异常：java.lang.ClassNotFoundException: org.apache.log4j.DailyRollingFileAppender，我觉得很奇怪，这种常用的类怎么可能会缺失。但是， 代码之多，无奇不有 ，遇到这种奇怪的问题也是检验我技术高低的良机，看我怎么步步排查，找到问题所在。本文开发环境基于 Java v1.8+、Spark v1.6.x、Maven v3.5.x 。问题出现 场景描述：一个常规的 Java 项目，单线程处理数据，一直以来都正常运行，某一天我做了小小的代码改动，接着运行就报错。错误日志信息如下：12345678910111213141516171819202122log4j:ERROR Could not instantiate class [org.apache.log4j.DailyRollingFileAppender].java.lang.ClassNotFoundException: org.apache.log4j.DailyRollingFileAppender at java.net.URLClassLoader.findClass (URLClassLoader.java:381) at java.lang.ClassLoader.loadClass (ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass (Launcher.java:338) at java.lang.ClassLoader.loadClass (ClassLoader.java:357) at java.lang.Class.forName0 (Native Method) at java.lang.Class.forName (Class.java:264) at org.apache.log4j.helpers.Loader.loadClass (Loader.java:178) at org.apache.log4j.helpers.OptionConverter.instantiateByClassName (OptionConverter.java:317) at org.apache.log4j.helpers.OptionConverter.instantiateByKey (OptionConverter.java:120) at org.apache.log4j.PropertyConfigurator.parseAppender (PropertyConfigurator.java:629) at org.apache.log4j.PropertyConfigurator.parseCategory (PropertyConfigurator.java:612) at org.apache.log4j.PropertyConfigurator.configureRootCategory (PropertyConfigurator.java:509) at org.apache.log4j.PropertyConfigurator.doConfigure (PropertyConfigurator.java:415) at org.apache.log4j.PropertyConfigurator.doConfigure (PropertyConfigurator.java:441) at org.apache.log4j.helpers.OptionConverter.selectAndConfigure (OptionConverter.java:468) at org.apache.log4j.LogManager.&lt;clinit&gt;(LogManager.java:122) at org.slf4j.impl.Log4jLoggerFactory.getLogger (Log4jLoggerFactory.java:64) at org.slf4j.LoggerFactory.getLogger (LoggerFactory.java:285) at org.slf4j.LoggerFactory.getLogger (LoggerFactory.java:305) at com.xxx.yyy.client.hbase.HBaseUtils.&lt;clinit&gt;(HBaseUtils.java:36)错误日志很多，主要看这一行信息：java.lang.ClassNotFoundException: org.apache.log4j.DailyRollingFileAppender，找不到 DailyRollingFileAppender 这个类，即类缺失。显然，这不可能是代码改动引起的问题，这种情况可能是虚拟机没有加载到类，或者加载了多个版本不一致的类导致冲突。查了很多网上的相同问题，都说是依赖包缺失，但是我觉得不太可能，因为这个 Java 项目中的其它模块都能正常使用【使用多个 Maven 模块管理整个 Java 项目，它们的环境一致】，于是想办法验证一下。先在 Java 项目中搜索类，可以看到能搜索到，说明不会缺失【此处不考虑打包过程中移除的情况】。再使用 mvn dependency:tree 生成依赖树信息，在依赖树信息中搜索查看，也能看到关于 slf4j 的两个依赖包以及关于 log4j 的一个依赖包，说明没有缺失。1234[INFO] | | +- org.slf4j:slf4j-api:jar:1.7.10:compile[INFO] | | +- org.slf4j:slf4j-log4j12:jar:1.7.10:compile...[INFO] +- log4j:log4j:jar:1.2.12:compile根据上面的操作分析，依赖没有缺失，而且，从搜索结果看只有一个类，从依赖树信息中看也没有多版本冲突，此时看似陷入了僵局。问题分析解决 我努力回想改动了什么代码或者配置才会导致这个问题，使用 Git 查一下，通过查看提交历史记录，发现了一处微小的改动，在 Maven 子模块的 pom.xml 文件中。这也是造成这个问题的罪魁祸首，下面详细说明。其实，此时需要考虑一个问题，本机查看的项目代码和打包后的可能不一样，比如冲突问题导致的版本选择，或者插件造成的部分无效依赖被移除等原因会造成前后差异。我也一直在回想我改动了什么代码或者配置，才触发了这个问题，果然，通过 Git 的提交记录找到了蛛丝马迹。通过仔细的对比，发现了问题所在，原来在 pom.xml 文件中，使用了 maven-shade-plugin 插件进行依赖瘦身，导致将 slf4j、log4j 相关的依赖全部被移除。归根结底，还是因为我在代码中没有使用 slf4j、log4j 的相关类【但是在父类中使用了】，maven-shade-plugin 插件误以为这两个依赖都是无用的，就全部移除了。等到程序启动运行的时候，发现找不到相关的类了。pom.xml 配置信息如下：123456789101112131415161718192021222324252627282930313233343536373839404142&lt;!-- shade 构件，打包时可以：包含依赖构件，重命名包名避免冲突，移除特定的类避免冲突 --&gt;&lt;!-- 具体参考:http://maven.apache.org/plugins/maven-shade-plugin/--&gt;&lt;!-- &lt;minimizeJar&gt;true&lt;/minimizeJar&gt; 可以自动移除无用的类，瘦身 jar 包 --&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;!-- 绑定 Maven 的 package 阶段 --&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;!-- 详细配置项 --&gt; &lt;configuration&gt; &lt;!-- 自动移除无用的依赖，坑：项目没用到 slf4j, 但是依赖的父类用到，却被移除 --&gt; &lt;!--&lt;minimizeJar&gt;true&lt;/minimizeJar&gt;--&gt; &lt;!-- 将指定文件以 append 方式加入到构建的 jar 包中 --&gt; &lt;transformers&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.AppendingTransformer&quot;&gt; &lt;resource&gt;reference.conf&lt;/resource&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;!-- 过滤匹配到的文件 --&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;!-- 附加所有构件，并指定后缀名，与主程序 jar 包区分开 --&gt; &lt;shadedArtifactAttached&gt;true&lt;/shadedArtifactAttached&gt; &lt;shadedClassifierName&gt;jar-with-dependencies&lt;/shadedClassifierName&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt;其中，&lt;minimizeJar&gt;true&lt;/minimizeJar&gt; 这个配置决定了打包的依赖保留还是移除，我把它配置为 true，打包时会自动帮我移除无用的依赖包，其中包括 log4j、slf4j，也就导致了本文开头的问题。看来，maven-shade-plugin 插件的依赖瘦身功能，还是要慎用，像今天这种情况就很是莫名其妙，只能靠细心、靠经验来发现问题、解决问题，如果是别人的代码还真难发现。解决方法很简单，只要把这个配置移除【或者设置为 false】，问题就解决了。还有另外一种解决方式，在代码中显式使用 log4j 的相关类，其实真实是使用 slf4j 里面的实现类，这样打包时 maven-shade-plugin 插件则不会移除相关的类。问题总结 在这种 ClassNotFoundException 异常现象的分析过程中，可以借助一款工具：Arthas（阿尔萨斯） ，这是一款由 阿里巴巴 开源的一款 Java 诊断工具，深受开发者喜爱。它可以解决类似如下的问题：这个类从哪个 jar 包加载的？为什么会报各种类相关的 Exception？我改的代码为什么没有执行到？难道是我没 commit？分支搞错了？遇到问题无法在线上 debug，难道只能通过加日志再重新发布吗？线上遇到某个用户的数据处理有问题，但线上同样无法 debug，线下无法重现！是否有一个全局视角来查看系统的运行状况？有什么办法可以监控到 JVM 的实时运行状态？比如针对我这个场景，我就可以快速查到 DailyRollingFileAppender 这个类有没有被虚拟机加载，以及从哪个 jar 包加载的。可以快速发现：虚拟机中并没有加载这个类，这个时候就可以断定类缺失，然后转换思路去查为什么类缺失。如果在项目中搜索、查看依赖树信息都没有发现类缺失的迹象，就可以怀疑是不是打包过程中被移除了，甚至可以怀疑是不是上传了错误的 jar 包去执行程序。这样就可以一步一步、有理有据地分析问题，直到解决问题，不至于全程懵逼，靠经验与猜测去碰运气。显然，解决问题的过程肯定是目的明确而且高效的。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Maven</tag>
        <tag>log4j</tag>
        <tag>slf4j</tag>
        <tag>ClassNotFoundException</tag>
        <tag>DailyRollingFileAppender</tag>
        <tag>maven-shade-plugin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Vultr 创建云主机详细步骤]]></title>
    <url>%2F2019072801.html</url>
    <content type="text"><![CDATA[我在以前的一篇博客中，详细记录了自己搭建翻墙梯子的过程，参考：使用 Vultr 搭建 Shadowsocks（VPS 搭建 SS） ，其中，我还顺便留下了我的 Vultr 推广链接：我的 10 美元推广链接 。可能是因为这篇博客的流量还不错，基本每个月都会有几个人通过我的推广链接注册，当然，注册后真正使用的人只有 1-2 个。 我看了一下我的账户，平均每个月都会收到一份有效注册所带来的优惠券，大概可以给我带来 $10 的收入，足够抵消我的月租消费了，这也是注册 Vultr 后并真正使用 Vultr 的人带给我的收入，我觉得这是意外的惊喜。所以，本着分享的理念以及「吃饭」的需要，我推荐大家使用 Vultr，但是担心可能有些读者第一次使用 Vultr，系统又是英文的，不太熟悉。在此，我把节点的创建过程再整理出来，图文并茂，并说明注意事项，给读者参考。读者参考本文一步一步操作，就可以从零开始创建 Vultr 主机，只需要再花费 $10 就可以使用半年的 Vultr 云主机。至于搭建梯子的过程还是请继续参考上面给出的以前的博客内容：使用 Vultr 搭建 Shadowsocks（VPS 搭建 SS） 。声明 ：2019 年 08 月 09 日发现 Vultr 官方不会再赠送 $10 的代金券给新注册用户，只会给我发放代金券，但是 $25 的代金券仍然有效，请读者选择 $25 对应的链接打开注册，以免错失了代金券。首先声明，本文中的文字与截图整理于 2019 年 07 月 26 日到 2019 年 08 月 03 日，以后 Vultr 这个产品的界面或者功能可能会变化，所以请读者以实际使用时的产品为准，本文仅供参考。但是我觉得无论 Vultr 这个产品再怎么变，本文描述的这几大核心功能也不会变，最多就是界面操作的变化，读者可以放心参考使用。开篇推荐 使用 Vultr 的云主机，最好选择洛杉矶地区的或者日本的服务器，我亲自测试这两个地区的服务器最稳定，已经推荐给很多人，而且网速相对来说较好，我的推广链接【可以获取 10 美元的代金券，只要充值 10 美元就能使用】：我的 10 美元推广链接 ，官网链接也在这里：Vultr 。 这里再多说点，如果使用上面的推广链接注册 Vultr 帐号，可以获取 10 美元的代金券，需要在 30 天之内使用，使用的条件就是充值 10 美元以上的钱。例如充值 10 美元就会获取 20 美元的帐号余额，这些钱如果购买 3.5 美元的主机可以使用半年了，挺划算的。此外还有一个限时的大优惠，如果准备长期使用 Vultr，肯定要充值多一点，我这里有一个限时的推广链接：我的 25 美元推广链接 ，可以获取 25 美元的代金券，使用条件就是充值 25 美元以上的金额。假如充值了 25 美元，总共获取 50 美元入账，购买 3.5 美元的主机可以使用 14 个多月，适合长期使用 Vultr 的。 以下列举 Vultr 的五大好处：扣费灵活 ，Vultr 有一个好处就是主机的费用并不是按照月份扣除的，而是按照天扣除的，每天扣除的费用是 月租 / 30。例如你的主机只用了 10 天，然后销毁不用了，实际只会扣除月租 1/3 的钱，这种方式很是灵活，哪怕主机的 IP 地址被屏蔽了也可以销毁重新生成一个，并不会浪费钱。它不像国内的云服务商，一般是按照月份扣费的。主机管理灵活 ，它不像国内的云服务商，购买一台云主机后，直接先扣费，然后分配一台主机，IP 地址是固定的，如果有问题只能重启。而在 Vultr 中是可以随意创建、销毁虚拟主机的，根据你自己的需求，选择配置、主机机房位置、操作系统，几分钟就可以生成一台主机，如果用了几天觉得不好，或者 IP 地址被封，再销毁重新创建即可，Vultr 只会扣除你几天的费用，非常人性化。价格优惠 ，根据配置的不同，价格有多个档次，有 $2.5 / 月 （只有 IP6 地址）、$3.5 / 月、$5 / 月 等等，更贵的也有，一般个人使用选择这三个中的一个就够用了，但是要注意便宜的经常售罄，而且最便宜的只支持 IP6，慎用。大家如果看到没有便宜的主机了不用着急，可以先买了贵的用着，反正费用是按照天数扣除的，等后续发现便宜的套餐赶紧购买，同时把贵的主机给销毁，不会亏钱的。 付费方式灵活 ，付费方式除了支持常见的 Paypal、 信用卡 等方式，它还支持 比特比 、 支付宝 、 微信 等方式。就问你是不是很人性化，作为一家国外的公司，还特意支持 支付宝 、 微信 的方式支付，也从侧面反映了随着中国的日益强大，中国的电子支付方式正在走向全球，越来越流行。机房分布全球 ，它的机房位置遍布全球，例如 日本 、 新加坡 、 澳大利亚 、 美国 、 德国 、 英国 、 加拿大 ，读者根据网络的需求可以灵活选择。至于使用 Vultr 云主机做什么，我想最大的用处就是自行搭建梯子，可以参考我以前的博客内容：使用 Vultr 搭建 Shadowsocks（VPS 搭建 SS） 。产品介绍 首先来看一下官网主页，官网主页地址为：Vultr ，目前看官网的 UI 和几个月前相比有变化，颜色在视觉上加深了，图标显得更加拟物化。登录的时候需要特别注意，它的验证码非常丧心病狂，一般是六位，而且很难看清楚，更加让人抓狂的是，有时候看不清还不能换一个，只能在输入认证错误后再重新输入，这种用户体验比不上国内的厂商。当然，在常用的网络环境中，Vultr 会检测出当前为常用网络，从而跳过验证码验证这一步骤，所以有时候不需要输入验证码。登录成功后，进入到主页，默认是在 Products 模块下面的 Instances 标签页。读者可以看到我这里已经有主机显示出来，如果读者是第一次注册后登录进去或者还没有创建主机的话，是看不到类似于我这里图中的 Server 列表的。不过不用着急，后面我会带领读者一步一步进行主机的创建。先来熟悉一下系统的所有模块与常用功能，毕竟界面是全英文的，读者熟悉后方便在后续的操作中熟练找到需要的模块与功能。注意查看上面的首页图，读者也可以在登录进入系统之后，随便点击浏览一下。先看左侧列表，分为五个大模块：Products、Billing、Support、Affiliate、Account，下面简单描述一下这五个大模块，读者看完后在心里可以有一个基本的概念：Products： 产品管理 模块，在里面可以管理主机、登录密钥、DNS、系统快照等信息，最常用的就是主机的创建、查看、销毁 Billing： 账单管理 模块，在里面可以查看历史消费记录、支付方式，最常用的就是欠费充值、查看历史消费记录 Support： 系统支持 模块，可以在里面浏览一些常见问题，或者联系客服，一般情况下用不到，目前无需关心 Affiliate： 营销推广 模块，可以利用 Vultr 为你生成的唯一链接，拉取新用户注册使用，然后就会给你返代金券，可抵扣消费，一般情况下用不到，不过可以分享给身边的人试试 Account： 帐号信息 模块，包括姓名、地址、邮箱等信息，每月的扣费记录会以账单的形式发送到你的邮箱，这个模块除了第一次设置，以后基本用不到，一般情况下每个月看一次邮箱即可 产品管理 产品管理 里面的功能很多，但是对于我这样需求简单的人来说，不需要那么多功能，我只需要创建主机、系统快照这两个功能足够，所以我也只会介绍这两个功能。主机创建 在 Products 模块的 Instances 标签页中，可以看到 Server 列表，也就是用户创建的主机列表，如果还没有创建，则显示为空。看看我的主机列表，已经有一台主机，如下图。读者可以注意到右侧有一个蓝色背景的圆形按钮，如果把鼠标的光标放上去，会显示 Deploy New Server，它就是用来创建主机的，下面我将一步一步演示创建的过程。点击上述蓝色按钮，会进入 Deploy New Instance 配置界面，读者在 Choose Server 中首先选择 Cloud Compute，它表示 云主机 ，也就是虚拟主机，它旁边还有三个产品类型可以忽略。接着在 Server Location 中选择机房位置，这个要根据网络的需要请读者自行选择，如果你在中国大陆，需要搭建梯子，建议选择日本或者美国洛杉矶的机房位置，我这里以日本为例。继续往下看，在 Server Type 中选择操作系统类型，我选择 CentOS 7x64。在 Server Size 中选择主机配置，同时也代表价格，我选择 $5/mon，表示每个月费用五美元，再详细看一下机器的配置：25GB SSD 表示固态硬盘大小、$0.007/h 表示每小时费用 0.007 美元、1 CPU 表示机器的 CPU 为一核、1024MB Memory 表示机器的内存大小、1000GB Bandwidth 表示机器的流量大小。最后的 Additional Features、Startup Script、SSH Keys、Server Hostname &amp; Label 可以不用设置，如果读者对 Linux 服务器有一定的了解并且会简单操作，可以在 Startup Script 设置启动脚本用来安装需要的软件环境，也可以在 SSH Keys 中设置密钥用来后续的免密登录，在此不在赘述。一切设置完成后，点击右下角的 Deploy Now 蓝色按钮，接着等待几分钟即可。在等待的过程中，可以回到前面的 Instances 标签页中查看 Server 列表，可以看到刚刚创建的机器正在初始化。大概需要几分钟的时间，主机就创建完成。主机创建完成，最重要的事情就是要得知它的 IP 地址、登录密码，才能进行下一步的操作。IP 地址其实在 Server 列表中已经可以看到，是 198.13.59.132，也可以直接点击主机，进入到主机的详情页。或者点击最右侧的三个点，选择 Server Details 进入主机的详情页。在 Overview 标签页，此时可以看到更多关于主机的信息，包括 IP 地址、登录用户名、登录密码、流量使用情况、CPU 消耗监控、当前扣费情况。系统快照 前面主机创建的过程读者看了可能会觉得很麻烦，需要设置那么多东西，如果以后机器有问题需要重新创建还要来一遍，体验多不好。况且，如果机器上面安装了一些软件，设置了一些参数，从头再来很麻烦的，有没有简单的方式可以复制一台已经存在的主机呢，除了 IP 地址不一样，其它配置完全一致，并且还要保留机器上面的软件、参数等信息。可以，当然可以，接下来， 系统快照 功能就要出场了。 系统快照 其实就是把操作系统在某一时刻的状态保存下来，包括系统的配置、安装的软件、参数的配置，生成的系统快照就可以随时重复使用，就像克隆一样。为了使用系统快照，必须先创建系统快照，在主机的详情页中，有一个 Snapshots 标签页，就是用来创建系统快照的。先在文本框中填入一个名字，用来标记这一个系统快照，然后点击右侧的 Take Snapshot 开始创建。在创建的过程中，可以在 Products 模块中的 Snapshots 标签页中查看系统快照的生成状态，这个过程一般需要很长时间，根据系统的复杂度而定。当然，如果系统中没有安装任何软件，创建起来还是很快的，几分钟就行。等系统快照创建完成，使用系统快照就简单得多了，在创建云主机的过程中，不需要选择那么多的参数，选择机房位置后，在 Server Type 中点击 Snapshot 标签页，可以看到系统快照列表，从中选择一个自己需要的系统快照，直接生成即可。使用系统快照创建的主机，除了机房位置、IP 地址与系统快照中的不一样，其它配置信息都是一样，也包括原来的软件、系统配置。账单管理 账单管理 模块包含设置支付方式、充值、消费记录查看等功能，一般都会用到充值功能，用户使用这种虚拟服务一般不会充值太多的钱，可能半年或者一年才会充值一次。支付方式选择 支付方式就是用户选择使用什么方式来支付，设置好就行，以后除非更换支付方式，否则再也用不到这个功能。除了常规的 信用卡 、Paypal 支付方式，Vultr 还支持 支付宝 、 微信 、 比特比 支付方式，这支付体验对于中国用户来说简直太友好了，直接下单扫码分分钟就能完成。在 Billing 模块选择 Make Payment 标签页，可以看到左侧列表中有多种支付方式可以选择，我这里已经绑定了信用卡。其中，Credit Card 表示信用卡、Bitcoin 表示比特币、Alipay 表示支付宝，Wechat Pay 表示微信支付，读者可以自行选择。绑定具体的支付方式时，需要填写帐号信息，读者根据要求填写即可。查看历史消费记录 查看历史消费记录，就是为了对对账，看看有没有额外的乱扣费现象，一般情况下通过邮箱查看就行，不用特意登录 Vultr 里面看。不过，账单默认是不会被发送到邮箱的，也不会通知用户，需要提前设置好通知方式，在 Billing 模块中的 History 标签页最底部有一个通知方式设置，选择通过邮箱发送即可，这个邮箱帐号是在 Account 模块中设置的，可以参考下文的相关内容。选择 Billing 模块中的 Histroy 标签页，可以看到我的消费记录，八月一日扣除了我七月份的费用，总计 $3.5，这算是很便宜的主机配置，以前我用的是 $5 的主机，后来发现流量用不完就换了便宜的主机。接着再看看我的邮箱，收到了费用扣除账单通知，一共扣除 $3.5。仔细看 History 标签页中的历史清单，可以发现其中还有一些是进账，这种情况可能是自己充值，或者是邀请新用户使用 Vultr 带来的代金券。可以看到在七月份有两笔钱进账，总计二十美元，这明确记得我在七月份没有充值，应该是 Vultr 发放给我的代金券，如果按照 $5 / 月 计算，这笔钱可以抵扣我四个月的主机费用了。 系统支持 我自始至终只使用了两次这个模块，都是为了调整我的消费额度，前提是必须把需求说明清楚，并整理成英文，然后类似于提交 工单 一样把需求从后台发给客服。我的需求就是把我的月消费额度下调至 $15 / 月 ，主机节点个数下调至 3 个，这样可以防止由于自己误操作导致的费用消耗过多，或者密码泄漏被人滥用。当然，这些情况理论上都不会发生，我只是图一个安心而已。 有了自己的需求，我整理了一段简单的说明，内容如下：12345678New Instance Limit:3New Monthly Instances Cost Limit:15Intended Usage:I want to increase my limit of instance cost,15 is my choose.然后在 Support 模块中，选择 Tickets 功能。可以看到在右上角有一个 Open New Ticket 按钮，是用来新建工单的，点击它。在跳转到的内容填写页面中填写你的需求即可，需要选择问题类型、主机、标题、内容，填写完成后点击下方的 Open Ticket 按钮即可。接下来就是等待了，客服不会及时回复的，一般需要等待 1 个工作日【24 个小时】。而且由于时差的原因，客服一般是在半夜回复，我们只能等到第二天再看。提交后也可以在工单列表中查看历史工单，并可以随时打开进行补充，它就像一个聊天对话系统，但不是实时的，读者要做的就是等待。给你们看一下客服给我的回复，提交工单、客服回复前后相差 32 个小时，这效率也是挺低的，还好这个功能基本不会用到，要不然等这么久会疯掉的。1234567891011Hello, The limits have been set as requested.Please let us know if you need further assistance. Our team is here and always happy to help.Thank you for choosing Vultr!Best Regards,NachelleVultr.com我这个申请的调整结果可以在 Billing 模块中的 Limits 标签页中查看，主机个数、消费上线都已经调整成我希望的。营销推广 营销推广 模块里面的功能基本不会用到，除非你能邀请到别人注册并充值使用 Vultr 这个产品，这样的话 Vultr 就会返给你代金券，可以抵扣消费，但是不能提现。所以我在此只简单介绍其中的两个小功能：分享链接、用户统计。分享链接 分享链接，就是把 Vultr 为你生成的唯一链接分享出去，别人点击你的链接注册后，就算作是你带来的用户，如果注册用户又充值并且在 Vultr 里面消费，你就会收到代金券。然而，用户只是注册是不行的，不算作有效用户，毕竟随便找几个邮箱就可以注册了，必须充值使用才算。在 Affilicate 模块中，选择 Linking Code 标签页，就可以看到 Vultr 为你生成的唯一链接了。例如我的唯一链接是：我的 Vultr $10 代金券链接 ，文本形式则是：https://www.vultr.com/?ref=7443790，如下图。 用户统计 用户统计就是查看自己邀请到的用户点击、用户注册、用户消费情况，选择 Stats 标签页，可以看到每个月的点击用户、注册用户、消费用户数据，并且 Vultr 已经使用条形统计图的方式展示出来。以此可以查看自己为 Vultr 带来的用户，以及自己能不能有代金券的收入。看看我的用户统计，七月份比较惨淡，共有二十多个用户点击了我的链接，但是只有一个用户注册了，至于这个用户有没有充值继续使用还不确定，要等两个月之后才能判断。帐号信息 帐号信息在 Account 模块中设置，能用到的也就是个人简介，在 Profile 标签页中。而且这个模块一般用不到，读者无需关心。我也就是填写姓名、邮箱、地址等信息。其中，邮箱帐号是很有必要的，在 账单管理 中设置账单的通知方式时，使用的就是这里填写的邮箱帐号，这样我每个月才会收到 Vultr 发来的扣费通知。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>Vultr</tag>
        <tag>VPS</tag>
        <tag>vps</tag>
        <tag>Shadowsocks</tag>
        <tag>Affiliate</tag>
        <tag>cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[番茄鸡蛋面做法总结 - 酸汤口味]]></title>
    <url>%2F2019072701.html</url>
    <content type="text"><![CDATA[面食是中国常见的一种传统主食，馒头、饼、包子、馍、面条、面叶，使用面粉做出来的类型多种多样。其中， 面条 可谓是最为流行，无论东北、西北、东南、西南、中原、华南，都可以看到 面条 身影，形式丰富，口味不一，这更是中华民族劳动人民智慧的结晶。其中， 汤面 又是一派，汤面最重要的是什么，除了面条本身之外，最重要的是汤。而汤，是非常难做的，耗时长且配料讲究，不仅要选骨头，还要放大料，一般一锅好汤都要熬制两个小时以上，更甚者要熬制四个小时。一般家庭吃面条是不可能这么折腾的，那么我这里有一种更为简单的方法：使用番茄鸡蛋做酸汤，再配合面条，做一碗酸汤面。这种方法操作起来简单，耗时短，又能保留汤的美味，适合自己在家做，本文记录做法总结。引言 我在网络上看到全国很多卖的火爆的汤面，看自媒体介绍，看采访报道，都是注重讲究汤的制作，还自夸是祖传，几十年积累改良下来的，无论他们怎么说，这可见汤的重要性。在广州的上下九、老西关，也各有一家不错的广式汤面，汤真的很不错，年头也比较久。目前市面上为什么很多快餐店的汤面、汤粉都不好吃，就是汤不行。我记得小时候在江南一带生活过一段时间，那里的快餐店的外墙上都会印着 民以食为天、面以汤为鲜 作为广告语，而且他们的汤做的真不错，一碗简单的青菜面都很好吃，例如上海清汤面。另外他们的小馄饨也很好吃，也是取决于汤很好，不过现在很难找到这么好吃的了。当然，也要考虑到成本，现在如果你舍得消费 40-50 元人民币去吃一碗面，在广州还是能找到比较美味的面的，比如上下九、老西关那两家，此外以前还有一家上海风味的店： 寻人启示 【在兆佳业广场，不知道现在还开不开】，味道也很正宗。汤固然很重要，但是普通人在家里做面条，不可能耗费那么多精力去熬制一锅汤出来，熬出来只做几碗面也很浪费。此时，可以有两个选择：一个是购买 号称高汤的调料 ，也就是使用一些浓缩骨粉、香料、调料混合制成的面汤专用调料，煮面的时候放在水里，可以把面汤调制成美味鲜香的汤。我小时候用过，当时觉得挺好吃的，但是现在吃起来明显味道不对，而且我担心健康问题。二是直接购买 浓缩高汤液体 ，一般的冷藏的，号称使用原汤制作，买回来尽快使用，不宜旧存，价格比较高，而且我也担心健康问题。除了这些有没有其他选择了呢，有，那就是我下面的重点，最简单的番茄鸡蛋汤。这种汤做法简单，汤味鲜美，用来下面最合适。食材准备 以下食材适合 2 人食用，我自己吃是直接吃了一盆：番茄 2 个，选择粉的，不要脆的 手工新鲜面条 1 斤，最好是刚做出来的湿面条，实在没有挂面也可以，挂面就用不了 1 斤，半斤足够 鸡蛋 1-2 个 食用盐 其它配菜任选，例如青菜、酱牛肉、榨菜、牛肉丸 番茄 2 个 手工湿面条 1 斤 制作步骤 从食材准备到出锅装盘，大概需要 20 分钟即可。番茄去皮切丁 番茄切十字花刀，把番茄放在沸水中煮 1 分钟，并使用勺子不断浇番茄上半部分，然后就很容易去皮了。去皮后切丁，放在盘子中备用。番茄切丁 番茄丁下油锅 如果有吃鸡蛋面汤的需求，先把鸡蛋炒好，捣碎备用，番茄炒好后再放进去，一起煮汤。但是我是直接吃煎蛋，所以这里不放鸡蛋了。开锅，加花生油，烧热后加入切好的番茄丁，快速翻炒，大概炒 1-2 分钟，番茄丁会产生糊状的酱汁，就可以准备加水了。炒制番茄丁 加水煮沸 立马加适量水，稍微搅拌一下，接着煮沸。下面条 汤煮沸后开始加面条，注意要一点一点加，并及时搅拌，否则面条很容易粘连，那一锅面条就废了，如果感觉水量不够要及时加水，不要犹豫。下面条 适当搅拌防止粘连 煮沸后加冷水 接着就是等待煮沸，此时需要反复三次煮沸，第一次煮沸后稍微加一点冷水，防止冒锅，后两次煮沸只要打开锅盖散气即可。 注意，这时候别忘记加盐。第一次煮沸加冷水 出锅装盆 最终出锅，装盆，一盆香喷喷的酸汤面就做好了。配上煎蛋，再切几个牛肉丸，人间美味。注意，我这不是普通的碗，我这是盆，这一盆吃下去我真的有点撑了。我一开始就不想混鸡蛋在里面，怕汤喝不完浪费了，于是只放了番茄做酸汤，另外煎了一个鸡蛋。一大盆被我吃光 其它方式 如果觉得汤里少了点什么，也可以选择一开始炒鸡蛋捣碎，然后放入汤中。别小看简单的鸡蛋番茄，做出来的汤非常好喝，一口气我可以喝两碗。锅里的样子 出锅装盆的样子 注意事项 1、番茄品种一定要选择 粉 的，不要 脆 的，这样更容易熬制出美味的酸汤。2、面条下锅后一定要迅速搅拌一下，防止面条粘连，特别是挂面，很容易就是一坨，导致里面的煮不熟。3、关于面条的选择，我更倾向于选择手工湿面条，更好吃，挂面不好吃。4、这种酸汤千万不要浪费，最好全喝了，营养又健康。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>patato</tag>
        <tag>noodles</tag>
        <tag>egg</tag>
        <tag>sugar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git pull 失败：RPC failed;SSL_ERROR_SYSCALL errno 10054]]></title>
    <url>%2F2019072301.html</url>
    <content type="text"><![CDATA[众所周知，Git 是一款非常流行的版本控制工具，现在的项目开发基本都离不开它，否则项目的协作开发将寸步难行，甚至会有专门的项目管理职位来规范项目的开发协作。如果不使用 Git，我的博客整理工作也会增加难度与复杂度，不得不说，我已经离不开它了。今天碰到一个关于 Git 的很奇怪的错误，本文记录解决的过程，整理完感觉经验技能又增长了。问题出现 我换了一台电脑，把项目代码下载下来，正常的 clone 后，一直使用，过了几天，突然出现下面的问题。在使用 git pull 命令同步最新代码时报错：1234error: RPC failed; curl 56 OpenSSL SSL_read: SSL_ERROR_SYSCALL, errno 10054fatal: The remote end hung up unexpectedlyfatal: early EOFfatal: unpack-objects failed我仔细观察了整个过程，一开始还是正常的，百分比进度在变化，然后就卡在那里一直不再动，最后就报错，紧接着 pull 流程被终止。初步看起来像是网络不好或者文件内容太大导致的网络连接超时失败。按照可能是网络问题这个方向，我重试了多次，全部都是 git pull 失败，然后我换成其它项目再做相同的操作就正常，我陷入了沉思：应该和环境无关，只和项目有关，这个 git pull 失败的项目到底有什么特殊之处。突然，我一拍脑门，想起来了，这个项目前一天晚上被我 commit 了很多张图片，应该有 100 张以上，总计 200MB 大小，看来这是问题所在。问题分析解决 循着这个线索，使用报错关键词 RPC failed; curl 56 OpenSSL SSL_read: SSL_ERROR_SYSCALL, errno 10054 去 stackoverflow 搜索一下，发现很多人都遇到过这个问题。原因在于 http 通信缓存设置的值太小，恰好我的项目是使用 http 协议进行 pull 的，而没有使用 ssh 的方式。这时候的解决方式就是设置一下缓存大小，参数名为：http.postBuffer，把它的值设置大一点【注意它的单位是 B，字节，进位是 1024 制的】：1234# 500MB, 如果配合使用 --global 参数可以全局生效 git config http.postBuffer 524288000# 1GBgit config http.postBuffer 1048576000根据官网对 http.postBuffer 这个参数的解释说明：Maximum size in bytes of the buffer used by smart HTTP transports when POSTing data to the remote system. For requests larger than this buffer size, HTTP/1.1 and Transfer-Encoding: chunked is used to avoid creating a massive pack file locally. Default is 1 MiB, which is sufficient for most requests.附官网链接：https://git-scm.com/docs/git-config ，参见对参数 http.postBuffer 的解释。可以看到这个参数的默认值为：1 MiB，对大部分项目都是合理的，但是对于我这个一次疯狂 commit 很多张图片的项目就无能为力了。配置完成后，也可以在项目的 .git/config 配置文件中查看这个参数的信息【如果设置了全局生效，则需要在家目录中寻找这个配置文件，即 home 目录】。1234567891011121314151617181920212223[core] repositoryformatversion = 0 filemode = false bare = false logallrefupdates = true symlinks = false ignorecase = true[remote &quot;origin&quot;] url = https://github.com/iplaypi/sources-playpi.git fetch = +refs/heads/*:refs/remotes/origin/*[branch &quot;master&quot;] remote = origin merge = refs/heads/master[gui] wmstate = normal geometry = 1061x563+30+30 233 255[credential] helper = store[user] name = iplaypi email = playpi@qq.com[http] postBuffer = 524288000好，接下来再进行 pull 操作，可以看到，最终正常了，没有再出问题【一开始我设置的是 500MB，还是不行，接着改为 1GB 就可以了】。由于网络速度问题或者中国大陆访问 GitHub 缓慢的原因，这次正常的 pull 使用了将近四十分钟才完成，等得我着急。可见，真的是我这个项目的内容太大了，同步的时候 http 通信缓存不足，导致出错。问题总结 1、此外，还有一个压缩参数：core.compression，可以用来设置压缩率，有 11 个取值。当然，如果把项目内容压缩了，由于压缩操作本身就会很耗时，会导致下载速度变慢，下载同步过程总的耗时也会随之增加。 官网说明：An integer -1..9, indicating a default compression level. -1 is the zlib default. 0 means no compression, and 1..9 are various speed/size tradeoffs, 9 being slowest. If set, this provides a default to other compression variables, such as core.looseCompression and pack.compression.2、我当前使用的是 http 方式，其实还有一种 ssh 方式，更方便，可以试试。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>PRC</tag>
        <tag>SSL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Java 获取 HBase 中多版本数据的方法]]></title>
    <url>%2F2019071101.html</url>
    <content type="text"><![CDATA[最近工作比较繁忙，在处理需求、写代码的过程中踩到了一些坑，不过问题都被我一个一个解决了，所以最近三周都没有更新博客内容。不过，我是整理了提纲、打了草稿，近期会陆续整理出来。今天就先整理出来一个简单的知识点：使用 Java API 从 HBase 中获取多版本【Version 的概念】数据的方法，开发环境基于 JDK v1.8、HBase v1.1.2、Zookeeper v3.4.6，在演示过程中还会使用原生的 HBase Shell 进行配合，加深理解。入门概念 先列举一些关于 HBase 的基础概念，有助于继续阅读下文，如果不太了解需要先回顾一下：列式分布式数据库，基于 Google BigTable 论文开发，适合海量的数据存储 Rowkey、Column Family、Qualifier、Timestamp、Cell、Version 的概念HBase Shell、Java API、Phoenix 示例代码 下面的演示会以 HBase Shell、Java API 这两种方式分别进行，便于读者理解。建表造数据 为了使用 Java API 获取多版本数据，我要先做一些基础工作：创建表、造数据、造多版本数据。为了尽量简化数据的复杂度，以及能让读者理解，我准备了 2 条数据，下面使用一个表格来整理这 2 条数据，读者可以看得更清晰：RowkeyColumn FamilyQualifierVersionValuerow01cfname1JIMrow01cfname2Jackrow02cfname1Lucyrow02cfage120从上表可以看出，一共 2 条数据，row01 有 1 列，2 个版本，row02 有 2 列，1 个版本。下面使用原生的 HBase Shell 开始逐步建表、造数据。1、进入交互式客户端 使用 hbase shell 进入交互式客户端，在输出的日志中可以看到当前环境 HBase 的版本号。登录成功后终端显示：2、创建表：学生表 使用 create &#39;TB_HBASE_STUDENT&#39;,&#39;cf&#39; 创建一张表，为了便于后面的操作，表名最好使用大写形式，否则涉及到表名的操作需要加单引号。由于 HBase 是列式存储结构，所以创建表时不需要指定具体的列名称，只要指定 Column Family 名称即可。执行后终端显示：120 row (s) in 2.5260 seconds =&gt; Hbase::Table - TB_HBASE_STUDENT3、查看表结构 使用 describe &#39;TB_HBASE_STUDENT&#39; 查看表结构，执行后终端显示：12345Table TB_HBASE_STUDENT is ENABLEDTB_HBASE_STUDENTCOLUMN FAMILIES DESCRIPTION&#123;NAME =&gt; &apos;cf&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125;1 row (s) in 0.0390 seconds可以看到表的基本信息，其中 Column Family 名称为 cf，最大版本 VERSIONS 为 1，这会导致只会存储一个版本的列数据，当再次插入数据的时候，后面的值会覆盖掉前面的值。4、修改最大版本 为了满足我的需求，需要更改表，把 cf 的最大版本数 VERSIONS 增加，设置为 3 。使用 alter &#39;TB_HBASE_STUDENT&#39;,{NAME=&gt;&#39;cf&#39;,VERSIONS=&gt;3} 命令即可。执行后终端显示：12345Updating all regions with the new schema...0/1 regions updated.1/1 regions updated.Done.0 row (s) in 3.7710 seconds修改成功后，我使用 describe &#39;TB_HBASE_STUDENT&#39; 再次查看表结构，终端显示：12345Table TB_HBASE_STUDENT is ENABLEDTB_HBASE_STUDENTCOLUMN FAMILIES DESCRIPTION&#123;NAME =&gt; &apos;cf&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;3&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125;1 row (s) in 0.0380 seconds这次可以看到，VERSIONS =&gt; &#39;3&#39; 表示 cf 已经支持存储 3 个版本的数据了。5、插入 2 条数据 HBase 的插入数据功能是使用 put 命令，每次插入 1 列，根据上述表格数据格式，需要执行 4 次 put 操作。1234put &apos;TB_HBASE_STUDENT&apos;,&apos;row01&apos;,&apos;cf:name&apos;,&apos;JIM&apos;put &apos;TB_HBASE_STUDENT&apos;,&apos;row01&apos;,&apos;cf:name&apos;,&apos;Jack&apos;put &apos;TB_HBASE_STUDENT&apos;,&apos;row02&apos;,&apos;cf:name&apos;,&apos;Lucy&apos;put &apos;TB_HBASE_STUDENT&apos;,&apos;row02&apos;,&apos;cf:age&apos;,&apos;20&apos; 执行后终端显示如下：123456789101112131.8.7-p357 :012 &gt; put &apos;TB_HBASE_STUDENT&apos;,&apos;row01&apos;,&apos;cf:name&apos;,&apos;JIM&apos;0 row (s) in 0.1600 seconds1.8.7-p357 :013 &gt; put &apos;TB_HBASE_STUDENT&apos;,&apos;row01&apos;,&apos;cf:name&apos;,&apos;Jack&apos;0 row (s) in 0.0180 seconds1.8.7-p357 :014 &gt; put &apos;TB_HBASE_STUDENT&apos;,&apos;row02&apos;,&apos;cf:name&apos;,&apos;Lucy&apos;0 row (s) in 0.0160 seconds1.8.7-p357 :015 &gt; put &apos;TB_HBASE_STUDENT&apos;,&apos;row02&apos;,&apos;cf:age&apos;,&apos;20&apos;0 row (s) in 0.0180 seconds1.8.7-p357 :016 &gt;命令行查看 1、先尝试使用 get 命令来获取这 2 条数据，分别执行 3 次 get 操作。123get &apos;TB_HBASE_STUDENT&apos;,&apos;row02&apos;,&apos;cf:name&apos;get &apos;TB_HBASE_STUDENT&apos;,&apos;row02&apos;,&apos;cf:age&apos;get &apos;TB_HBASE_STUDENT&apos;,&apos;row01&apos;,&apos;cf:name&apos; 执行后终端显示如下：123456789101112131415161718191.8.7-p357 :026 &gt; get &apos;TB_HBASE_STUDENT&apos;,&apos;row02&apos;,&apos;cf:name&apos;COLUMN CELLcf:name timestamp=1566118670447, value=Lucy1 row (s) in 0.0160 seconds1.8.7-p357 :027 &gt; get &apos;TB_HBASE_STUDENT&apos;,&apos;row02&apos;,&apos;cf:age&apos;COLUMN CELLcf:age timestamp=1566118677185, value=201 row (s) in 0.0060 seconds1.8.7-p357 :028 &gt; get &apos;TB_HBASE_STUDENT&apos;,&apos;row01&apos;,&apos;cf:name&apos;COLUMN CELLcf:name timestamp=1566118661397, value=Jack1 row (s) in 0.0080 seconds1.8.7-p357 :029 &gt;可以看到，此时并没有获取到 row01 的 2 个版本的数据，只获取了最新版本的结果。2、使用 get 获取多版本数据，执行 get 时需要加上 VERSIONS 相关的参数。1get &apos;TB_HBASE_STUDENT&apos;,&apos;row01&apos;,&#123;COLUMN=&gt;&apos;cf:name&apos;,VERSIONS=&gt;3&#125;执行后终端显示如下：123456781.8.7-p357 :029 &gt; get &apos;TB_HBASE_STUDENT&apos;,&apos;row01&apos;,&#123;COLUMN=&gt;&apos;cf:name&apos;,VERSIONS=&gt;3&#125;COLUMN CELLcf:name timestamp=1566118661397, value=Jackcf:name timestamp=1566118652009, value=JIM2 row (s) in 0.0140 seconds1.8.7-p357 :030 &gt;可以看到，2 个版本的数据都读取出来了。3、使用 scan 扫描数据 此外还有一个 scan 命令可以扫描表中的数据，使用 scan &#39;TB_HBASE_STUDENT&#39;,{LIMIT=&gt;5} 尝试扫描 5 条数据出来。执行后终端显示如下：1234567891.8.7-p357 :031 &gt; scan &apos;TB_HBASE_STUDENT&apos;,&#123;LIMIT=&gt;5&#125;ROW COLUMN+CELLrow01 column=cf:name, timestamp=1566118661397, value=Jackrow02 column=cf:age, timestamp=1566118677185, value=20row02 column=cf:name, timestamp=1566118670447, value=Lucy2 row (s) in 0.0420 seconds1.8.7-p357 :032 &gt;由于表中只有 2 条数据，所以只显示出 2 条，而且 scan 默认也是获取最新版本的数据结果。4、如果想退出 HBase Shell 交互式客户端，使用 !quit 命令即可。代码示例 上面使用原生的 HBase Shell 操作演示了创建表、插入数据、读取数据的过程，下面将使用 Java API 演示读取数据的过程，而创建表、插入数据的过程就不再演示。这里需要特别注意，为了正常使用 Java API 的相关接口，Java 项目需要依赖 hbase-client、commons-configuration、hadoop-auth、hadoop-hdfs 等组件。我的代码已经上传至 GitHub，详见：TestHBase.java ，搜索类名 TestHBase 即可。1、代码示例 代码结构比较简单，分为：构造查询请求、发送请求、解析结果输出几部分，注释中也注明了各个部分的作用，总计也就 50 行代码左右。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586/** * HBase Java API Get 测试 */public void testGet () &#123; String hTableName = &quot;TB_HBASE_STUDENT&quot;; IplaypiStudyConfig configuration = IplaypiStudyConfig.getInstance (); byte [] cfbyte = &quot;cf&quot;.getBytes (); byte [] col01byte = &quot;name&quot;.getBytes (); byte [] col02byte = &quot;age&quot;.getBytes (); try &#123; // 构造查询请求，2 条数据，多个版本 List&lt;Get&gt; getList = Lists.newArrayList (); Get get = new Get (Bytes.toBytes (&quot;row01&quot;)); get.addColumn (cfbyte, col01byte); // 设置最大版本数，默认为 1 get.setMaxVersions (3); getList.add (get); Get get2 = new Get (Bytes.toBytes (&quot;row02&quot;)); get2.addColumn (cfbyte, col01byte); get2.addColumn (cfbyte, col02byte); getList.add (get2); // 发送请求，获取结果 HTable hTable = new HTable (configuration, hTableName); Result [] resultArr = hTable.get (getList); /** * 以下有两种解析结果的方法 * 1 - 通过 Result 类的 getRow () 和 getValue () 两个方法，只能获取最新版本 * 2 - 通过 Result 类的 rawCells () 方法返回一个 Cell 数组，可以获取多个版本 * 注意，高版本不再建议使用 KeyValue 的方式，注释中有说明 */ // 1- log.info (&quot;====get result by first method&quot;); for (Result result : resultArr) &#123; log.info (&quot;&quot;); log.info (&quot;--------&quot;); String rowStr = Bytes.toString (result.getRow ()); log.info (&quot;====row:[&#123;&#125;]&quot;, rowStr); // 如果包含 name 列，则获取输出 if (result.containsColumn (cfbyte, col01byte)) &#123; String valStr = Bytes.toString (result.getValue (cfbyte, col01byte)); log.info (&quot;====name:[&#123;&#125;],getValue&quot;, valStr); // 以下方式不建议使用，但是可以获取多版本 List&lt;KeyValue&gt; keyValueList = result.getColumn (cfbyte, col01byte); for (KeyValue keyValue : keyValueList) &#123; log.info (&quot;====name:[&#123;&#125;],getColumn -&gt; getValue&quot;, Bytes.toString (keyValue.getValue ())); &#125; &#125; // 如果包含 age 列，则获取输出 if (result.containsColumn (cfbyte, col02byte)) &#123; String valStr = Bytes.toString (result.getValue (cfbyte, col02byte)); log.info (&quot;====age:[&#123;&#125;],getValue&quot;, valStr); // 以下方式不建议使用，但是可以获取多版本 List&lt;KeyValue&gt; keyValueList = result.getColumn (cfbyte, col02byte); for (KeyValue keyValue : keyValueList) &#123; log.info (&quot;====age:[&#123;&#125;],getColumn -&gt; getValue&quot;, Bytes.toString (keyValue.getValue ())); &#125; &#125; &#125; // 2- log.info (&quot;&quot;); log.info (&quot;====get result by second method&quot;); for (Result result : resultArr) &#123; log.info (&quot;&quot;); log.info (&quot;--------&quot;); String rowStr = Bytes.toString (result.getRow ()); log.info (&quot;====row:[&#123;&#125;]&quot;, rowStr); //name 列 List&lt;Cell&gt; cellList = result.getColumnCells (cfbyte, col01byte); // 1 个 cell 就是 1 个版本 for (Cell cell : cellList) &#123; // 高版本不建议使用 log.info (&quot;====name:[&#123;&#125;],getValue&quot;, Bytes.toString (cell.getValue ())); //getValueArray: 数据的 byte 数组 //getValueOffset:rowkey 在数组中的索引下标 //getValueLength:rowkey 的长度 String valStr = Bytes.toString (cell.getValueArray (), cell.getValueOffset (), cell.getValueLength ()); log.info (&quot;====name:[&#123;&#125;],[getValueArray,getValueOffset,getValueLength]&quot;, valStr); log.info (&quot;====timestamp:[&#123;&#125;],cell&quot;, cell.getTimestamp ()); &#125; //age 列不演示了，省略... &#125; &#125; catch (IOException e) &#123; log.error (&quot;!!!!error: &quot; + e.getMessage (), e); &#125;&#125;2、运行结果 执行运行，可以看到结果输出，与数据表中一致，多版本数据结果也可以全部获取：123456789101112131415161718192021222324252627282930312019-08-18_17:54:18 [main] INFO test.TestHBase:58: ====get result by first method2019-08-18_17:54:18 [main] INFO test.TestHBase:60: 2019-08-18_17:54:18 [main] INFO test.TestHBase:61: --------2019-08-18_17:54:18 [main] INFO test.TestHBase:63: ====row:[row01]2019-08-18_17:54:18 [main] INFO test.TestHBase:67: ====name:[Jack],getValue2019-08-18_17:54:18 [main] INFO test.TestHBase:71: ====name:[Jack],getColumn -&gt; getValue2019-08-18_17:54:18 [main] INFO test.TestHBase:71: ====name:[JIM],getColumn -&gt; getValue2019-08-18_17:54:18 [main] INFO test.TestHBase:60: 2019-08-18_17:54:18 [main] INFO test.TestHBase:61: --------2019-08-18_17:54:18 [main] INFO test.TestHBase:63: ====row:[row02]2019-08-18_17:54:18 [main] INFO test.TestHBase:67: ====name:[Lucy],getValue2019-08-18_17:54:18 [main] INFO test.TestHBase:71: ====name:[Lucy],getColumn -&gt; getValue2019-08-18_17:54:18 [main] INFO test.TestHBase:77: ====age:[20],getValue2019-08-18_17:54:18 [main] INFO test.TestHBase:81: ====age:[20],getColumn -&gt; getValue2019-08-18_17:54:18 [main] INFO test.TestHBase:86: 2019-08-18_17:54:18 [main] INFO test.TestHBase:87: ====get result by second method2019-08-18_17:54:18 [main] INFO test.TestHBase:89: 2019-08-18_17:54:18 [main] INFO test.TestHBase:90: --------2019-08-18_17:54:18 [main] INFO test.TestHBase:92: ====row:[row01]2019-08-18_17:54:18 [main] INFO test.TestHBase:98: ====name:[Jack],getValue2019-08-18_17:54:18 [main] INFO test.TestHBase:103: ====name:[Jack],[getValueArray,getValueOffset,getValueLength]2019-08-18_17:54:18 [main] INFO test.TestHBase:104: ====timestamp:[1566118661397],cell2019-08-18_17:54:18 [main] INFO test.TestHBase:98: ====name:[JIM],getValue2019-08-18_17:54:18 [main] INFO test.TestHBase:103: ====name:[JIM],[getValueArray,getValueOffset,getValueLength]2019-08-18_17:54:18 [main] INFO test.TestHBase:104: ====timestamp:[1566118652009],cell2019-08-18_17:54:18 [main] INFO test.TestHBase:89: 2019-08-18_17:54:18 [main] INFO test.TestHBase:90: --------2019-08-18_17:54:18 [main] INFO test.TestHBase:92: ====row:[row02]2019-08-18_17:54:18 [main] INFO test.TestHBase:98: ====name:[Lucy],getValue2019-08-18_17:54:18 [main] INFO test.TestHBase:103: ====name:[Lucy],[getValueArray,getValueOffset,getValueLength]2019-08-18_17:54:18 [main] INFO test.TestHBase:104: ====timestamp:[1566118670447],cell备注 1、在使用 Java API 时注意低版本、高版本之间的差异，必要时及时升级，就像上文代码中的 Result.getColumn、KeyValue.getValue ()、Cell.getValue () 这几个方法。2、Phoenix 是一款基于 HBase 的工具，在 HBase 之上提供了 OLTP 相关的功能，例如完全的 ACID 支持、SQL、二级索引等，此外 Phoenix 还提供了标准的 JDBC 的 API。在使用 Phoenix 时，可以很方便地像操作 SQL 那样操作 HBase。 使用 Phoenix 创建表、查询数据示例如图。创建表，使用：CREATE TABLE IF NOT EXISTS TB_HBASE_STUDENT (&quot;pk&quot;varchar primary key,&quot;cf&quot;.&quot;name&quot;varchar,&quot;cf&quot;.&quot;age&quot;varchar);查询示例，使用：select * from&quot;TB_HBASE_STUDENT&quot;limit 5;3、本示例的代码放在 GirHub，详见：TestHBase.java ，搜索类名 TestHBase 即可。参考 GitHub 的代码时，注意在 iplaypistudy-common-config 模块中增加自己的配置文件，如果开发环境的版本不匹配，也要升级版本，在 pom.xml 更改即可。4、想要使用 HBase Shell 删除表时，必须先使用 disable YOUR_TABLE_NAME 来禁用表，然后再使用 drop YOUR_TABLE_NAME 删除表，直接删除表是不被允许的。]]></content>
      <categories>
        <category>大数据技术知识</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>HBase</tag>
        <tag>version</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录一次 AbstractMethodError 异常]]></title>
    <url>%2F2019070401.html</url>
    <content type="text"><![CDATA[场景描述：在某一天我觉得我的 Java 项目的依赖太冗余了，决定删除一些无用的依赖，让整个项目瘦身，以后打包发布的时候也更快速、节省时间。接着我按照自己的经验，把大部分依赖全部删除，此时编译会报一些错误，这是必然的，但是我不担心，根据报错信息把缺失的依赖再一个一个添加进来即可。忙活了一阵，终于解决了所有的报错，编译、打包一气呵成，不再有错误，看了一下打包后的文件大小，足足比原先小了 30%，我略感满意。但是此时我仍旧悬着一颗心，因为编译、打包成功不代表什么，后面的运行才是大问题，运行时往往会暴露一些隐藏的问题。而且项目里面有好几个功能，只要有一个功能运行失败那就说明依赖还是有问题，改造不成功。【千万不要以为编译、打包成功就万事大吉了，运行时的异常才是大问题，一定要有未雨绸缪的准备】果然，刚启动第一个功能就出现了我想象中的异常信息：java.lang.AbstractMethodError。问题出现 异常突然出现，我差点就懵了，昨天还好好的，今天怎么就这样了，程序又不是女朋女，不可能说变就变。此时，我又想起了一个段子，程序员的一般心里活动：运行失败了，这 TM 怎么可能会失败呢？运行成功了，这 TM 怎么就成功了呢？作为一名资深的工程师，我还是决定试试，看看能不能走个狗屎运，于是我在本机、测试环境、正式环境分别做了测试，发现都是报一样的错误，接着我就意识到问题的严重性，不能再心存侥幸，要拿出我真实的技术来说话了。前面的处理方式就像重启系统一样，只不过是碰运气的方式，我连报错信息都没有仔细看，接下来就要认真处理了。既然报错了，那就耐心查看， 办法总比困难多 。下面列出完整的错误信息：1234567891011121314151617181920212223242019-07-04_20:05:26 [main] INFO yarn.Client:58: Application report for application_1561431414509_0020 (state: ACCEPTED)2019-07-04_20:05:26 [shuffle-server-2] ERROR server.TransportRequestHandler:191: Error sending result RpcResponse&#123;requestId=5206989806485258134, body=NioManagedBuffer&#123;buf=java.nio.HeapByteBuffer [pos=0 lim=47 cap=47]&#125;&#125; to dev6/172.18.5.206:55124; closing connectionjava.lang.AbstractMethodError at io.netty.util.ReferenceCountUtil.touch (ReferenceCountUtil.java:73) at io.netty.channel.DefaultChannelPipeline.touch (DefaultChannelPipeline.java:107) at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:810) at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:723) at io.netty.handler.codec.MessageToMessageEncoder.write (MessageToMessageEncoder.java:111) at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0 (AbstractChannelHandlerContext.java:738) at io.netty.channel.AbstractChannelHandlerContext.invokeWrite (AbstractChannelHandlerContext.java:730) at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:816) at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:723) at io.netty.handler.timeout.IdleStateHandler.write (IdleStateHandler.java:302) at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0 (AbstractChannelHandlerContext.java:738) at io.netty.channel.AbstractChannelHandlerContext.invokeWrite (AbstractChannelHandlerContext.java:730) at io.netty.channel.AbstractChannelHandlerContext.access$1900 (AbstractChannelHandlerContext.java:38) at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write (AbstractChannelHandlerContext.java:1089) at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write (AbstractChannelHandlerContext.java:1136) at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run (AbstractChannelHandlerContext.java:1078) at io.netty.util.concurrent.AbstractEventExecutor.safeExecute (AbstractEventExecutor.java:163) at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks (SingleThreadEventExecutor.java:403) at io.netty.channel.nio.NioEventLoop.run (NioEventLoop.java:462) at io.netty.util.concurrent.SingleThreadEventExecutor$5.run (SingleThreadEventExecutor.java:858) at java.lang.Thread.run (Thread.java:748)其中，重点只需要看这几行内容：12342019-07-04_20:05:26 [shuffle-server-2] ERROR server.TransportRequestHandler:191: Error sending result RpcResponse&#123;requestId=5206989806485258134, body=NioManagedBuffer&#123;buf=java.nio.HeapByteBuffer [pos=0 lim=47 cap=47]&#125;&#125; to dev6/172.18.5.206:55124; closing connectionjava.lang.AbstractMethodError at io.netty.util.ReferenceCountUtil.touch (ReferenceCountUtil.java:73) at io.netty.channel.DefaultChannelPipeline.touch (DefaultChannelPipeline.java:107)我定睛一瞧，AbstractMethodError 这种异常类型我还没见过，这怎么行，抓紧去查了 Java 的官方文档，查过之后，才明白这个异常的含义。官方定义内容如下：Thrown when an application tries to call an abstract method. Normally, this error is caught by the compiler; this error can only occur at run time if the definition of some class has incompatibly changed since the currently executing method was last compiled.大概意思就是说 Java 程序在 运行时 发现 方法的定义 与 编译时 的不一致，怎么会有这种现象呢，最大的可能也就是依赖冲突了，至于为何造成冲突，还需要进一步检查。我又回过头去仔细看一下这个功能的前后逻辑，很简单，只是使用 Spark 处理 HDFS 里面的数据，然后把处理结果再写回到 HDFS，实际运行时处理的数据量也不大，看起来不会有功能性的问题。而且，前不久这个功能还运行的好好的，只在我重构更改后才起不来的，原因基本可以定位在依赖方面：冲突、缺失。问题解决 顺着依赖冲突这个突破点看下去，可能是因为我在清理依赖时把某个依赖清除掉了，然后又自己添加一个不同版本的，导致与原先的依赖版本不匹配。而且看到异常信息里面都是和 netty 有关的，可以猜测可能是 netty 的相关依赖出问题了。接着再多看一点点异常信息，还发现有一些额外的有效信息：1234567891011121314151617181920212223242526272829303132333435362019-07-04_20:05:23 [main] INFO spark.SecurityManager:58: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set (Administrator, dota); users with modify permissions: Set (Administrator, dota)2019-07-04_20:05:23 [main] INFO yarn.Client:58: Submitting application 20 to ResourceManager2019-07-04_20:05:23 [main] INFO impl.YarnClientImpl:274: Submitted application application_1561431414509_00202019-07-04_20:05:24 [main] INFO yarn.Client:58: Application report for application_1561431414509_0020 (state: ACCEPTED)2019-07-04_20:05:24 [main] INFO yarn.Client:58: client token: N/A diagnostics: N/A ApplicationMaster host: N/A ApplicationMaster RPC port: -1 queue: default start time: 1562241921232 final status: UNDEFINED tracking URL: http://dev6:8088/proxy/application_1561431414509_0020/ user: dota2019-07-04_20:05:29 [main] INFO yarn.Client:58: Application report for application_1561431414509_0020 (state: ACCEPTED)2019-07-04_20:05:30 [main] INFO yarn.Client:58: Application report for application_1561431414509_0020 (state: ACCEPTED)2019-07-04_20:05:31 [main] INFO yarn.Client:58: Application report for application_1561431414509_0020 (state: ACCEPTED)2019-07-04_20:05:32 [main] INFO yarn.Client:58: Application report for application_1561431414509_0020 (state: FAILED)2019-07-04_20:05:32 [main] INFO yarn.Client:58: client token: N/A diagnostics: Application application_1561431414509_0020 failed 2 times due to AM Container for appattempt_1561431414509_0020_000002 exited with exitCode: 10For more detailed output, check application tracking page:http://dev6:8088/cluster/app/application_1561431414509_0020Then, click on links to logs of each attempt.Diagnostics: Exception from container-launch.Container id: container_e19_1561431414509_0020_02_000001Exit code: 10Stack trace: ExitCodeException exitCode=10: at org.apache.hadoop.util.Shell.runCommand (Shell.java:576) at org.apache.hadoop.util.Shell.run (Shell.java:487) at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute (Shell.java:753) at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer (DefaultContainerExecutor.java:212) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call (ContainerLaunch.java:303) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call (ContainerLaunch.java:82) at java.util.concurrent.FutureTask.run (FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624) at java.lang.Thread.run (Thread.java:748)从上文的日志信息来看，程序的 Driver 端已经提交了 Spark 任务到 Yarn 集群，然后 Yarn 集群分配了资源，但是在后续的通信过程中，不知道哪里出问题了，导致通信中断，进而导致 Spark 任务失败。结合上面我猜测的和 netty 依赖有关，那就从这里入手吧，先把项目的依赖树梳理出来，使用 mvn dependency:tree &gt; tree.txt，把依赖树的信息保存在文件 tree.txt 中，然后在依赖树信息中搜索 netty。可以看到关于 netty 依赖的信息。再全局搜索一下项目中的类【在 Windows 下使用 Eclipse 模式的快捷键 Ctrl + Shift + t】，异常信息对应的那个类：ReferenceCountUtil，可以看到存在两个同名的类：类名称一致【都是 ReferenceCountUtil】，包名称一致【都是 io.netty.util】，只不过对应的 jar 包依赖不一致，一个是 io.netty:netty-common:4.1.13.Final.jar【这个是我的 org.elasticsearch.client:transport:5.5.0 传递依赖过来的】，另一个是 io.netty:netty-all:4.0.29.Final.jar【这个是 Spark 自带的，只不过我重新指定了版本】，这两个类肯定会冲突的。解决办法很简单，直接去除多余的依赖即可，但是要注意去除后会不会引发其它的依赖缺失问题。我在我的项目里面移除了所有的 io.netty:netty-* 依赖，这些依赖也是传递过来的，版本都为 v4.1.13，如下图：如果不全部移除而是选择只移除 netty-common，还会有问题，因为这些依赖之间也互相依赖，看 common 这个命名就知道了，这就是： 一荣俱荣，一损俱损 。我把这些依赖移除后，netty 相关的依赖冲突就没有问题了，但是又遇到了一个小问题：1java.lang.ClassNotFoundException: org.elasticsearch.spark.rdd.EsPartitionSpark 任务正常启动后，运行过程中出现了上述错误，导致 Spark 任务失败，乍一看是类缺失。但是如果在项目中搜索的话，也能搜索到这个类，是不是觉得很奇怪。其实不要多想，这个是典型的 Yarn/Spark 集群环境问题，项目中使用的 jar 包【特定版本的，我这里是：org.elasticsearch.client:transport:jar:5.5.0】在集群环境中没有，如果切换一个集群环境中存在版本就可以了【例如 v5.6.8】。或者一定要使用这个版本的话，就把这个 jar 包复制到 Yarn/Spark 集群环境每台机器的 lib 库中去。但是一般情况下，公司的环境是统一的，会避免使用多版本的依赖，以免引起一连串的未知冲突问题，浪费大家的时间。在实际生产环境中，可能还会遇到一个更加糟糕的问题，即项目本身的依赖非常混乱，并且有大量的重复，可能去除一个还有一个，会造成大量重复的工作，所以在查看依赖树时可以使用 -Dverbose 参数，完整的命令：mvn dependency:tree -Dverbose &gt; tree.txt，把原始的所有传递依赖全部列出来，这样就可以对症操作，一次性把所有依赖移除。当然，会有人觉得这样操作也是很麻烦，能不能来个插件，直接配置一下即可，至于去除的操作过程我也不关心，只要能帮我去除就行。当然，这对于想偷懒并提高效率的技术人员来说是必备的，这个东西就是插件 maven-shade-plugin。在 configuration 里面配置 artifactSet -&gt; excludes -&gt; exclude -&gt; jar 包坐标 即可。 但是要注意，插件要使用高版本的：v3.1.0，我一开始使用的是 v2.4.3，怎么配置都无效，搞了半天发现低版本不支持。此外，还要注意 JDK 的版本也要 v1.8+，这样才能保证使用其它的特性，例如打包压缩：&lt;minimizeJar&gt;true&lt;/minimizeJar&gt;。使用这个参数可以自动把无用的依赖 jar 排除掉，给代码瘦身，同时也节约打包时间，非常好用。我的 jar 在使用打包压缩参数后，由原本的 313MB 被压缩到了 191MB，压缩率超过 30%，我觉得非常好用。此外，maven-shade-plugin 插件是一款非常优秀的插件，最常用的莫过于 影子别名 功能，对于复杂的依赖冲突解决有奇效。例如对于上面的依赖冲突问题，可以不用找原因一点一点解决，直接使用 影子别名 功能把传递依赖的 netty jar 包改个名字即可，这样它们就可以共存了，简单粗暴却有奇效。推荐大家使用，这里不再赘述。问题总结 总结一下问题，就是同名的类存在了不同版本的 jar 包中，等到运行的时候，Java 虚拟机发现异常，便抛出异常信息，停止运行程序。此外，在没有十足的把握或者时间人力不充足的情况下，千万不要想着重构代码，后果你不一定能承担，带来的好处可能大于带来的灾难，这也叫 好心办坏事 。再回顾一下，我这个问题是 Spark 任务运行在 Yarn 集群上面才发现的，如果使用 local 模式运行 Spark 任务是不会有问题的。所以当时出问题后我也是疑惑，反复测试了好几次才敢确认，主要是因为使用 Yarn 模式时，同时也会使用集群中提供的 jar 包依赖，如果项目本身打包时又打进了相同的 jar 包，就极有可能引发冲突【版本不一致，而且 netty 包的冲突本身就是一个大坑】。]]></content>
      <categories>
        <category>踩坑系列</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spark</tag>
        <tag>netty</tag>
        <tag>AbstractMethodError</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[郴州东江湖之行]]></title>
    <url>%2F2019062301.html</url>
    <content type="text"><![CDATA[从 2019 年 06 月 21 日 18:30 点到 2019 年 06 月 23 日 18:30 点，我与公司部门的同事们进行了一次为期两天两夜的郴州东江湖之旅，一行总共 25 人，其中还有两位同事的老家就在东江湖周边，这也为此次旅途带来了一些特别的安排。以前每天在喧嚣的城市中工作，面对的都是闪烁霓虹的高楼大厦，车水马龙的街道，现在选择一个合适的时间点，为自己放一个小假，去拥抱大自然的风光，走走路动动腿，看看山山水水。本文简单记录这次旅途的过程，仅为存念，请读者跟着我的文字，带你们领略 小东江 自然风光的旖旎。注意，本文图片较多，我已经尽量把每张图片都压缩在几百 KB 的大小，浏览时请耐心等待。出发 去程记录 2019 年 06 月 21 日 18:00，大家开始吃晚饭或者零食，补充体力，我吃了一份简单的盒饭、一个粽子、一杯酸奶，算是勉强吃饱。 我的晚饭 接着从公司出发，四人组队打车去 广州南站 ，由于是在下班期间，而且还是在周五，所以路上难免堵车，我们也预估了时间，提前出发，以免迟到而赶不上车。打出租车不好打，网约车也不好叫，我们等了十几分钟才坐上车，我选择了走高速，因为之前已经咨询过同事，走高速不会堵车的，只有高速出口会堵一会。从上车到到达，全程消耗了 45 分钟，加上等车的十几分钟，总共一个小时的时间，顺利提前到达 广州南站 。由于提前到达了高铁站，还需要等待 40 分钟，才能进站乘车。还有一些没吃晚饭的同事们去找吃的了，我就留在长凳上照看行李。中途我还去周围逛了一下，发现便利店的饮料真的贵，和火车上的价格差不多了，这比上海火车站可差远了。我在上海火车站候车室见到的快餐店、便利店、奶茶店，不仅种类繁多，价格也和外面平常的店铺一样，只贵了一点点。广州南站便利店饮料价目表 时间走到了 20:40，终于可以上车了，坐高铁前往郴州，整个过程大概一个半小时，预计 22:10 到达 郴州西站 。在高铁上，大家有的看电影，有的玩游戏，有的玩手机，全程轻松愉悦，不必细说。22:10 顺利到达 郴州西站 ，我们换乘预定的大巴，紧接着前往住宿地点，大概 23:30 到达住宿地点。关于交通工具的选择 如果坐高铁，全程换乘两次，转车也痛苦，并没有节约时间，但是对于晕车的人来说是首选。如果坐大巴，可以从公司门口出发，直接开往目的地，但是也要 5 个小时以上，坐大巴的好处是不用换乘，上车就睡，直接到达目的地，但是有些人可能会晕车。总的来说，从广州到郴州 东江湖 ，全程要 5-6 个小时，我们是 18:30 出发，23:30 到达住宿地。是的，就是需要这么久，主要是 广州高铁站 离出发地、 郴州西高铁站 离目的地都太远了，下了班打的去 广州高铁站 将近一个小时【25 公里】，到了 郴州西高铁站 再坐大巴去景区住宿地也要一个小时【40 公里】，晚上郴州的大巴是限速的，最高 40km / 小时。我在高铁上和一个郴州本地人聊天，他说去 东江湖 游玩千万不要住太远，如果住市区的话，早上要起的很早赶过去，太累了。看来我们一开始就选择住在景区门口【从住宿地出来走路 2 分钟就到景区大门口】还是明智的，住的这么近还要在 05:45 起床，06:00 吃早餐，06:10 出发，否则赶不上看 小东江 的雾景了【07:00 左右景色最好】。可以想象，如果住在市区，早上五点前就要起床了吧。到达 折腾了五个小时，转了两次车【打的转高铁，高铁转大巴】，终于到达了住宿地点【东江芳鑫酒店】，开始分房间，领了钥匙，坐下来聊聊天、喝几口水。接着看到有同事拎着一篮子杨梅，在分发，我上前问了问，原来是大哥的表妹【我们公司 CTO 旭日大哥的表妹，接送我们，并协助我们全程游玩】专门带来给我们品尝的，从她家的果园刚刚摘出来，我吃了几个，不得不说，真好吃，放在嘴里咬一口，杨梅汁渗出来，酸酸甜甜，这才是正宗的杨梅。此时已经 23:40 了，我们是真的疲惫，但是又感到饥饿，于是想着出去逛逛，吃点宵夜。问了几个人，果然也有想出去吃点东西的人，于是我们一合计，就一起出去吃宵夜吧。凑了八九个人，就一起出去了，其中包括旭日大哥，我一想坏了，肯定又少不了喝酒吃肉，今天的睡眠时间注定不足了。由熟悉地形的同事【家就在附近】带我们来到一个叫做 吊桥 的地方，他说这里有整条街的大排档，通常到夜里一两点仍旧人声鼎沸。听着这个描述，我感觉与住宿周围的环境形成鲜明的对比，因为住宿周围基本没有人出来活动，除了路灯，所有地方基本都是漆黑一片。步行大概十几分钟，转了一个弯，突然霓虹闪烁、人声鼎沸，聊天的声音、吵闹的声音、商家宣传的声音不绝于耳，行人、车辆来来往往，感觉是到了另外一个世界。旭日大哥先带我们来到江边吹风，给我们介绍 吊桥 ，没错，它是一个标准的小桥，只不过在这个时间被封路了，不能上去，我们只能站在旁边观看、吹风。其实，周围漆黑一片，啥也看不清，只能吹吹风。稍微休息了一会儿，我们就选定了一家大排档，开吃开喝。当然，基本所有的菜都是辣的，特意叮嘱了服务员不要放那么多辣，毕竟在场的广东人居多。点了两斤小龙虾，买二送一。这里的啤酒是以桶为单位的，一桶五公斤，我们一群人喝了两桶，当然我由于过敏没喝。聊天吹水，吃饱喝足，结账走人。接着回住宿点，往回走，此时已经第二天凌晨 01:30，路上也没人了，只有我们一群孤单的身影。回到酒店，已经 01:40，洗漱完毕，看到桌子上还剩下一些杨梅，为了不浪费，我又吃了一些杨梅，一口气吃了十几个。由于刚刚吃过宵夜，又吃了杨梅，一开始睡不着，熬到凌晨 02:30 才睡着。我觉得这真的是作，只能睡三个小时了，不知道第二天还能不能起来，体力还能不能跟得上。住了一晚，第二天【06 月 22 日】早上 05:30 起床，我多睡了十分钟，最后还是坚持起床了，因为其他人都已经出发了，我是最后一个。到旁边的早餐店吃早餐：鱼粉、鸡蛋、咸菜，说真的，鱼粉虽然材料简单，味道真不错，吃完早餐，准备上山【景点都在高处】。步行几分钟，就到了景区的大门口，住得近就是好，走两步就到。注意图中，我们有人拉着行李箱过来玩的。小东江 进了景区的大门，接着就是乘坐大巴上山，当然你也可以选择步行，但是距离太远了，如果不是特意为了徒步锻炼还是坐大巴上去吧【就像在韶关丹霞山，可以选择徒步爬上去而不坐大巴也不坐缆车】。大概十分钟左右，就可以到达 小东江 景点。刚下车，我就被眼前的景色惊到了，同时旁边有一群游客大姐特别激动，不断地发出惊讶的表情与赞美之声，场面一度非常欢乐。小东江 是东江湖的一条支流，岸边有几公里的人行道，可以散步，看 雾漫小东江 的景色。中途还会有延伸到江心的平台，可以近距离感受小东江的雾气，以及拍照留念。先放出来几张图：为了配合游客拍照，景点特意安排工作人员驾小舟在江心撒网捕鱼，当然，只是做动作，并不是真的捕鱼，主要就是为了配合游客拍照。我拿着一个相机竟然不会使用连拍功能，还是旁边的大叔提醒我：不使用连拍你怎么可能拍得到想要的画面，我当场查看了相机的所有设置，也没有找到连拍的设置在哪，最后只能悻悻地走了。请看下图，靠我眼疾手快拍到的：在江心的平台上稍作休息，我第一次拿起了自拍杆，试拍效果。可以看到环境监测大屏显示的环境信息，空气质量超级好，温度适宜，湿度太高【大雾的原因】，负氧离子超级多，停下来每呼吸一口，感觉都是赚的。这条线路如果慢慢走，边走边赏景、边拍照，可以走大概两个小时。如果不是特别沉迷其中，没有必要走完全程，因为看到的景色都差不多，而且随着时间的消逝，雾气也会渐渐消散，后面就没有什么看头了。再者，还要适当保存体力进行后面的活动，不能在一个景点中消耗过多的时间。前后算起来，我们徒步了大概一个多小时，最后在一个江心平台合照留念，996 专属团队，引起了路人的围观。蹲在 C 位拿着白色自拍杆的就是我。龙景峡谷 看完小东江雾景，我们来到一个中途服务点，稍作休息，此处有水果、饮料、小吃出售。休息几分钟，接着坐车前往 龙景峡谷 ，当然你也可以选择步行过去【去的路途中我看到有人背着包在步行】，但是实在是太远了，而且没有行人徒步专用道，都是直接走在机动车道上，太危险。让人意外的是，第一辆车来到很快就坐满了【51 座的大巴，来到时上面已经有部分游客，应该是从上一个服务点过来的】，我们还有几个人没有上去，只好选择等待下一班车。但是，下一班车竟然是公交车，而且人已经满了，我们还是硬挤上去了，没想到在山上挤了一趟公交车。我上车就来一张自拍，纪念一下山上的公交车。坐车大概十几分钟，我们来到了 龙景峡谷 入口处的游客服务中心，准备进入游览。龙景峡谷 ，其实就是一个峡谷，顺带有一座小山可以爬，爬的过程中可以看到小瀑布、周围的风景。围绕着小山的阶梯绕一圈，全程大概四十分钟。在爬山图中，看到了小瀑布，很对得起这个 小 字。在山上的某个角落看看山下的景色。接着全程就很热了，太阳很晒，没有风，衣服基本被汗水浸透。而且，我还背着重重的背包，非常消耗体力，我还一路奇怪，我只睡了三个小时，是怎么坚持得了的。从山上下来后，经过 浮桥 ，也就是一些空桶做成的水中走道，可以从上面走过去。由于天气太热，可以看到上面基本没人，但是为了不虚此行，我们几个人还是冒着烈日上去走了一趟，上去之前我穿上了防晒衣。浮桥的场景，看看这天气有多热。在浮桥上，转头又看到一个小瀑布。从 龙景峡谷 下来后，回到入口，此时全身都湿透了，汗水哗哗地往下流。我们买了饮料、雪糕，给自己降降温。没想到我们几个人是率先下山的，四人组自拍，这镜头显胖。接着我们几个人就先回到游客中心，在大厅等着其他人回来，准备一起坐船去 兜率岛 。我在游客中心洗了脸，衣服全部湿透了，看看我背后的风景，再看看懵逼的我。兜率岛灵岩溶洞 等人全部到齐后，我们一起坐游船前往 兜率岛【读音 Doushuai，但门票上印的却是 Tushi】，沿途风景很好，还可以穿上救生衣，走上甲板，吹吹风，无论是近处还是远处的景色都一览无余，尽收眼底。兜率岛 是东江湖中的一座岛，总面积 5.6 平方公里，也是湖南第一大岛，岛上山清水秀，景色宜人。有人形容东江湖的景象： 山中有湖，湖中有岛，岛上有庙，庙里有洞，洞中有洞 ，这里的岛就是 兜率岛 。走到这里，必须要把行程路线说明一下，我们购买的是 精品线路 2 通票，等坐游船前往 兜率岛 玩一圈后，理论上就应该返程了，因为已经把所有景点全部游览一遍。但是我们不走一般的路线，后面还会有自由行路线，这全靠大哥的表妹在安排。坐船出发啦，在游船起步的几分钟内，可以看到 东江大坝 。在坐船的过程中，我只顾着看风景、拍视频了，基本没有拍照片，可惜不能把大好河山展现出来。坐船过程大概 20 分钟。船上的小伙伴，可能是刚刚在 龙景峡谷 太累了，或者是晕船，坐下来就不想动了。甲板上的小伙伴，连带后面的风景。此情此景，老大们怎能不登场。游船大概行驶 20 分钟后，到达 兜率岛 。先随便来两张照片看看这里的秀丽风景。由于有人把票弄丢了，几个人陪着在补票，所以没跟着同一艘船过来，我们一群人要等着他们。看到有人在卖水果，老大请客，先买西瓜吃。由于路途劳顿，汗水流出来了，再看着这切开的西瓜，口水又流出来了。看着这一群吃瓜群众，在这个时刻吃上一块西瓜，简直就是人生中一大幸福事。吃好后，后面的几个人也来到了，我们先休息一会儿。此时此刻，我们多数人已经很累了，坐在地上不想动，也不想说话，有些人已经表现出一副生无可恋的样子。休息好后，我们开始前往 灵岩溶洞 ，从大门入口处走几十个台阶就是溶洞的入口，观赏全程大概 30-40 分钟。 灵岩溶洞 又是一处神奇的景色，它处在一座庙中，里面的钟乳石光怪陆离、千姿百态、变化莫测，看了让人叹为观止。走在里面，一会儿迂回狭窄，一会儿豁然开朗，周围的环境怪石嶙峋、冷风习习，一会儿让人毛骨悚然，一会儿让人震撼不已。溶洞的入口小门： 福地洞天 。刚刚走进去的时候，就感觉到凉风习习，阴森森的，有人惊叹：是不是开空调啦！不得不说， 灵岩溶洞 里面真是凉快，温度估计 20 摄氏度都不到，而且有风，里面绝对没有开空调、放冷气之类的，全部是自然的凉爽，但是确实有点冷。在行走的过程中，洞顶会有水滴滴下来，简直是冰水滴，滴下来很冷，如果你用手摸一下路边的石头，也是很冰的。至于景观，主要是一些钟乳石、笋石、石柱，在五颜六色的灯光的照射下，显示出各种形态。当然，最终你看到的是什么全靠自己的想象，因为解说员在整个过程中重复了好多次：三分神似，七分想象，不同的人眼里看到的是不同的形态。在这里面拍照也是拍不清，一片漆黑，灯光太暗，除了石头上面会打一些光。不信？来，看看这些都是什么，我反正是看不出来。一路走下来，我就一个感觉：真是太凉快了，把整个人的温度都降下来了，出去的时候，眼镜瞬间被蒙了一层水雾，可见里外的温差有多大。在里面走的久了，我就觉得此地不宜久留，疲惫加上饥饿，如果再被冷气压制一下，很担心下一步就感冒。所以加快步伐，想快点出去。总结一下：洞内真的像一个天然的冰箱，非常的凉爽，特别是在出洞口，从洞内一阵阵凉风刮来，纯天然空调啊，站在洞口让人挪不开脚步。不过这样的巨大温差，如果还站在洞口直接吹冷风，估计很容易就感冒了，反正我是吹了一会就站的远一点，不敢吹了。我们几个人出来后，发现很多人还没出来，只好坐在路边，边休息边等。有几个人在门口吹风，试图自拍： 道法自然 。灵岩溶洞 观赏结束后，所有人到齐，我们将要前往 水上乐园 。水上乐园 水上乐园 本来是为游客开发的水上项目，可以在这里体验各种项目的玩法，例如 水上摩托艇 、 香蕉船 ，当然是要额外收费的。由于时间点不合适，我们也没有提前安排这类活动，所以没有关心具体的项目。给大家看一下 兜率岛 的地图指引，了解一下 水上乐园 的位置，其实它就在 返程码头 的旁边。从 灵岩溶洞 走过去，大概十几分钟，而且路上有树荫有风，不是很热。沿途风光大好，美不胜收，随便拍一张照片都是壮阔的景色。如果按照正常的官方线路图，到这里已经结束了，可以直接在 返程码头 乘坐游船返程，还不耽误回去吃午饭。但是我们至此才开始自由行，由大哥的表妹安排，先在 水上乐园 门口乘坐自己租的小船，前往我们预定的 东江湖避暑休闲农庄 ，在那里度过下午和明天。在 水上乐园 门口，有很多卖特产的，水果、鱼干、烤鱼，可以买来吃。直奔东江湖避暑休闲农庄 一开始我们走错路了，直接走到 返程码头 了，以为会在那里上船。其实不是，而是在另外一个简陋的停靠点，算不上码头，大家依次上小船。上了小船，先来一张自拍合照。在柴油机的轰隆声，大概过了二十分钟，我们到达了 休闲农庄 。给大家看一下地图， 休闲农庄 的位置，它的所在地其实是一个半岛，如果从另外一个方向过去是可以直接开车的，但是从我们这个方向【兜率岛的位置】过去只能坐船。到了休闲农庄，时间大概是 12:00，我们大部分人又累又饿，已经不想动了。在等午饭的过程中，我就躺在沙发上立即睡着了，大概迷迷糊糊睡了十几分钟。到达农庄里的小宿舍，我在二楼看风景。这里设施很齐全，有乒乓球桌、K 歌设备、麻将桌、吊床、躺椅、游泳池、自家果园，大家可以在这里面朝大湖，谈笑风声。吃午饭自由活动 终于等到开饭了，时间大概是 13:00，非常丰盛，非常好吃，大家饱餐一顿。吃完午饭，时间大概是 14:00，大家自由活动，可以回房间休息，可以去游泳，可以打牌，可以到周围逛一逛。我实在是太累了，不想动了，把换掉的衣服简单洗了一下，然后就准备睡觉了。衣服刚洗完，到走廊外面晾衣服，突然发现烈日消失了，天空开始出现了乌云，大风也刮起来了，看来是要下雨了，那些出去逛的小伙伴也要回来了吧。用下面的几张照片来多角度展现当时的情景，乌云密布，简直就是 黑云压城城欲摧，山雨欲来风满楼 。我下楼看了看，看着这湖里逐渐泛起了波浪，远处的乌云也已经压过来，一场风雨不可避免了。接着我就撑不住了，回到房间倒在床上就睡着了，时间大概是 15:30。吃晚饭自由活动 我一觉睡了三个小时，到 18:30 才醒来，而且睡得很稳，睡眠质量超级好。接着刚洗漱完毕就听到有人喊下楼吃晚饭，时间大概是 19:00。吃完晚饭又是自由活动，还有一部分人在玩喝酒游戏，还有两条大黄狗在趴着。大部分人还是很累就回去继续休息了，准备明天的活动，剩下的人在 K 歌、打麻将、游泳、逗狗。此时，吹着微风，听着水声，让人有一种 面朝大海，唱 K 打牌 的错觉。大家玩到 00:00 左右，陆续有人回去休息了。我大概 03:30 才睡，主要是下午睡太久了，睡不着，再加上聊天、唱歌、吃东西，变得有点亢奋。虽然身体上很累，但是精神上有点亢奋，回顾这一天的活动，真的感觉累成狗，还好现在年轻，还能撑得住。看，农庄的两条大黄狗都困得不行，不理我们了。游泳晨跑去果园 第三天的早上我们定的行程是 09:00 出发，因为要坐船、坐大巴前往 郴州火车站 【预计两个小时】，赶 12:10 的火车，还要吃午饭。本来是早上 07:00 吃早餐，然后自由活动，大家可以去跑步、逛果园、游泳，但是我太累了，早上醒来看到外面又在下雨，于是接着睡了。当然，他们大部分人还是起来了，进行各自的活动，只有包括我在内的几个人在睡觉。我大概在 08:30 起床，然后被人催着吃早饭，啥也没玩，听说早上一直在下小雨，导致小部分人没起床，也就是直接睡到了饭点。虽然下着小雨，风景还是不错，几个人在乡间的小路上跑步。吃完早饭，09:00，大家收拾东西返程，准时出发，去赶火车。我们在返程的小船上再来一张合照。返程 说真的，不知道为啥，吃早餐的时候天气还不错，然后从上船那一刻起，雨就变大了。在船上下雨，换乘大巴还在下雨，整个返程的路上一直在下雨，但是到了火车站雨就停了。经过两个小时的舟车劳顿，我们大概在 11:20 到了 郴州火车站 ，由于经费紧张，我们回程是坐普通火车，没有再坐高铁，预计需要四个半小时，在 16:40 可以到达 广州东站 ，反正时间也比较充足。到了火车站，大家各自吃午饭，或者买点零食、饮料带着。我和几个人一起去找了一个小店【佳兴鱼粉】，我吃了 招牌黄鸭叫鱼粉 ，香辣口味的，真的有鱼，味道很好，汤很鲜。吃了一半我觉得有点辣也有点热，就配了一根油条，不得不说，人间美味。临走时，发现店里的 香辣猪脚 【或者叫猪蹄】才十二块钱一只，忍不住买了一只打包带着，准备在火车上吃。不得不说，真的买对了，太好吃了，比广州的还好吃。有一些小伙伴准时进站后，发现火车晚点了 20 分钟，然后先到的小伙伴就在群里通知了大家，让大家吃午饭的的速度慢下来，不要着急。郴州火车站 的车站面积很小，安检、检票、进站全程不到五分钟。插播一个小意外，我在候车室拍视频，持续拍了几分钟，然后被工作人员拦下来，他拦了一下我的手机，提示我说： 收起来，在这里不要乱拍 。在我专心寻找画面的时候，这突如其来的提醒着实把我吓了一跳。到达广州 经过四个半小时的火车旅途，我们到达 广州东站 ，由于火车晚点了二十分钟，我们到达火车站的时间大概是 17:00 。没想到刚出火车站，就是狂风大作、乌云密布，看样子又是一场暴风雨。由于住的地方离地铁站比较远，我一看这天气等会步行也不太方便，决定坐公交回去。在火车站广场走了一圈，发现以前的公交站消失了，取而代之的是一片施工围栏。我跟着人流走了一会，发现公交站搬迁了，原来的公交站位置被围栏围起来，听说在修地铁站。新的公交站在草暖公园，以前是常规的停车场，不过距离有点远，估计步行要十分钟以上。都已经走了这么远，也不能回去坐地铁，看着天气也快下雨了，于是加快步伐走向公交站，在路上看到几乎所有人都在奔跑。果然，刚到公交站，暴雨就来了，还好我已经上车了。顺利到家，到家一下子就放松了。终结 此刻放下打字的键盘，需要好好休息，明天又要开始拿起打代码的键盘，回归工作。]]></content>
      <categories>
        <category>游玩</category>
      </categories>
      <tags>
        <tag>Hunan</tag>
        <tag>Chenzhou</tag>
        <tag>fun</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次愚蠢的 HBase 错误]]></title>
    <url>%2F2019061701.html</url>
    <content type="text"><![CDATA[最近在整理一个单独的 Maven 项目，是从其它项目迁移过来的，主要存放的是和业务强相关的代码，略显混乱的代码是必不可少的，我的职责就是把现存的代码抽象出来，尽量删除一些无用的代码，只保留少量的可复用的代码。在整理的过程中，我几乎把所有的代码全部删除了，只保留一些高复用性的 util、constant 之类的代码，但是在整理和 HBase 相关的代码时，遇到了一个诡异的问题，报错信息截取片段：Cannot get replica 0 location for，后来经过排查发现不是现象诡异，而是自己太愚蠢，本文记录这个过程。代码问题 代码逻辑很简单，根据批量的 pk 值 去 HBase 表中查询数据，只需要查询指定的 列簇 ，然后对返回结果进行解析，按行输出为文本文件。具体的代码逻辑：根据 pk 构造 Get 对象，并指定 列簇 根据 HBase 表名称以及环境配置参数构造 HTable 对象 调用 HTbale 对象的 get 方法获取结果 解析 3 中返回的结果 在测试的时候，发现总是在第 3 个步骤出现大量的异常信息：12019-06-17_23:09:40 [Executor task launch worker-11] ERROR client.AsyncProcess:927: Cannot get replica 0 location for &#123;&quot;cacheBlocks&quot;:true,&quot;totalColumns&quot;:1,&quot;row&quot;:&quot;ef7e0077a525929788b387dda294b9bb&quot;,&quot;families&quot;:&#123;&quot;r&quot;:[&quot;publish_date&quot;]&#125;,&quot;maxVersions&quot;:1,&quot;timeRange&quot;:[0,9223372036854775807]&#125;此外我还看到一些 HBase 查询时的错误，只列出了 HBase 相关的片段：1234567org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1007 actions: IOException: 1007 times, at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.makeException (AsyncProcess.java:228) at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.access$1700 (AsyncProcess.java:208) at org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.getErrors (AsyncProcess.java:1605) at org.apache.hadoop.hbase.client.HTable.batch (HTable.java:936) at org.apache.hadoop.hbase.client.HTable.batch (HTable.java:950) at org.apache.hadoop.hbase.client.HTable.get (HTable.java:911)而且是所有的查询请求全部都是这种错误，看起来像是查询不到数据，但是不知道为何如此。我一开始猜测是环境的原因，可能是哪里把环境参数的配置文件搞错了，于是检查了一遍配置文件，没有发现任何问题。后来猜测可能是集群的问题，但是检查了一遍，集群一直正常运行。折腾了一个多小时，最终实在没有办法，只好把这段查询数据的逻辑代码单独拆出来，重新手写一遍，查询 2 条测试数据的发表时间：publish_date，并单步调试，看看究竟发生了什么。1234567891011121314151617181920212223242526@Test public void TestHBase () &#123; String string = &quot;XX_POST_TABLE_NAME&quot;; try &#123; HTable hTable = new HTable (XxxConfig.getInstance (), TableName.valueOf (string)); List&lt;Get&gt; getList = Lists.newArrayList (); Get get = new Get (Bytes.toBytes (&quot;0000135807e05492e830ade76a8a0c38x&quot;)); get.addColumn (XxxConsts.R, &quot;publish_date&quot;.getBytes ()); getList.add (get); Get get2 = new Get (Bytes.toBytes (&quot;0000135807e05492e830ade76a8a0c38&quot;)); get.addColumn (XxxConsts.R, &quot;publish_date&quot;.getBytes ()); getList.add (get2); Result [] resultArr = hTable.get (getList); for (Result result : resultArr) &#123; Cell cell = result.getColumnLatestCell (RhinoETLConsts.R, &quot;publish_date&quot;.getBytes ()); if (null != cell) &#123; System.out.println (&quot;====&quot; + Bytes.toString (cell.getValueArray (), cell.getValueOffset (), cell.getValueLength ())); &#125; else &#123; System.out.println (&quot;====null&quot;); &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace (); &#125;&#125;经过运行测试，发现是正常的，没有任何错误信息出现，需要的 发表时间 字段可以正常查询出来，我立即懵了，这可怎么办。懵了 3 秒，我立刻缓过神来，作为一个有三年工作经验的工程师，我觉得这都是小场面。而且，此刻我的内心已经有了答案： 没出问题就说明有问题 ，至于哪里有问题则很容易确认，要么是旧代码有问题，要么是我手写的新代码有问题，只要仔细对比一下真相就出来了。接着我仔细对比了一下两份代码的不同之处，没用 3 分钟就发现了蹊跷之处，而且是很低级的错误，且看下一小节的分析。错误代码片段 在上面的开头我已经列出来了代码逻辑的 4 个步骤，本以为出错原因在第 3 个步骤，没想到真实原因在第 2 个步骤：根据 HBase 表名称以及环境配置参数构造 HTable 对象，代码如下：1234567891011121314151617181920212223242526/** * 获取 HBase 连接 * * @param htableStr * @return */public static HTable getHTable (String htableStr) &#123; HTable hTable = null; try &#123; hTable = new HTable (XxxConfig.getInstance (), TableName.valueOf (htableStr)); &#125; catch (Exception e) &#123; LOG.error (e.getMessage (), e); &#125; finally &#123; if (null != hTable) &#123; try &#123; hTable.close (); &#125; catch (IOException e) &#123; e.printStackTrace (); &#125; &#125; &#125; return hTable;&#125;代码乍一看好像没有问题，就那么几行，但是千万要留意 finally 代码块中的代码，它是在做什么？它把刚刚创建好的 HTable 链接关了，关闭之前还友好地判断了一下是不是为 null，我真是一口老血喷出来。注意，这里的关闭操作完全不影响后续的查询请求，在代码层面是判断不出来有什么问题的，即不会产生 编译错误 ，直到运行起来真正去查询的时候才发现报错，但是报错信息又很模糊，没有表明具体的原因。个人思考 我回顾了一下，这个问题明显是一个很低级的错误，但是为什么出现在我身上呢，总结原因有二：一是直接复制了别处的代码，稍做改动，结果改错了；二是在晚上整理的代码，已经工作了一天，状态不够好，容易犯小错误。从小事中吸取教训，总结如下：1、在状态不好的时候，还是先休息好最重要，否则坚持写出来的代码会有一些低级错误，而且还自信地认为没有问题，给后续的排查留下坑。同理，做其它事情也是一样，对于一些要求严格的事情，为了保证质量，一定要在一天中状态最好的时间段去处理，才能最大程度地避免出问题。2、对自己写出的代码不要过于自信，特别是一些简单的代码，自己想当然地认为不可能有问题，不舍得花费几分钟检查一下，或者测试一下，这会为自己带来浪费时间的风险，就像现在这样，早晚要为自己的失误买单。做其它事情也是一样，一定要反复检查自己负责的部分，如果有时候局限于时间排期，没有充足的时间检查，无法保证可靠性的话，宁愿延期解决，也不要坑自己和别人。当然，也不要因此自卑或者退缩，该是自己负责的时候一定要积极，按时保质保量完成。]]></content>
      <categories>
        <category>大数据技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个诡异的 ES-Hadoop 问题]]></title>
    <url>%2F2019061301.html</url>
    <content type="text"><![CDATA[最近在处理 Elasticsearch 数据的时候，使用的是 ES-Hadoop 组件，方便快捷，但是今天遇到一个小问题，让我着实折腾了一番。折腾的原因在于我本以为一切顺利，确实没想到会有一些奇怪的事情发生，这也让我积累了经验，其中错误的核心内容为：Incompatible types found in multi-mapping: Field [your_field] has conflicting types，本文详细记录分析问题的过程，文中内容涉及的开发环境为 Elasticsearch v5.6.8、Windows7 X64。问题出现 代码的逻辑很简单，使用 Spark 连接 Elasticsearch 集群【使用 ES-Hadoop 组件】，读取数据，然后简单处理一下就写入 HDFS 中，没有任何复杂的逻辑。但是在程序运行的过程中，出现了异常：1org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Incompatible types found in multi-mapping: Field [query.bool.must.match.content] has conflicting types of [OBJECT] and [KEYWORD].根据图中的异常信息，可以猜测是字段问题：类型冲突，下面来逐步分析。问题分析 看起来是存在不兼容的 type，根本原因是字段类型冲突，一个字段同时存在两种类型：OBJECT、KEYWORD，但是这个字段名称也太诡异了：query.bool.must.match.content，不用说，肯定是有人在查询时误把查询语句作为数据 put 到了 Elasticsearch 数据库中，导致产生了这种奇怪的字段名称，去数据库查询一下就知道。查询数据量 由于从异常信息中无法得知其它有效信息，只能使用 exists 查询语句，看看有几条这种数据，查询语句如下：12345678910111213&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;exists&quot;: &#123; &quot;field&quot;: &quot;query.bool.must.match.content&quot; &#125; &#125; ] &#125; &#125;&#125;通过查询，可以看到有一条数据，这一看就是一条标准的查询语句，被作为数据存入了 Elasticsearch 数据库，应该是有人误操作。可以看到，整个索引别名【底下可能会有多个真实索引名称】里面只有这一条数据，那为什么会冲突呢？其实，不能只看数据量，因为可能数据被删除了，但是 mapping 中仍旧保留着字段信息【Elasticsearch 的 mapping 无法针对字段粒度进行删除、更新】，所以要进一步查看索引别名下面的每个真实索引名称对应的 mapping 中是不是都有这个字段。因此，直接查看 mapping 更为准确。查看索引配置 这里需要特别注意一个问题，现在很多索引的 mapping 都是使用 匹配模版 构造的，即定义了一些规则【例如字段名称以什么开头、以什么结尾就会存储成对应的类型】，然后字段都以这些规则自动生成，例如如果写入一条数据，里面的内容字段以 _content 结尾，则会自动分词，方便检索。这种方式的好处是可以综合考虑多种情况，提前全部设置为模版，不仅管理起来方便，也为以后的字段扩展留下余地。一般的模版信息格式如下，了解一下即可：1234567891011121314151617181920212223242526272829303132333435363738394041&#123; &quot;mappings&quot;: &#123; &quot;your_index_name&quot;: &#123; &quot;_source&quot;: &#123; &quot;excludes&quot;: [ &quot;content&quot;, &quot;author&quot; ] &#125;, &quot;dynamic_templates&quot;: [ &#123; &quot;template_1&quot;: &#123; &quot;mapping&quot;: &#123; &quot;index&quot;: &quot;not_analyzed&quot;, &quot;type&quot;: &quot;string&quot; &#125;, &quot;match&quot;: &quot;*&quot;, &quot;match_mapping_type&quot;: &quot;string&quot; &#125; &#125;, &#123; &quot;content1&quot;: &#123; &quot;mapping&quot;: &#123; &quot;analyzer&quot;: &quot;wordsEN&quot;, &quot;type&quot;: &quot;text&quot; &#125;, &quot;match&quot;: &quot;*_content&quot; &#125; &#125;, &#123; &quot;price&quot;: &#123; &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;float&quot; &#125;, &quot;match&quot;: &quot;*_price&quot; &#125; &#125; ] &#125; &#125;&#125;模版里面的内容其实是一个 JSON 数组，可以设置多个匹配规则，方便字段的规范管理。接着使用 head 插件查看 mapping，把几个真实的索引下面的 mapping 都检查了一遍【一共四个】，只在两个索引下面找到了期望的 mapping 信息，如下：第一处：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&#123; &quot;properties&quot;: &#123; &quot;query&quot;: &#123; &quot;properties&quot;: &#123; &quot;bool&quot;: &#123; &quot;properties&quot;: &#123; &quot;must&quot;: &#123; &quot;properties&quot;: &#123; &quot;match&quot;: &#123; &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;properties&quot;: &#123; &quot;query&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;type&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125; &#125;, &quot;range&quot;: &#123; &quot;properties&quot;: &#123; &quot;publish_date&quot;: &#123; &quot;properties&quot;: &#123; &quot;from&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;include_lower&quot;: &#123; &quot;type&quot;: &quot;boolean&quot; &#125;, &quot;include_upper&quot;: &#123; &quot;type&quot;: &quot;boolean&quot; &#125;, &quot;to&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125;第二处：123456789101112131415161718192021222324252627282930&#123; &quot;properties&quot;: &#123; &quot;query&quot;: &#123; &quot;properties&quot;: &#123; &quot;bool&quot;: &#123; &quot;properties&quot;: &#123; &quot;must&quot;: &#123; &quot;properties&quot;: &#123; &quot;match&quot;: &#123; &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125;, &quot;term&quot;: &#123; &quot;properties&quot;: &#123; &quot;site_id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125;虽然只找到了两处，但是足够造成前面的异常，通过对比可以发现其中的细微不同之处，核心的地方在于 content 的类型不一致，对比如下：1234567891011121314151617181920-- OBJECT&#123; &quot;content&quot;: &#123; &quot;properties&quot;: &#123; &quot;query&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;type&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125;&#125;-- KEYWORD&#123; &quot;content&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;&#125;好了，破案了，问题根本原因被找到，那么怎么解决呢？问题解决 直接查看源码，先看看 ES-Hadoop 是怎么处理的，根据异常信息里面的方法调用，主要就是看 MappingSet.addToFieldTable ()，我的环境依赖的 ES-Hadoop 坐标为：12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-hadoop&lt;/artifactId&gt; &lt;version&gt;5.6.8&lt;/version&gt;&lt;/dependency&gt;我查询到的源代码如下：1234567891011121314151617181920212223242526272829303132333435363738@SuppressWarnings (&quot;unchecked&quot;) private static void addToFieldTable (Field field, String parent, Map&lt;String, Object []&gt; fieldTable) &#123; String fullName = parent + field.name (); Object [] entry = fieldTable.get (fullName); if (entry == null) &#123; // Haven&apos;t seen field yet. if (FieldType.isCompound (field.type ())) &#123; //visit its children Map&lt;String, Object []&gt; subTable = new LinkedHashMap&lt;String, Object []&gt;(); entry = new Object []&#123;field, subTable&#125;; String prefix = fullName + &quot;.&quot;; for (Field subField : field.properties ()) &#123; addToFieldTable (subField, prefix, subTable); &#125; &#125; else &#123; //note that we saw it entry = new Object []&#123;field&#125;; &#125; fieldTable.put (fullName, entry); &#125; else &#123; // We&apos;ve seen this field before. Field previousField = (Field) entry [0]; //ensure that it doesn&apos;t conflict if (!previousField.type ().equals (field.type ())) &#123; throw new EsHadoopIllegalArgumentException (&quot;Incompatible types found in multi-mapping: &quot; + &quot;Field [&quot;+fullName+&quot;] has conflicting types of [&quot;+previousField.type ()+&quot;] and [&quot;+ field.type ()+&quot;].&quot;); &#125; // If it does not conflict, visit it&apos;s children if it has them if (FieldType.isCompound (field.type ())) &#123; Map&lt;String, Object []&gt; subTable = (Map&lt;String, Object []&gt;) entry [1]; String prefix = fullName + &quot;.&quot;; for (Field subField : field.properties ()) &#123; addToFieldTable (subField, prefix, subTable); &#125; &#125; &#125;&#125;看到这里就没有什么办法了，因为 Elasticsearch 数据不规范【本来是正常的，后来人为因素破坏了 mapping 数据结构】，导致 ES-Hadoop 无法处理，从而抛出异常。但是，仔细思考一下，ES-Hadoop 的这种处理逻辑显然有点问题，因为客户端在读取数据的时候，可以指定同一个索引下的多个类型，当然，也可以同时指定多个索引。然而，有时候为了方便，会把很多索引的别名设置成同一个，这样在查询或者取数的时候就不用指定索引名称的列表了。如果是这样，多个索引下面的 mapping 不能保证一致，由于是手动设置的索引别名，索引数据可能多种多样【另一层面的知识点，Elasticsearch 官方是不允许同一个索引下的多个类型拥有不同的字段属性的，而且，6.x 取消了索引类型的概念】，但是客户端在读取数据的时候是可以过滤字段的，使用 es.read.field.include、es.read.field.exclude 参数分别设置必要的字段、过滤的字段。这样的话，开发者就可以把可能有问题的字段去除掉，避免影响程序的正常运行。然而可以看到，ES-Hadoop 没有给任何机会，遇到类型冲突的字段直接抛出异常，程序无法正常运行。我觉得应该在日志中给出警告，提醒开发者可能出现的问题，但是程序仍旧可以正常运行，在运行的过程中，如果真的遇到字段冲突的问题【例如同时读取了不同索引中的相同字段，但是字段类型不一致，无法处理】，程序自会抛出运行时异常，而如果从头至尾没有任何字段问题，程序就可以正常运行了，开发者甚至毫无感知发生了什么。于是，接着我找到一个 GitHub 的讨论帖子：https://github.com/elastic/elasticsearch-hadoop/issues/1074、https://github.com/elastic/elasticsearch-hadoop/issues/1192，发现早就有人遇到同样的问题了，并且提出了建议，作者也把它作为开发特性，计划在以后的版本发布。目前来看，应该在 v6.4.2、v6.5.0 修复了这个问题，但是我使用的还是 v5.6.8，而且在帖子中也可以看到一些人同样是 v5.6.0、v5.6.1、v5.6.5 版本有问题。此时，我要么升级版本，要么更改源码，要么重建数据源，这些方式对于我来说都有未知的风险，我陷入了沉思。突然，一阵灵光闪现，我觉得可以适当降低小版本号，可能以前 ES-Hadoop 是没有这个限制的，以防走弯路，同时我又参考了别的项目代码，发现 v5.5.0 可以使用。于是，我更改了构件的版本号，其它地方不用变动【要确保低版本的构件可以支持高板的 Elasticsearch】，测试了一下，果然可以，遇到字段冲突不会抛出异常，程序可以正常运行。此时，我再想查查源代码是怎么处理的，发现已经找不到 v5.6.8 那个 MappingSet 类了。依赖构件坐标如下：12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-hadoop&lt;/artifactId&gt; &lt;version&gt;5.5.0&lt;/version&gt;&lt;/dependency&gt;这种情况虽然看起来有潜在的危险，我也知道，但是我在自定义配置中，使用 es.read.field.include 参数只读取少量的字段，就可以保证有冲突的字段不影响我的业务处理逻辑，也认为对整个应用程序没有什么危害。]]></content>
      <categories>
        <category>大数据技术知识</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Elasticsearch</tag>
        <tag>es-hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 让进程在后台运行的几种方法]]></title>
    <url>%2F2019051501.html</url>
    <content type="text"><![CDATA[在 Linux 系统中，运行程序时经常需要把进程放在后台运行，并且退出终端等待，也可以说是守护进程，daemon 的概念，可能过几个小时或者十几个小时之后再去观察。此时，需要把进程放入后台运行，并且为了防止进程挂起，也需要设置进程忽略挂起信号【使用 nohup 命令】，这样就可以保证进程在我退出终端后仍旧能正常运行，无论我多久之后再来观察，仍可以看到进程的信息。本文记录关于进程后台运行【daemon】的几种方法，并给出实际操作示例。回顾 在我刚开始步入工作的路途上，算是一个职场新手，很多东西都不会，一路上披荆斩棘，现在总算积累了一些经验。记得在刚开始工作时，经常碰到这样的问题，使用 telnet 或者 ssh 登录了远程的 Linux 服务器【很多工作需要在 Linux 上面完成】，在上面跑了几个 Shell 脚本，或者起了几个 Java 进程，而且这些脚本或者进程耗时都比较长，可能需要几个小时或者几天。我一开始的做法就是打开 XShell 工具的多个会话，分别跑脚本或者起进程，不仅不能关掉，而且还要担心网络问题导致会话断开，一切工作就白费了。结果被其他同事看到了，说我这种做法太蠢了，可以使用后台挂起的命令【即 nohup 加上 &amp;】，这样就可以让进程自由在在地运行，我也可以安心做其它事情了。我第一次受到了经验方面的冲击，觉得这种方式太酷了，一开始我为什么不多思考一下或者查询一下，我想这些实用的知识点肯定还有很多，从此以后我更加努力。在后来的工作或者生活当中，我又接触到了很多类似的知识点或者说是小技巧，不仅提高了我的工作效率，还丰富了我的认知。其中，基于进程的后台运行这个需求，还有很多很好的工具可以使用，还有很多实际操作的小技巧可以使用，以下篇幅会一一介绍。继续阅读之前，可以先了解一下信号的概念，也可以直接参考我的另外一篇博文：Linux 之 kill 命令入门实践 。 前言 在工程师或者运维人员的生涯中，肯定会碰到这样的场景：使用 ssh 或者 telnet 登录了远程服务器，然后在上面跑一些任务或者脚本。如果是临时任务或者几分钟就能搞定的任务，基本不会有什么问题，但是如果是耗时比较长的任务，就会因为网络不稳定或者手抖退出了会话，从而导致任务失败，还要从头再来，遇到这种问题所有人都是崩溃的。那么我不禁思考，有没有什么办法可以让任务在提交后不受网络中断、会话退出的影响呢，可以一直保持在后台稳定运行，直到结束。肯定是有的，读者在工作中一定也见过周围的大神同事操作，下面列举一些常用的方式，读者可以参考，选择自己喜欢的方式使用。内容中涉及到的 SIGHUP 信号，先来了解一下它的由来：在 Unix 的早期版本中，每个终端都会通过 modem 和系统通讯，当用户 logout 时，modem 就会挂断（hang up）电话。同理，当 modem 断开连接时，就会给终端发送 hangup 信号来通知其关闭所有子进程。这里的子进程包含前台子进程、后台子进程，前台子进程是被直接关闭的，后台子进程要根据操作系统的 huponexit 设置而定，不一定会被关闭。其中，这里的后台子进程包括正在运行的子进程（使用 jobs 工具查看 处于 running 状态）、暂停的子进程（使用 jobs 工具查看 处于 stopped 状态）。再看一下维基百科给它的定义：nohup is a POSIX command to ignore the HUP (hangup) signal. The HUP signal is, by convention, the way a terminal warns dependent processes of logout.Output that would normally go to the terminal goes to a file called nohup.out if it has not already been redirected.直接忽略挂起信号 众所周知，当发生用户注销会话、网络断开等事件时，终端会收到 SIGHUP 信号从而关闭其所有的子进程【通过把 SIGHUP 信号发送给所有 后台 子进程实现， 前台 子进程直接停掉】。到这里，我就能想到两种解决方案：一是让进程忽略掉 SIGHUP 信号，二是让进程独立运行，附属于其它进程，脱离当前终端对应的进程，这两种方法都可以让进程不受外界因素影响，稳定地运行。nohup 方式 思路有了，首先能想到的就是 nohup 工具，顾名思义，nohup 这个工具的作用就是让进程忽略掉 SIGHUP 信号。先来看一下帮助文档信息，使用 命令查看：1 待整理 图。。setsid 方式 待整理。&amp; 方式 待整理。忽略挂起的后悔药 后台运行的例子，使用 disown 也可以达到效果。使用 Ctrl + z 命令，把正在前台运行的进程暂停，并放在后台，程序并没有被杀死。其实这个组合快捷键是一种控制信号，编号为 19，标识为 SIGSTOP，读者可以参考我的另外一篇博文：Linux 之 kill 命令入门实践 。当然，如果使用终端工具，再开一个会话窗口，使用 ps 命令查询这个进程的 pid 编号，然后使用 kill -19 pid 命令发送一个 SIGSTOP 信号给进程也可以达到把程序暂停并放在后台的效果。 批量管理进程 前面的描述都是单个进程或者几个进程，管理起来也挺方便，但是如果遇到大量的进程需要管理，例如运维人员日常需要手动管理大量的进程，几百个几千个都是有可能的，那么怎么办呢。为了简化管理，并且保证进程能在后台稳定运行，此时就需要通过 screen 工具来操作，这是一个利器。首先我们来看一下帮助文档信息，使用 man screen 命令输出：1xx图。。备注 1、关于挂起的测试 我使用 XShell 测试退出终端时，发现并不会挂起普通的后台进程【说明后台进程没有接收到 SIGHUP 信号】，反而把进程的父进程设置为了 init【进程号为 1】，这样进程就不会退出，下次登录的时候还能查看。但是这个现象违反了前面的知识点：退出终端时所有子进程会收到 SIGHUP 信号，后来我发现，原来这个操作是针对 前台任务 而言的，如果是 后台任务 则不一定，要看系统的参数设置：shopt | grep huponexit，huponexit 这个参数决定了是否向后台任务发送 SIGHUP 信号。而在大多数 Linux 系统中，这个参数一般默认是关闭的，所以才出现了终端退出后台进程没有挂起的现象。2、读者可以继续探索一下 tmux 工具的使用，这类工具可以更加高效、安全地帮助我们管理后台进程，从而让我们脱离手动管理后台程序的苦海。]]></content>
      <categories>
        <category>大数据技术知识</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>nohup</tag>
        <tag>setsid</tag>
        <tag>disown</tag>
        <tag>screen</tag>
        <tag>daemon</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[指南页面的自动收集]]></title>
    <url>%2F2019050401.html</url>
    <content type="text"><![CDATA[最近给博客站点增加了一个独立的页面： 指南 ，用来记录本站点的所有文章，包括发表时间与文章链接，并按照发表时间倒序排列，仅供快速查找以及核对使用。其中，还会有一个文章编号，就是 url 的数字部分，由日期与编号组成，例如 2019050201 表示 2019 年 05 月 02 日发表的第 1 篇文章，这个文章编号对我来说很有用，用来核对查重。但是，为了简化这个页面的整理工作以及后续的自动生成，我需要把这个流程自动化，本文记录实现思路与实现方式。实现思路 为了简化难度，我准备使用 Shell 解决这个问题。大概思路：遍历目录的文件、解析 id 与 title、搜索匹配指南 index 文件、替换或者追加。1、遍历指定目录【_post 目录】中的所有 markdown 文件，对每个文件执行 2。2、对单个文件，使用 grep 正则搜索： ^id: [0-9]{10}，使用 awk 获取 id 值，判断 id 值是否在 index 文件中，在则跳过，否则执行 3。3、再使用 grep 正则搜索： ^title: ，使用 awk 获取 title 值，执行 4，把 id 值和 title 值追加到 index 文件中。4、先使用 grep id 值 index 文件 搜索 3 中的 id 是否已经在 index 文件中，不在才能追加进去。追加 index 文件时，先重命名 index 文件为 index_bak，逐行读取。对每一行数据使用 grep 正则搜索：1echo $line | grep -E &apos;^- [0-9]&#123;10&#125;，\[&apos; | awk -F&apos;，&apos; &apos;&#123;print $1;&#125;&apos; | awk -F&apos; &apos; &apos;&#123;print $2;&#125;&apos;如果搜索无内容的直接将当前行写入新文件，命名为 index。再结合使用 awk 获取当前行的 id 值，只要 3 中的 id 值大于此 id 值并且年份一致，则将 3 中的 id 值和 title 值写入新文件【构造特定格式的数据行】，接着再将当前行写入新文件。如果年份一致且 3 中的 id 值最小，则按照 5 写入当年的数据最后一行。5、相同年份作一个标记，是否已经写入新文件作一个标记，如果遇到读取的下一行已经不是当年的数据【正则搜索无结果】，则把构造的特定格式的数据行写入新文件，再把当前行写入新文件。6、4 中的文件逐行读取完成后，把 4 中一开始重命名的文件 index_bak 删除，只保留新文件 index，此时 index 文件中已经增加了一行新内容。具体实现 Shell 脚本内容参考如下，注释都已经标明处理逻辑，通俗易懂：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#!/bin/bash# index 文件 index=./source/guide/index.mdindex_bak=./source/guide/index_bak.md# 文本格式 content_pattern=&apos;- id，[title](https://www.playpi.org/id.html)&apos;# 遍历文件夹内的所有文件 for file in ./source/_posts/*.mddo if [-f $file]; then # 获取单个文件的 id【在第 3 行】 和 title【在第 2 行】 echo &apos;================================================================&apos; # echo &apos;====read file:&apos; $file var1=$(grep -nE &apos;^id: [0-9]&#123;10&#125;&apos; $file | grep -E &apos;^3:id: [0-9]&#123;10&#125;&apos; | awk -F&apos;: &apos; &apos;&#123;print $2;&#125;&apos;) # echo &apos;====read id:&apos; $var1 var2=$(grep -n &apos;^title: &apos; $file | grep &apos;^2:title: &apos; | awk -F&apos;: &apos; &apos;&#123;print $2;&#125;&apos;) # echo &apos;====read title:&apos; $var2 # 判断非空必须使用双引号，否则逻辑错误 if [-n &quot;$var1&quot;] &amp;&amp; [-n &quot;$var2&quot;]; then has=$(grep $var1 $index) # 为空，表示 id 不在 index 文件中，has 变量切记使用双引号 if [-z &quot;$has&quot;]; then # 字符串搜索替换，待搜索字符串是变量，不是字符串本身，// 表示替换所有 content=$&#123;content_pattern//id/$var1&#125; content=$&#123;content/title/$var2&#125; # 追加到 index 文件中 echo &apos;====prepare append to index:&apos; $content # 重命名 index 文件 mv $index $index_bak # 标记是否写入 / 是否同一年份 has_write=&apos;&apos; is_same_year=&apos;&apos; while read line do match_id=$(echo $line | grep -E &apos;^- [0-9]&#123;10&#125;，\[&apos; | awk -F&apos;，&apos; &apos;&#123;print $1;&#125;&apos; | awk -F&apos; &apos; &apos;&#123;print $2;&#125;&apos;) # 搜索到匹配内容并且还没写入 if [-n &quot;$match_id&quot;] &amp;&amp; [-z &quot;$has_write&quot;]; then # echo &apos;====compare,match_id:&apos; $match_id # 判断是否相同年份 if [$&#123;var1:0:4&#125; == $&#123;match_id:0:4&#125;]; then is_same_year=&apos;1&apos; # 比较大小 if [$var1 -gt $match_id]; then echo &apos;====gt match_id append to index:&apos; $content echo $content &gt;&gt; $index echo $line &gt;&gt; $index has_write=&apos;1&apos; else echo $line &gt;&gt; $index fi else echo $line &gt;&gt; $index fi elif [-n &quot;$is_same_year&quot;] &amp;&amp; [-z &quot;$has_write&quot;]; then # 当前行没有搜索到匹配内容，并且同一年份，并且还没写入，说明已经是当前年份的最后一行了，直接写入即可 echo &apos;====last append to index:&apos; $content echo $content &gt;&gt; $index echo $line &gt;&gt; $index has_write=&apos;1&apos; else # 没有搜索到匹配内容，或者不同年份，或者已经写入，直接写入即可 echo $line &gt;&gt; $index fi done &lt; $index_bak # 删除 index_bak 文件，此时只有最新的 index 文件 rm $index_bak else # 在 index 文件中已经存在，无需处理 echo &apos;====index has:&apos; $var1 fi else echo &apos;!!!!invalid var:&apos; $var1 $var2 fi else echo &apos;!!!!invalid file:&apos; $file fidone 执行脚本时会打印解析出来的每条 id 与 title，以及写入的原因【比较后写入、最后一条写入】，第一次执行脚本耗时久一点，后续执行会自动跳过已经收集过的，耗时可以忽略。由于实现思路简化了，所以要求 index 文件在每一年都至少已经有一个完整的记录，否则没法对比写入。注意，正则搜索时在必要情况下使用 -E 选项开启扩展模式，否则正则无效，仍旧是普通的字符串。日期格式的字符串比较大小，由于格式规范，都是十位数字，可以直接转为数字比较，大的就代表日期最新。当然，日期格式的字符串比较大小，也可以先转为时间戳数字，再进行比较，示例：1date +% s -d &apos;2019010101&apos;虽然这种格式得到的结果不是真正的时间戳，但是只要是数字就可以比较了。另外，经过查找帮助手册，没有发现可以指定格式的参数选项，也就是无法把 YYMMddHH 格式的日期字符串转为对应的真实时间戳。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>guide</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Java 代码迁移微博图床到 GitHub 图床]]></title>
    <url>%2F2019050201.html</url>
    <content type="text"><![CDATA[由于微博图床开启了防盗链，导致我的博客里面的图片全部不可见，因此要切换图床。当然，一开始我使用的是极其简单的方法，直接设置博客页面的 referer 属性即可【设置为 noreferrer】，这样微博图床就检测不到引用来源，也就不会拒绝访问了。但是后续又遇到了其它问题，这些内容我在前几天的博客里面都记录了：解决微博图床防盗链的问题 。后来我实在找不到更为恰当的解决方案，于是决定直接迁移图床。本来一开始准备使用 PicGo 这个工具，但是发现有问题，在我比较着急的情况下，决定自己写一写代码，完成迁移操作。本文就记录这些代码的逻辑。 依赖构件 为了减少代码量，精简代码，需要引入几个第三方 jar 包，当然不引入也行，如果不引入有一些繁琐而又简单的业务逻辑需要自己实现，有点浪费时间了。主要要依赖几个 jar 包：处理文件的 io 包、处理网络请求的 httpclient 包、处理 git 的 jgit 包，pom.xml 配置文件内容如下：123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.5.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.eclipse.jgit&lt;/groupId&gt; &lt;artifactId&gt;org.eclipse.jgit&lt;/artifactId&gt; &lt;version&gt;4.8.0.201706111038-r&lt;/version&gt;&lt;/dependency&gt;代码结构 写代码也比较简单，主要有四个步骤：读取 Markdown 文件内容并利用正则抽取微博图床的图片链接、下载所有图片并上传至 GitHub、替换内容中抽取出的所有图片链接为 GitHub 的图片链接、内容写回新文件。使用 Java 处理不需要多少代码，大概有不到 200 行代码，真正的业务逻辑代码更少，当然，关于网络请求的部分还是不够精简，目前我觉得能用就行。代码放在 GitHub 上面，仅供参考：MigratePic.java ，搜索 MigratePic 类即可。代码主体调用：1234567891011121314151617181920212223242526public static void main (String [] args) &#123; // String dir = &quot;e:\baktest&quot;; // String outDir = &quot;e:\baktest-out&quot;; String dir = &quot;e:\bak&quot;; String outDir = &quot;e:\bak-out&quot;; Set&lt;File&gt; fileSet = getAllFiles (dir); LOGGER.info (&quot;==== 文件个数:&quot; + fileSet.size ()); for (File file : fileSet) &#123; try &#123; // 1 - 读取文件，抽取微博图床的链接与图片名称 String content = FileUtils.readFileToString (file, &quot;utf-8&quot;); Map&lt;String, String&gt; imgMap = extractImg (content); // 2 - 下载图片并上传至 GitHub Map&lt;String, String&gt; urlMap = uploadGithub (imgMap); // 3 - 替换所有链接 content = replaceUrl (content, urlMap); // 4 - 内容写回新文件 String outFile = outDir + File.separator + file.getName (); FileUtils.writeStringToFile (new File (outFile), content, &quot;utf-8&quot;); LOGGER.info (&quot;==== 处理文件完成:&#123;&#125;, 获取新浪图床链接个数:&#123;&#125;, 上传 GitHub 个数:&#123;&#125;&quot;, file.getAbsolutePath (), imgMap.size (), urlMap.size ()); &#125; catch (IOException e) &#123; e.printStackTrace (); &#125; &#125;&#125;需要指定输入、输出目录。截图如下：其中，getAllFiles 方法是获取指定目录的所有文件：1234567891011121314/** * 获取指定文件夹内的所有文件 * * @param dir * @return */private static Set&lt;File&gt; getAllFiles (String dir) &#123; Set&lt;File&gt; fileSet = new HashSet&lt;&gt;(); File file = new File (dir + File.separator); for (File textFile : file.listFiles ()) &#123; fileSet.add (textFile.getAbsoluteFile ()); &#125; return fileSet;&#125;在代码的细节中，可以看到我是每个文件单独处理的，比较耗时间的就是下载图片、上传到 GitHub 这两个过程，而且由于我是文件分开处理，所以总的时间更长了。如果想节约点时间，可以一次性把所有的图片全部下载完成，最后一次提交到 GitHub 即可，这样就节约了多次频繁地与 GitHub 建立连接、断开连接所消耗的时间，如果是几次提交无所谓，但是几十次提交就多消耗很多时间了。例如按照我这个量，78 个文件，500-600 张图片，运行程序消耗了十几分钟，但是我估计如果一次性处理完成，时间应该在 5 分钟以内。接下来分别描述四个步骤。读取文件抽取图片链接 Markdown 文件其实也就是普通的文本文件，没有特殊的格式，这就给程序处理带来了极大方便，直接使用工具包读取就行。此外，抽取微博图床的图片链接需要使用正则表达式，代码内容如下：1234567891011121314151617181920private static Pattern PATTERN = Pattern.compile (&quot;https://[0-9a-zA-Z]&#123;3&#125;\.sinaimg\.cn/large/[0-9a-zA-Z]&#123;8,50&#125;\.jpg&quot;);/** * 抽取微博图床的图片链接与图片文件名 * * @param string * @return */private static Map&lt;String, String&gt; extractImg (String string) &#123; Map&lt;String, String&gt; imgMap = new HashMap&lt;&gt;(); Matcher matcher = PATTERN.matcher (string); while (matcher.find ()) &#123; String oldUrl = matcher.group (); int index = oldUrl.lastIndexOf (&quot;/&quot;); if (0 &lt; index) &#123; String imgName = oldUrl.substring (index + 1); imgMap.put (oldUrl, imgName); &#125; &#125; return imgMap;&#125; 这里列举一个图片链接的例子：https://ws1.sinaimg.cn/large/b7f2e3a3gy1g2hlkwnfm9j214a0hr75v.jpg 。下载图片并上传新图床 这是一个很重要的步骤，需要把上一个步骤完成后获取到的图片下载下来，并且提交到 GitHub 上面去【提交可以不使用代码，直接手动提交也行】，然后获取新图片链接。为了完成这个步骤，需要先在 GitHub 上面新建一个项目，专门用来存放图片，然后把这个项目 clone 到本地，用来存放下载的图片，最后直接提交即可。下载图片并提交到 GitHub：123456789101112131415161718192021222324252627private static String githubUrl = &quot;https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/old/&quot;;/** * 提交本地的图片到 GitHub, 并拼接新的图片链接 * * @param imgMap * @return */private static Map&lt;String, String&gt; uploadGithub (Map&lt;String, String&gt; imgMap) &#123; String imgDir = &quot;E:\img\img-playpi\img\old\&quot;; Map&lt;String, String&gt; urlMap = new HashMap&lt;&gt;(); for (Map.Entry&lt;String, String&gt; entry : imgMap.entrySet ()) &#123; String oldUrl = entry.getKey (); String imgName = entry.getValue (); boolean isSuc = downloadImg (oldUrl, imgDir, imgName); if (isSuc) &#123; String newUrl = githubUrl + imgName; urlMap.put (oldUrl, newUrl); &#125; &#125; LOGGER.info (&quot;==== 开始上传文件到 GitHub, size: &#123;&#125;&quot;, urlMap.size ()); // 统一上传到 GitHub, 这一步骤可以省略，留到最后手动提交即可 boolean gitSuc = JGitUtil.commitAndPush (&quot;add and commit by Java client,img size: &quot; + urlMap.size ()); if (!gitSuc) &#123; urlMap.clear (); &#125; return urlMap; &#125;注意下载图片需要指定本地项目的路径，方便提交到 GitHub，例如我这里是 E:\img\img-playpi\img\old\，拼接 GitHub 的图片链接时需要指定固定的域名部分、用户名、分支名、子目录，例如我这里是：https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/old/ 。这里列举一个 GitHub 图片链接的例子：https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/old/20190502183444.png 。下载图片的详细逻辑：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/** * 下载图片到指定的文件目录 */public static Boolean downloadImg (String url, String dir, String fileName) &#123; Boolean isSuc = false; HttpClient httpclient = null; int retry = 5; while (0 &lt; retry--) &#123; try &#123; httpclient = new DefaultHttpClient (); HttpGet httpget = new HttpGet (url); httpget.setHeader (&quot;User-Agent&quot;, &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.79 Safari/537.1&quot;); httpget.setHeader (&quot;Accept&quot;, &quot;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&quot;); HttpResponse resp = httpclient.execute (httpget); if (HttpStatus.SC_OK == resp.getStatusLine ().getStatusCode ()) &#123; HttpEntity entity = resp.getEntity (); InputStream in = entity.getContent (); isSuc = savePicToDisk (in, dir, fileName); return isSuc; &#125; &#125; catch (Exception e) &#123; e.printStackTrace (); LOGGER.error (&quot;!!!! 下载失败，重试一次 & quot;); &#125; finally &#123; httpclient.getConnectionManager ().shutdown (); &#125; &#125; return isSuc;&#125;/** * 根据输入流，保存内容到指定的目录文件 * * @param in * @param dirPath * @param filePath */private static Boolean savePicToDisk (InputStream in, String dirPath, String filePath) &#123; try &#123; File dir = new File (dirPath); if (dir == null || !dir.exists ()) &#123; dir.mkdirs (); &#125; // 拼接文件完整路径 String realPath = dirPath.concat (filePath); File file = new File (realPath); if (file == null || !file.exists ()) &#123; file.createNewFile (); &#125; FileOutputStream fos = new FileOutputStream (file); byte [] buf = new byte [1024]; int len = 0; while ((len = in.read (buf)) != -1) &#123; fos.write (buf, 0, len); &#125; fos.flush (); fos.close (); return true; &#125; catch (IOException e) &#123; e.printStackTrace (); LOGGER.error (&quot;!!!! 写入文件失败 & quot;); &#125; finally &#123; try &#123; in.close (); &#125; catch (IOException e) &#123; e.printStackTrace (); &#125; &#125; return false;&#125;提交图片到 GitHub 的代码：12345678910111213141516171819202122/** * 提交并推送代码至远程服务器 * * @param desc 提交描述 * @return */public static boolean commitAndPush (String desc) &#123; boolean commitAndPushFlag = false; try (Git git = Git.open (new File (LOCAL_REPOGIT_CONFIG))) &#123; UsernamePasswordCredentialsProvider provider = new UsernamePasswordCredentialsProvider (GIT_USERNAME, GIT_PASSWORD); git.add ().addFilepattern (&quot;.&quot;).call (); // 提交 git.commit ().setMessage (desc).call (); // 推送到远程，不报错默认为成功 git.push ().setCredentialsProvider (provider).call (); commitAndPushFlag = true; &#125; catch (Exception e) &#123; e.printStackTrace (); LOGGER.error (&quot;Commit And Push error!&quot; + e.getMessage ()); &#125; return commitAndPushFlag;&#125;注意这里需要指定本地项目的配置文件路径，例如我的是 E:\img\img-playpi\.git，与前面的下载路径是在同一个父目录，另外还需要指定用户名密码。使用新链接替换旧链接 如果前面的步骤完成，就说明图片已经被成功迁移到 GitHub 上面，并且获取到了新的图片链接，接着直接替换掉旧链接即可。代码逻辑如下：123456789101112131415/** * 替换所有的图片链接 * * @param string * @param urlMap * @return */private static String replaceUrl (String string, Map&lt;String, String&gt; urlMap) &#123; for (Map.Entry&lt;String, String&gt; entry : urlMap.entrySet ()) &#123; String oldUrl = entry.getKey (); String newUrl = entry.getValue (); string = string.replaceAll (oldUrl, newUrl); &#125; return string;&#125;替换后内容写回新文件 写入新文件是很简单的，直接调用 io 包即可完成，但是为了安全起见，文件放在新的目录中，不要直接替换掉原来的文件，否则程序出现意外就麻烦了。迁移结果 随意打开一篇博客，使用文件对比工具查看替换前后的区别，可以看到除了图片链接被替换掉，其它内容没有任何变化。在本地仓库查看，图片已经全部下载。在 GitHub 的仓库中查看，图片全部推送。任意打开一篇博客，里面的图片已经可以全部正常显示，只不过有一些太大的图片【超过 1MB 的】加载速度有点慢，还可以接受。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>weibo</tag>
        <tag>image</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决微博图床防盗链的问题]]></title>
    <url>%2F2019042701.html</url>
    <content type="text"><![CDATA[对于不少自己搭建博客的人来说，图床的选择可真是一个大难点，以前还有各种免费好用的图床工具，例如七牛云、又拍云、SM.MS、Imgur、GitHub、微博图床等，当然还有腾讯云、阿里云的云存储服务，但是免费的意味着不稳定，说不定哪天图片就没有了，有一些国外的访问速度又不行，国内的云存储服务商收费又比较高，还有的必须绑定认证的域名才能使用。本来搭建一个小小的博客，只为了记录知识，传播技术，遇到耗财或者耗精力的这种问题，都比较头疼。后来纠结了好几天，最终决定使用免费的 微博图床 ，一是因为新浪微博这家厂商体量大，微博图床短期内应该不会出问题，二是看到好多网友说他们已经稳定使用微博图床 3-5 年了，没有出过问题。我大概使用的时间还没有一年，以前都是本地化的，没有整理成完整的文章，后来开始慢慢整理并部署上线。没想到最近【2019 年 4 月 24 日左右发现】微博图床出问题了，访问图片链接全部是返回 403 状态码，表示拒绝访问，其实是微博图床开启了防盗链，本文就记录这个现象以及可行的解决方案。微博图床防盗链开启 初始现象 在 2019 年 4 月 24 日的时候，我发现一个严重的问题，我的博客里面的图片显示不出来了，并不是被封了，如果被封也会显示图片的，只不过是马赛克图片。发现这个问题的缘由是新写了一篇博客，本地生成测试的时候，发现图片全部不显示了，一开始还以为是网络问题。博客里面的图片全部无法正常显示 接着我随机抽了一些图片链接在浏览器中直接打开看，发现是可以看到图片的，然后在博客中还是看不到图片，如果在博客中选择图片链接，使用右键 在新标签页中打开 ，也是不能看到。这就说明微博图床开始检测请求的合法性了，对于不正常的请求统统拒绝。当然，如果直接使用图片的链接在浏览器中单独打开，是可以看到图片的，紧接着在博客中就可以看到对应的图片了，但是这并不是说明图片可以使用了，其实是浏览器的缓存作用，如果及时清除浏览器的缓存，发现又不能使用了。复制图片地址在浏览器中打开，图片可以正常显示 分析现象 接着使用浏览器的调试工具查看详细的请求信息，按 F12 按键，调出调试工具，刷新网页，使用 jpg 过滤无效内容，可以看到所有的图片访问请求结果都是 403，也就是拒绝访问。随便点开一个链接的请求信息，查看 Status Code 为 403，也就是拒绝访问，注意查看请求头的 Referer 参数，值是一个链接，表示当前请求所属的页面，即 引用来源 ，而新浪微博恰好会检测这个参数，拒绝所有的外链请求，即不是从新浪的站点发送的图片请求。原来，近期微博图床对图片 CDN 添加了引用来源【Referer】检测，非微博站内引用将会返回 403 错误码，即拒绝访问。那能不能伪造或者清除这个参数呢，其实是可以的，只不过伪造、清除都需要增加一些 Javascript 动态脚本来处理，需要一些技术支持。如果选择清除 Referer 参数，可以先验证一下，把图片的链接直接复制到浏览器中访问，就不会有这个参数，发现可以正常访问，没有 403 错误。注意，一开始我发现使用浏览器能直接访问，紧着着博客里面的图片也能访问了，我还以为是需要单独访问一次图片，然后就可以任意访问了，后来发现其实是浏览器缓存的作用，空欢喜一场。也看到有说法是，微博图床仅仅针对开启 SSL 的链接【即 HTTPS】实行站外禁止访问，而普通的 HTTP 链接仍旧安然无恙，这种说法是错误的【但是确实有这种现象出现】。我测试了一下，的确是有这样的现象，前提是来源页面开启了 SSL，而图床链接使用基本的 HTTP，这样的话由于 Referer 的特性，请求图片链接时不会传输 Referer 这个参数的值【即来源页面的信息不会传递给请求页面】，微博图床自然也就无法检测了。所以最简单的方案就是把所有微博图床的链接全部由 HTTPS 替换为 HTTP，但是由于我的博客全面开启了 SSL，为了加绿锁，因此不引用普通的 HTTP 链接，这种简单的方案我就无法采用了，只能遗憾舍弃。解决方案 考虑切换图床，免费的已经基本没有了，收费的比较贵，或者找到方案先临时使用，不然会给查看博客的人带来很大困扰，毕竟没有图片的博客怎么能看，这也影响博客的质量与声誉。尝试清除来源引用 在静态网页的 头部 代码中【即 head 标记】添加如下配置项：1&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot; /&gt;它的作用就是阻止浏览器发送 Referer 信息，对整个页面的所有链接生效【当然也有针对单个链接设置的方法：&lt;a rel=”noreferrer” href=”your-website-url” /&gt;，这里不采用】，这样一来微博图床就不知道请求的引用来源了，可以达到和直接在浏览器中访问一样的效果。 但是要注意，不是每种浏览器都支持这种语法的，此设置对有的浏览器来说无效。那么在 Hexo 框架中怎么增加呢，显然不会有相关配置项，只能更改源代码，而且使用了 Next 主题，应该要更改主题的源代码，以保证 Hexo 在渲染静态页面为每个页面都增加这个配置。查阅文档，了解了渲染模板所在位置，打开 themes/next/layout/_partials/head.swig 文件，在里面添加 meta 标记就行。修改完成后查看页面的源代码，已经有这个属性了，并且所有的图片都可以正常访问了，完美。但是我觉得这肯定不是长久之计，以后说不定还会有幺蛾子出现，所以要随时准备着。尝试其他方案 先观察一段时间，这段时间要考虑其他方案的可行性和成本。建议 微博图床开启防盗链，个人博客对于新浪图床的依赖时代基本要告别了，虽然有其他免费图床可以使用，但稳定性和可持续性上显然无法与大企业维护的图床相比。为了博客内容稳定考虑，还是考虑后续逐渐把图片迁移到其他云存储空间，费用方面能承受就行。其他知识点 微博图床简单介绍 对于大多数个人博客维护者而言，免费的图床既节省成本，也能够提升页面访问的速度，而新浪微博图床则成了首选。新浪微博由于本身体量大，其图床免费无限容量，只需要有一个微博账号就可使用。同时具备全网 CDN 加速，支持 HTTPS，无论是国内还是国外网络访问，速度都很不错。而且新浪如此企业，不会像其他个人或者团队经营的免费图床一样随时可能会关掉。基于这些优势，不少人会优先选择新浪微博图床作为网站提供图片服务。毕竟直接挂 CDN 或者自建图床的话，也是一个持久的付费维护，如果一旦被攻击，更是造成费用暴增。RefererReferer 首部包含了当前请求页面的来源页面的地址，即表示当前页面是通过此来源页面里的链接进入的。服务端一般使用 Referer 首部识别访问来源，可能会以此进行统计分析、日志记录以及缓存优化等。同时也让服务器能够发现过时的和错误的链接并及时维护。需要注意的是 referer 实际上是 referrer 的误拼写，它可能是 HTTP 协议中第一个被拼写错误的标准头，为保持向下兼容就将错就错了。可以参见 RFC 文档的 Referer 的介绍：https://tools.ietf.org/html/rfc2616#section-14.36 ，原文有这样的描述：the “referrer”, although the header field is misspelled.此外还可以参考维基百科的相关介绍：https://zh.wikipedia.org/wiki/HTTP% E5%8F%83% E7%85% A7% E4% BD%8D% E5%9D%80 。在以下几种情况下，Referer 不会被发送 来源页面采用的协议为表示本地文件的 file 或者 data URI当前请求页面采用的是非安全协议，而来源页面采用的是安全协议【HTTPS】为整个页面设置 &lt;meta name=”referrer” content=”no-referrer” /&gt;为单个链接设置 &lt;a rel=”noreferrer” href=”your-website-url” /&gt;注意第二种情况，如果你的博客开启了 SSL，可以使用 HTTP 的图片链接，就可以正常访问了。但是要牺牲你的博客的安全性，因为浏览器会检测到你的博客内容里面有普通的 HTTP 链接，就会导致不可信【尽管存在有效的证书，也没有用】，小绿锁会消失，并给出警告。例如我的博客，为了测试，使用了一个 HTTP 图片链接，其它的图片都是 HTTPS 链接，可以发现 HTTP 的图片可以正常访问，其它的图片仍旧被拒绝访问了。此时，发现博客的小绿锁已经没有了，并且给出了警告提示。后记 使用上述的解决方案后，我又发现了一个严重的问题，由于清除了引用来源 referer，博客文章的地址就不会发送出去，导致我的 不蒜子 统计失效，也就是每篇文章的阅读数、整个站点的访问量【pv】、整个站点的访客数【uv】都会停止统计。这会导致整个博客的动态流量不可见，对于写博客的我来说内心会有一点点失落，所以我要想办法解决这个问题。已经知道问题的根源了，解决起来也是很容易的，直接开启引用来源 referer 即可，但是由于和微博图床的图片防盗链冲突，不能同时开启。也就是说除了微博图床的防盗链要关闭 referer，其它的链接仍旧正常开启，看看能不能想办法只把微博图床的链接关闭 referer。标记 a 可以增加 ref=”noreferrer” 属性，但是在 Hexo 中我无法找到合适的方式来完成这个操作。本来准备在 _macro/post.swig 中对渲染后的标记属性进行替换，示例 swig 语句：1&#123;&#123; post.content|replace (&apos;group&apos;, &apos;noreferrer&apos;, &apos;g&apos;) &#125;&#125;把 ref=”group” 替换为 ref=”noreferrer”，但是测试后发现行不通，传递过来的 content 只包含 p 标记，并没有 a 标记，也就是说明 a 标记是在其它地方渲染的。而如果直接在渲染标记 a 的地方进行选择性替换，发现微博图床的图片链接，就把 ref 属性替换掉，需要去更改 Hexo 的源代码，其中有一个 markdown (str) 方法，显然这种临时方案不合理，也很麻烦。为了稳定地解决这个问题，我还是决定更换图床，然后使用第三方工具进行图片迁移。更换图床 和以前一样，挑选了一圈，也是很纠结，最终还是下定决心直接使用 GitHub 了，稳定又方便。其实就是新建一个仓库，专门用来存放图片，只不过需要考虑一下图片过多、图片过大会不会被 GitHub 限制。去 GitHub 搜索帮助文档，帮助文档信息 ，可以得知仓库最大为 100GB，但是官方建议保持在 1GB 以下，单个文件低于 100MB，因此用来存放文件绰绰有余。另外需要注意，仓库文件超过 1GB 时会收到 GitHub 的提醒邮件，超过 75GB 时，每次在提交时都会收到警告。 原文描述如下：We recommend repositories be kept under 1GB each. Repositories have a hard limit of 100GB. If you reach 75GB you’ll receive a warning from Git in your terminal when you push. This limit is easy to stay within if large files are kept out of the repository. If your repository exceeds 1GB, you might receive a polite email from GitHub Support requesting that you reduce the size of the repository to bring it back down.In addition, we place a strict limit of files exceeding 100 MB in size.既然有这种限制，最好还是把图片压缩一下，推荐使用图片压缩工具：Imagine ，这个工具可以实时看到压缩效果，而且压缩率还不错，能到 50%。但是，如果想要保持图片的色彩度、还原度，压缩效果肯定是不行的，甚至有时候压缩后的图片比压缩前的还大。压缩图片示例 迁移图片 迁移图片本来是个很麻烦的事情，要把图片迁移、博客文章里面的链接替换掉，但是还好有现成的工具可以使用，在这里推荐：PicGo ，这个工具本来不是做图片迁移的，仅仅是图片上传生成链接而已，但是有人开发了插件，专门用来迁移 markdown 文件里面的图片，会自动迁移图片并且更新 markdown 里面的图片链接。这个插件是：picgo-plugin-pic-migrater ，而且，还可以支持批量迁移，指定一个文件夹，直接迁移文件夹里面的所有 markdown 文件。迁移过程 详细的迁移步骤就不再记录，几个重要的步骤：在 GitHub 建立仓库、使用 PicGo 工具迁移图片，重新整理 markdown 文件。操作前切记备份好自己的 markdown 文件，以免迁移出现问题导致文件丢失。在使用 PicGo 的过程中，发现总是迁移失败，重试了多次之后确定是因为在 markdown 语法中增加了注释，相当于给图片链接增加了 alt 属性【生成时图片会有一个 img 标记】，导致 PicGo 的插件识别不了，迁移失败。我已经在 GitHub 的项目中提了 issue：https://github.com/PicGo/picgo-plugin-pic-migrater/issues/1 ，作者也回复了，后续会修复。而我比较着急，等不了，又不可能把这些注释全部清除，也不好，所以我决定自己迁移，通过 Java 写代码解决。写代码也比较简单，主要有四个步骤：读取 markdown 文件内容并利用正则抽取微博图床的图片链接、下载所有图片并上传至 GitHub、替换内容中抽取出的所有图片链接为 GitHub 的图片链接、内容写回新文件。使用 Java 处理不需要多少代码，主要要依赖几个 jar 包：处理文件的 io 包、处理网络请求的 httpclient 包、处理 git 的 jgit 包。详细内容可以参考我的另外一篇博客：使用 Java 代码迁移微博图床到 GitHub 图床 。 小细节 针对 PicGo 的使用还有一些小细节可以注意一下：自定义域名、子文件夹路径、图片压缩【不压缩针对 GitHub 速度会很慢，能压缩到 200KB 最好】、文件重命名。未来考虑 迁移完成之后，以后新的图片就直接使用 PicGo 上传到 GitHub 图床了，同时需要注意区分子文件夹。在 GitHub 仓库中，暂时每年新建一个文件夹，以年份数字为名称。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>https</tag>
        <tag>weibo</tag>
        <tag>referer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mapreduce 错误之 bin bash-line 0-fg-no job control]]></title>
    <url>%2F2019042401.html</url>
    <content type="text"><![CDATA[今天在开发 mapreduce 程序的过程中，为了快速开发，程序的整体框架是从别的业务复制过来的，自己增加一些数据处理逻辑以及环境的参数配置。接着就遇到问题，在本地本机测试的时候，Job 作业无法启动，总是抛出异常，然后进程退出。本机系统为 Windows 7 X64。异常错误信息简略如下：12Exit code: 1Exception message: /bin/bash: line 0: fg: no job control本文记录这个现象以及解决方案。问题出现 在本地本机启动 Job 时无法正常运行作业，直接抛出异常后退出进程，完整错误信息如下：12345678910111213141516171819202122Diagnostics: Exception from container-launch.Container id: container_e18_1550055564059_0152_02_000001Exit code: 1Exception message: /bin/bash: line 0: fg: no job controlStack trace: ExitCodeException exitCode=1: /bin/bash: line 0: fg: no job control at org.apache.hadoop.util.Shell.runCommand (Shell.java:576) at org.apache.hadoop.util.Shell.run (Shell.java:487) at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute (Shell.java:753) at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer (DefaultContainerExecutor.java:212) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call (ContainerLaunch.java:303) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call (ContainerLaunch.java:82) at java.util.concurrent.FutureTask.run (FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624) at java.lang.Thread.run (Thread.java:748)Container exited with a non-zero exit code 1Failing this attempt. Failing the application.2019-04-22_22:46:04 [main] INFO mapreduce.Job:1385: Counters: 0其中的重点在于：Exception message: /bin/bash: line 0: fg: no job control，由于我不了解这种错误，只能靠搜索引擎解决了。问题解决 问题解决很容易，在 Job 的配置中增加一项：mapreduce.app-submission.cross-platform，取值为 true，截取代码片段如下：1234Configuration conf = job.getConfiguration ();conf.set (&quot;mapreduce.job.running.map.limit&quot;, &quot;50&quot;);// 本机环境测试加上配置，否则会抛出异常退出：ExitCodeException: /bin/bash: line 0: fg: no job controlconf.set (&quot;mapreduce.app-submission.cross-platform&quot;, &quot;true&quot;);这个配置的含义就是跨平台，保障 Job 作业可以在 Windows 平台顺利运行。备注 参考：stackoverflow 讨论一例 。]]></content>
      <categories>
        <category>踩坑系列</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于 httpcore 的 Maven 依赖冲突问题解决]]></title>
    <url>%2F2019042201.html</url>
    <content type="text"><![CDATA[今天，又遇到一个 Maven 冲突的问题，这种问题我遇到的多了，每次都是因为项目依赖管理混乱或者为新功能增加依赖之后影响了旧功能，这次就是因为后者，新增加的依赖的传递依赖覆盖了原有的依赖，导致了问题的产生。大家如果搜索我的博客，搜索关键词 maven 或者 mvn，应该可以看到好几篇类似的文章，每次的情况都略有不同，每次解决问题的过程也是很崩溃。不过，每次崩溃之后都是一阵喜悦，毕竟感觉自己的经验又扩充了一些，以后遇到此类问题可以迅速解决。问题出现 写了一个 mapReduce 程序从 HBase 读取数据，写入到 Elasticsearch 中，整体的框架是从别的项目复制过来的，自己重写了处理逻辑以及环境相关的参数，但是跑起来的时候，map 过程很顺利，几百个 task 全部成功完成，但是 reduce 过程直接挂了，几十个 task 全部失败，重试了还是失败。我只能去查看日志，去 Hadoop 监控界面，看到对应任务的报错日志如下：123456789101112131415161718192021222324252627282930313233343536373839402019-04-22 16:01:30,469 ERROR [main] com.datastory.banyan.spark.ScanFlushESMRV2$FlushESReducer: org/apache/http/message/TokenParserjava.lang.NoClassDefFoundError: org/apache/http/message/TokenParser at org.apache.http.client.utils.URLEncodedUtils.parse (URLEncodedUtils.java:280) at org.apache.http.client.utils.URLEncodedUtils.parse (URLEncodedUtils.java:237) at org.apache.http.client.utils.URIBuilder.parseQuery (URIBuilder.java:111) at org.apache.http.client.utils.URIBuilder.digestURI (URIBuilder.java:181) at org.apache.http.client.utils.URIBuilder.&lt;init&gt;(URIBuilder.java:91) at org.apache.http.client.utils.URIUtils.rewriteURI (URIUtils.java:185) at org.apache.http.impl.nio.client.MainClientExec.rewriteRequestURI (MainClientExec.java:494) at org.apache.http.impl.nio.client.MainClientExec.prepareRequest (MainClientExec.java:529) at org.apache.http.impl.nio.client.MainClientExec.prepare (MainClientExec.java:156) at org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.start (DefaultClientExchangeHandlerImpl.java:125) at org.apache.http.impl.nio.client.InternalHttpAsyncClient.execute (InternalHttpAsyncClient.java:129) at org.elasticsearch.client.RestClient.performRequestAsync (RestClient.java:343) at org.elasticsearch.client.RestClient.performRequestAsync (RestClient.java:325) at org.elasticsearch.client.RestClient.performRequestAsync (RestClient.java:268) at org.elasticsearch.client.RestHighLevelClient.performRequestAsync (RestHighLevelClient.java:445) at org.elasticsearch.client.RestHighLevelClient.performRequestAsyncAndParseEntity (RestHighLevelClient.java:423) at org.elasticsearch.client.RestHighLevelClient.bulkAsync (RestHighLevelClient.java:206) at com.datastory.banyan.client.es.ESBulkProcessor.lambda$new$0 (ESBulkProcessor.java:154) at org.elasticsearch.action.bulk.Retry$RetryHandler.execute (Retry.java:230) at org.elasticsearch.action.bulk.Retry.withAsyncBackoff (Retry.java:87) at org.elasticsearch.action.bulk.BulkRequestHandler$AsyncBulkRequestHandler.execute (BulkRequestHandler.java:138) at org.elasticsearch.action.bulk.BulkProcessor.execute (BulkProcessor.java:350) at org.elasticsearch.action.bulk.BulkProcessor.executeIfNeeded (BulkProcessor.java:341) at org.elasticsearch.action.bulk.BulkProcessor.internalAdd (BulkProcessor.java:276) at org.elasticsearch.action.bulk.BulkProcessor.add (BulkProcessor.java:259) at org.elasticsearch.action.bulk.BulkProcessor.add (BulkProcessor.java:255) at org.elasticsearch.action.bulk.BulkProcessor.add (BulkProcessor.java:241) at com.datastory.banyan.client.es.ESBulkProcessor.addIndexRequest (ESBulkProcessor.java:237) at com.datastory.banyan.spark.ScanFlushESMRV2$FlushESReducer.reduce (ScanFlushESMRV2.java:212) at com.datastory.banyan.spark.ScanFlushESMRV2$FlushESReducer.reduce (ScanFlushESMRV2.java:158) at org.apache.hadoop.mapreduce.Reducer.run (Reducer.java:171) at org.apache.hadoop.mapred.ReduceTask.runNewReducer (ReduceTask.java:627) at org.apache.hadoop.mapred.ReduceTask.run (ReduceTask.java:389) at org.apache.hadoop.mapred.YarnChild$2.run (YarnChild.java:168) at java.security.AccessController.doPrivileged (Native Method) at javax.security.auth.Subject.doAs (Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs (UserGroupInformation.java:1709) at org.apache.hadoop.mapred.YarnChild.main (YarnChild.java:162)截图如下：看到关键部分：java.lang.NoClassDefFoundError: org/apache/http/message/TokenParser，表面看是类未定义，但是真实情况是什么还要继续探索，例如依赖缺失、依赖冲突导致的类不匹配等。问题解决 初步分析 先搜索类 TokenParser 吧，看看能不能搜索到，在 IDEA 中搜索，我的环境是使用 ctrl + shift + t 快捷键，搜索之后发现存在这个类，记住对应的 jar 包坐标以及版本：1org.apache.httpcomponents:httpcore:jar:4.3.2这里需要注意一点，如果你的项目是由多个子项目聚合而成的，此时使用 IDEA 的搜索功能并不准确，会搜索出来其它子项目的同名依赖，从而误导你的视线，所以还是使用依赖分析插件比较好，例如：depedency，下面也会讲到。既然类已经存在，说明有极大可能是依赖冲突导致的 NoClassDefFoundError。继续从错误日志中寻找蛛丝马迹，看到 at org.apache.http.client.utils.URLEncodedUtils.parse (URLEncodedUtils.java:280) 这里，接着搜索类 URLEncodedUtils 并查看第 280 行的 parse 方法。1org.apache.httpcomponents:httpclient:jar:4.5.2上面是依赖坐标以及版本，看到这里有经验的工程师已经可以发现问题所在了：两个同类型的依赖 jar 包版本差别太大，这里暂且不分析。接着查看源码：好，到这里已经把基本情况分析清楚了，程序异常里面的 NoClassDefFoundError 并不是类缺失，所以没有报错 ClassNotFound。根本原因是类版本不对，导致 URLEncodedUtils 找不到自己需要的特定版本的类，尽管有一个同名的低版本的类存在，但是对于 Java 虚拟机来说这是完全不同的两个类，这也是容易误导人的地方。再延伸一下话题，如果真的是类不存在，使用 IDEA 查看源码时会显示红色字体提示的，如图：详细分析 接下来就使用依赖分析插件 dependency 来分析这两个 jar 包的来源以及版本差异，在项目的根目录执行 mvn dependency:tree -Dverbose &gt; tree.txt ，把依赖树信息重定向到 tree.txt 文件中，里面的 -Dverbose 参数可以使我们更为清晰地看到版本冲突的 jar 包以及实际使用的 jar 包。找到 httpclient 和 httpcore 的来源，依赖树片段截取如下：12345678910[INFO] +- com.company.commons3:ds-commons3-es-rest:jar:1.2:compile[INFO] | +- org.apache.httpcomponents:httpclient:jar:4.5.2:compile...... 省略 [INFO] | +- org.apache.httpcomponents:httpasyncclient:jar:4.0.2:compile[INFO] | | +- org.apache.httpcomponents:httpcore:jar:4.3.2:compile[INFO] | | +- (org.apache.httpcomponents:httpcore-nio:jar:4.3.2:compile - omitted for duplicate)[INFO] | | +- (org.apache.httpcomponents:httpclient:jar:4.3.5:compile - omitted for conflict with 4.5.2)[INFO] | | \- (commons-logging:commons-logging:jar:1.1.3:compile - omitted for duplicate)可以看到 httpclient 来自于 ds-commons3-es-rest，版本为 4.5.2，而 httpcore 来自于 httpasyncclient，版本为 4.3.2。特别注意：httpasyncclient 里面还有一个 4.3.5 版本的 httpclient 由于版本冲突被忽略了，这也是导致问题的元凶。依赖树片段截图如下：到这里已经可以知道问题所在了，httpclient、httpcore 这两个依赖的版本差距太大，前者 4.5.2，后者 4.3.2，导致前者的类 URLEncodedUtils 在调用后者的类 TokenParser 时，找不到满足条件的版本，于是抛出异常：NoClassDefFoundError。解决方案 那这个问题也是很容易解决的，指定版本接近的两个依赖即可，但是还是要根据实际情况而来。本来最简单的方案就是移除所有相关依赖，然后在 pom.xml 中显式地指定这两个依赖的版本。但是这么做太简单粗暴了，因为这两个依赖不是一级依赖，而是传递依赖，不必手动管理。所以要适当地移除某一些传递依赖，保留另一些传递依赖，让它们不要交叉出现。我的做法就是移除 ds-commons3-es-rest 里面的传递依赖，保持 httpasyncclient 里面的传递依赖，这样它们的版本号接近，而且是同一个依赖里面传递的，基本不可能出错。pom.xml 配置如图：httpclient 的小版本号是可以比 httpcore 高一点的，继续查看依赖树，可以看到 httpclient 的版本为 4.3.5，httpcore 的版本为 4.3.2。引申插件 除了 dependency 插件外，还有另外一个插件也非常好用：enforcer，插件的坐标如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;!-- 帮助分析依赖冲突的插件，可以在编译时期找到依赖问题 --&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-enforcer-plugin&lt;/artifactId&gt; &lt;version&gt;1.4.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;enforce-ban-duplicate-classes&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;enforce&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;!-- 设置规则，否则没法检查 --&gt; &lt;rules&gt; &lt;!-- 检查重复类 --&gt; &lt;banDuplicateClasses&gt; &lt;!-- 忽略一些类 --&gt; &lt;ignoreClasses&gt; &lt;ignoreClass&gt;javax.*&lt;/ignoreClass&gt; &lt;ignoreClass&gt;org.junit.*&lt;/ignoreClass&gt; &lt;ingoreClass&gt;org.aspectj.*&lt;/ingoreClass&gt; &lt;ingoreClass&gt;org.jboss.netty.*&lt;/ingoreClass&gt; &lt;ingoreClass&gt;org.apache.juli.*&lt;/ingoreClass&gt; &lt;ingoreClass&gt;org.apache.commons.logging.*&lt;/ingoreClass&gt; &lt;ingoreClass&gt;org.apache.log4j.*&lt;/ingoreClass&gt; &lt;ingoreClass&gt;org.objectweb.asm.*&lt;/ingoreClass&gt; &lt;ingoreClass&gt;org.parboiled.*&lt;/ingoreClass&gt; &lt;ingoreClass&gt;org.apache.xmlbeans.xml.stream.*&lt;/ingoreClass&gt; &lt;ingoreClass&gt;org.json.JSONString&lt;/ingoreClass&gt; &lt;/ignoreClasses&gt; &lt;!-- 除了上面忽略的类，检查所有的类 --&gt; &lt;findAllDuplicates&gt;true&lt;/findAllDuplicates&gt; &lt;/banDuplicateClasses&gt; &lt;!-- JDK 在 1.8 以上 --&gt; &lt;requireJavaVersion&gt; &lt;version&gt;1.8.0&lt;/version&gt; &lt;/requireJavaVersion&gt; &lt;!-- Maven 在 3.0.5 以上 --&gt; &lt;requireMavenVersion&gt; &lt;version&gt;3.0.5&lt;/version&gt; &lt;/requireMavenVersion&gt; &lt;/rules&gt; &lt;fail&gt;true&lt;/fail&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;!-- 官方的默认规则 --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;extra-enforcer-rules&lt;/artifactId&gt; &lt;version&gt;1.0-beta-6&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/plugin&gt;这个插件需要配置在 pom.xml 中，并且绑定 Maven 的生命周期，默认是绑定在 compile 上面，然后需要给 enforcer 配置一些规则，例如检查重复的类。接着在编译期间，enforcer 插件就会检验项目的依赖中所有的类【可以设置忽略容器中的类，例如作用域为 provided 的依赖包】，如果有重复的类，就会报错，编译不会通过。注意，这个插件除了可以检查依赖、类的冲突【通过设置规则 rule 来实现】，还可以设置一些其它的开发规范，例如规定 JDK 版本、开发系统环境必须为 Windows、使用的 Maven 版本等等。此外，官方也提供了一些规则列表可以参考：http://maven.apache.org/enforcer/enforcer-rules/index.html ，而且还有 API 允许我们自定义规则，非常灵活。问题总结 抽象总结 总结一下现象，其实就是项目本来依赖了 B 包，B 包里面有传递依赖包 1、包 2，由于包 1、包 2 都来自于 B 包，所以版本差别不大，很适配。包 1 的类调用包 2 的类很顺利，不会有问题。后来由于其它功能需要，项目又加入了 A 包，此时没有注意到 A 包里面也有包 1，而且比 B 包里面的包 1 版本高，这本来不是问题，只是潜在风险。但是，编译打包时 A 包里面的包 1 把 B 包里面的包 1 覆盖了，包 2 仍旧是来自于 B 包，这就出问题了，风险变成灾难了。当程序运行时包 1 需要调用包 2，由于版本差别过大，找不到符合条件的类了，抛出异常：NoClassDefFoundError。这里面的验证机制浅显地描述就是每个类都会有自己的序列化编号，如果有严格要求同版本依赖的类，调用方法时会严格验证。关于编译的疑问 到这里，读者会有疑问，为什么编译不报错，能顺利通过呢？其实从上面就能看到答案了，这种依赖包之间相互引用的类，类是存在的，只是版本不一致而已，编译时并不能检测出来。如果是你自己写的类源码，引用了别的依赖包的类，同时对版本要求严格的话，编译是一定会报错的。但是，如果你提前知道了是哪个类，一般不可能知道，只有报错了才会知道，而且会有不止一个类，这也是令人头疼的地方。如果进一步分析异常信息，发现它归属于 ERROR，并不是运行时异常，更不用谈编译时异常了，这种错误和 OutOfMemoryError 类似，是虚拟机运行时出现问题，比较严重。感悟 找到这种问题的原因是没有什么难度的，一眼就可以看出来是依赖冲突。但是解决过程可谓是难度极大，而且可以让人崩溃，对于初学者来说可以放弃了，折腾三天可能都不会有结果的。特别在依赖庞大的情况下，几百个依赖包，几百 M 大小，这时候找起来特别麻烦，有时候改动了一点会影响到其它的依赖，引起连锁反应，可能问题还没解决，又引发了其它问题。所以，在项目开发的初始阶段，一定要管理好项目的依赖，并且在依赖变更时要一起讨论，否则后患无穷。此外，在解决依赖冲突的过程中，有 2 个插件工具很好用：dependency、enforcer。]]></content>
      <categories>
        <category>踩坑系列</category>
      </categories>
      <tags>
        <tag>httpcore</tag>
        <tag>maven</tag>
        <tag>dependency</tag>
        <tag>enforcer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 之 kill 命令入门实践]]></title>
    <url>%2F2019042101.html</url>
    <content type="text"><![CDATA[最近在实际应用场景中，需要使用 Linux 系统的 kill 命令来控制程序的生命周期，例如 ctrl + c、ctrl + z、kill -9 pid 等，而这些命令在日常的工作当中也是非常常见的并且很好用。为了多了解一些 Linux 中信号常量的知识点，以及 kill 命令的基本原理，我整理了这一篇博客。基础知识 信号 根据 kill 的实际使用来初步了解一下信号的概念。首先要清楚一个基本知识点：kill 命令只是用来向进程发送信号的，而不是直接杀死进程的，实际操控进程生命的仍旧是系统内核以及信号常量的规范动作【进程本身注册的信号动作：默认、忽略、捕捉自定义】。kill 命令使用户能够向进程发送信号，信号是发送给进程以中断进程并使其作出反应的信息。如果进程被设计为对发送给它的该类型信号作出反应，则它将作出反应；否则，该进程将终止。对于进程对信号做出正常反应的情况，例如对一个进程发送编号为 9 的信号，则该进程会终止。而对一个进程发送编号为 19 的信号【SIGSTOP】，则该进程会退到后台暂停，接着使用编号为 18 的信号【SIGCONT】可以激活进程继续运行【也可以直接使用 fg /bg 这一对命令】。对于进程不能对信号做出反应而终止的情况，例如对一个进程发送编号为 10 的信号【SIGUSR1】，这个信号本来是给用户自定义的，而普通的进程没有被设计为对这个信号做出反应，因此进程将终止运行【另一方面，在 PHP 中，后台进程会对这个信号做出反应，是因为官方发布的程序实现了这个信号的指令，并为进程注册了这个信号】。对于 Linux 来说，实际上信号是软中断，许多重要的程序都需要处理信号。信号，为 Linux 提供了一种处理异步事件的方法。每个信号都有一个名字和编号，这些名字都以 SIG 开头，例如 SIGINT、SIGKILL 等等。信号定义在 signal.h【/usr/include/asm/signal.h】头文件中，信号编号都定义为正整数，从 1 开始。当然，也有编号为 0 的信号，但是它对于 kill 有特殊的应用。使用 kill -l 可以查看所有的信号常量列表，其中，前面 32 个是基本的，后面 32 个是扩展的【做底层驱动开发时能用到】。常用信号常量 以下列举一些常用的信号常量以及解释说明：信号编号 信号名称 信号解释 1SIGHUP 挂起信号【hang up】，终端断线，经常在退出系统前使用，会终止进程。但是，一般启动程序时为了让程序继续运行，会指定 nohup 就是为了不让程序接收挂起信号而终止，这样在退出系统时程序仍旧能正常运行 2SIGINT 中断【与键盘快捷键 ctrl + c 对应】，表示与键盘中断 3SIGQUIT 退出【与键盘快捷键 ctrl + \ 对应】9SIGKILL强制终止，程序必须终止【无需清除】，只有进程属主或者超级用户发出该命令时才起作用 15SIGTERM 停止，要求进程自己退出【需要先清除】，所以可能停止失败，只有进程属主或者超级用户发出该命令时才起作用 10SIGUSR1 用户自定义信号 111SIGSEGV段错误信号，在操作内存、硬盘资源出错时会出现，例如硬盘空间不足、内存读取无权限时 12SIGUSR2 用户自定义信号 218SIGCONT继续【与命令 fg/bg 对应，搭配 jobs 一起使用】19SIGSTOP暂停【与键盘快捷键 ctrl + z 对应】，可以使用信号 18 来继续运行，或者使用 fg/bg 来调度到前 / 后台继续运行【搭配 jobs 一起使用】也可以在 Linux 机器上面使用 man 7 signal 可以查看帮助文档，有更为详细的解释说明。在所有的信号中，只有编号为 9 的信号【SIGKILL】才可以 无条件终止 进程，编号为 15 的信号【SIGTERM】也可以 停止 进程，但是可能终止失败。对于编号为 9 的信号【SIGKILL】和编号为 19 的信号【SIGSTOP】，进程无法选择忽略，必须做出反应，而对于其它的信号，进程都有权利选择忽略。信号处理动作详解 对于信号的处理有三种方式：忽略、捕捉、默认。忽略信号，大多数信号可以使用这个方式来处理，但是有两种信号不能被忽略：9 号【SIGKILL】、19 号【SIGSTOP】。因为这两个信号向内核和超级用户提供了 终止 和 停止 的 可靠 方法，如果被忽略了，那么这个进程就变成了没人能管理的的进程，显然这是内核设计者不希望看到的场景。捕捉信号，需要告诉内核，程序希望如何处理某一种信号，其实就是写一个信号处理函数，里面写上自定义的处理逻辑，然后将这个函数告诉内核【注册函数】。当该信号产生时，由内核来调用用户的自定义函数，以此来实现某种信号的自定义处理。说到底，就是进程捕捉信号，自定义处理，不使用内核默认的处理方式。系统默认动作，对于每个信号来说，系统都对应有默认的处理动作。当发生了该信号，系统会自动执行。不过，对系统来说，大部分的处理方式都比较粗暴，就是直接杀死该进程。信号的实际使用 以上把信号的基本概念了解清楚了，但是在实际中程序是怎么使用的呢？为了配合使用，必须有两方面程序：一是信号发送方【即负责发送信号的工具，例如 kill 就可以】，另一方是接收方【即能接收信号并且做出反应的程序，基本所有运行在 Linux 上的程序都可以】。接下来就以 c 语言编程，写两个例子，模拟发送方【封装 kill】、接收方【信号处理函数注册】，来观察一下信号的实际应用。信号处理函数注册 信号处理函数的注册，使用入门版的接口，signal 函数原型如下：123#include &lt;signal.h&gt;typedef void (*sighandler_t)(int);sighandler_t signal (int signum, sighandler_t handler);根据函数原型可以看出，由两部分组成，一个是真正处理信号的函数，另一个是注册函数。对于 sighandler_t signal (int signum, sighandler_t handler) 函数来说，signum 显然是信号的编号，handler 是处理函数的指针。同样地，在 typedef void (*sighandler_t)(int) 这个处理函数的原型中，有一个参数是 int 类型，显然也是信号的编号，在实现函数时要根据信号的编号进行不同的操作。只需要实现真正的处理信号的方法即可，以下是示例，信号处理只是打印，方便观察：123456789101112131415161718192021222324252627#include&lt;signal.h&gt;#include&lt;stdio.h&gt;#include &lt;unistd.h&gt;void handler(int signum) &#123; // 处理函数只把接收到的信号编号打印出来 if(signum == SIGIO) printf("SIGIO signal: % d\n", signum); else if(signum == SIGUSR1) printf("SIGUSR1 signal: % d\n", signum); else printf("error\n");&#125;int main(void) &#123; // 忽略 SIGINT, 默认处理 SIGTERM, 其它信号不注册都会导致程序退出 signal (SIGIO, handler); signal (SIGUSR1, handler); signal (SIGINT, SIG_IGN); signal (SIGTERM, SIG_DFL); printf("SIGIO=% d,SIGUSR1=% d,SIGINT=% d,SIGTERM=% d\n", SIGIO, SIGUSR1, SIGINT, SIGTERM); // 以下是无限循环 for(;;)&#123; sleep (10000); &#125;return 0;&#125;使用 gcc 编译器编译【如果 Linux 环境不带需要自行安装】：gcc -o signal_test signal_test.c ，然后就可以执行了：./signal_test 。接着使用 ctrl + c 快捷键【被进程忽略】，使用 kill 命令发送 29 号信号【被接收并打印出来编号】、10 号信号【被接收并打印出来编号】、2 号【被接收并忽略】、15 号【被接收并按照系统默认动作停止进程】，具体看下面的两张图片。使用 kill 命令发送信号 进程接收信号的处理方式 接着演示 kill 发送一个程序没有注册的信号 12 号【SIGUSR2】，可以观察到程序直接退出。kill 发送 12 号信号 进程直接退出 信号发送工具模拟 信号发送工具比较简易，其实就是模拟封装 kill，观察效果，先看一下 kill 函数的原型：123#include &lt;sys/types.h&gt;#include &lt;signal.h&gt;int kill(pid_t pid, int sig);可以看到函数原型很简单，有两个参数，pid 是信号接受者的 pid，sig 是信号编号，接着就实现一个简单的脚本，里面直接调用 kill 函数，内容如下：1234567891011121314151617181920212223#include &lt;sys/types.h&gt;#include &lt;signal.h&gt;#include&lt;stdio.h&gt;#include &lt;unistd.h&gt;int main(int argc, char** argv) &#123; // 接收的参数个数不足 if(3 != argc)&#123; printf("[Arguments ERROR!]\n"); printf("\tUsage:\n"); printf("\t\t% s &lt;Target_PID&gt; &lt;Signal_Number&gt;\n", argv [0]); return -1; &#125; int pid = atoi (argv [1]); int sig = atoi (argv [2]); // 这里增加一个对编号判断的逻辑 if(pid &gt; 0 &amp;&amp; sig &gt; 0)&#123; kill (pid, sig); &#125;else&#123; printf("Target_PID or Signal_Number MUST bigger than 0!\n"); &#125; return 0;&#125;在此特殊说明一下，关于 pid 的取值范围，上述代码示例把 pid 限制在正整数，防止出错。其实 pid 的取值范围很广，各有特殊含义，请参考文末的备注。使用 gcc 编译后【gcc -o signal_kill signal_kill.c】直接运行，观察能否把信号正常发送给运行的进程。运行脚本发送信号 运行的进程可以正常接收到信号 经过观察，是可以的，至此信号函数的使用演示完成。操作实践 详细认识信号的基本知识后，接下来进行实践会更加知其所以然，那就回归到正题，我来使用 kill 命令进行实践操作一下，演示一下常用的信号以及处理效果。在日常工作中，一般会使用信号 1、信号 3、信号 3、信号 9、信号 15，这五个比较常用，就不再演示，只是需要留意一下它们对应的键盘快捷键，信号 2 是 ctrl + c，信号 3 是 ctrl + \ 。我想重点演示一下信号 18、信号 19 以及 bg、fg、jobs 命令。演示 开启三个进程，分别使用 ctrl + z 命令暂停它们的运行，在暂停时输出的日志中会有 Stopped 标记，并且会有进程的编号分配，在方括号中的就是【有时候暂停时还会有 核心已转储 、core dumped 的提示】。使用 jobs 命令查看暂停的进程，此时每个进程会有编号，此时的三个进程分别是 2、3、4。使用 kill 发送 18 号信号给编号为 4 的暂停进程，然后再次使用 jobs 命令查看，发现这个进程的状态已经由 Stopped 变为了 Running，说明这个进程继续运行了【但是是后台运行，没有占用终端】。发送 18 号信号 编号为 4 的进程后台运行中 接着使用 fg、bg 命令把编号为 4 的进程调到前台运行、返回后台运行。此时可以发现，fg、bg 命令和信号 18 的作用是等价的，而且更为丰富，可以把进程在前台【占用终端】、后台【不占用终端】之间调换。总结 总结一下：对于正在运行的进程，并且等待终端的输入，此时如果使用 ctrl + c 就会导致进程退出，所以可以使用 ctrl + z 让进程暂停，并退到后台等待，此时终端被释放，可以继续输入命令。接着可以使用 jobs 命令查看有哪些被暂停的进程【此时进程会有编号，从 1 开始】，可以使用 bg num 命令让第 num 个进程在后台运行，可以使用 fg num 让第 num 个进程在前台运行【继续占用终端】。当然，如果使用 bg、fg 时不加序号参数，则默认对最后一个进程操作。备注 段错误 在某一次的实际场景中，想从本地上传文件到远程服务器，具体的操作是登录远程服务器后，在终端中使用 lrz 命令【环境为 CentOS 系统，需要自行安装这个工具】，然后在弹出的文件浏览器中选择本地的文件。在上传的过程中，刚刚开始没多久就报错： 段错误 (core dumped)【如果使用英文表示，为：Segmentation fault，后面括号里面的 core dumped 是核心已转储，在进程退出或者暂停时会出现】，紧接着上传进度中断，上传进程停止。然后检查发现服务器上传文件指定目录的硬盘空间已经没有了，使用 df -h 命令查看，磁盘使用率 100%，所以无法再继续上传文件。上面的错误： 段错误 (core dumped)，我猜测可能是和信号 SIGSEGV 有关，下面就以 c 语言为基础写一个简单的例子，在代码中特意非法操作内存，让内核主动发送 SIGSEGV 信号给进程。代码示例如下，已经写好注释：123456789101112#include &lt;stdio.h&gt;int main()&#123; char *str = "hello"; // 非法赋值，想改变字符串内存地址的字符串值，不被允许 *str = 'h'; printf("% s\n", str); // 新定义字符串就可以 char *str2 = "world"; printf("% s\n", str2); return 0;&#125;使用 gcc 编译：gcc -o seg_error seg_error.c，然后运行：./seg_error，就可以发现报错：Segmentation fault。Segmentation fault 报错截图 如果不确定是哪几行代码出了问题，可以简单调试一下，重新编译时加上 -g 参数，再使用 gdb 调试器工具：gdb seg_error，开启调试模式，然后输入 r 运行，接着就可以看到具体的报错信息以及报错位置。从下图中可以看到，程序在运行中接收到 SIGSEGV 信号而退出，并抛出 Segmentation fault 错误信息，异常代码在第 6 行：*str = ‘h’;，这 1 行代码在非法操作内存【字符串是不可改变的量，被分配在内存区域的数据段，当向该只读数据区域进行写操作即为非法】，操作系统内核【kernel】会通过 kill 命令向进程发送编号为 11 的信号，即 SIGSEGV【段错误】信号，进程被内核终止。除了内核在检测到非法操作时发送这个信号给进程，如果我手动发送这个信号给进程会发生什么呢，不妨试一下。随便起动一个进程【我使用 tail -f seg_error.c 查看文件内容】，然后使用 kill 命令发送 SIGSEGV 信号给这个进程。可以从上图中看到，进程由于接收到 SIGSEGV 信号而退出。进程号取值 在使用 kill 命令时，pid 参数就是系统给进程分配的编号，但是这个参数除了正常的正整数之外，其它的取值有各自特殊的含义。pid 大于 0，将信号发送给进程 id 为 pid 的进程 pid 等于 0，将信号发送给与发送进程属于同一进程组的所有进程【即进程组 id 相等的进程】pid 等于 - 1，将该信号发送给系统内所有的进程【前提是有发送信号权限的，并且不包括系统进程集中的进程】pid 小于 - 1，将该信号发送给其进程组 id 等于 pid 绝对值的所有进程【针对进程组】 可靠信号与不可靠信号 以上内容在讨论信号的知识点与实际演示时，都没有考虑到信号的可靠性问题，默认都是能送达的。但是，信号是区分可靠信号、不可靠信号的。不可靠信号，信号可能会丢失，而一旦信号丢失【多次信号不排队】，进程是无法接收这个信号的。Linux 的信号机制基本上是从 Unix 系统中继承过来的，早期 Unix 系统中的信号机制比较简单和原始，后来在实践中逐渐暴露出一些问题。因此，把那些建立在早期 Unix 信号机制上的信号叫做 不可靠信号 ，信号值小于 SIGRTMIN【不同系统会有微小的差别，例如在 CentOS 中是 34】的信号都是不可靠信号。可靠信号，也称为阻塞信号，当发送了一个阻塞信号，并且该信号的动作是系统默认动作或捕捉该信号，则信号从发出以后会一直保持未决的状态，直到该进程对此信号解除了阻塞，或将对此信号的动作更改为忽略。随着时间的发展，实践证明了有必要对信号的原始机制加以改进和扩充。所以，后来出现的各种 Unix 版本分别在这方面进行了研究，力图实现 可靠信号 。由于原来定义的信号已有许多应用，不好再做改动，最终只好又新增加了一些信号，并在一开始就把它们定义为可靠信号，这些信号支持排队，不会丢失，信号值的范围在 SIGRTMIN 和 SIGRTMAX 之间。同时，信号的发送和安装也出现了新版本：信号发送函数 sigqueue () 以及信号的安装函数 sigaction () 。]]></content>
      <categories>
        <category>Linux 命令系列</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>kill</tag>
        <tag>jobs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JNI 字段描述符基础知识]]></title>
    <url>%2F2019041301.html</url>
    <content type="text"><![CDATA[平时在做 Java 开发的时候，难免遇到异常信息中包含一种特殊的表达字符串，例如：1method: createWorker signature: (Ljava/util/concurrent/Executor;) Lorg/jboss/netty/channel/socket/nio/AbstractNioWorker;或者 1java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.JavaType.isReferenceType () Z 可以看到，异常信息中有一种特殊的字符串出现了：L 后面跟着类名 、 方法后面跟了一个 Z。其实，这就是 JNI 字段描述符【Java Native Interface FieldDescriptors】，它是一种对 Java 数据类型、数组、方法的编码。此外，在 Android 逆向分析中，通过反汇编得到的 smali 文件，里面的代码也会遵循这种方式，即 Dalvik 字节码。本文就记录一些数据类型、数组、方法的编码方式以及解释说明，方便以后查阅。基本概念 这种编码方式把 Java 中的基本数据类型、数组、对象都使用一种规范来表示：八种基本数据类型都使用一个大写字母表示 void 使用 V 表示 数组使用左方括号表示 方法使用一组圆括号表示，参数在括号里，返回类型在括号右侧 对象使用 L 开头，分号结束，中间是类的完整路径，包名使用正斜杠分隔 基本编码 基本编码如下表格，并配有解释说明：Java 类型 JNI 字段描述符booleanZbyteBcharCshortSintIlongJfloatFdoubleDvoidVObject 以 L 开头，以；结尾，中间是使用 / 隔开的完整包名、类型。例如：Ljava/lang/String;。如果是内部类，添加 $ 符号分隔，例如：Landroid/os/FileUtils$FileStatus;。数组 [ 方法 使用 () 表示，参数在圆括号里，返回类型在圆括号右侧，例如：(II) Z，表示 boolean func (int i,int j)。举例说明 数据类型 1、[I：表示 int 一维数组，即 int []。2、Ljava/lang/String;：表示 String 类型，即 java.lang.String。3、[Ljava/lang/Object;：表示 Object 一维数组，即 java.lang.Object []。4、Z：表示 boolean 类型。5、V：表示 void 类型。 方法1、() V：表示参数列表为空，返回类型为 void 的方法，即 void func ()。2、(II) V：表示参数列表为 int、int，返回类型为 void 的方法，即 void func (int i,int j)。3、(Ljava/lang/String;Ljava/lang/String;) I：表示参数列表为 String、String，返回类型为 int 的方法，即 int func (String i,String j)。4、([B) V：表示参数列表为 byte []，返回类型为 void 的方法，即 void func (byte [] bytes)。5、(ILjava/lang/Class;) J：表示参数列表为 int、Class，返回类型为 long 的方法，即 long func (int i,Class c)。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>JNI</tag>
        <tag>字段描述符</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[预估 Mysql 数据表的数据大小和索引大小]]></title>
    <url>%2F2019041001.html</url>
    <content type="text"><![CDATA[最近接到一个小的新需求，需求很容易实现，就是定时把一些分析得出的指标从 Elasticsearch 中离线存储到 Mysql 数据库中，方便以后查询。离线存储的原因是因为资源不足，Elasticsearch 会自动删除 15 天以前的原始数据，而且 Elasticsearch 每天都会新产生数十万到数百万的数据，依据这些原始数据只会产生几十条分析结果，显然离线存储到 Mysql 中更为合理。在处理这个需求时，接着就遇到了一个小问题，当前业务组没有数据库资源，需要申请，而且由于资源不足，不能随便申请，要给出合理的预估值。这样，就涉及到数据库占用空间大小的预估了，本文记录一种简单的方法。数据大小和索引大小预估 我当前使用的是 Mysql 数据库，其它数据库产品查询方式可能会有所不同，请根据实际情况操作。在数据库中，使用系统数据库的表 TABLES 进行查询：1234SELECT data_length,index_lengthFROM information_schema.TABLES tWHERE table_schema=&apos;your_db_name&apos;AND table_name = &apos;your_table_name&apos;;其中，系统数据库是 information_schema，存储表信息的表是 TABLES，data_length、index_length 这 2 个字段表示数据大小、索引大小，单位是字节 B。当然，如果使用可视化的数据库连接管理工具，也可以通过管理工具直接鼠标点击查看，其实背后的逻辑仍旧是查询 TABLES 表，例如我通过 Navicat 工具查看。可见，无论使用哪种方式，都可以把需要的信息查询出来，然后就可以预估数据大小了。我截图的信息显示，数据大小 8.5MB，索引大小 0MB，还要结合数据条数，我查了一下有 10000 条数据，因此可以粗略估计每条数据的大小为 0.85KB。这里需要注意一下，预估数据大小之前要保证数据的字段取值接近真实情况，最好能有数据示例可以参考，而且数据量要尽量大一些，例如几万条，不能只有几十条、几百条。如果确实没有数据示例参考，需要自己模拟生成，尽量把字段的取值多生成一些实际中可能出现的值。例如字符串类型如果是 vachar，要把每种长度的取值都生成一些，或者根据实际场景，某些长度的字符串出现的可能性大一点，那就多生成一些。如果觉得这样计算比较麻烦的话，其实还有一种更简单的方法，直接查询 avg_row_length 字段，这个字段表示数据表的平均行大小，和上面自己计算的结果类似。总之，就是为了接近真实，才能更为准确地预估出数据占用的空间大小，实际去申请资源时才能有理有据。此外，这个 TABLES 表里面的内容很丰富的，有需要的可以查询一下，查看数据表的字段信息 SQL 语句：1SHOW COLUMNS FROM TABLES;]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>数据库</tag>
        <tag>database</tag>
        <tag>space</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 JDK 命令行工具分析内存泄漏或内存溢出问题]]></title>
    <url>%2F2019040301.html</url>
    <content type="text"><![CDATA[最近遇到一个棘手的问题，有业务方在调用存储系统封装的 SDK 取数的过程中，遇到了 OOM 问题，但是数据量很小，只有 12000 条。同时进程启动时申请的内存高达 12g，使用 Xmx、Xms 参数控制，实际指定参数取值为：-Xms12g -Xmx12g。但是如果只看报错日志信息，抛出异常的代码位置指向了 SDK 的内部代码。根据这个现象，我猜测可能是业务方的处理逻辑问题、SDK 内部处理逻辑问题、申请的内存过小问题，这些问题归根结底，要么是内存不够【内存溢出】，要么是内存不当使用【内存泄漏】。所以，我要在 Java 虚拟机参数方面或者业务方代码逻辑方面入手，一步一步测试，找出问题的元凶。本文就记录这一过程，以及适当引申一些关于 JVM 的知识。解释说明一下，上述中的 SDK 表示存储系统独立封装的取数、查询接口，它屏蔽了 Elasticsearch 自带的接口，并封装成公共组件，提供给各个业务方使用。各个业务方在使用前，需要申请开通 token 验证码，存储系统会根据业务方的使用量分配合适的资源，业务方在调用时需要传入 token 验证。这样做的好处，一是可以监控所有的业务方的取数、查询情况，收集所有的请求日志，统计一些常用的指标，然后反过来指导存储系统的改进，例如根据业务方的调用情况进行资源分配的伸缩、针对常用的数据类型进行索引优化。二是可以保障整个数据库集群的正常运行，由于屏蔽了 Elasticsearch 自带的接口，业务方不能随意操作超大额的数据量，SDK 会做限制，因此不会产生某些不合理的查询、取数请求，从而不对数据库造成巨大的压力。三是限制了一些不需要的查询、取数方式，在保障业务方基本需求的情况下又可以保障数据库集群的稳定，例如多层聚合、日期聚合等操作，这些操作不合理，而且会对数据库集群造成压力【无论数据量大小都可能会出事】。问题出现 简单描述现象，查看日志，猜测可能的原因。问题解决 分析现象 一开始没有指定 JVM 参数，因为使用的是 JDK1.7 版本的参数，不会生效，这就导致分配的默认堆取值偏小。JVM 参数设置：12345678JAVA=$&#123;JAVA_HOME&#125;/bin/java# 设置 jvm 的参数 #HEAP_OPTS=&quot;-Xms12g -Xmx12g&quot;HEAP_OPTS=&quot;-Xms6g -Xmx6g -Xmn2g&quot;# JDK8 以后取消了 PermSize#PERM_OPTS=&quot;-XX:PermSize=1024M -XX:MaxPermSize=2048m -XX:+UseConcMarkSweepGC -XX:+UseParNewGC&quot;#JDK8 的 MetaspaceSizePERM_OPTS=&quot;-XX:MetaspaceSize=1024m -XX:MaxMetaspaceSize=2048m -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+HeapDumpOnOutOfMemoryError&quot;待整理，重试，重现现象，确保问题准确复现。掌握内存分析工具 待整理。各种工具介绍，举例，截图。对症下药解决问题 待整理。减小内存，使用普通的 list。重试，使用命令行工具查看现象，截图。问题总结 不同版本的参数不一致 主要是针对 JDK 来说的，不同的 JDK 版本的参数会有一些不同，例如以下 2 个虚拟机参数，在 JDK1.8 的环境中是 -XX:MetaspaceSize=1024m -XX:MaxMetaspaceSize=2048m，已经不是 -XX:PermSize=1024M -XX:MaxPermSize=2048m 了【JDK 1.7 以及以前的版本】，在进程启动的时候查看日志会有警告信息的，提示参数设置无效，会被忽略。CopyOnWriteArrayList占用内存，待整理。模拟内存溢出 待整理。进程已杀死问题 加大内存，机器内存不够分配，进程异常退出，待整理。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>JDK</tag>
        <tag>内存泄漏</tag>
        <tag>内存溢出</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[青椒炒蛋做法总结]]></title>
    <url>%2F2019033101.html</url>
    <content type="text"><![CDATA[青椒炒蛋，是一道非常普通的家常菜，基本家家户户都会做。有的家庭喜欢吃辣，放的是稍微辣一点的辣椒，有的家庭不喜欢吃辣，就放菜椒或者甜椒，总之，对于辣椒的选择非常多。对于辣椒的处理方式，有的人喜欢切小块，有的人喜欢斜切小段，还有的人直接剁碎，做法也多种多样。本文记录青椒炒蛋的做法总结，使用的是菜椒【不辣微甜】，由于故意多放了生抽，做出来的口味是咸香的。食材准备 以下食材的份量为一大盘，足够 2 人吃，食材非常简单：鸡蛋 3 个 青椒 2 棵 大蒜 6 粒 食用盐、生抽酱油 全部的食材 制作过程 这是一道快菜，制作过程根据家庭灶的火力大小，2-5 分钟即完成。处理食材备用 青椒洗净去籽切小快，鸡蛋液加少量食用盐搅拌均匀，大蒜切片。青椒切块，大蒜切片 鸡蛋液 炒鸡蛋备用 锅里加油，要多加一点，鸡蛋液很吸油，烧热后下鸡蛋液，定型后炒散，炒散后盛出备用。由于鸡蛋液里面已经加了食用盐，就不用再加盐调味了。鸡蛋液下锅 定型炒散 鸡蛋盛出备用 炒青椒大蒜 锅里留底油，如果炒完鸡蛋后不够再适当加点油，烧热，大蒜片和青椒同时下锅翻炒【注意是同时下锅】，翻炒至青椒 5 成熟，关小火准备加生抽调味。青椒大蒜同时下锅 翻炒至青椒 5 成熟 加生抽调味后下鸡蛋 青椒翻炒至 5 成熟时，关小火，加生抽调味。注意一定要加多一点生抽，那种大的汤勺可以加将近一汤勺，然后开大火翻炒，把青椒炒至 8 成熟，生抽遇到大火热量时会散发独特的香味。加生抽炒至青椒 8 成熟 此时倒入前面炒过的鸡蛋，混合翻炒，如果觉得不够味再加一点食用盐，我加的生抽已经够味，不再加食用盐了。倒入鸡蛋 翻炒均匀 出锅装盘 翻炒均匀后可以出锅装盘了。装盘侧视图 装盘俯视图 配上剩下的几块红烧肉，美滋滋。注意事项 1、炒青椒时和大蒜片一起下锅，不需要先下大蒜片爆香，这很关键。2、油量，炒鸡蛋的时候一定要多放一点油，才能保持鸡蛋的嫩滑，因为鸡蛋液吸油很厉害。如果油加的少，会导致鸡蛋炒的有点干有点糊，影响口感。3、炒青椒的时候为什么要多加一点生抽呢，毕竟会影响这道菜的颜色，一般炒菜都只会在最后调味时加一点点。因为使用青椒来炒鸡蛋，这种青椒是没有什么味道的，那样只会有鸡蛋的香味，显得太单调，而生抽遇到热量会散发出独特的香味，同时生抽里面又有盐分，从而达到了咸香的效果。就如北方有些地方做番茄炒蛋的时候，是做成咸味的，也会加入大量的生抽，味道也非常好，特别是拌面吃，既是菜又能调味。 致谢 感谢微博用户 @开心的柠檬日记 ，在微博上放了很多做菜的方子，微博主页为：开心的柠檬日记的微博 。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>青椒炒蛋</tag>
        <tag>辣椒炒蛋</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[参加 Elastic 社区第三次线下活动广州站]]></title>
    <url>%2F2019033001.html</url>
    <content type="text"><![CDATA[在 2019 年 3 月 30 日，我去参加了 Elastic 社区第三次线下活动广州站的分享会，活动简介：Elastic 社区第三次线下活动广州站 。看到各位行业顶尖分享者的分享，不能说受益匪浅，至少给我打开了一些思路，拓展了我的知识面，同时我也学到了一些知识，既包括技术方面的，也包括处事方面的。这篇博文就简单记录一下这个过程。 出发 先看一下地图指引 到达公交站，上冲南站，天气不错 走路路过特斯拉服务站，听说最近交付的特斯拉电动车有很多问题 到达 到达的比较早，因为要帮忙安排桌子凳子，一切准备就绪后，一起吃了个午饭。13:30 开始签到，签到现场 我充当了一会儿签到员，坐着的那个是我 各种各样的 Elasticsearch 贴纸 这是一种比较特殊的 Elasticsearch 贴纸 静听分享 先简单看一下这个分享会的大概流程与分享内容 分享一 Elasticsearch 在数说全量库的应用实践 现场场景一 现场场景二 现场场景三 分享二 Elasticsearch 在慧算账技术运营中的应用 现场场景 分享三 Elasticsearch 在大数据可视化分析中的应用 现场场景 分享四 打造云原生的 Elasticsearch 服务 现场场景 分享五 Elasticsearch 集群在雷达大数据平台的演进 现场场景 分享者合影留念 认真的观众 分享者合影留念 我的思考以及学到的东西 0、虽然有一些分享听不懂，例如腾讯云的 Elasticsearch 云服务，做了什么优化、达到了什么效果，或者是数说雷达的架构演进，这些目前对于我来说都太不切实际，因为还没接触到这么高深的知识，平时也使用不到，所以听起来云里雾里。但是，能从中提取 1-2 个重要知识点也是有用的，例如腾讯云的索引碎片化，导致读写速度严重下降，这与我在工作当中遇到的问题一模一样。再例如数说雷达演进过程中遇到的坑，某个字段没有做 doc_values，导致不支持 aggregation 查询，这与我很久之前遇到的问题一模一样，此时又加深了我的认知。1、多版本 Elasticsearch 的兼容解决办法，需要设置拦截器，把请求的不兼容参数部分替换掉，可以使用 SpringBoot 整合，需要注意已知版本的种类。2、针对 long 类型字段的聚合【即 aggregation】请求根据自己的业务场景，如果判断为实际上没有必要【例如只是对年份、月份、日做聚合，并不考虑时区、毫秒时间戳的问题】，可以换一种思路，转化为字符串存储，针对字符串做聚合操作效率就高多了。3、在现场提问时，有的人是带着自己业务实际遇到的问题来提问探讨的，提问时描述问题已经消耗了将近 10 分钟。接下来如果真的探讨起来，估计没有半个小时一个小时搞不定，这显然是在浪费大家的时间。所以分享者也及时打断了提问，并留下联系方式，分享会后线下接着再讨论。这种做法很得体，虽然不能在现场解答【为了节约大家的时间】，但是会后讨论也是一样，有时候根据实际情况就是需要这样的取舍。4、在 Elasticsearch 中，字段类型是可以节约存储空间与请求耗时的，例如 integer、long、short 的合理使用，但是切记存储的目的最终都是为了使用。 备注 如果需要查看分享者的 PPT 文档，可以在 Elastic 社区下载：https://elasticsearch.cn/slides 。]]></content>
      <categories>
        <category>游玩</category>
      </categories>
      <tags>
        <tag>Elastic</tag>
        <tag>线下活动</tag>
        <tag>广州</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FFmpeg 使用总结]]></title>
    <url>%2F2019032701.html</url>
    <content type="text"><![CDATA[FFmpeg 是一款开源的软件，可以进行多种格式的视频、音频编码转换、片段剪辑。它包含了 libavcodec – 这是一个用于多个项目中音频和视频的解码器库，以及 libavformat – 一个音频与视频格式转换库。FFmpeg 这个单词中的 FF 指的是 Fast Forward。FFmpeg 官网：https://ffmpeg.org ，下载时会跳转到这里：https://ffmpeg.zeranoe.com/builds ，请选择合适的版本下载使用。本文记录 FFmpeg 的使用方法，基于 Windows X64 平台。下载安装 下载 在 https://ffmpeg.zeranoe.com/builds 下载页面，选择适合自己操作系统的版本，我这里选择 Windows X64 的 static zip 包，解压后直接使用，无需安装。解压配置环境变量 下载到指定的目录【最好放在方便管理的目录，不显得混乱】，直接解压，得到一个文件夹，里面有 bin、doc、presets 这 3 个子文件夹，其中 bin 里面就包含了主程序：ffmpeg、ffplay、ffprobe，这里不涉及安装的概念，程序可以直接使用。解压主目录 子文件夹 bin为了方便使用这 3 个主程序，需要把 bin 所在目录配置到环境变量 PATH 中【我这里是 D:\Program Files\ffmpeg\bin】，这里就不再赘述，如果不配置，每次使用命令时都要给出完整的目录，我觉得很麻烦。使用示例 ffmpeg 的命令行参数的位置会影响执行的结果，例如时间参数，这与我所知道的其它工具不一样，所以参数位置不能乱放。此外，还需要注意涉及到转码的操作会比较耗时，几十分钟的视频不是几分钟能处理完的，和视频的清晰度也有关系，这个要有一定的心理准备。1、把 mkv 格式的视频文件转为 mp4 格式的文件，视频使用 libx264 编码。12-- 如果没有配置环境变量 PATH, 命令需要指定 D:\Program Files\ffmpeg\bin\ffmpegffmpeg -i imput.mkv -c:v libx264 output.mp4 里面的字幕信息如果是和视频一起的，会自动携带输出。2、查看视频文件的流信息，包括视频、音频、字幕。12-- 其中类似 Stream #0:0 格式的内容就是流信息，指定参数时可以直接使用数字编号表示流 ffmpeg -i input.mkv3、mkv 文件剪辑，截取片段，指定音轨。12-- -ss 表示开始时间，-to 表示结束时间，ffmpeg -ss 01:22:08 -to 01:32:16 -accurate_seek -i in.mkv -map 0:v -map 0:a:1 -codec copy -avoid_negative_ts 1 out.mkv其中，-accurate_seek 表示画面帧数校准，-avoid_negative_ts 1 表示修复结尾可能的空白帧，-map 0:v 表示截取所有视频，-map 0:a:1 表示截取第 2 道音轨。此外，如果把时间参数放在 -i 前面，结果总会多截取 1-2 秒【如上面示例】。但是如果放在后面，截取的视频片段时间准确了，然而开头的音频正常，视频有 20-30 秒的漆黑一片，不知道为啥。注意，如果视频带有内嵌字幕【mkv 携带的一般是 ASS 字幕】，也需要一起剪辑的话，需要指定参数：-map 0:s，格式和指定视频、音频的格式一致。如果是其它格式的字幕，只要确保 ffmpeg 支持即可使用字幕相关的参数，那么怎么查看呢，很简单，使用 ffmpeg -codecs |grep title 命令即可搜索。4、rmvb 文件转为 mp4 文件，涉及到编码转换。12-- 视频使用 h264 编码，音频使用 aac 编码 ffmpeg -i input.rmvb -c:v h264 -c:a aac out.mp4这里需要注意，涉及到编码转换的比较消耗 CPU，上面这个命令把我的 CPU 消耗到 100%，动态视频详见微博：FFmpeg 视频转码 CPU 飙升到 100% 。其中，留意流输出信息：123Stream mapping: Stream #0:1 -&gt; #0:0 (rv40 (native) -&gt; h264 (libx264)) Stream #0:0 -&gt; #0:1 (cook (native) -&gt; aac (native))此外，FFmpeg 不支持 rmvb 格式的文件，只能转码为 mp4 的格式再使用，这里的不支持不是指不能处理，而是不能直接输出 rmvb 格式的文件，处理输入是可以的。5、多个 mp4 文件拼接，先转为同样的编码格式的 ts 流，再拼接 ts 流接着转换为 mp4 格式的输出。123ffmpeg -i 1.mp4 -vcodec copy -acodec copy -vbsf h264_mp4toannexb 1.tsffmpeg -i 2.mp4 -vcodec copy -acodec copy -vbsf h264_mp4toannexb 2.tsffmpeg -i &quot;concat:1.ts|2.ts&quot; -acodec copy -vcodec copy -absf aac_adtstoasc output.mp4简单高效，而且视频质量没有损失。其它1、如果只是为了转换 mkv 文件的格式为 mp4，也可以使用一款软件：MkvToMp4 。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>FFmpeg</tag>
        <tag>视频剪辑</tag>
        <tag>音频剪辑</tag>
        <tag>视频转码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分别 是为了再次相聚]]></title>
    <url>%2F2019032702.html</url>
    <content type="text"><![CDATA[这是我还在大学读书的时候，有一年过年回家，组织了高中同班同学的聚会，其实主要目的就是一起吃个饭，玩半天，交流一下。我还隐隐约约记得当时提前约好了十几个人，然后到了当天早上再确认的时候，只有八个人能来了，刚好凑一桌。那次一别，以后再也没见过，只在微信上聊过，大家天各一方，有的求学做科研，有的成家立业。而如今，当我碰巧再拾起这段文字的时候，只觉得沙流指尖，微风拂面。当然，我再也写不出这么稚嫩的、无病呻吟的文字了，因为现在整天在写代码，我的思维也变化了很多。最近三年，在工作环境中经历了多次的相聚离别，现在年后又是离职大潮，刚好整理出这篇旧笔记，提醒自己的成长之路。开篇 我应该算是一个善感的人，但不多愁。此时努力寻找记忆中的碎片，感受那些我能看到的瞬间，尝试从记忆中寻找值得书写的部分，以作想念。每次只要有久违的聚会，之前我都会想象见面时的情景，以及每个人的样子，认为这样能追寻到由于长时间不见面而淡去的亲切感。同时内心也会积聚起来丰富的情感，构思出许多可以表达的文字，藏在心底，待到意兴浓的时候说出来。相聚 我记得那天早晨起的特别早，是假期中最早的一次，可能是内心的激动，亦或是天气转暖，也可能是因为我定了三个闹钟。出发的时候由于出现了三个不在我计划中的意外，导致我的出发时间比我预计的晚了一个小时，这已经使我的内心感到些许急躁。后来在路上没想到会堵车，又耽搁了 20 分钟，望着拥挤的车流，我再也按捺不住急躁的内心，便直接下车走完了最后一段路，到达的时候已经是中午 12 点了。在路上有点风，微风拂面，我能感觉到微微颤抖，为自己没有围围巾而感到后悔，这可是冬天，温度只有几摄氏度。可是到了集合地点太阳都出来了，阳光明媚，冬风和煦，的确是个好天气，相当适合聚会，内心的寒冷一扫而空。走到集合的地点，那是一个破旧的学校操场，我曾在这里度过一年光阴，扶了扶眼镜，向四周望了望，看到在操场旁的屋檐下，他们正在打桌球。毕竟两年多没见过面了，他们的样子在我脑海中渐渐模糊，此时努力搜寻，看着一张张熟悉又陌生的脸庞，心中情感翻涌，暗暗对着他们的名字，辛福感扑面而来。但准备好的话一句也没有说出口，这就是我的性格，见了面在一起比什么语言都美好，直接加入他们，我想这也是最好的表达。快速从脑海中搜索记忆，在我仔细看来，大家仍然是高中的模样，没有怎么变化，还是那么青春，还是那么快乐，没有老练，没有隔阂，脸上洋溢着年少时的单纯感，都像这个年龄应该有的样子。我装出很「酷」的样子，至少我是这么认为，可毕竟长时间没有打过桌球了，手生，出杆结果总是不尽人意，我总感觉自己处于尴尬的境地，可是仍然装出不在乎的样子，说几句玩笑话，淡淡一笑，希望没人看到，说到底还是生疏了。本应该可以去吃饭了，但大家兴致未尽，在「混乱」的状态下又玩了几十分钟，随着黑色八号球的入袋，结束了应该有的结局，已经过了午饭的时间。说实话，此时我已经饥肠辘辘，但大家看起来都很亢奋，兴奋喜悦的表情洋溢于脸，在说笑中集体寻找吃饭的地点。八个年轻人一起走在马路上，蹦蹦跳跳，三两成堆，不时对旁边的人吐露心声，开着不着边际的玩笑，说着没心没肺的笑话。此时我的脑海中浮现出一幅和谐美好的画面，也必将印在我的心底。走走停停，说说笑笑，乘坐免费的公交，不多时便到达了新建落成的七彩世界，映入眼帘的是花花绿绿的色彩，心情豁然开朗，已经做好了吃喝的准备。进入大楼入口，乘坐电梯，直奔饮食区域。首先寻找最佳的位置，足够容纳八个人入座，围在一起，谈笑风生。我实在是饿坏了，便独自一人去点了一碗面，首先尝尝咸淡。没想到我等了一会儿，他们大部分人都来了，都要吃面，那就点吧。七七八八大家议论了一番，各自点了一碗面。等待上面的过程中，几个男生又去点了一些副食，女生去点了饮品，相互搭配，应该足够果腹了。东西上齐后，本想随心所欲，大快朵颐，可是不知怎么的，先前的食欲减小了，不过以我的大饭量还是能消耗很多食物的。吃饭的过程就是吃饭的过程。吃饱喝足之后，大家聊了一会儿，有人提议打牌，反正闲着没事儿干，打就开打。本来准备去购买纸牌的，没想到王飞的书包里居然带着这玩意儿，看来他那书包里面装着很多现金也是真的了。打牌真的是体力活也是脑力活，不仅需要工于计算，还需要相互配合，可是牌不好什么都白搭。在这个过程，我可以说是遭遇了打牌生涯的滑铁卢，无论怎么样打，都是输牌。当然，值得说明的是，输牌并不是因为牌技不好，而是因为牌不好，就算什么都算出来了也打不赢，这不能怪我。身为理科生，我清楚地知道这只是概率问题，风水会轮流转，但为了缓解气氛，我也可以承认这可能是人品的问题。况且竟然牌从头到尾一直不好，想想这也是一个小概率事件，我不得不承认可能确实和我的人品有一点关系。输输输……输了牌，就会有惩罚，惩罚过后，换了一波人继续打。可是万万没想到，我这一门虽然换了人打，牌还是不好，虽然比我打的时候好了那么一点点，仍然是稳输不赢，牌差的简直令人发指。打牌成绩的最顶峰也就是赢了一个而已，很快又变成负数了，真是令人伤悲。分别 不知不觉中，在说说笑笑、打打闹闹的氛围下过去了几个小时，天也已经快黑了，是时候该分别了，各自回家，要不然有些人恐怕赶不上末班车了（十八线小县城过了六点就没车了）。而后大家收拾东西，开始动身，临走时总要合照一张吧。合照一没有我 合照二没有我 每一张照片，王飞的脖子和头看起来很奇怪，我没法挑出效果更好的了。第一张解丰的全身没有入镜，当然，这怪我，是我拍的。请看下面零散的几张照片，当时的红米手机拍照效果也就这样了，每张照片的大小只有 800KB 左右。大家闲聊 二位在比划啥 二位正经的样子 正经人 这里面有我 这是干啥 下了楼，大家一起往车站行走，途中许梦宇有事首先分别，王继雪顺车回家第二个分别。剩下的六人继续走在繁华的街头，沿着热闹的商铺，趁着剩下不多的时间聊天说笑。本来是一段很短的路程，我们却走了很长时间，可能大家心里都想着多聊一会儿吧，才故意放慢了脚步。当然，最终这也导致了我错过了回家的末班车。到了车站，大家又聊了一会儿，当作告别的寒暄。我不得不乘坐另外一辆车，和李欢一起，只是到达离家还有十多公里的永兴街上，然后让家里的亲人来接。结尾 「天下没有不散的宴席」，这句老话大家都知道，可大概只有经历过的人才能理解其中蕴含着的意义吧。时间总是很快，最终的时刻总要分别。 分别，是为了再次相聚。下一次再相聚会是什么时候呢？下一次再相聚大家会变吗？静静等待着再次相聚。哦，对了，我离开家乡去学校的那天，火车开动的时候，家乡飘起了小雪。后来火车远离家乡的时候，听说雪已经很大了，可惜我没有见到。是为记。2015 年 02 月 26 日 鹏飞]]></content>
      <categories>
        <category>游玩</category>
      </categories>
      <tags>
        <tag>聚会</tag>
        <tag>利辛一中</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爆炒花甲做法总结]]></title>
    <url>%2F2019032501.html</url>
    <content type="text"><![CDATA[爆炒花甲是一道做法简单、快速出菜、可口下饭的家常菜，一般晚上去大排档吃夜宵，基本都会有这道菜。本文记录爆炒花甲的家常做法，以及需要注意的地方。食材准备 以下份量为一盘：花甲 1.5 斤 - 2 斤，花甲的价格一般在 6-10 块钱之间【广州地区】二荆条辣椒 2 个，细长条那种，不是很辣 小米椒 5 个，我用干辣椒和辣椒酱代替了 豆瓣酱、蚝油、食用盐、淀粉 大蒜、香葱，香葱我用青椒代替了 辣椒食材 辣椒切碎备用 准备工作中最重要的就是让花甲吐沙，一般买回来的花甲里面都会有大量的沙子，所以要让花甲把沙子吐出来，这样吃起来才不会硌牙，否则会严重影响口感，没法吃。花甲吐沙方法主要有两种：方法 1【时间长，简单】：放入清水、食用盐、香油，浸泡一个小时，基本可以把沙吐干净。方法 2【时间短，麻烦】：放入锅中，加入清水、料酒、姜片、香油，小火煮热【不要很热，保持不烫手的温度，否则会严重影响肉质】，持续 15 分钟，也可以把沙吐干净。我一般选择第一种，花甲买回来之后放在盆里，加入盐、香油等着就行了，可以先去处理其它食材了，不用管，很方便。制作步骤 花甲焯水 花甲吐沙完成之后，捞出进行焯水，焯水之后花甲基本熟了，然后捞出花甲迅速过冷水，过冷水的目的是保持花甲肉的鲜嫩。注意焯水不要太久，否则花甲肉就老了，影响口感。我把花甲焯水的时候还加了姜片和料酒去腥。爆香锅底 油烧热，一定要比平时炒菜多放一点油，毕竟是爆炒，加入大蒜、辣椒、姜片，十几秒爆香，我这里省略了姜片，因为焯水的时候用的水里面加了姜片和料酒。大蒜也不加了。豆瓣酱或者蚝油调味 关小火，加入豆瓣酱，调和味道，我没有豆瓣酱，就用蚝油代替了。爆炒花甲 味道调和好后，加入过冷水的花甲，开大火，爆炒。爆炒 3 分钟后关小火调味【时间实际依据灶的火力大小，家用灶一般火不够大，要多炒一会】，加入食用盐【如果豆瓣酱或者生抽够味就不用加盐了】、生抽、辣椒酱，大火翻炒十几秒。我又补了一点辣椒酱 最后一步也很关键，使用淀粉液勾芡，目的是让味道均匀包裹在花甲的表面，否则味道都会遗留在锅里，导致花甲味道偏淡。勾芡可以实现真正入味的效果。出锅装盘 如果有香葱的话，出锅前再放一点点香葱，更好看，味道也更好。我这里没放香葱，使用青椒代替了。其它配菜 顺手做了一道红烧肉，加了香菇和 2 个鸡蛋，有点偏卤肉的口味了。红烧肉收汁阶段 红烧肉成品 注意事项 花甲吐沙 花甲吐沙一定要做好，不可匆匆了事，否则花甲吃起来全部都是沙子就不好了，严重影响口感，另外还要注意在爆炒的时候会有一些花甲壳碎掉，混在花甲肉里面，吃的时候也要注意一下，虽然不是沙子，但是也会硌牙。花甲焯水 花甲焯水，可以适当放一点姜片、料酒，去腥味。当然，如果吐沙的方法采用的是温水慢煮的方式，可以不用再放任何调料了，直接焯水就行了。焯水时要切记水不能太沸腾，或者不要一直放在沸水里面，要适当用漏勺翻一下，否则会导致花甲肉全部脱离花甲壳了，这个和花甲新不新鲜无关，水太沸腾会导致大量的花甲肉脱离花甲壳。勾芡 勾芡，才能保证味道均匀分布在花甲表面，吃起来才有入味的效果，否则调味料大部分都粘在锅的表面了。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>爆炒花甲</tag>
        <tag>麻辣花甲</tag>
        <tag>炒花甲</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Valine 给 Hexo 博客添加评论系统]]></title>
    <url>%2F2019032001.html</url>
    <content type="text"><![CDATA[我的博客已经搭建得差不多了，一些配置也固定下来了，最近重点一直在补充博客内容，把以前的笔记都整理出来。然后有一天我就想，好像总感觉少点什么，发现评论这个功能是没有的。以前是为了追求简洁的风格，而且评论这个功能不稳定，主要是评论系统不好选择，很多都关闭了。思前想后，考虑了好几天，最终还是决定先加上评论功能，实验一阵子，看看有没有必要，后续再决定是取消还是继续，反正也就是改一下配置就行了，没有多大工作量。接下来查了一下当前还活着的评论系统的种类，最后选择了 Valine 这个评论系统。它不需要登录，无后台管理，非常简洁，比较符合我追求的理念。参考相关内容：https://github.com/xCss/Valine 、https://valine.js.org 、https://leancloud.cn 。注册帐号创建应用 Valine 诞生于 2017 年 8 月 7 日，是一款基于 Leancloud 的快速、简洁且高效的无后端评论系统。 所以，第一步就需要注册 Leancloud 账号，然后才能申请应用的 appid 和 appkey。注册过程我就不赘述了，和注册普通的账号一样，官网地址：https://leancloud.cn 。接下来重点来了，需要申请免费的应用【有钱的话也可以购买收费的版本】，这里面有一些需要注意的地方，否则最后评论的时候没有效果，会导致 Leancloud 后台接收不到评论数据。1、登录 Leancloud 系统，进入系统的控制台，然后创建应用。从主页进入控制台 创建应用，我这里已经创建好一个应用了。2、填写、选择应用的参数，这里需要填写应用的名字，选择开发版本【免费版本，限制请求并发数】。3、创建完成后，进入设置详情页面。点击齿轮，进入设置详情页面。在设置详情页面里面，选择 设置 -&gt; 应用 Key，就可以看到应用的 appid 和 appkey，这 2 个字符串要记下来，等一下在 Hexo 里面配置的时候有用。4、在 存储 -&gt; 数据 里面查看默认的 Class 信息，有一些默认的 Class，例如 _User、_File、_Role 等，这些都用不到，而 Hexo 的评论功能需要一个名称为 Comment 的 Class，现在发现没有这个 Class，要不要手动配置一个呢。其实不用担心，经过我的测试 Hexo 会自动生成这个 Class，所以不需要自己手动配置了。5、在 设置 -&gt; 安全中心 ，把 文件上传、短信服务、推送服务、实时通信 这几个服务全部关闭，因为用不到。然后需要特别注意的就是 Web 安全域名 这一个选项，里面一定要填写自己站点的域名，并且带上端口号，例如 http 请求的默认端口就是 80，htps 请求的默认端口就是 443。这里如果没有配置好，评论的时候也会失败的。配置 Hexo 参数 上一步骤已经把 Leancloud 里面的应用申请好了，并且设置了重要的选项，获取到 appid 和 appkey，接下来配置 Hexo 就简单多了。打开 Hexo 主题的配置文件 _config.yml，搜索一下 Valine，找到默认配置【这是因为 Hexo 已经自动集成了 Valine 评论系统，不需要安装什么，如果没有请升级 Hexo 版本】。默认是关闭的，把配置更改如下图，更为详细内容参考：https://valine.js.org/configuration.html 。主要配置的内容如下【重点是 appid、appkey、placeholder，至于验证、邮件提醒就按照自己的需要来配置吧】：123456789101112131415161718valine: # 开启 Valine 评论 enable: true # 设置应用 id 和 key appid: CCCJixxxxxxXXXxxxXXXX000-gzGzo000 appkey: AA1RXXXXXhPXXXX00F0XXXJSq # mail notifier , https://github.com/xCss/Valine/wiki # 关闭提醒与验证 notify: false verify: false # 文本框占位文字 placeholder: 没有问题吗？ # 需要填写的信息字段 meta: [&apos;nick&apos;,&apos;mail&apos;] # 默认头像 avatar: wavatar # 每页显示的评论数 pageSize: 10这里面我发现一个问题，就是有一些配置项不生效，例如：meta、avatar，我也不知道是 Hexo 的问题还是 Valine 的问题，我也不懂，就先不管了，因为不影响评论这个功能。另外还有一个就是评论的时候总会强制检验邮箱和 url 的规范性，如果没填或者填的不规范就弹框提示，我不知道怎么取消，只好在在 GitHub 提了一个 Issue，详见：https://github.com/xCss/Valine/issues/168 ，但是作者一直没回。等了几天，作者回复了，说是我的 Valine 版本太低，让我升级。我看了本地的 Valine 的版本，已经是 v1.3.5 了，然后我就怀疑可能是 Hexo 的版本问题，但是我自己做了很多自定义的配置，改了很多 css、js 文件，不能随便升级，等以后有时间做一个大版本的升级，再好好整理。那怎么才能让博客文章的底部显示评论对话框呢，其实很简单，什么都不用做，Hexo 默认是给每个页面都开启评论的【前提是在 Hexo 的配置文件中开启了一种评论系统】。它背后的配置就是 Markdown 文件的 comments 属性，默认设置是 true，所以不用配置了，如果非要配置也可以，如下图。此外，还需要注意，如果博客还有除正文内容之外的页面存在，例如关于、分类、标签，要把他们的 Markdown 文件的 comments 属性设置为 false，否则这些页面在展示的时候也会有评论的功能出现，总不能让别人随便评论吧。测试效果 打开任意一篇博客文章，可以看到底部已经有评论的文本框了。试着填写内容，评论一下，可以看到评论列表的内容。好了，此时可以再回到 Leancloud 系统，看一下评论数据吧。直接在 存储 -&gt; 数据 -&gt;Comment 里面，可以看到已经有评论数据了。由于 Valine 是无后端的评论系统，所以数据直接被存储到了 Leancloud 系统的数据库表里面，看看就行了，不方便管理。如果评论数据很多，为了更方便管理评论数据，能收到更友好的邮件通知提醒，可以使用 Valine-Admin 来实现，我暂时先不用。经过几天的测试，可以看到应用的请求量统计信息。附加 Valine-Admin 进行评论数据管理 这个插件我现在先不使用，因为还不知道评论数据会怎么样呢，等以后如果确实有需要再考虑增加，参考项目：https://github.com/zhaojun1998/Valine-Admin 。后记 还记得我上面提到的 Valine 版本过低，导致评论的时候总会强制检验邮箱和 url 的规范性这个现象。经过一段时间的观察，以及 2019-06-21 爆发的 leanCloud 域名误被封禁事件，导致彻底无法评论，我终于找到了升级 Valine 版本的方法，只要直接更改主题的 swig 脚本文件，具体路径在 themes/next/layout/_third-party/comments/valine.swig，把里面的旧版本的 js 引用移除【有 2 个文件引用】，改为新版本的 js 引用。注意，作者特意在官网说明最新版本的 Valine 只需要引用一个 unpkg 库的 Valine.min.js 文件即可，其它的不再需要【我升级到当时最新的版本 v1.3.7】。升级完成后，再去评论区看看，不会再强制验证邮箱和 url，而且会收集显示评论用户的操作系统、浏览器信息，同时右下角也新增了 Valine 的版本信息，可以更为清晰地看到版本。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Valine</tag>
        <tag>评论系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 客户端设置 Windows 下的字符编码]]></title>
    <url>%2F2019031901.html</url>
    <content type="text"><![CDATA[在 Linux 以及大多数托管网站上，默认的字符编码均是 UTF-8，而 Windows 系统默认编码不是 UTF-8，一般是 GBK。如果在 Windows 平台使用 Git 客户端，不设置 Git 字符编码为 UTF-8，Git 客户端在处理中文内容时会出现乱码现象，很是烦人。但是，如果能正确设置字符编码，则可以有效解决处理中文和中文显示的问题。大多数技术从业者应该都遇到过各种各样的编码问题，后来渐渐习惯了使用英文，尽量避免中文，但是也有一些场景是必须使用中文的。本文就记录解决 Git 中文处理和中文显示的问题的过程，系统环境基于 Windows7 X64，Git 基于 v2.18.0。乱码现象 Git 是一款非常好用的分布式版本控制系统，为了更好地使用它，一般都需要 Git 客户端的配合，下载使用参考：https://git-scm.com/downloads 。 在 Windows 平台使用 Git 客户端的过程中，有一个问题你一定逃不掉，那就是乱码问题。这是因为 Windows 系统的编码是 GBK，而 Git 客户端的编码是 UTF-8，当两种不同的编码相遇，必然有一方会乱码。如果设置 Git 客户端的编码为 GBK，那么在使用 Git 客户端处理系统文件的时候可以正常显示，但是处理 Git 版本控制内容的时候，就会乱码，无法支持中文。如果反过来呢，把 Git 客户端的编码设置为 UTF-8，那么处理版本控制内容就可以有效支持中文，但是处理系统文件的时候又会乱码。Git 客户端设置 UTF-8 编码，处理系统文件显示乱码 解决方式 这样看起来似乎没有解决方法，其实不是的，还是有很好的解决方法的。我这里为了完全支持版本管理系统，版本管理优先，肯定要统一设置为 UTF-8 编码，然后通过 Git 客户端的编码自动转换来支持系统的 GBK 编码。这里先提前说明，在使用 Git 客户端的时候，Git 的安装目录【一般默认是 C:\Program Files\Git】，也就是 Git 的根目录。在使用 ls 等命令处理文件时，如果携带了 / 字符，其实就表示从 Git 的安装目录开始。例如在里面寻找 etc 目录，如果是使用 Git Bash 打开的，可以直接使用根目录的方式，cd /etc/。再例如 vi /etc/git-completion.bash 不是表示从系统的根目录开始寻找文件【Windows 系统也没有根目录的概念】，而是表示从 Git 的安装目录开始寻找文件。设置 Git 客户端 打开 Git 客户端的主页面，右键打开菜单栏【或者点击窗口的左上角也可以打开】，选择 Options 选项。接着选择 Text 参数配置，把编码方式由 GBK 改为 UTF-8【locale 也要设置为 zh_CN】。设置完成后，一定会导致一个现象，那就是使用 ls 查看系统文件时，带有中文的目录和带有中文的文件，一定是乱码的，根本看不清楚显示的是什么。不过不用担心，后面会通过设置让它恢复正常的。接下来要解决的是显示的问题，目的是保证 Windows 的 GBK 编码可以在 Git 客户端正常显示。由于 Git 客户端被设置为了 UTF-8 编码，使用 ls 命令查看目录文件详情的时候，一定是乱码的，什么也看不出来【数字和英文不受影响】。那就需要设置 ls 命令的参数，让它按照 Git 客户端的编码来显示，不支持的字符也要显示，这样再使用 ls 命令的时候，就会自动把 GBK 编码转为 UTF-8 编码，那么带有中文的目录、带有中文的文件都能正常显示了。最简单的做法，就是需要指定 ls 命令的附加参数【–show-control-chars】，为了方便，直接更改配置文件 /etc/git-completion.bash 【没有的话新建一个既可】，在行尾增加配置项 alias ls=”ls –show-control-chars –color” 。其实就是通过新建别名这个技巧把 ls 命令的含义扩展了，让它可以根据 Git 客户端的编码转换系统的编码【在这里就是把 GBK 转为 UTF-8】。12vi /etc/git-completion.bashalias ls=&quot;ls --show-control-chars --color&quot;更改完成后，可以看到能正常显示系统中的带有中文名称的文件了。但是还要注意一点，如果使用 Git 客户端的 Bash 处理其它命令，一定会乱码的，因为不像 ls 那样做了编码转换。以下 2 例【分别时使用 elasticsearch、java 命令】：那这个现象有没有办法解决呢，网上大多数解决办法都是说把 Git 客户端的编码设置为和 Windows 系统一样，一般设置为 GBK，这显然是又倒退回去了【为了满足 Git 一定要设置为 UTF-8】。其实唯一的解决办法就是从命令的参数下手，把原生的命令利用别名机制给加上编码有关的参数，和修改 ls 命令的做法一致。以下供参考：1234-- 在文件最后追加，不要修改文件原有的内容 vi /etc/bash.bashrcalias javac=&quot;javac -J-Dfile.encoding=UTF-8&quot;alias java=&quot;java -Dfile.encoding=UTF-8&quot;设置 Git接下来就是设置 Git 进行版本控制时使用的编码方式，例如提交信息时支持输入中文日志、输出 log 可以正常显示中文。设置 Git 有两种方式，一种是通过更改配置文件，另一种是通过 Git 自带的命令来配置参数。为了显得没有手动去破坏 Git 的原有配置文件，我就使用 Git 自带的命令来配置编码。当然，通过更改配置文件的方式也会一同描述出来。1、通过命令行把 Git 的各种编码都设置为 UTF-812345git config --global core.quotepath false # 显示 status 编码 git config --global gui.encoding utf-8 # 图形界面编码 git config --global i18n.commit.encoding utf-8 # 处理提交信息编码 git config --global i18n.logoutputencoding utf-8 # 输出 log 编码 export LESSCHARSET=utf-8 # 因为 git log 默认使用 less 分页，所以需要 bash 对 less 命令处理时使用 utf-8 编码 2、如果通过配置文件的方式来更改，则需要编辑配置文件 /etc/gitconfig 【没有则新建一个】，在里面设置以下内容。1234567[core] quotepath = false [gui] encoding = utf-8 [i18n] commitencoding = utf-8 logoutputencoding = utf-8另外还需要在配置文件 /etc/profile 中新增 1export LESSCHARSET=utf-83、特殊说明gui.encoding = utf-8 是为了解决 git gui 和 gitk 中的中文乱码问题，如果发现代码中的注释显示乱码，可以在所属项目的根目录中 .git/config 文件中添加：12[gui] encoding = utf-8i18n.commitencoding = utf-8 是为了设置 commit log 提交时使用 UTF-8 编码。i18n.logoutputencoding = utf-8 是为了保证在 git log 时使用 UTF-8 编码。export LESSCHARSET=utf-8 是为了保证 git log 翻页时使用 UTF-8 编码，这样就可以正常显示中文了【配合前面的 i18n.logoutputencoding 设置】。 验证 add 执行的时候 Git 输出的日志都是中文显示的，特别是带有中文名称的文件。 验证提交时填写日志信息，可以直接填写中文日志，另外 Git 的输出日志也是以中文来显示的，可以看到哪些文件变更了。验证使用 git log 查看历史日志时正常显示中文内容 注意事项1、此外，Cygwin 在 Windows 平台上也有同样的问题，设置方式也是类似的。当然，如果只是查看目录文件，使用基本的命令，请尽量脱离带有中文的目录和带有中文的文件，避免踩坑，这样还可以把编码直接设置为 GBK 了，但是遇到特殊的情况还是脱离不了 UTF-8 编码。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Windows</tag>
        <tag>中文乱码</tag>
        <tag>gbk</tag>
        <tag>utf-8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[麻婆豆腐做法总结]]></title>
    <url>%2F2019031601.html</url>
    <content type="text"><![CDATA[麻婆豆腐是一道川菜，口味特色就是麻、辣、鲜，而且非常下饭。我常听说正宗的麻婆豆腐要用郫县豆瓣酱【回锅肉也是这样】，才能做出来正宗的味道，但是我身边没有那么多材料，只能做一道简单版的麻婆豆腐。本文就记录麻婆豆腐的做法总结。食材准备 食材就很简单了，以下的量为一盘：嫩豆腐一块，一般 2 块钱左右 瘦肉 50 克，剁成肉沫 豆瓣酱，我没有豆瓣酱就用一种混合调味酱【辣椒酱、豆瓣酱、胡椒粉】替代了 辣椒粉，或者辣椒酱 青花椒，最好用青花椒，才够麻的味道【买的散装青花椒里面会有一些其它植物的种子，要细心挑拣出去】一块嫩豆腐 青花椒 制作步骤 初步处理食材 豆腐切小块，稍微焯水，备用。如果豆腐质量比较好的话，可以一整块冲一下水就行，不用再焯水了。我买的这个豆腐有点碱的味道【类似魔芋一样】，所以稍微焯一下水为好。焯水时不能用大火，否则豆腐会碎掉的，也可以在焯水时稍微放一点点盐进去。豆腐切小块 豆腐简单焯水 瘦肉剁成肉沫，稍微腌制一下，备用。我为了保持肉沫的鲜嫩，还裹了一层淀粉液。瘦肉剁成肉沫 肉沫腌制 肉沫裹淀粉液 炒肉沫 锅里放油烧热，稍微多放一点油，然后倒入肉沫翻炒，基本 30 秒就可以了。肉沫下锅 肉沫翻炒 炒豆腐 一般的做法应该是接着放豆瓣酱，炒出红油，然后加青花椒、辣椒粉，加水煮了一段时间后，再下豆腐。但是我就不搞这么复杂的过程了，直接炒一下豆腐，把豆腐和肉沫混合在一起。豆腐下锅炒 豆腐肉沫混合 加混合调味酱、辣椒酱、调味料、青花椒 加水开煮 因为我的豆腐已经焯水了，所以很容易就熟了，接着再加热水煮开，转为小火再煮 5 分钟就行了。切记别加太多热水，否则变汤了，我这个加的有点多，要多煮一会儿水才能蒸发。加热水，刚刚淹没豆腐 煮开后转为小火 小火慢煮 收汁出锅装盘 煮了 5 分钟就可以准备收汁了，接着还要进行勾芡，我使用淀粉液进行勾芡。勾芡完成稍微再煮 30 秒就可以关火，出锅装盘。收汁 调淀粉液，少量淀粉加水 调淀粉液成品 勾芡完成，可以看到有点浓稠 出锅装盘 注意事项1、为了保证肉沫的鲜嫩，千万不要炒太久，下锅后稍微炒一下就行了，因为后续还要加水煮很久呢。我这里没有采用炒豆瓣酱出红油的做法，所以就用淀粉液裹了一下，肉沫炒熟后直接下豆腐。2、切豆腐时豆腐一般都会粘在刀上，所以有一个技巧就是从手心往手指的方向反着切，切完一刀就可以用手压住，这样豆腐就不会粘在刀上了。参考如下图【我是左手持刀，右手压豆腐】：3、收汁时最好使用淀粉液勾芡一下，这样才能保证调味料都裹在豆腐上，达到入味的效果。否则味道可能都遗落在汤汁里面了，导致豆腐没有什么味道。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>麻婆豆腐</tag>
        <tag>豆腐</tag>
        <tag>麻辣豆腐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[青椒肉丝做法总结]]></title>
    <url>%2F2019031101.html</url>
    <content type="text"><![CDATA[青椒肉丝，本来是一道做法非常简单的菜肴，但是想做好却不容易，为什么呢？一是因为肉丝和青椒丝很多人切不出来，可能切出来的是条状的，那就做不出来青椒肉丝；二是因为腌制肉的时候没有进行裹淀粉的步骤【有条件裹鸡蛋清当然更好】，导致肉刚下锅就老，炒不出鲜嫩滑爽的效果；三是因为炒的时候油量和油温没有控制好，导致肉炒老了，不好吃。本文则记录青椒肉丝的炒制过程以及需要注意的地方，请观察一下我是怎么做出来一道家常的青椒肉丝的。食材准备 食材就很简单了，以下是一盘的份量：纯瘦肉 200 克 大青椒一棵 大红椒一棵【为了好看，搭配一棵红椒】淀粉少量、花生油少量 制作步骤 食材处理 主要就是切肉丝，切青椒丝，腌制肉丝，注意肉丝要切细一点。切肉丝的时候先把肉切片【一只手掌按着肉，刀躺着从手掌下划过】，最后用肉片去切肉丝，可以保证肉丝的质量。腌制肉丝的时候除了调味料，还需要加一点点水和淀粉【淀粉不要多放，否则菜炒出来会偏甜】，用手抓均匀，让淀粉液充分裹在肉丝表面【有条件就用鸡蛋清替代更好，但是肉丝少的时候就没必要了，用不完一个鸡蛋】。最后还要加一点点花生油，防止肉丝下锅的时候粘锅。肉丝腌制 10 分钟。青红椒切丝 腌制肉丝加调料 腌制肉丝加水和淀粉 抓均匀后放一点花生油 肉丝炒制，盛出备用 锅里加油，多加一点，先烧热，然后关火让油冷却一下。冷却到 3 成热再倒入肉丝，然后开大火开始翻炒，基本 30 秒就可以把肉丝炒熟了【取决于肉丝切得好不好】。然后盛出备用。油加热后冷却 倒入腌制好的肉丝 不停地翻炒 盛出备用 青椒炒制，混合翻炒 由于一开始加了偏多的油，此时不需要再放油，或者根据实际情况放一点点也行。油烧热后放入青红椒丝，大火快速翻炒，基本 1 分钟以内就可以把青红椒丝炒至断生。关小火，放入备用的肉丝，翻炒几下，开始调味，放入食用盐、鸡精、耗油，接着大火快速翻炒几下，放几滴香醋，准备出锅。放入青红椒丝翻炒 翻炒至断生【为了拍图青红椒丝炒太熟了】放入肉丝调味翻炒 出锅装盘 盛出装盘 还配了一道麻婆豆腐 注意事项1、买肉一定要买纯瘦肉，最好不要带一丝肉筋，并且形状要规整，薄厚均匀，这样才容易切片进而切肉丝。买肉的时候肯定不需要店方帮忙切肉了，因为他们不可能有时间给你切肉丝出来。此外一定要保证刀比较锋利，锋利的刀更容易处理，如果刀用了很久都没磨过，恰好找这个机会磨磨刀。2、腌制肉丝的时候可以适当加一点水【用来溶解淀粉】，然后加一点淀粉，抓均匀，让淀粉充分裹上肉丝。当然，有条件的直接使用鸡蛋清最好，不需要水和淀粉了，味道还更香。抓均匀后再加一点花生油，防止下锅的时候粘锅。3、肉丝下锅之前要确保油温不高，如果油温过高要开小火让油冷却一下，否则裹着淀粉的肉丝一下锅表面就会糊掉。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>青椒肉丝</tag>
        <tag>青红椒</tag>
        <tag>炒肉丝</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[腊肠炒饭做法总结]]></title>
    <url>%2F2019030901.html</url>
    <content type="text"><![CDATA[蛋炒饭，是一种常见的菜肴，日常生活中听到的最多的就是扬州炒饭、传统炒饭。其实炒饭的做法非常多，口味也非常多，还被改成了很多版本，例如：虾仁炒饭、滑蛋炒蛋、老干妈炒饭。但无论怎么改变，它们的共同点都是做法简单，准备米饭和配菜就行了，炒出来吃起来香喷喷的，口感极好，也不用单独配其它的菜了，很方便。本文就记录腊肠炒饭的家常做法。食材准备 以下准备的是两人份的食材：腊肠，我这里选择的皇上皇腊肠，口味偏甜了，不适合炒饭，去买的时候广州酒家的咸香腊肠卖光了，先凑合着用 鸡蛋 2 个 米饭 2 小碗 青菜 1 小棵，普通的青菜即可，或者放绿豌豆也行，主要为了点缀一下 胡萝卜 1 小段 腊肠，我买的这种偏甜了，不适合炒饭，下次还是买广州酒家的咸香腊肠比较好。米饭 全部食材 制作步骤 1、处理配菜 配菜全部都切好备用：青菜切碎，腊肠切斜片，胡萝卜切丁，鸡蛋打入碗里搅拌，米饭捣碎。这里需要注意的有三点：一是胡萝卜要去皮，更能凸显颜色，而且没有胡萝卜皮的影响，炒饭吃起来口感也更好。二是米饭要捣碎，让米粒分开，不要一整块下锅，所以最好选择比较干硬的米饭来做炒饭，炒起来更方便，口感也更好。三是搅拌鸡蛋之前可以稍微放一点点食用盐，这样做为了鸡蛋更入味。2、炒鸡蛋备用 开火，锅里放油，一定要多放点油，因为鸡蛋非常吃油。放多点没关系，因为后续炒饭可以少放点。油烧热后，把鸡蛋液倒放进去，摇晃炒锅，让鸡蛋在里面呈圆形，防止鸡蛋液堆在一起。如果鸡蛋液太多了，可以在底部的鸡蛋液成型后，用锅铲拨到一边，让上边的鸡蛋液流下来，继续成型。鸡蛋基本成型后，就可以用锅铲搅拌捣碎。实际上用炒勺做就方便一点，如果是用锅铲就不太方便。这里需要注意，鸡蛋不要炒制太熟，要让它保持嫩嫩的，因为等一下还要和米饭一起重新下锅。捣碎后盛出备用，如果有时间的话，可以把鸡蛋的蛋清和蛋黄分别炒制，做出来的颜色会更好看。3、炒配菜和饭 鸡蛋盛出后，锅里表面其实还有大量的油，接着再稍微放一点点油就行了。继续加热，放入腊肠片、胡萝卜、碎青菜，大火炒 30 秒。加腊肠片 加胡萝卜青菜【我忘记放碎青菜了，出锅前才补上，看后面的图】接着就开始加入米饭和刚才的鸡蛋，我这个米饭看起来是一块一块的，其实一碰就散了，开大火不停地翻炒。翻炒 3 分钟左右【如果一开始米饭没有捣碎，或者刚从冰箱拿出来的冷米饭，炒起来会比较慢】，基本就熟了，关小火，准备调味。加入米饭和鸡蛋 炒熟了，准备调味 4、调味出锅 放入盐、鸡精、生抽、老抽，然后继续开大火，翻炒几十秒，出锅装盘，美滋滋。我还放了一点榨菜和辣椒酱。放入调味料，因为放了老抽，可以看到颜色有一点点变化 前面忘记放青菜了，补回来 出锅装盘，这图片有点糊了 注意事项1、米饭的选择，米饭不是随便都适合做炒饭的，要选择那种稍微干硬一点的，米粒都分开的，炒出来会更香。如果是剩米饭，已经是一整块了，千万不要一整块的下锅，很难分开，要提前捣碎，处理好再下锅炒。2、一开始炒鸡蛋的时候，不要炒制太熟，要保持嫩嫩的，因为后面还要和米饭一起下锅。3、腊肠的选择，不要选择偏甜的口味，否则最终的炒饭吃起来会有点腻，所以还是选择咸香的口味比较好。广州酒家的那种咸香的腊肠，用来炒饭真的很合适。4、关于分量的建议，一般做炒饭至少两人的份量。因为如果只炒一份，各种菜只能放一点，放多了就不是炒饭了。那问题来了，剩下的菜不好办【半个胡萝卜、半棵青菜】，又不能存放太久，只能下次接着炒，甚至连续吃炒饭。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>腊肠炒饭</tag>
        <tag>蛋炒饭</tag>
        <tag>鸡蛋炒饭</tag>
        <tag>炒饭</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Github 的 WebHooks 实现代码自动更新]]></title>
    <url>%2F2019030601.html</url>
    <content type="text"><![CDATA[我的静态博客为了百度爬虫单独部署了一个镜像，放在了我的 VPS 上面【在 vultr 购买的主机】，并单独设置了二级域名 blog.playpi.org。但是，每次 GitHub 有新的提交时【基本每周都会有至少三次提交】，为了及时更新，我都会登录到 VPS 上面，到指定的项目下做一下拉取更新的操作，即执行 git pull。这样操作了三五次，我就有点不耐烦了，自己身为做技术的人，怎么能忍受这个呢，做法既低效又不优雅。于是，我就在想有没有更好的方法来实现自动拉取更新。一开始想到，直接在 VPS 起一个周期性脚本不就行了，比如每隔 1 分钟自动执行 git pull，但是立马又被我否定了，虽然做法很简单，但是太不优雅了，而且极大浪费 CPU。后来想到，GitHub 自带了 WebHooks 功能，概念类似于回调钩子，可以给 GitHub 的项目设置各种各样的行为，满足一定的场景才会触发【例如当有新的 push 时，就会向设置的 url 发送请求，并且在请求体中携带 push 的相关信息】。我的自动化构建就是这样的原理，每当 source 分支有提交时，都会通知 tavis-ci【这就是一个行为】，然后在 travis-ci 中设置好脚本，自动运行脚本，就完成了自动生成、部署的操作。根据这个思路，就可以给 GitHub 的项目设置一个 WebHooks，每当 master 分支有提交时【代表着静态博客有更新了】，会根据设置的链接自动发送消息到 VPS 上面，然后 VPS 再执行拉取更新，这样的话就优雅多了。但是问题又来了，满足这种场景还需要在 VPS 设置一个后台服务，用来接收 GitHub 的消息通知并执行拉取更新的操作。我想了一下，既然 VPS 上面已经起了 Nginx 服务，那就要充分利用起来，给 Nginx 设置好反向代理，把指定的请求转给另外一个服务就行了。那这个服务怎么选呢，当然是选择 PHP 后台了，毕竟 PHP 号称世界上最好的语言， PHP 后台搭建起来也容易。本文就记录从基础环境安装配置到成功实现自动拉取更新的整个过程，本文涉及的系统环境是 CentOS 7 x64，软件版本会在操作中具体指明。配置服务器的 PHP 支持 VPS 上面的 Nginx 已经安装好了，就不再赘述过程，不清楚的可以参考我的另外一篇文章：GitHub Pages 禁止百度蜘蛛爬取的问题 。配置 PHP 的后台服务支持主要有三个步骤：一是配置安装 PHP，包括附加模块 PHP-FPM，二是配置启动 PHP-FPM 模块，三是配置重启 Nginx。由于我的机器资源问题【配置太低】，在这个过程踩了很多坑，我也会一一记录下来。 毕竟我是新手，有很多地方不是太懂，所以先参考了官网和一些别人的博客，有时候看多了也会迷惑，有些内容大家描述的不一样，所以要结合自己的实际环境来操作，有些步骤是可以省略的。这些链接我放在这里给大家参考：参考 PHP 官网 、CentOS 7.2 环境搭建实录 (第二章：php 安装) 、PHP-FPM 与 Nginx 的通信机制总结 、 使用 Github 的 WebHooks 实现生产环境代码自动更新 。 先安装软件仓库，我的已经安装好了，重复安装也没影响。1yum -y install epel-release踩着坑安装 PHP1、下载指定版本的 PHP 源码，我这里选择了最新的版本 7.3.3，然后解压。1234-- 下载 wget http://php.net/get/php-7.3.3.tar.gz/from/this/mirror -O ./php-7.3.3.tar.gz-- 解压 tar zxvf php-7.3.3.tar.gz2、configure【配置】，指定 PHP 安装目录【默认是 /usr/local/php，使用 --prefix 参数】和 PHP 配置目录【默认和 PHP 安装目录一致，使用 --with-config-file-path 参数】，我这里特意指定各自的目录，更方便管理。12-- 配置，并且开启 PHP-FPM 模块 [使用 --enable-fpm 参数]./configure --prefix=/site/php/ --with-config-file-path=/site/php/conf/ --enable-fpm遇到报错：configure: error: no acceptable C compiler found in $PATH。竟然缺少 c 编译器，那就安装吧。12-- 安装 gcc 编译器 yum install gcc安装 gcc 编译器成功 安装 gcc 编译器完成后，接着执行配置，又报错：configure: error: libxml2 not found. Please check your libxml2 installation.。这肯定是缺少对应的依赖环境库，接着安装就行。123-- 安装 2 个，环境库 yum install libxml2yum install libxml2-devel -y安装依赖环境库成功 接着就重复上述的配置操作，顺利通过配置。3、编译、安装。12-- 编译，安装一起进行 make &amp;&amp; make install遇到报错：12345cc: internal compiler error: Killed (program cc1)Please submit a full bug report,with preprocessed source if appropriate.See &lt;http://bugzilla.redhat.com/bugzilla&gt; for instructions.make: *** [ext/fileinfo/libmagic/apprentice.lo] Error 1这是由于服务器内存小于 1G 所导致编译占用资源不足【好吧，我的服务器一共就 512M 的内存，当然不足】。解决办法：在配置【configure】参数后面加上一行内容 --disable-fileinfo，减少内存的开销。接着执行编译安装又报错：12345cc: internal compiler error: Killed (program cc1)Please submit a full bug report,with preprocessed source if appropriate.See &lt;http://bugzilla.redhat.com/bugzilla&gt; for instructions.make: *** [Zend/zend_execute.lo] Error 1这是因为虚拟内存不够用，我的主机只有 512M。没办法了，降低版本试试，先降为 v7.0.0【或者开启 swap 试试，后面发现不用了，切换低版本后就成功了】，接着重新下载、配置、编译、安装，从头再来一遍。12-- 下载的时候更改版本号就行 wget http://php.net/get/php-7.0.0.tar.gz/from/this/mirror -O ./php-7.0.0.tar.gz更换了版本后，一切操作都很顺利，就不再考虑开启 swap 了，最终执行编译、安装完成。真正开始配置 配置、编译、安装完成后，开始编辑各个模块的配置文件，更改默认参数，包括配置 PHP 与 PHP-FPM 模块。确认配置无误，再启动对应的服务或者重新加载对应的配置【也可以使用命令验证参数配置是否正确，下文会有描述】。PHP 配置文件 在执行编译安装的目录，复制配置文件 php.ini-development 粘贴到 PHP 的配置目录【如果一开始 configure 时没有显示指定 PHP 的配置目录，默认应该和 PHP 的安装目录一致，也就是要复制粘贴在 /usr/local/php 中，而我指定了 PHP 的配置目录 /site/php/conf】。1cp php.ini-development/site/php/conf/php.ini更改 PHP 的配置文件，修改部分参数，更改 cgi.fix_pathinfo 的值为 0，以避免遭受恶意脚本注入的攻击。12vi /site/php/conf/php.inicgi.fix_pathinfo=0PHP-FPM 配置文件 在 PHP 的安装目录中，找到 etc 目录【如果在一开始的 configure 时没有显示指定 PHP 的安装目录，默认安装在 /usr/local/php 中，则需要到此目录下寻找 etc 目录，而我指定了 PHP 的安装目录 /site/php/】，复制 PHP-FPM 模块的配置文件 php-fpm.conf.default，内容不需要更改。123-- PHP 的附加模块的配置默认安装在了 etc 目录下 cd /site/php/etccp php-fpm.conf.default php-fpm.conf在上面的 etc 目录中，继续复制 PHP-FPM 模块的默认配置文件。因为在上述的配置文件 php-fpm.conf 中，指定了 include=/site/php/etc/php-fpm.d/*.conf，也就是会从此目录 /site/php/etc/php-fpm.d/ 加载多份有效的配置文件，至少要有一份存在，否则后续启动 PHP-FPM 的时候会报错。12-- 先直接使用模板，不改配置参数，后续需要更改用户和组 cp php-fpm.d/www.conf.default php-fpm.d/www.conf配置完成后，开始启动 PHP-FPM 模块，在 PHP 的安装目录中执行。123456789-- PHP 的附加模块的脚本默认安装在了 sbin 目录下 -- 为了方便可以添加环境变量，把 sbin、bin 这 2 个目录都加进去 cd /site/php-- 配置文件合法性测试 ./sbin/php-fpm -t-- 启动，现在还不能使用 service php-fpm start 的方式，因为没有把此模块配置到系统里面 ./sbin/php-fpm-- 检验是否启动 ps aux|grep php-fpm配置文件合法性检测 可以看到正常启动了 那怎么关闭以及重启呢，PHP 5.3.3 以后的 PHP-FPM 模块不再支持 PHP-FPM 以前具有的 ./sbin/php-fpm (start|stop|reload) 等命令，所以不要再看这种古老的命令了，需要使用信号控制：INT，TERM，立刻终止 QUIT 平滑终止USR1 重新打开日志文件USR2 平滑重载所有 worker 进程并重新载入配置和二进制模块 注意，这里的信号标识和 Unix 系统中的一样，被 kill 命令所使用，其中 USR1、USR2 是用户自定义信号，PHP-FPM 模块需要自定义实现，仅供参考。其中，根据 Unix 基础知识，INT【2】表示中断信号，等价于 Ctrl + C，TERM【15】表示终止信号【清除后正常终止，不同于编号 9 KILL 的强制终止而不清除】，QUIT【3】表示退出信号，等价于 Ctrl + \，USR1【10】、USR2【12】这 2 个表示用户自定义信号。所以可以使用命令 kill -INT pid 来停止 PHP-FPM 模块，pid 的值可以使用 ps aux|grep php-fpm 获取。当然，也可以使用 kill -INT pid 配置文件路径 来停止 PHP-FPM 模块，pid 配置文件路径 可以在 php-fpm.conf 中查看，pid 参数 ，默认是关闭的。为了能使用 service php-fpm start|stop|restart|reload 的方式来进行启动、停止、重启、重载配置，这种方式显得优雅，需要把此模块配置到系统里面。在 PHP 的编译安装目录，复制文件 sapi/fpm/init.d.php-fpm ，粘贴到系统指定的目录即可。1234567cd /site/php-7.0.0-- 复制文件 cp sapi/fpm/init.d.php-fpm/etc/init.d/php-fpm-- 添加执行权限 chmod +x /etc/init.d/php-fpm-- 添加服务 chkconfig --add php-fpmNginx 的配置文件 接下来就是更改 Nginx 的配置文件，让 Nginx 支持 PHP 请求，并且同时设置好反向代理，把请求转给 PHP-FPM 模块处理【前提是在不影响 html 请求的情况下】，在 server 中增加一个配置 location。12345678910111213-- 打开配置文件 vi /etc/nginx/nginx.conf-- 更改 server 模块的内容，增加 php 的配置 -- 80 端口就不用管了，直接在 443 端口下配置 location ~* \.php$ &#123; fastcgi_index index.php; fastcgi_pass 127.0.0.1:9000; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param SCRIPT_NAME $fastcgi_script_name; &#125;-- 重新加载 nginx 配置，不需要重启 nginx -s reload这样配置好，就会把所有的 PHP 请求转给 PHP-FPM 模块处理，同时并不会影响原来的 html 请求。额外优化配置项 此外，还有一些环境变量配置、开机启动配置，这里就不再赘述了，这些配置好了可以方便后续的命令简化，不配置也是可以的。12345678910-- 设置开机启动的 chkconfig 方法，以下是添加服务 cp sapi/fpm/init.d.php-fpm/etc/init.d/php-fpmchmod +x /etc/init.d/php-fpmchkconfig --add php-fpm-- 设置开机启动 chkconfig php-fpm on-- 添加环境变量，之后 php 的相关命令就可以直接使用了 vi /etc/profileexport PATH=$PATH:/site/php/bin:/site/php/sbinsource /etc/profilePHP 脚本 先在静态站点的根目录下，添加默认的 index.php 文件，用来测试，内容如下，内容的意思是输出 PHP 的所有信息。注意，PHP 文件的格式是以 &lt;?php 开头，以 ?&gt; 结尾。12vi index.php&lt;?php phpinfo (); ?&gt;打开浏览器访问，可以看到成功，这就代表着 PHP 与 Nginx 的配置都没有问题，已经能正常提供服务。接下来就来测试一下复杂的脚本，可以用来自动拉取 GitHub 的提交。再创建一个 auto_pull.php 文件，内容如下，会自动到执行目录拉取 GitHub 的更新，这样就能实现镜像的自动更新了，还加入了秘钥验证【先不用管功能性是否可用，而是先测试一下复杂的 PHP 脚本能不能正常执行，脚本内容后续还要优化更改】，内容大致如下。123456789101112131415161718192021222324252627282930313233343536vi auto_pull.php&lt;?php// 生产环境 web 目录 $target = &apos;/site/iplaypi.github.io&apos;;// 密钥，验证 GitHub 的请求 $secret = &quot;test666&quot;;// 获取 GitHub 发送的内容 $json = file_get_contents (&apos;php://input&apos;);$content = json_decode ($json, true);// GitHub 发送过来的签名 $signature = $_SERVER [&apos;HTTP_X_HUB_SIGNATURE&apos;];if (!$signature) &#123; return http_response_code (404);&#125;list ($algo, $hash) = explode (&apos;=&apos;, $signature, 2);// 计算签名 $payloadHash = hash_hmac ($algo, $json, $secret);// 获取分支名字 $branch = $content [&apos;ref&apos;];// 判断签名是否匹配，分支是否匹配 if ($hash === $payloadHash &amp;&amp; &apos;refs/heads/master&apos; === $branch) &#123; $cmd = &quot;cd $target &amp;&amp; git pull&quot;; $res = shell_exec ($cmd); $res_log = &apos;Success:&apos;.PHP_EOL; $res_log .= $content [&apos;head_commit&apos;][&apos;committer&apos;][&apos;name&apos;] . &apos; 在 & apos; . date (&apos;Y-m-d H:i:s&apos;) . &apos; 向 & apos; . $content [&apos;repository&apos;][&apos;name&apos;] . &apos; 项目的 & apos; . $content [&apos;ref&apos;] . &apos; 分支 push 了 & apos; . count ($content [&apos;commits&apos;]) . &apos; 个 commit：&apos; . PHP_EOL; $res_log .= $res.PHP_EOL; $res_log .= &apos;=======================================================================&apos;.PHP_EOL; echo $res_log;&#125; else &#123; $res_log = &apos;Error:&apos;.PHP_EOL; $res_log .= $content [&apos;head_commit&apos;][&apos;committer&apos;][&apos;name&apos;] . &apos; 在 & apos; . date (&apos;Y-m-d H:i:s&apos;) . &apos; 向 & apos; . $content [&apos;repository&apos;][&apos;name&apos;] . &apos; 项目的 & apos; . $content [&apos;ref&apos;] . &apos; 分支 push 了 & apos; . count ($content [&apos;commits&apos;]) . &apos; 个 commit：&apos; . PHP_EOL; $res_log .= &apos; 密钥不正确或者分支不是 master, 不能 pull&apos;.PHP_EOL; $res_log .= &apos;=======================================================================&apos;.PHP_EOL; echo $res_log;&#125;?&gt;接下来先手工测试一下 PHP 文件的访问是否正常，可以使用 curl 模拟请求，或者直接使用 GitHub 的 WebHooks 请求。我这里为了简单，先使用 curl 命令来测试，后续的步骤才使用 GitHub 来真正测试。1curl -H &apos;X-Hub-Signature:test&apos; https://blog.playpi.org/auto_pull.php可以看到，访问正常，先不管功能上能不能正常实现，至少保证 PHP 可以正常提供服务，后面会和 GitHub 对接。测试 WebHooks 效果 在 GitHub 中使用 WebHooks，为了表现出它的效果是什么样，我画了一个流程图，可以直观地看到它优雅的工作方式。在上一步骤中，自动拉取更新的脚本已经写好，并且使用 curl 测试过模拟访问可用，那接下来就测试功能是否可用，当然，踩坑是避免不了的，优化脚本内容也是必要的。特别要注意用户权限和脚本内容这两方面，用户权限方面我直接使用 nginx 用户，踩坑比较少，脚本内容方面要保证你的服务器支持 shell_exec () 这个 PHP 函数，可以在 index.php 文件中加一段代码 echo shell_exec (‘ls -la’);，测试一下。我的机器经过测试时支持的。在 GitHub 设置 WebHooks在 GitHub 对应项目的设置【Settings】中，找到 Webhooks 选项，可以看到已经有一些设置完成的 WebHook，这里面就包括 travis-ci 的自动构建配置。然后点击新建按钮，创建一个新的 WebHook【这个过程需要重新填写密码确认】，填写必要的参数，url 地址、秘钥、触发的事件，然后确认保存即可。注意，秘钥只是为了测试使用，实际应用时请更改，包括 WebHooks 的秘钥设置和 PHP 脚本里面的秘钥字符串。如果是第一次创建完成，还没有触发请求的历史记录，可以先手动在 master 分支做一次变更提交，然后就会触发一次 WebHooks 事件。我这里已经有触发历史了，拿一个出来看就行了。注意，为了方便测试，只要有一次请求就行了，因为如果后续更改了脚本，不用再手动向 master 分支做一次变更提交，可以直接点击重新发送【redeliver】。触发请求的信息，就是 http 请求头和请求体 VPS 的 PHP 后台服务返回的信息，可以看到正常处理了 WebHooks 请求，但是没有做拉取更新的操作，原因可能是秘钥不对或者分支不对。 测试功能是否可用 以下内容所需要的 PHP 脚本：index.php、auto_pull.php 。12345&lt;?phpecho shell_exec (&quot;id -a&quot;);echo shell_exec (&apos;ls -la&apos;);phpinfo ();?&gt;12345678910111213141516171819202122232425262728293031323334353637383940&lt;?php// 生产环境 web 目录 $target = &apos;/site/iplaypi.github.io&apos;;// 密钥，验证 GitHub 的请求 $secret = &quot;test666&quot;;// 获取 GitHub 发送的内容，解析 $json = file_get_contents (&apos;php://input&apos;);$content = json_decode ($json, true);// GitHub 发送过来的签名，一定要大写，虽然 http 请求里面是驼峰法命名的 $signature = $_SERVER [&apos;HTTP_X_HUB_SIGNATURE&apos;];if (!$signature) &#123; return http_response_code (404);&#125;// 使用等号分割，得到算法和签名 list ($algo, $hash) = explode (&apos;=&apos;, $signature, 2);// 在本机计算签名 $payloadHash = hash_hmac ($algo, $json, $secret);// 获取分支名字 $branch = $content [&apos;ref&apos;];// 日志内容 $logMessage = &apos;[&apos; . $content [&apos;head_commit&apos;][&apos;committer&apos;][&apos;name&apos;] . &apos;] 在 [&apos; . date (&apos;Y-m-d H:i:s&apos;) . &apos;] 向项目 [&apos; . $content [&apos;repository&apos;][&apos;name&apos;] . &apos;] 的分支 [&apos; . $content [&apos;ref&apos;] . &apos;] push 了 [&apos; . count ($content [&apos;commits&apos;]) . &apos;] 个 commit&apos; . PHP_EOL;$logMessage .= &apos;ret:[&apos; . $content [&apos;ref&apos;] . &apos;],payloadHash:[&apos; . $payloadHash . &apos;]&apos; . PHP_EOL;// 判断签名是否匹配，分支是否匹配 if ($hash === $payloadHash &amp;&amp; &apos;refs/heads/master&apos; === $branch) &#123; // 增加执行脚本日志重定向输出到文件 $cmd = &quot;cd $target &amp;&amp; git pull&quot;; $res = shell_exec ($cmd); $res_log = &apos;Success:&apos; . PHP_EOL; $res_log .= $logMessage; $res_log .= $res . PHP_EOL; $res_log .= &apos;=======================================================================&apos;.PHP_EOL; echo $res_log;&#125; else &#123; $res_log = &apos;Error:&apos; . PHP_EOL; $res_log .= $logMessage; $res_log .= &apos; 密钥不正确或者分支不是 master, 不能 pull&apos; . PHP_EOL; $res_log .= &apos;=======================================================================&apos;.PHP_EOL; echo $res_log;&#125;?&gt;上面已经测试了访问正常，但是为了保证 PHP 脚本的功能正常执行，接下来要优化 PHP 脚本内容了。我分析一下，根据脚本的内容，只有当秘钥正确并且当前变更的分支是 master 时才会执行拉取更新操作，看返回结果也是这样的。当前没有执行拉取更新的操作，但是我的这一个触发通知里面是表明了 master 分支【根据 ref 参数】，那就是秘钥的问题了，需要详细看一下秘钥计算的那段 PHP 代码。如果怕麻烦，直接把加密这个流程去掉【会导致恶意请求，浪费 CPU 资源】，GitHub 并没有要求一定要填写秘钥，但是我为了安全，仍旧填写。我看了一下代码，并没有发现问题，于是加日志把后台处理的一些结果返回，看看哪里出问题了。最终发现竟然是分支名字的问题，PHP 代码通过 $content 没有获取到任何内容，包括分支名字、项目名字、提交信息等，而秘钥签名的处理是正常的。思考了一下，然后我就发现，竟然是创建 WebHooks 的时候内容传输类型【Content type】设置错误，不能使用默认的，要设置为 application/json，否则后台的 PHP 代码处理不了内容解析，获取的全部是空内容。好，一切准备就绪，再来试一次，问题又来了，果然用户权限问题是逃不了的。这个问题我早有防备，本质就是没有设置好 PHP 的用户，导致 PHP 执行脚本的时候，没有权限获取与 Git 有关的信息【执行脚本的用户没有自己的家目录，也没有存储 ssh 认证信息】。接下来就简单了，去设置 PHP 的执行用户，可能还要涉及到 Nginx。先在原先的 index.php 脚本中增加内容 echo shell_exec (“id -a”);，用来输出当前用户信息，发现是 nobody，那就和我想的一样了。为了规范起来便于管理，还是改为和 Nginx 同一个用户比较好，还记得 PHP-FPM 模块的配置文件吗 /site/php/etc/php-fpm.d/www.conf ，去里面找到用户和组的配置项 user、group，把 nobody 改为 nginx。为什么选择 nginx 用户呢，因为我的 Nginx 服务使用的就是 nginx 用户，这样就不用再创建一个用户了，可以去配置文件 /etc/nginx/nginx.conf 里面查看。其实，用户的设置是随意的，如果把 PHP-FPM 的用户设置为 root 更方便，但是这样有很大风险，所以不要这么做。如果非要使用 nobody 也是可以的，我只是为了方便管理用户，和 Nginx 服务共同使用一个用户。一切配置完成后别忘记重启 PHP-FPM 模块。接着就是最重要的步骤了，把本地的 GitHub 项目所属用户设置为 nginx，并且保证 nginx 用户的家目录有 ssh 认证相关的秘钥信息，这样在以后的自动拉取更新时才能畅通无阻。我把原先的项目删掉，然后使用 sudo 命令给 nginx 用户生成 ssh 认证信息，并且重新克隆项目，克隆的同时指定所属用户为 nginx。【由于用户 nginx 没有登录 Shell 的权限，所以不能直接使用 nginx 用户登录后再操作的方式解决】12345678910-- 目录不存在先创建，赋给 nginx 用户权限 mkdir -p /home/nginx/.ssh/chown nginx:nginx -R /home/nginx/.ssh/-- H 参数表示设置家目录环境，u 参数表示用户名 cd /site/sudo -Hu nginx ssh-keygen -t rsa -C &quot;plapyi@qq.com&quot;sudo -Hu nginx git clone https://github.com/iplaypi/iplaypi.github.io.git-- 如果没有 iplaypi.github.io 目录的权限，也要赋予 nginx 用户 mkdir iplaypi.github.iochown nginx:nginx iplaypi.github.io好，一切准备就绪，我再来试一次。可以看到，完美执行，热泪盈眶。为了方便，本来我把这 2 个 php 文件直接放在项目里面了，放在 source 分支，再更新一下 travis-ci 的配置文件，把它们提交到 master 分支去。但是这样做的风险就是把秘钥暴露出去了，显然不可取，所以折中的办法就是把这 2 个文件当做模板，把秘钥隐去，放在 source 分支，以后用的时候直接复制就行了。我想了一下，这个秘钥哪怕暴露出去看起来也没有什么大的危害，除了能伪造请求，产生多余的 pull 操作，浪费机器资源。这里面还有一点需要注意，请确保 iplaypi.github.io 目录里面的文件特别是 auto_pull.php 文件的所属用户都是 nginx，主要是因为 php 脚本里面有 pull 操作，如果有文件所属用户不是 nginx，会导致 pull 时因为文件覆盖的权限问题而失败。而且，失败了也没有错误信息【不像前面的 Permission denied 有提示】，也就是说在 GitHub 里面的 WebHook 查看返回日志是看不到错误信息的，一片空白，说明没有成功 pull。我就遇到了这个问题，找不到原因折腾了很久，最后发现是文件权限的问题。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Github</tag>
        <tag>PHP</tag>
        <tag>WebHooks</tag>
        <tag>自动更新</tag>
        <tag>钩子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx 配置 SSL 证书实现 HTTPS 访问]]></title>
    <url>%2F2019030501.html</url>
    <content type="text"><![CDATA[由于 GitHub Pages 把百度爬虫屏蔽了，导致百度爬虫爬取不到我的个人主页，所以被百度收录的内容很少，能收录的基本都是我手动提交的。后来我的解决办法就是自己搭建了一台 Web 服务器，然后在 DNSPod 中把百度爬虫的访问流量引到我的 Web 服务器上面，服务器主机是我自己购买的 VPS，服务器应用我选择的是强大的 Nginx。本文就记录 Web 服务器搭建以及配置 SSL 证书这个过程。安装 NginxNginx 官方网站：https://www.nginx.com/resources/wiki/start/topics/tutorials/install 。我的 VPS 是 CentOS 7 X64 版本的，所以安装 Nginx 的过程比较麻烦一点，需要自己下载源码、编译、安装，如果需要用到附加模块【例如 http_ssl 证书模块】，还需要重新编译，整个过程比较耗时。如果不熟悉的话，遇到问题也要折腾半天才能解决。所以，我在不熟悉的 Nginx 的情况下选择了一种简单的方式，直接自动安装，并自带了一些常用的模块，例如 ssl 证书模块。但是缺点就是安装过程稍微长一点，在网络好的情况下可能需要 3-5 分钟。我还参考了别人的文档：https://gist.github.com/ifels/c8cfdfe249e27ffa9ba1 ，但是仅供参考，因为我发现也有一些不能使用的地方。创建源配置文件 在 /etc/yum.repos.d/ 目录下创建一个源配置文件 nginx.repo，如果不存在这个目录，先使用 mkdir 命令创建目录，然后在目录中添加一个文件 nginx.repo，使用命令：1vi nginx.repo进入编辑模式，填写如下内容：12345[nginx]name=nginx repobaseurl=http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck=0enabled=1编辑完成后保存即可。自动安装 Nginx接下来就是使用命令自动安装 Nginx 了【敲下命令，看着就行了，会有刷屏的日志输出】：1yum install nginx -y安装完成后，使用以下命令启动：1service nginx start可以使用命令 service nginx status 查看 Nginx 是否启动：然后你就能看到 Nginx 的主页了，默认是 80 端口，直接使用 ip 访问即可【如果这里打不开，可能是端口 80 没有开启，被防火墙禁用了，需要重新开启，开启方法参考后面的章节】。获取 SSL 证书、配置参数 SSL 证书获取 证书的获取可以参考我的文章：利用阿里云申请免费的 SSL 证书 。我在阿里云获取的证书是免费的、有效期一年的，等证书过期了可以重新申请【不知道能不能自动续期】，因为我有阿里云的帐号，所以就直接使用了。当然，通过其它方式也可以获取 SSL 证书，大家自行选择。 直接下载即可，下载后上传到站点的任意目录，但是要记住文件的位置，因为等一下配置 Nginx 的时候需要指定证书的位置。我把它们放在了 /site/ 目录，一共有 2 个文件：.key 文件时私钥文件，.pem 文件时公钥文件。Nginx 参数配置 更改配置文件，打开文件【使用 vi 命令会自动创建不存在的文件】，进入编辑模式：12# 配置 vi /etc/nginx/nginx.conf填写内容如下【我这里只是配置基本的参数 server 有关内容，大家当然可以根据实际需要配置更为丰富的参数】，留意证书的公钥与私钥这 2 个文件的配置：123456789101112131415161718192021222324252627# 80 端口是用来接收基本的 http 请求，里面做了永久重定向，重定向到 https 的链接 server &#123; listen 80; server_name blog.playpi.org; access_log /site/iplaypi.github.io.http-blog-access.log main; rewrite ^/(.*)$ https://blog.playpi.org/$1 permanent; &#125;# 443 端口是用来接收 https 请求的 server &#123; listen 443 ssl;# 监听端口 server_name blog.playpi.org;# 域名 access_log /site/iplaypi.github.io.https-blog-access.log main; root /site/iplaypi.github.io; ssl_certificate /site/1883927_blog.playpi.org.pem;# 证书路径 ssl_certificate_key /site/1883927_blog.playpi.org.key;#key 路径 ssl_session_cache shared:SSL:1m;# 储存 SSL 会话的缓存类型和大小 ssl_session_timeout 5m;# 配置会话超时时间 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;# 为建立安全连接，服务器所允许的密码格式列表 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on;# 依赖 SSLv3 和 TLSv1 协议的服务器密码将优先于客户端密码 #减少点击劫持 add_header X-Frame-Options DENY; #禁止服务器自动解析资源类型 add_header X-Content-Type-Options nosniff; #防 XSS 攻击 add_header X-Xss-Protection 1; &#125;只要按照如上的配置，就可以同时接收 http 请求与 https 请求【实际上 http 的请求被永久重定向到了 https】，我的配置如下图【请忽略 www 二级域名的配置项】：验证参数是否准确 有时候配置了参数，可能因为字符、参数名问题导致启动失败，然后再回来改配置文件，比较繁琐，所以可以直接使用 Nginx 提供的命令来验证配置文件的内容是否合法，如果有问题可以在输出警告日志中看到，改起来也非常方便。1nginx -t可以看到，配置项正常，接下来就可以启动 Nginx 了。开启端口、启动 Nginx在上面的步骤中，如果在一开始想启动 Nginx，虽然启动成功了，但是却访问不了 Nginx 的主页，那很大可能是服务器的端口没有开启，导致访问请求被拒绝，所以需要适当开启必要的端口【如果没有安装防火墙工具 firewall 请自行安装】。1234567891011121314# 查看已经开启的端口 firewall-cmd --list-ports# 开启端口 80firewall-cmd --permanent --zone=public --add-port=80/tcp# 开启端口 443firewall-cmd --permanent --zone=public --add-port=443/tcp# 重载更新的端口信息 firewall-cmd --reload# 这种方式可以，启动 Nginxservice nginx start# 停止 Nginxservice nginx stop# 如果需要重启，直接使用下面的更方便 nginx -s reload大家看一下我的服务器的端口开启信息：验证站点 打开站点 https://blog.playpi.org ，可以愉快地访问了，可以看到 https 链接的绿锁。接着查看一下 SSL 证书的信息。题外话 重定向问题思考 关于开启 https 的访问，我一开始也配置了 www 的二级域名，但是通过日志发现没有通过 301 重定向访问 https://www.playpi.org 的请求，一直不明白原因。后来发现，因为做重定向的时候还是重定向到 GitHub 上面了。同理，如果使用 ip 直接访问，可以观察到自动跳转到 https://www.playpi.org 了，查看证书还是 GitHub 的证书。所以后来直接把百度爬虫的请求转发到 blog 的二级域名还是明智的【www 的二级域名就不用自己再搞一套了】，否则百度爬虫还是抓取不到。如果百度爬虫直接使用 https 链接抓取还是可以的，但是看百度站长里面的说明，是通过 http 的 301 重定向抓取的。Nginx 的 https 模块安装 由于我使用的是简单小白的安装方式，不需要关心额外用到的模块，例如 http_ssl 模块，因为安装包里面自带了这个模块，可以使用 nginx -V 命令查看。因此，如果大家有使用源码编译安装的方式，注意 https 模块不能缺失，否则不能开启 https 的方式。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>https</tag>
        <tag>ssl</tag>
        <tag>证书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用阿里云申请免费的 SSL 证书]]></title>
    <url>%2F2019030401.html</url>
    <content type="text"><![CDATA[在搭建博客的过程中，一开始是全部使用 GitHub，因为这样做就什么也不用考虑了，例如主机、带宽、SSL 证书，全部都交给 GitHub 了，自己唯一需要做的就是写 Markdown 文档。但是，后来发现 GitHub 把百度爬虫给禁止了，也就是百度爬虫爬取不到 GitHub 的内容，导致我的站点没有被百度收录。后来为了专门给百度爬虫搭建一条线路，自己搭建了一个镜像服务，也就是和 GitHub 上面的内容一模一样站点，是专门给百度爬虫使用的。而且，为了测试方便，在 DNSPod 中还增加了一条 blog 二级域名的解析记录，blog 的访问全导向自己的镜像，这样就可以方便观察部署是否成功。后来还把百度爬虫的 www 访问通过 CNANE 跳转到 blog 去，这样就不用单独再搞一个 www 了，因为挺麻烦的（域名解析线路问题、测试问题、证书确认问题，都挺麻烦）。而在这个过程中，就产生了使用阿里云申请免费的 SSL 证书这一流程（有效期一年），记录下来给大家参考。注册阿里云、开启实名认证 这个步骤就不多说了，需要证书总得注册一个帐号吧，也方便后续管理。此外，国内的证书服务商都要求实名认证，这个也没办法。如果不想实名认证，可以使用开源的 Lets Encrypt ，只不过有效期只能是 3 个月，也就是说每隔 3 个月就要更新一次，GitHub Pages 使用的就是它。阿里云的官网链接：https://www.aliyun.com 。购买 SSL 证书 1、在阿里云系统找到关于 SSL 证书的服务， 产品与服务 -&gt; 安全（云盾）-&gt;SSL 证书（应用安全）。2、进入后，点击右上角的 购买证书 。3、按照我截图中的步骤 1、2、3 选择，这里需要注意，这个免费的选项隐藏的很深，直接勾选是不会出现的，要按照我标识的步骤来勾选才行，这里看到出现的费用很贵不用害怕，等一下接着选择对了就会免费的。最终选择 免费型 DV SSL，按照我下图中的选项，可以看到费用是 0 元。选择后，下单即可，虽然要走购买流程，但是是不用付钱的。绑定证书信息、等待审核 1、下单完成后开始 申请 ，这里的 申请 的意思是申请使用它，要填写一些基本的信息，包括个人信息和网站信息，后续还需要验证身份，看你有没有权限管理你配置的网站。如果不申请 使用 ，证书其实就一直闲置在那里。填写个人信息，主要就是我个人的联系方式。填写网站信息，由于我使用的是自己的服务器上面搭建的 Web 服务，既没有使用阿里云也没有使用其它云服务，所以我选择了 文件验证 ，即需要把验证文件上传到我的域名对应的目录下面，用来证明这个站点是我管理的。当然，验证通过后，这个文件可以删除。2、填写完成后，会生成一个文件 fileauthor.txt，我需要把这个文件下载下来，然后上传到我的服务器对应的目录中，才能点击 验证 按钮，如果通过了，说明这个站点就是我管理的，也就是一个权限验证。由于在验证 www 证书对应的文件的时候，需要把 fileauthor.txt 文件上传到服务器，但是由于在 DNSPod 中设置的域名解析是解析到 GitHub 的（没有专门针对阿里的设置），所以总是验证失败。后来就干脆临时把所有的 www 解析都指向我自己的服务器，等通过了验证再改回去，整个过程很是折腾。折腾了一大圈，最后还发现了更简单的方法，直接放弃 www 证书的申请，在 DNSPod 中把百度的流量通过 CNAME 直接引到 blog 上面去就行了，这样只要维护一个 blog 的 Web 服务就行了。这样只需要增加一条解析，而且 blog 的证书验证过程也方便简单。DNSPod 解析示例 在这个过程中，我还发现验证过程需要一定的时间，一开始显示失败，但是不告诉我原因，还以为是自己的服务器的问题，重试了多种方法，包括重启 Web 服务。我等了十几分钟，证书就莫名其妙审核通过了，然后还发送了短信通知（到这里我猜测阿里云的 Web 界面显示的内容是滞后的，短信通知的内容才是实时的）。证书申请成功，可以使用了。下载证书、上传到自己的服务器 下载证书、上传到自己的服务器这一步骤就不多说了，主要就是复制粘贴的工作。着重要说一下 Nginx 的配置，主要就是 server 属性的配置，由于我把 www、blog 这 2 个二级域名都保留了，所以需要分开配置。其实，这里配置的 www 的二级域名根本没有用，因为不会有流量过来的，重在测试证书的安装。Nginx 的配置内容参考（2 个子域名分开配置，有 2 份 SSL 证书）：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253 server &#123; listen 80; server_name www.playpi.org; access_log /site/iplaypi.github.io.http-www-access.log main; rewrite ^/(.*)$ https://www.playpi.org/$1 permanent; &#125; server &#123; listen 80; server_name blog.playpi.org; access_log /site/iplaypi.github.io.http-blog-access.log main; rewrite ^/(.*)$ https://blog.playpi.org/$1 permanent; &#125; server &#123; listen 443 ssl;# 监听端口 server_name www.playpi.org;# 域名 access_log /site/iplaypi.github.io.https-www-access.log main; root /site/iplaypi.github.io; ssl_certificate /site/1884603_www.playpi.org.pem;# 证书路径 ssl_certificate_key /site/1884603_www.playpi.org.key;#key 路径 ssl_session_cache shared:SSL:1m;# 储存 SSL 会话的缓存类型和大小 ssl_session_timeout 5m;# 配置会话超时时间 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;# 为建立安全连接，服务器所允许的密码格式列表 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on;# 依赖 SSLv3 和 TLSv1 协议的服务器密码将优先于客户端密码 #减少点击劫持 add_header X-Frame-Options DENY; #禁止服务器自动解析资源类型 add_header X-Content-Type-Options nosniff; #防 XSS 攻击 add_header X-Xss-Protection 1; &#125;server &#123; listen 443 ssl;# 监听端口 server_name blog.playpi.org;# 域名 access_log /site/iplaypi.github.io.https-blog-access.log main; root /site/iplaypi.github.io; ssl_certificate /site/1883927_blog.playpi.org.pem;# 证书路径 ssl_certificate_key /site/1883927_blog.playpi.org.key;#key 路径 ssl_session_cache shared:SSL:1m;# 储存 SSL 会话的缓存类型和大小 ssl_session_timeout 5m;# 配置会话超时时间 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;# 为建立安全连接，服务器所允许的密码格式列表 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on;# 依赖 SSLv3 和 TLSv1 协议的服务器密码将优先于客户端密码 #减少点击劫持 add_header X-Frame-Options DENY; #禁止服务器自动解析资源类型 add_header X-Content-Type-Options nosniff; #防 XSS 攻击 add_header X-Xss-Protection 1; &#125;配置完成后重启 Nginx（使用 nginx -s reload），去浏览器查看证书信息，看到有效期一年。打开链接，看到左上角的小绿锁，好了，网站是经过验证的了。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>https</tag>
        <tag>阿里云</tag>
        <tag>SSL证书</tag>
        <tag>Lets Encrypt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年关于技术的未解之谜]]></title>
    <url>%2F2019030101.html</url>
    <content type="text"><![CDATA[由于技术能力的限制，平时会遇到一些自己觉得非常诡异的问题，感觉到莫名其妙。其实到头来发现，归根结底还是自己的认知问题：可能是技术水平不够，或者考虑不周全，甚至是一些低级别的错误判断。总而言之，遇到这些问题后，有时候请教人、查资料之后仍旧不得解，只能先记录下来，留做备注说明，等待以后解决。当然，随着时间的流逝，有些问题可能就被忘记了，有些问题在之后的某一个时间点被解决了。本文就是要记录这些问题，并在遇到新问题或者解决老问题之后，保持更新。常用链接 在这里先列出一些常用的网站链接，方便查看：es-hadoop 官网：https://www.elastic.co/guide/en/elasticsearch/hadoop/5.6/configuration.html ；xes-spark 读取 es 数据后 count 报错 使用 es-hadoop 组件，起 Spark 任务去查询 es 数据，然后过滤，过滤后做一个 count 算子，结果就报错了。而且，在报错后又重试了很多次（5 次以上），一直正常，没法重现问题。这个任务需要经常跑，以前从来没遇到过这样的异常，初步怀疑是 es 集群不稳定，具体原因不得而知。错误截图：完整错误信息如下（重要包名称被替换）：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556572019-02-26_15:01:44 [main] ERROR spokesman3.SpokesAndBrand:510: !!!!Spark 出错: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name at [Source: org.apache.commons.httpclient.AutoCloseInputStream@2687cf14; line: 1, column: 17581]org.elasticsearch.hadoop.rest.EsHadoopParsingException: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name at [Source: org.apache.commons.httpclient.AutoCloseInputStream@2687cf14; line: 1, column: 17581] at org.elasticsearch.hadoop.rest.RestClient.parseContent (RestClient.java:171) at org.elasticsearch.hadoop.rest.RestClient.get (RestClient.java:155) at org.elasticsearch.hadoop.rest.RestClient.targetShards (RestClient.java:357) at org.elasticsearch.hadoop.rest.RestRepository.doGetReadTargetShards (RestRepository.java:306) at org.elasticsearch.hadoop.rest.RestRepository.getReadTargetShards (RestRepository.java:297) at org.elasticsearch.hadoop.rest.RestService.findPartitions (RestService.java:241) at org.elasticsearch.spark.rdd.AbstractEsRDD.esPartitions$lzycompute (AbstractEsRDD.scala:73) at org.elasticsearch.spark.rdd.AbstractEsRDD.esPartitions (AbstractEsRDD.scala:72) at org.elasticsearch.spark.rdd.AbstractEsRDD.getPartitions (AbstractEsRDD.scala:44) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply (RDD.scala:239) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply (RDD.scala:237) at scala.Option.getOrElse (Option.scala:120) at org.apache.spark.rdd.RDD.partitions (RDD.scala:237) at org.apache.spark.SparkContext.runJob (SparkContext.scala:1929) at org.apache.spark.rdd.RDD.count (RDD.scala:1157) at org.apache.spark.api.java.JavaRDDLike$class.count (JavaRDDLike.scala:440) at org.apache.spark.api.java.AbstractJavaRDDLike.count (JavaRDDLike.scala:46) at com.package.to.class.SpokesAndBrand.getMention (SpokesAndBrand.java:508) at com.package.to.class.SpokesAndBrand.runCelebrityByBrand (SpokesAndBrand.java:185) at com.package.to.class.SpokesAndBrand.execute (SpokesAndBrand.java:116) at com.package.to.class.SpokesmanAnalyzer.execute (SpokesmanAnalyzer.java:162) at com.package.to.class.SpokesmanAnalyzeCli.execute (SpokesmanAnalyzeCli.java:154) at com.package.to.class.SpokesmanAnalyzeCli.start (SpokesmanAnalyzeCli.java:75) at com.package.to.class.util.AdvCli.initRunner (AdvCli.java:191) at com.package.to.class.job.client.BasicInputOutputSystemWorker.run (BasicInputOutputSystemWorker.java:79) at com.package.to.class.model.AbstractDataReportWorker.run (AbstractDataReportWorker.java:122) at com.package.to.class.buffalo.job.AbstractBUTaskWorker.runTask (AbstractBUTaskWorker.java:63) at com.package.to.class.report.cli.TaskLocalRunnerCli.start (TaskLocalRunnerCli.java:110) at com.package.to.class.util.AdvCli.initRunner (AdvCli.java:191) at com.package.to.class.report.cli.TaskLocalRunnerCli.main (TaskLocalRunnerCli.java:43)Caused by: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name at [Source: org.apache.commons.httpclient.AutoCloseInputStream@2687cf14; line: 1, column: 17581] at org.codehaus.jackson.JsonParser._constructError (JsonParser.java:1433) at org.codehaus.jackson.impl.JsonParserMinimalBase._reportError (JsonParserMinimalBase.java:521) at org.codehaus.jackson.impl.JsonParserMinimalBase._reportInvalidEOF (JsonParserMinimalBase.java:454) at org.codehaus.jackson.impl.Utf8StreamParser.parseEscapedFieldName (Utf8StreamParser.java:1503) at org.codehaus.jackson.impl.Utf8StreamParser.slowParseFieldName (Utf8StreamParser.java:1404) at org.codehaus.jackson.impl.Utf8StreamParser._parseFieldName (Utf8StreamParser.java:1231) at org.codehaus.jackson.impl.Utf8StreamParser.nextToken (Utf8StreamParser.java:495) at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.mapObject (UntypedObjectDeserializer.java:219) at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.deserialize (UntypedObjectDeserializer.java:47) at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.mapArray (UntypedObjectDeserializer.java:165) at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.deserialize (UntypedObjectDeserializer.java:51) at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.mapArray (UntypedObjectDeserializer.java:165) at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.deserialize (UntypedObjectDeserializer.java:51) at org.codehaus.jackson.map.deser.std.MapDeserializer._readAndBind (MapDeserializer.java:319) at org.codehaus.jackson.map.deser.std.MapDeserializer.deserialize (MapDeserializer.java:249) at org.codehaus.jackson.map.deser.std.MapDeserializer.deserialize (MapDeserializer.java:33) at org.codehaus.jackson.map.ObjectMapper._readValue (ObjectMapper.java:2704) at org.codehaus.jackson.map.ObjectMapper.readValue (ObjectMapper.java:1286) at org.elasticsearch.hadoop.rest.RestClient.parseContent (RestClient.java:166) ... 29 more2019-02-26_15:01:44 [main] INFO rdd.JavaEsRDD:58: Removing RDD 3086 from persistence listHexo 生成 html 静态页面目录锚点失效 我这些所有的博客文档是先写成 Markdown 文件，然后使用 Hexo 渲染生成 html 静态页面，再发布到 GitHub Pages 上面，还有一些是发布到我自己的 VPS 上面（为了百度爬虫）。但是最近我发现一个现象，有一些文章的锚点无效，也就是表现为目录无法跳转，例如想直接查看某一级目录的内容，在右侧的 文章目录 中直接点击对应的标题，不会自动跳转过去。这个问题我发现了很久，但是一直没在意，也没有找到原因。最近才碰巧发现是因为标题内容里面有空格，这才导致生成的 html 静态页面里面的锚点失效，我随机又测试了几次其它的页面，看起来的确是这样。下面列出一些示例：123https://www.playpi.org/2019022501.html ，Hexo 踩坑记录的 https://www.playpi.org/2018121901.html ，js 字符串分割方法 https://www.playpi.org/2019020701.html ，itchat 0 - 初识 但是，我又发现其他人的博客，目录标题内容中也有空格，却可以正常跳转，我很疑惑。现在我猜测是 Hexo 的问题，或者哪里需要配置，等待以后的解决方法吧。别人的博客示例：https://blog.itnote.me/Hexo/hexo-chinese-english-space/ 。邮件依赖的诡异异常 在项目中新引入了邮件相关的依赖【没有其它任何变化】，这样就可以在需要时发送通知邮件，依赖内容如下：123456&lt;!-- 邮件相关依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-email&lt;/artifactId&gt; &lt;version&gt;1.3.3&lt;/version&gt;&lt;/dependency&gt;然后神奇的事情发生了，实际执行时，程序抛出异常【去掉这个依赖则正常】：123456789101112131415161718Exception in thread &quot;main&quot; java.lang.StackOverflowError at sun.nio.cs.UTF_8$Encoder.encodeLoop (UTF_8.java:619) at java.nio.charset.CharsetEncoder.encode (CharsetEncoder.java:561) at sun.nio.cs.StreamEncoder.implWrite (StreamEncoder.java:271) at sun.nio.cs.StreamEncoder.write (StreamEncoder.java:125) at java.io.OutputStreamWriter.write (OutputStreamWriter.java:207) at java.io.BufferedWriter.flushBuffer (BufferedWriter.java:129) at java.io.PrintStream.write (PrintStream.java:526) at java.io.PrintStream.print (PrintStream.java:669) at java.io.PrintStream.println (PrintStream.java:806) at org.slf4j.impl.SimpleLogger.write (SimpleLogger.java:381) at org.slf4j.impl.SimpleLogger.log (SimpleLogger.java:376) at org.slf4j.impl.SimpleLogger.info (SimpleLogger.java:538) at org.apache.maven.cli.logging.Slf4jLogger.info (Slf4jLogger.java:59) at org.codehaus.plexus.archiver.AbstractArchiver$1.hasNext (AbstractArchiver.java:464) at org.codehaus.plexus.archiver.AbstractArchiver$1.hasNext (AbstractArchiver.java:467) at org.codehaus.plexus.archiver.AbstractArchiver$1.hasNext (AbstractArchiver.java:467) at org.codehaus.plexus.archiver.AbstractArchiver$1.hasNext (AbstractArchiver.java:467)而根据这个异常信息，我搜索不到任何有效的信息，一直无法解决。最后，我对比了其它项目的配置，发现 手动设置 maven-assembly-plugin 插件的版本为 2.6 即可。而之前是没有设置这个版本号的，默认去仓库获取的最新版本，这个默认的版本可能刚好有问题。Python 入门踩坑 在一开始使用 Python 的时候，没有使用类似 Anaconda、Winpython 这种套件来帮我自动管理 Python 的第三方工具库，而是从 Python 安装开始，用到什么再用 pip 安装什么。整个过程真的可以把人搞崩溃，工具库之间的传递依赖、版本的不兼容等问题，令人望而却步，下面给出一些难忘的经历。出现错误：12Install packages failed: Installing packages: error occurrednumpy.distutils.system_info.NotFoundError: no lapack/blas resources found需要先手动安装 numpy+mkl，再手动安装 scipy，下载文件链接：http://www.lfd.uci.edu/~gohlke/pythonlibs 。我下载了 2 个文件：numpy-1.11.3+mkl-cp27-cp27m-win32.whl、scipy-0.19.0-cp27-cp27m-win32.whl，然后手动安装。一开始我下载的是 64 位的安装包，结果发现我的 Windows 安装的 Python 是 32 位的，导致不支持【下载时没有选择位数，直接下载的默认的包】。另外，直接进入 Python 的命令行环境时也会打印出版本信息的。使用 import pip; print (pip.pep425tags.get_supported ()); 可以获取到 pip 支持的文件名和版本。注意安装 scipy 之前还需要各种第三方库，官方介绍：Install numpy+mkl before installing scipy.。在 Shell 中验证安装第三方库是否成功，例如 numpy：from numpy import *。scipy 包安装：pip install scipy==0.16.1【不推荐】，成功完成安装，如果缺少第三方包会报很多错误。网上查询后的总结：安装 numpy 后安装 scipy 失败，报错：numpy.distutils.system_info.NotFoundError，一般是缺少一些系统库，需要安装：libopenblas-dev、liblapack-dev、libatlas-dev、libblas-dev。常见第三方库介绍：pandas，分析数据 sklearn，机器学习，各种算法jieba，分词工具gensim nlp word2v，模块训练词向量模型scipy，算法库，数学工具包numpy，数据分析matlptop，图形可视化Python 中的编码：2.X 版本，python 编码过程： 输入 –&gt; str –&gt; decode –&gt; unicode –&gt; encode –&gt; str –&gt; 输出。3.X 版本，不一样，直接是 unicode。Python 中代码有 print u’xx’ + yy，yy 是中文，直接跑的时候打印到 Shell 不报错，但是使用后台挂起跑的时候，重定向到文件时，会报错，因为 Python 获取不到输出流的编码。Spark UI 无法显示 使用 yarn-client 模式起了一个 Spark 任务，在 Driver 端看到异常日志：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394952019-01-16_14:53:31 [qtp192486017-1829 - /static/timeline-view.js] WARN servlet.DefaultServlet:587: EXCEPTION java.lang.IllegalArgumentException: MALFORMED at java.util.zip.ZipCoder.toString (ZipCoder.java:58) at java.util.zip.ZipFile.getZipEntry (ZipFile.java:583) at java.util.zip.ZipFile.access$900 (ZipFile.java:60) at java.util.zip.ZipFile$ZipEntryIterator.next (ZipFile.java:539) at java.util.zip.ZipFile$ZipEntryIterator.nextElement (ZipFile.java:514) at java.util.zip.ZipFile$ZipEntryIterator.nextElement (ZipFile.java:495) at java.util.jar.JarFile$JarEntryIterator.next (JarFile.java:257) at java.util.jar.JarFile$JarEntryIterator.nextElement (JarFile.java:266) at java.util.jar.JarFile$JarEntryIterator.nextElement (JarFile.java:247) at org.spark-project.jetty.util.resource.JarFileResource.exists (JarFileResource.java:189) at org.spark-project.jetty.servlet.DefaultServlet.getResource (DefaultServlet.java:398) at org.spark-project.jetty.servlet.DefaultServlet.doGet (DefaultServlet.java:476) at javax.servlet.http.HttpServlet.service (HttpServlet.java:707) at javax.servlet.http.HttpServlet.service (HttpServlet.java:820) at org.spark-project.jetty.servlet.ServletHolder.handle (ServletHolder.java:684) at org.spark-project.jetty.servlet.ServletHandler$CachedChain.doFilter (ServletHandler.java:1507) at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter (AmIpFilter.java:164) at org.spark-project.jetty.servlet.ServletHandler$CachedChain.doFilter (ServletHandler.java:1478) at org.spark-project.jetty.servlet.ServletHandler.doHandle (ServletHandler.java:499) at org.spark-project.jetty.server.handler.ContextHandler.doHandle (ContextHandler.java:1086) at org.spark-project.jetty.servlet.ServletHandler.doScope (ServletHandler.java:427) at org.spark-project.jetty.server.handler.ContextHandler.doScope (ContextHandler.java:1020) at org.spark-project.jetty.server.handler.ScopedHandler.handle (ScopedHandler.java:135) at org.spark-project.jetty.server.handler.GzipHandler.handle (GzipHandler.java:264) at org.spark-project.jetty.server.handler.ContextHandlerCollection.handle (ContextHandlerCollection.java:255) at org.spark-project.jetty.server.handler.HandlerWrapper.handle (HandlerWrapper.java:116) at org.spark-project.jetty.server.Server.handle (Server.java:366) at org.spark-project.jetty.server.AbstractHttpConnection.handleRequest (AbstractHttpConnection.java:494) at org.spark-project.jetty.server.AbstractHttpConnection.headerComplete (AbstractHttpConnection.java:973) at org.spark-project.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete (AbstractHttpConnection.java:1035) at org.spark-project.jetty.http.HttpParser.parseNext (HttpParser.java:641) at org.spark-project.jetty.http.HttpParser.parseAvailable (HttpParser.java:231) at org.spark-project.jetty.server.AsyncHttpConnection.handle (AsyncHttpConnection.java:82) at org.spark-project.jetty.io.nio.SelectChannelEndPoint.handle (SelectChannelEndPoint.java:696) at org.spark-project.jetty.io.nio.SelectChannelEndPoint$1.run (SelectChannelEndPoint.java:53) at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob (QueuedThreadPool.java:608) at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run (QueuedThreadPool.java:543) at java.lang.Thread.run (Thread.java:748)2019-01-16_14:53:31 [qtp192486017-1829 - /static/timeline-view.js] WARN servlet.ServletHandler:592: Error for /static/timeline-view.jsjava.lang.NoClassDefFoundError: org/spark-project/jetty/server/handler/ErrorHandler$ErrorPageMapper at org.spark-project.jetty.server.handler.ErrorHandler.handle (ErrorHandler.java:71) at org.spark-project.jetty.server.Response.sendError (Response.java:349) at javax.servlet.http.HttpServletResponseWrapper.sendError (HttpServletResponseWrapper.java:118) at org.spark-project.jetty.http.gzip.CompressedResponseWrapper.sendError (CompressedResponseWrapper.java:291) at org.spark-project.jetty.servlet.DefaultServlet.doGet (DefaultServlet.java:589) at javax.servlet.http.HttpServlet.service (HttpServlet.java:707) at javax.servlet.http.HttpServlet.service (HttpServlet.java:820) at org.spark-project.jetty.servlet.ServletHolder.handle (ServletHolder.java:684) at org.spark-project.jetty.servlet.ServletHandler$CachedChain.doFilter (ServletHandler.java:1507) at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter (AmIpFilter.java:164) at org.spark-project.jetty.servlet.ServletHandler$CachedChain.doFilter (ServletHandler.java:1478) at org.spark-project.jetty.servlet.ServletHandler.doHandle (ServletHandler.java:499) at org.spark-project.jetty.server.handler.ContextHandler.doHandle (ContextHandler.java:1086) at org.spark-project.jetty.servlet.ServletHandler.doScope (ServletHandler.java:427) at org.spark-project.jetty.server.handler.ContextHandler.doScope (ContextHandler.java:1020) at org.spark-project.jetty.server.handler.ScopedHandler.handle (ScopedHandler.java:135) at org.spark-project.jetty.server.handler.GzipHandler.handle (GzipHandler.java:264) at org.spark-project.jetty.server.handler.ContextHandlerCollection.handle (ContextHandlerCollection.java:255) at org.spark-project.jetty.server.handler.HandlerWrapper.handle (HandlerWrapper.java:116) at org.spark-project.jetty.server.Server.handle (Server.java:366) at org.spark-project.jetty.server.AbstractHttpConnection.handleRequest (AbstractHttpConnection.java:494) at org.spark-project.jetty.server.AbstractHttpConnection.headerComplete (AbstractHttpConnection.java:973) at org.spark-project.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete (AbstractHttpConnection.java:1035) at org.spark-project.jetty.http.HttpParser.parseNext (HttpParser.java:641) at org.spark-project.jetty.http.HttpParser.parseAvailable (HttpParser.java:231) at org.spark-project.jetty.server.AsyncHttpConnection.handle (AsyncHttpConnection.java:82) at org.spark-project.jetty.io.nio.SelectChannelEndPoint.handle (SelectChannelEndPoint.java:696) at org.spark-project.jetty.io.nio.SelectChannelEndPoint$1.run (SelectChannelEndPoint.java:53) at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob (QueuedThreadPool.java:608) at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run (QueuedThreadPool.java:543) at java.lang.Thread.run (Thread.java:748)2019-01-16_14:53:31 [qtp192486017-1829 - /static/timeline-view.js] WARN server.AbstractHttpConnection:552: /static/timeline-view.jsjava.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted () Z at org.spark-project.jetty.servlet.ServletHandler.doHandle (ServletHandler.java:608) at org.spark-project.jetty.server.handler.ContextHandler.doHandle (ContextHandler.java:1086) at org.spark-project.jetty.servlet.ServletHandler.doScope (ServletHandler.java:427) at org.spark-project.jetty.server.handler.ContextHandler.doScope (ContextHandler.java:1020) at org.spark-project.jetty.server.handler.ScopedHandler.handle (ScopedHandler.java:135) at org.spark-project.jetty.server.handler.GzipHandler.handle (GzipHandler.java:264) at org.spark-project.jetty.server.handler.ContextHandlerCollection.handle (ContextHandlerCollection.java:255) at org.spark-project.jetty.server.handler.HandlerWrapper.handle (HandlerWrapper.java:116) at org.spark-project.jetty.server.Server.handle (Server.java:366) at org.spark-project.jetty.server.AbstractHttpConnection.handleRequest (AbstractHttpConnection.java:494) at org.spark-project.jetty.server.AbstractHttpConnection.headerComplete (AbstractHttpConnection.java:973) at org.spark-project.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete (AbstractHttpConnection.java:1035) at org.spark-project.jetty.http.HttpParser.parseNext (HttpParser.java:641) at org.spark-project.jetty.http.HttpParser.parseAvailable (HttpParser.java:231) at org.spark-project.jetty.server.AsyncHttpConnection.handle (AsyncHttpConnection.java:82) at org.spark-project.jetty.io.nio.SelectChannelEndPoint.handle (SelectChannelEndPoint.java:696) at org.spark-project.jetty.io.nio.SelectChannelEndPoint$1.run (SelectChannelEndPoint.java:53) at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob (QueuedThreadPool.java:608) at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run (QueuedThreadPool.java:543) at java.lang.Thread.run (Thread.java:748)这个日志在反复打印，也就是在任务的运行过程中，一直都有这个错误。它引发了什么问题呢，我检查了一下，对 Spark 任务的实际功能并没有影响，任务跑完后功能正常实现。但是，我发现在任务的运行过程中，Spark UI 页面打开后不正常显示【异常信息的开头就是关于某个 js 文件问题】：点击进去，直接显示 Error 500：服务器的 Driver 端日志截图：日志截图 1日志截图 2日志截图 3等了几天，又遇到同样的问题，除了这 2 次，其它时间点就没遇到过了：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394952019-01-24_22:51:49 [qtp697001207-1591 - /static/spark-dag-viz.js] WARN servlet.DefaultServlet:587: EXCEPTION java.lang.IllegalArgumentException: MALFORMEDat java.util.zip.ZipCoder.toString (ZipCoder.java:58)at java.util.zip.ZipFile.getZipEntry (ZipFile.java:583)at java.util.zip.ZipFile.access$900 (ZipFile.java:60)at java.util.zip.ZipFile$ZipEntryIterator.next (ZipFile.java:539)at java.util.zip.ZipFile$ZipEntryIterator.nextElement (ZipFile.java:514)at java.util.zip.ZipFile$ZipEntryIterator.nextElement (ZipFile.java:495)at java.util.jar.JarFile$JarEntryIterator.next (JarFile.java:257)at java.util.jar.JarFile$JarEntryIterator.nextElement (JarFile.java:266)at java.util.jar.JarFile$JarEntryIterator.nextElement (JarFile.java:247)at org.spark-project.jetty.util.resource.JarFileResource.exists (JarFileResource.java:189)at org.spark-project.jetty.servlet.DefaultServlet.getResource (DefaultServlet.java:398)at org.spark-project.jetty.servlet.DefaultServlet.doGet (DefaultServlet.java:476)at javax.servlet.http.HttpServlet.service (HttpServlet.java:707)at javax.servlet.http.HttpServlet.service (HttpServlet.java:820)at org.spark-project.jetty.servlet.ServletHolder.handle (ServletHolder.java:684)at org.spark-project.jetty.servlet.ServletHandler$CachedChain.doFilter (ServletHandler.java:1507)at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter (AmIpFilter.java:164)at org.spark-project.jetty.servlet.ServletHandler$CachedChain.doFilter (ServletHandler.java:1478)at org.spark-project.jetty.servlet.ServletHandler.doHandle (ServletHandler.java:499)at org.spark-project.jetty.server.handler.ContextHandler.doHandle (ContextHandler.java:1086)at org.spark-project.jetty.servlet.ServletHandler.doScope (ServletHandler.java:427)at org.spark-project.jetty.server.handler.ContextHandler.doScope (ContextHandler.java:1020)at org.spark-project.jetty.server.handler.ScopedHandler.handle (ScopedHandler.java:135)at org.spark-project.jetty.server.handler.GzipHandler.handle (GzipHandler.java:264)at org.spark-project.jetty.server.handler.ContextHandlerCollection.handle (ContextHandlerCollection.java:255)at org.spark-project.jetty.server.handler.HandlerWrapper.handle (HandlerWrapper.java:116)at org.spark-project.jetty.server.Server.handle (Server.java:366)at org.spark-project.jetty.server.AbstractHttpConnection.handleRequest (AbstractHttpConnection.java:494)at org.spark-project.jetty.server.AbstractHttpConnection.headerComplete (AbstractHttpConnection.java:973)at org.spark-project.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete (AbstractHttpConnection.java:1035)at org.spark-project.jetty.http.HttpParser.parseNext (HttpParser.java:641)at org.spark-project.jetty.http.HttpParser.parseAvailable (HttpParser.java:231)at org.spark-project.jetty.server.AsyncHttpConnection.handle (AsyncHttpConnection.java:82)at org.spark-project.jetty.io.nio.SelectChannelEndPoint.handle (SelectChannelEndPoint.java:696)at org.spark-project.jetty.io.nio.SelectChannelEndPoint$1.run (SelectChannelEndPoint.java:53)at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob (QueuedThreadPool.java:608)at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run (QueuedThreadPool.java:543)at java.lang.Thread.run (Thread.java:748)2019-01-24_22:51:49 [qtp697001207-1591 - /static/spark-dag-viz.js] WARN servlet.ServletHandler:592: Error for /static/spark-dag-viz.jsjava.lang.NoClassDefFoundError: org/spark-project/jetty/server/handler/ErrorHandler$ErrorPageMapperat org.spark-project.jetty.server.handler.ErrorHandler.handle (ErrorHandler.java:71)at org.spark-project.jetty.server.Response.sendError (Response.java:349)at javax.servlet.http.HttpServletResponseWrapper.sendError (HttpServletResponseWrapper.java:118)at org.spark-project.jetty.http.gzip.CompressedResponseWrapper.sendError (CompressedResponseWrapper.java:291)at org.spark-project.jetty.servlet.DefaultServlet.doGet (DefaultServlet.java:589)at javax.servlet.http.HttpServlet.service (HttpServlet.java:707)at javax.servlet.http.HttpServlet.service (HttpServlet.java:820)at org.spark-project.jetty.servlet.ServletHolder.handle (ServletHolder.java:684)at org.spark-project.jetty.servlet.ServletHandler$CachedChain.doFilter (ServletHandler.java:1507)at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter (AmIpFilter.java:164)at org.spark-project.jetty.servlet.ServletHandler$CachedChain.doFilter (ServletHandler.java:1478)at org.spark-project.jetty.servlet.ServletHandler.doHandle (ServletHandler.java:499)at org.spark-project.jetty.server.handler.ContextHandler.doHandle (ContextHandler.java:1086)at org.spark-project.jetty.servlet.ServletHandler.doScope (ServletHandler.java:427)at org.spark-project.jetty.server.handler.ContextHandler.doScope (ContextHandler.java:1020)at org.spark-project.jetty.server.handler.ScopedHandler.handle (ScopedHandler.java:135)at org.spark-project.jetty.server.handler.GzipHandler.handle (GzipHandler.java:264)at org.spark-project.jetty.server.handler.ContextHandlerCollection.handle (ContextHandlerCollection.java:255)at org.spark-project.jetty.server.handler.HandlerWrapper.handle (HandlerWrapper.java:116)at org.spark-project.jetty.server.Server.handle (Server.java:366)at org.spark-project.jetty.server.AbstractHttpConnection.handleRequest (AbstractHttpConnection.java:494)at org.spark-project.jetty.server.AbstractHttpConnection.headerComplete (AbstractHttpConnection.java:973)at org.spark-project.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete (AbstractHttpConnection.java:1035)at org.spark-project.jetty.http.HttpParser.parseNext (HttpParser.java:641)at org.spark-project.jetty.http.HttpParser.parseAvailable (HttpParser.java:231)at org.spark-project.jetty.server.AsyncHttpConnection.handle (AsyncHttpConnection.java:82)at org.spark-project.jetty.io.nio.SelectChannelEndPoint.handle (SelectChannelEndPoint.java:696)at org.spark-project.jetty.io.nio.SelectChannelEndPoint$1.run (SelectChannelEndPoint.java:53)at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob (QueuedThreadPool.java:608)at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run (QueuedThreadPool.java:543)at java.lang.Thread.run (Thread.java:748)2019-01-24_22:51:49 [qtp697001207-1591 - /static/spark-dag-viz.js] WARN server.AbstractHttpConnection:552: /static/spark-dag-viz.jsjava.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted () Zat org.spark-project.jetty.servlet.ServletHandler.doHandle (ServletHandler.java:608)at org.spark-project.jetty.server.handler.ContextHandler.doHandle (ContextHandler.java:1086)at org.spark-project.jetty.servlet.ServletHandler.doScope (ServletHandler.java:427)at org.spark-project.jetty.server.handler.ContextHandler.doScope (ContextHandler.java:1020)at org.spark-project.jetty.server.handler.ScopedHandler.handle (ScopedHandler.java:135)at org.spark-project.jetty.server.handler.GzipHandler.handle (GzipHandler.java:264)at org.spark-project.jetty.server.handler.ContextHandlerCollection.handle (ContextHandlerCollection.java:255)at org.spark-project.jetty.server.handler.HandlerWrapper.handle (HandlerWrapper.java:116)at org.spark-project.jetty.server.Server.handle (Server.java:366)at org.spark-project.jetty.server.AbstractHttpConnection.handleRequest (AbstractHttpConnection.java:494)at org.spark-project.jetty.server.AbstractHttpConnection.headerComplete (AbstractHttpConnection.java:973)at org.spark-project.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete (AbstractHttpConnection.java:1035)at org.spark-project.jetty.http.HttpParser.parseNext (HttpParser.java:641)at org.spark-project.jetty.http.HttpParser.parseAvailable (HttpParser.java:231)at org.spark-project.jetty.server.AsyncHttpConnection.handle (AsyncHttpConnection.java:82)at org.spark-project.jetty.io.nio.SelectChannelEndPoint.handle (SelectChannelEndPoint.java:696)at org.spark-project.jetty.io.nio.SelectChannelEndPoint$1.run (SelectChannelEndPoint.java:53)at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob (QueuedThreadPool.java:608)at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run (QueuedThreadPool.java:543)at java.lang.Thread.run (Thread.java:748)此外，还有一点值得注意，Chrome 浏览器的某些端口是禁止访问的，所以遇到过有一个 Spark 任务使用了 4045 端口【locked】，在 Chrome 浏览器是看不了任务状态的，页面无法打开，被 Chrome 浏览器屏蔽了，此时并不是 Spark 的问题。关于 Git 的小问题 1、本地版本落后，而且又与远程仓库冲突，git pull 报错警告，需要 merge，无法直接更新最新版本。下面的操作直接覆盖本地文件，强制更新到最新版本，本地未提交的更改会丢失。12git fetch --allgit reset --hard origin/master2、在 2018 年 9 月的某一天，发现 Git 的代码推送总是需要输入帐号和密码，哪怕保存下来也不行，每次 push 都需要重新输入，感觉很奇怪。后来发现是版本太旧了，当时的版本是 v2.13.0，升级后的版本是 v2.18.0，升级后就恢复正常了。后来无意间在哪里看到过通知，说是 TSL 协议升级了，所以针对旧版本强制输入用户名密码，升级就可以解决。 备注一下，HTTPS 是在 TCP 和 HTTP 之间增加了 TLS【Transport Layer Security，传输层安全】，提供了内容加密、身份认证和数据完整性三大功能。TLS 的前身是 SSL【Secure Sockets Layer，安全套接字层】，由网景公司开发，后来被 IETF 标准化并改名。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Git</tag>
        <tag>ElasticSearch</tag>
        <tag>es</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 的踩坑经验]]></title>
    <url>%2F2019022501.html</url>
    <content type="text"><![CDATA[大家知道，我是使用 Hexo 来构建我的静态站点的，每次使用 Markdown 语法书写 md 文档即可。写完后在本地使用 hexo g &amp; hexo s 命令【在本地生成并且部署，默认主页是 localhost:4000】来验证一下是否构建正常。如果有问题或者对页面效果不满意就返回重新修改，如果没有问题就准备提交到 GitHub 上面的仓库里面【在某个项目的某个分支】，后续 travid-cli 监控对应的分支变化，然后自动构建，并推送到 master 分支。至此，更新的页面就发布完成了，本人需要做的就是管理书写 md 文档，然后确保没问题就提交到 GitHub 的仓库。问题清单 前言描述的很好，很理想，但是有时候总会出现一些未知的问题，而我又不了解其中的技术，所以解决起来很麻烦，大部分时候都是靠蒙的【当然，也可以直接在 Hexo 的官方项目上提出 Issue，让作者帮忙解决】。下面就记录一些遇到的问题，以及我自己找到的原因。1-Markdown 语法不规范 这个错误有在 travis 上面出现过，在 travis 的 116 号、117 号错误：https://travis-ci.org/iplaypi/iplaypi.github.io/builds/476399853 。在使用 hexo 框架的时候，一定要确保 markdown 文件里面的代码块标识【标记代码的类型，例如：java、bash、html 等】使用正确。否则使用 hexo g 生成静态网页的时候，不会报错，但是却没有成功生成 html 静态网页，虽然 html 静态文件是有的，但是却查看不了，显示一片空白。代码块示例：Java 格式 xml 格式bash 格式 例如我把图一的 java 误写成了 bash，hexo g 的时候没有报错，但是生成的 html 静态网页却是空白一片，打开了什么也看不到。空白页面 但是如果把 java 误写成了 xml，在本地执行 hexo g 的时候不会报错，生成的 html 静态网页也是正常的。而一旦使用 travis-cli 执行自动构建的时候，构建是失败的【在 travis 的 116 号、117 号错误：https://travis-ci.org/iplaypi/iplaypi.github.io/builds/476399853 】，并且可以看到错误信息，图四，但是我看不懂错误原因，只能猜测找到问题所在，比较耗时。travis-cli 报错日志【我看不懂】：travis-cli 日志 1travis-cli 日志 2travis-cli 日志 3此外，写 Markdown 文档，使用代码块标记的时候，使用 3 个反单引号来标记，如果不熟悉代码块里面的编程语言，可以省略类型，例如 java、bash、javascript，不要填写，否则填错了生成的 html 静态文件是空白的。还有就是如果代码块里面放的是一段英文文本，和编程语言无关，也不要填写类型，否则生成的 html 静态文件也是空白的。2-Hexo 报错奇怪 这个错误还没有到 travis 上面，所以 travis 上面没有记录；在本地测试过程中，无论是 hexo s 还是 hexo g 都会报错，错误信息如图：看着这个信息，很像在当前项目的目录中找不到 hexo 命令，和 java 类似，我就怀疑是不是安装的 hexo 被什么时候卸载了，其实不是的，在其它项目中还能用。后来我发现是当前项目使用的模块缺失，为什么会缺失我也不知道，由于这些缺失的模块是通过 hexo 引入的，所以直接报错：hexo not found，给人以误导。总的来说，就是报错有误导性，没有报模块缺失，而我又不懂这些，查了一些资料，手动测试了一些方法，总算找到原因所在。找到原因，那解决办法很简单了，直接安装缺失的模块即可，使用 nmp install 命令安装 package.json 里面的模块。3-Hexo 配置错误引起的误导性 这个错误还没有到 travis 上面，所以 travis 上面没有记录；这个错误和上面的类似，但是如果从报错信息上面看，也具有误导性。在更改了 _config.yml 配置文件后，按照正常步骤去生成、部署的时候【使用 hexo g &amp; hexo s 命令，直接报错了，把我整蒙了，报错信息如下：关键配置部分如下，后续找到问题确实出在这里：从图中看信息，我也看不到什么原因，因为确实不懂。注意，我为了测试，发现 hexo g 是没有问题的，也就是生成没问题，那问题就出在部署步骤了，它会不认这个 hexo s 命令？我查了资料，发现大部分人都说缺失 hexo server 模块，我通过检查可以确保本机有这个模块，而且卸载了重新装，所以不是这个问题。最后发现是配置信息里面的参数【官方定义的关键词】错误了，里面的 Plugins 这个参数应该使用首字母大写，这谁能想到，正确的配置参数如下图：4-travis 配置问题 这个错误有在 travis 上面出现过，在 travis 的 27 号：https://travis-ci.org/iplaypi/iplaypi.github.io/builds/448152737 。在使用 travis 自动构建时，有一次突发奇想，想使用最新版本的 node_js，于是在 travis.yml 配置文件中，把 node_js 设为了 stable，即稳定版本，这样在构建的时候会使用最新稳定版本的 node_js，没想到就出问题了。node_js 的配置如下：travis 报错日志如下：重要部分：12error nunjucks@3.1.3: The engine "node" is incompatible with this module. Expected version "&gt;= 6.9.0 &lt;= 11.0.0-0". Got "11.0.0"error Found incompatible module看来还是在搞清楚新旧版本之间的差异后再想着升级版本，不要随意来，要不然浪费的是自己的时间。后来解决办法就是手动指定 node_js 的版本。5 - 无缘无故出现的问题 这个错误有在 travis 上面出现过，在 travis 的 133 号、134 号错误、135 号错误、136 号错误，举例：https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498318318 ；日志部分截图：这错误信息里面对我来说确实看不到有效的内容，还没找到解决办法，看似是文件路径不存在，但是项目配置也没变过。123456789npm ERR! path /home/travis/.nvm/versions/node/v10.10.0/lib/node_modules/hexo-cli/node_modules/highlight.js/tools/build.jsnpm ERR! code ENOENTnpm ERR! errno -2npm ERR! syscall chmodnpm ERR! enoent ENOENT: no such file or directory, chmod '/home/travis/.nvm/versions/node/v10.10.0/lib/node_modules/hexo-cli/node_modules/highlight.js/tools/build.js'npm ERR! enoent This is related to npm not being able to find a file.npm ERR! enoent npm ERR! A complete log of this run can be found in:npm ERR! /home/travis/.npm/_logs/2019-02-25T18_45_08_713Z-debug.log等待找问题的原因。好，仔细看了日志、找了博客文档，没有解决方法，我也不懂，看到可能是版本原因【我不能升级 nodejs 版本，与 yarn 有关】，可能是权限问题。我用 sudo npm install -g hexo-cli 试了试，明显不行：https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498794142 ，然后我就放弃了，直接改回来提交了，没想到无缘无故就可以了，构建日志：https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498796865 。准备发邮件问问 travis 客服，我现在单方面怀疑是 travis 的环境问题或者构建脚本所依赖的环境问题。由于时差问题，先记录几个时区的缩写，方便查看邮件内容的时候核对时间：UTC【世界标准时间】、EST【东部标准时间，UTC-5】、CET【欧洲中部时间，UTC+1】。我发送的邮件内容如下【发送于北京时间 2019-02-28 14:42:00】：完整文字版供参考 123456789101112131415161718192021Automatic building is failedHello,I hava a repository in GitHub,and i use travis-ci to build it automatically.I configured the correct script,and it has been built successfully more than one hundred times.My script is :https://github.com/iplaypi/iplaypi.github.io/blob/source/.travis.yml ;But it built failed at 2019-02-26,the all log as follows (i retry it three times,but still failed):https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498267014https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498278045https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498297576https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498318318I cannot find any usefuI information in my log.I am very sad and helpless.But i retry it at 2019-02-27,it actually build successfully,amazing.I swear I have not changed any files,the successful log is:https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498796865 ;So i am puzzled,i donnot know why,i suspect it is a problem with the machine.Can you help me?Best wishes. 发送后对方自动有一个回复，告知我他们的工作时间【中国北京时间与对方时差 + 13】：等了好几天，对方终于回复了【回复于北京时间 2019-03-04 11:00:00】，对方回复内容如下：对方回复重要文字内容 123456789Hey there,Thanks for reaching out and I&apos;m sorry for these spurious failures you have experienced.I think it could be an issue with the package itself or the NPM registry on that specific day. I&apos;ve looked at NPM&apos;s status and their was an incident on Feb. 27th. See https://status.npmjs.org/incidents/ptnlj2rtwfwm. Maybe it was already happening on Feb. 26th? Sorry for not having a better explanation.Please let us know if this issue resurfaces again, we would be happy to have another look.Thanks in advance and happy building! 看起来技术支持也没发现是啥问题，只是说有可能是 NPM 的问题，还给了一个链接：https://status.npmjs.org/incidents/ptnlj2rtwfwm，根据链接可以看到 NPM 的状态在某个时间点出问题了【时间点为 2019-02-27 15:46:00 UTC，也就是北京时间 2019-02-27 23:46:00】：但是我那个自动构建的问题是出在北京时间 2019-02-26 凌晨的，时间点也对不上，所以技术支持只是怀疑，也没有结论，那我也就不管了，继续观察以后有没有相同的问题出现。6 - 排版问题 1、在 Markdown 文件中关于链接的，要使用 []、() 这 2 个完整的标记，不要直接放一个链接出来，会导致生成的 html 文件带链接的内容居中对齐，导致文字分散开来，不好看。2、中文括号不要使用，也会导致居中对齐的问题，文字排版不好看，使用方括号吧：【内容示例】。7 - 草稿问题 我在使用 Hexo 的草稿功能时，发现一个问题，操作完成发布时，发现 Markdown 文档的头部描述信息变化了。例如我本来设置的 id 又变回了日期【可以理解，因为模板就是这样设置的】，然后 tags 的中括号中的标签变为了无需列表【不可理解】。暂时还没发现内容的变化，可能是内容中没有特殊符号。导致的问题就是草稿发布后【内容已经变化了】，提交到 source 分支，自动构建时，提交到主分支 master 后，这些文章的链接变为了日期的乱格式【因为是基于错误的 Markdown 文件构建的】。所以以后还是不要使用草稿功能了，没有必要，还麻烦，没写完也可以发布嘛，没啥大问题。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>markdown</tag>
        <tag>java</tag>
        <tag>bash</tag>
        <tag>xml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[鸡蛋饼做法总结]]></title>
    <url>%2F2019021001.html</url>
    <content type="text"><![CDATA[鸡蛋饼算是一种小吃，做法多种多样，可以煎，可以烙，可以蒸；吃法也多种多样，有的地方会卷配菜吃，有的地方会配粥吃，有的地方会直接吃。总而言之，鸡蛋饼算是一种万能美食，全国各地都有，大家也都喜欢吃，本文就记录鸡蛋煎饼的做法总结，本文记录的做法是采用煎的方式，另外还会额外放点葱花。食材准备 以下准备的食材可以煎 15 个鸡蛋饼左右：1、鸡蛋 4 个（喜欢的话多放点也可以）；2、200-300 克面粉（可以煎 15 个左右，面粉不能确定量，是因为如果面糊没有配好，就适量加水或者加面，调整好为止，具体会用到多少看情况了）；3、小葱 5 根（根据个人口味添加，多点少点都行）；适量的食材 制作步骤 制作步骤很简单，只要能煎好一张饼，重复进行就行了，煎好一张饼大概需要 3 分钟。当然，如果是第一次煎饼，可能煎前面几张饼的时候需要练习一下，也可能需要重新调制面糊，所以时间会长一点，但是为了煎饼成功，麻烦一点也值了。调制面糊的过程就不记录了，就是加盐（4 勺）、葱花、面粉、鸡蛋、水（最好可以用凉白开，别直接使用自来水）搅拌即可，如果里面有很多面疙瘩，不用担心，静置 10 分钟搅拌一次，重复 3 次左右面疙瘩即全部溶于水。面糊调制初步，还有很多面疙瘩 面糊调制完成 粘稠度大概这样，不会很粘稠，和液体差不多 1，锅里加油烧热，只要半勺即可（吃饭的那种小汤勺），多了会腻，然后火力转小火，并一直持续小火。 半勺油的量 2，放入一汤勺面糊（烧汤那种大汤勺，或者电饭煲自带的那种粥勺），如果发现煎出来的饼太厚了或者太大了，可以适当少放一点点面糊，具体放多少自己把握。然后适当转动煎锅，让面糊呈圆形（一定要快，10 秒内完成，否则因为受热不均匀，饼可能会散开变成多块，或者是一个圆环饼套着一个小圆饼），等逐渐凝固后就成了圆饼，然后接触锅的那一面就变得金黄，这个过程大概 1 分钟。 一大汤勺面糊 加面糊到锅里 转动成型 3，等凝固后就可以翻身了，这个步骤说简单也简单，说难也难，如果直接用锅不方便翻身的话，可以借助铲子，翻身后，可以看到饼的上一面已经煎好了，金黄的。这个时候注意要适当把饼转动一下，吸收一下油，避免粘锅。 给饼翻身 4，翻身后再煎 1 分钟左右，就可以出锅了，如果看到饼上面哪里煎的不均匀，还没熟，可以再着重煎几十秒。切记不能煎太久，要不然饼就糊了。 翻身后继续煎 1 分钟，再根据实际情况着重煎一下，准备出锅 5，出锅装盘，继续下一张鸡蛋饼。 全部出锅装盘 注意事项1、特别注意，如果调制面糊不是特别熟练的话，可能面糊的粘稠度不适合，或者调味偏淡偏咸，这样都不好，所以最好尝试着煎一个，然后品尝一下，如果味道不对再加调料，如果煎出来的饼不对，再加水或者面粉。多试几次，确保煎出来的饼自己满意。如果一味地煎饼，最后发现不好吃，那就浪费了；2、如果有两个锅可以用，为了节省时间，最好两个锅同时煎，要不然整个过程很枯燥，因为有一半的时间都在等待；3、有时候可能看着好像煎糊了，不用担心，不影响吃，因为出锅后等一会儿，褐色就会变成金黄色，非常好看；4、整个过程一定要确保是小火，否则饼很快就糊了；5、难点在于翻身，只要一出错一张饼就废了（或者变成了一堆碎饼）。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>鸡蛋饼</tag>
        <tag>鸡蛋葱饼</tag>
        <tag>鸡蛋煎饼</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ItChat 系列 0 - 初识 ItChat]]></title>
    <url>%2F2019020701.html</url>
    <content type="text"><![CDATA[微信已经是我们日常生活中常用的 APP 之一，每天都离不开。作为掌握技术的理工科人员，有时候总想着是否可以利用微信的接口完成一些重复的工作，例如群发消息、自动回复、接入机器人自动聊天等。当然，这些都可以实现，而且只要是人工可以做到的事情，基本都可以做到自动化（前提是微信提供了对应的接口，反例就是自动收发红包不行，当然微信不会直接提供 API 接口，需要自己寻找）。本文就讲解为了做到这些，需要的入门知识点，主要就是利用 ItChat 工具（屏蔽了微信的 API 接口，简化了使用微信接口的过程，不懂技术的普通人也可以轻松掌握），当然本文只是一个入门的例子而已（完成后对自己来说很实用而且有成就感），后续会讲解更加深入与广泛的内容。本文基于 Windows 7 操作系统，Python 2.7 版本（为了兼容性与易维护性，我推荐使用 Python 3.x 版本）ItChat 简介 摘录官方文档描述：itchat 是一个开源的微信个人号接口，使用 python 调用微信从未如此简单；使用不到三十行的代码，你就可以完成一个能够处理所有信息的微信机器人；当然，该 api 的使用远不止一个机器人，更多的功能等着你来发现；该接口与公众号接口 itchatmp 共享类似的操作方式，学习一次掌握两个工具；如今微信已经成为了个人社交的很大一部分，希望这个项目能够帮助你扩展你的个人的微信号、方便自己的生活。当然，我是觉得上面的描述有一些语句不通顺，但是不影响我们理解作者的原意。其实微信官方并没有提供详细的 API 接口，ItChat 是利用网页版微信收集了接口信息，然后独立封装一层，屏蔽掉底层的接口信息，提供一套简单的使用接口，方便使用者调用，这不仅提升了效率，还扩展了使用人群。使用入门 以下使用入门包括基础环境的安装、itcaht 的安装、代码的编写、实际运行，当然，为了避免赘述，不会讲解的很详细，如果遇到一些问题，自行利用搜索引擎解决。安装 Python 环境 下载 Python去官网：https://www.python.org/downloads/windows ，选择自己需要的版本，我这里选择 Windows 系统的版本（64 位操作系统），Python 2.7（这是一个很古老的版本了，推荐大家使用 3.x 版本）；我选择的版本 下载过程就和下载普通的文件、视频等一样，根据网速的限制有快有慢。安装 Python就像安装普通程序一样，直接双击下载的程序文件，选择安装即可，这里就不再赘述详细的安装过程了；如果你们的环境不是 Windows 7 系统的，可以自行使用搜索引擎搜索教程；这里一定要注意安装的版本是否适配自己的操作系统（包括系统类型与系统位数）；在 Windows 系统的 程序和功能 中查看已经安装完成的 Python 程序（2.7 版本，我是使用 Anaconda2 安装的，所以看起来有些不一样）：配置环境变量 如果这一步忽略了，使用 Python 或者 Python 自带的插件的时候（比如安装 ItChat 的时候就会用到 pip 工具），会找不到应用程序，只能先进入到 Python 目录或者插件所在的目录再使用对应的工具（例如进入 Python 所在的目录或者 pip 所在的目录），比较麻烦，所以在此建议大家配置一下环境变量；配置环境变量的过程也不再赘述，大家自己利用搜索引擎获取，下图是基于 Windows 7 版本的配置截图示例；系统属性 高级系统设置 环境变量 ，我这里编辑用户环境变量 PATH 的内容（如果不存在就新建，当然编辑系统环境变量 PATH 的内容也是可以的），切记内容一定是英文格式下的，多个使用英文逗号分隔 用户环境变量 ，我这里需要填写 2 条内容，使用英文逗号隔开（如果是直接安装的 Python，pip 和 python 应该在同一个路径下面，所以只需要 1 条就行了）我的环境需要配置 2 条内容 内容解释：1234--pip 所在目录 D:\Anaconda2\Scripts\;--python 所在目录 D:\Anaconda2;安装 ItChat 工具 在 Python 安装完成的情况下，才能进行接下来的操作，因为 ItChat 是基于 Python 环境运行的；为了验证 Python 是否正确安装，可以在命令行中输入 python，如果看到以下内容，就说明 Python 安装成功：接下来利用 pip 工具（Python 自带的）直接安装 itchat，非常简单，使用命令（如果 pip 命令不可用，请检查 Python 的安装目录是否存在 pip.exe 文件）：1pip install itchat安装 ItChat如果看到以下内容，说明 ItChat 安装成功：入门代码示例 一切准备就绪，接下来就可以写代码了，当然，入门代码非常简单实用（我会尽可能多的添加注释说明）：123456789101112131415161718192021222324252627282930313233343536#-*-coding:utf-8 -*-# 从 python 环境中导入 itchat 包，re 正则表达式包 import itchat, re# 从 itchat.content 中导入所有类、常量 (例如代码中的 TEXT 其实就是 itchat.content.TEXT 常量)from itchat.content import *# 导入时间包里面的 sleep 方法 from time import sleep# 导入随机数包 import random# 注册消息类型为文本 (即只监控文本消息，其它的例如语音 / 图片 / 表情包 / 文件都不会监控)# 也就是说只有普通的文字微信消息才能触发以下的代码 # isGroupChat=True 开启群聊模式，即只是监控群聊内容 (如果不开启就监控个人聊天，不监控群聊)@itchat.msg_register ([TEXT], isGroupChat=True)# @itchat.msg_register ([TEXT])def text_reply(msg): # msg 是消息体，msg ['Text'] 用来获取消息内容 # 第一个单引号中的内容是关键词，使用正则匹配，可以自行更改 (我使用.* 表示任意内容), 如果使用中文注意 2.x 版本的 Python 会报错，需要 u 前缀 message = msg ['Text'] print (message) match = re.search ('.*', message) # match = re.search (u'年 | 春 | 快乐', message) # 增加睡眠机制，随机等待一定的秒数 (1-10 秒) 再回复，更像人类 second = random.randint (1,10) sleep (second) if match: # msg ['FromUserName'] 用来获取用户名，发送消息给对方 from_user_name = msg ['FromUserName'] print (from_user_name) itchat.send (('====test message'), from_user_name) # 第一个单引号中的内容是回复的内容，可以自行更改 # 热启动，退出一定时间内重新登录不需要扫码 (其实就是把二维码图片存下来，下次接着使用)itchat.auto_login (hotReload=True)# 开启命令行的二维码 itchat.auto_login (enableCmdQR=True)# 运行 itchat.run ()代码截图如下：演示 登录扫码 登录成功 群聊自动回复（正则是任意内容，所以总是会自动回复）退出 重新登录继续聊天（由于开启了热启动，不需要重新扫码）继续聊天 小问题总结 1、部分系统可能字幅宽度有出入，可以通过将 enableCmdQR 赋值为特定的倍数进行调整：12# 如部分的 linux 系统，块字符的宽度为一个字符 (正常应为两字符), 故赋值为 2itchat.auto_login (enableCmdQR=2)2、Python 2.7 版本的中文报错问题（在 Python 2.7 环境下使用中文需要额外注意，坑比较多）： 例如代码中正则匹配带中文（由于编码问题导致无法匹配，或者会抛出异常）12# 正则搜索带中文，直接单引号在 Python 2.7 环境下是不行的 match = re.search ('年 | 春 | 快乐', message)实际运行时就会报错（报错信息如果不捕捉后台是看不到的）或者匹配结果不是想象中的（仅针对 Python 2.x 环境）需要使用 u 前缀 123# 正则搜索带中文，直接单引号在 Python 2.7 环境下是不行的 # 增加 u 前缀，表示 unicode 编码，才行 match = re.search (u'年 | 春 | 快乐', message)3、如果不开启热启动，每次重新登录时都会生成新的二维码，直接在 Wimdows 的命令行中，可能由于窗口太小显示不完整，此时需要拉伸一下命令行的窗口：4、有些人的电脑设置问题，命令行环境背景为白色，生成的二维码的颜色黑白色是相反的，导致扫码时无法识别，此时需要设置代码：12# 默认控制台背景色为暗色 (黑色)，若背景色为浅色 (白色)，可以将 enableCmdQR 赋值为负值 itchat.auto_login (enableCmdQR=-1) 接入机器人 一般读者做到上面的内容就算入门了，可以实现自动回复，并且关于 ItChat 也了解了一些，可以独自参考文档进行更加深入的开发了。但是，自动回复的内容毕竟太固定了，而且只能覆盖极少的内容，没办法实现真正的自动化。要想做到真正的自动化回复，机器人是少不了了，那么接下来讲解的就是如何接入一个第三方机器人，实现机器人自动回复。当然，代码内容也会稍显复杂，操作步骤也会稍显繁琐。接入机器人代码示例 接入机器人时为了换种方式，先把群聊模式关闭，使用个人聊天监控模式（方便聊天内容的随意性，更能提现机器人的可用性）：1@itchat.msg_register ([TEXT])还要导入网络请求相关的包：1import requests需要使用图灵机器人的核心配置（注册图灵机器人的过程不在此赘述，官网链接：http://www.tuling123.com ）：1234567891011121314151617181920# 封装一个根据内容调用机器人接口，返回回复的方法 def get_response(msg): # 构造了要发送给服务器的数据 apiUrl = 'http://www.tuling123.com/openapi/api' data = &#123; 'key' : APIKEY, 'info' : msg, 'userid' : 'wechat-robot', &#125; try: r = requests.post (apiUrl, data=data).json () # 字典的 get 方法在字典没有 'text' 值的时候会返回 None 而不会抛出异常 return r.get ('text') # 为了防止服务器没有正常响应导致程序异常退出，这里用 try-except 捕获了异常 # 如果服务器没能正常交互 (返回非 json 或无法连接), 那么就会进入下面的 return except Exception,err: # 打印一下错误信息 print (err) # 将会返回一个 None return完整代码示例（代码会封装的更好，格式更加规范，易读）：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#-*-coding:utf-8 -*-# 从 python 环境中导入 itchat 包，requests 网络请求包 import itchat, requests# 从 itchat.content 中导入所有类、常量 (例如代码中的 TEXT 其实就是 itchat.content.TEXT 常量)from itchat.content import *# 导入时间包里面的 sleep 方法 from time import sleep# 导入随机数包 import random# 机器人的 apikeyAPIKEY = '376cb2ca51d542c6b2e660f3c9ea3754'# 封装一个根据内容调用机器人接口，返回回复的方法 def get_response(msg): # 构造了要发送给服务器的数据 apiUrl = 'http://www.tuling123.com/openapi/api' data = &#123; 'key' : APIKEY, 'info' : msg, 'userid' : 'wechat-robot', &#125; try: r = requests.post (apiUrl, data=data).json () # 字典的 get 方法在字典没有 'text' 值的时候会返回 None 而不会抛出异常 return r.get ('text') # 为了防止服务器没有正常响应导致程序异常退出，这里用 try-except 捕获了异常 # 如果服务器没能正常交互 (返回非 json 或无法连接), 那么就会进入下面的 return except Exception,err: # 打印一下错误信息 print (err) # 将会返回一个 None return# 注册消息类型为文本 (即只监控文本消息，其它的例如语音 / 图片 / 表情包 / 文件都不会监控)# 也就是说只有普通的文字微信消息才能触发以下的代码 # isGroupChat=True 开启群聊模式，即只是监控群聊内容 (如果不开启就监控个人聊天，不监控群聊)# @itchat.msg_register ([TEXT], isGroupChat=True)@itchat.msg_register ([TEXT])def tuling_reply(msg): # msg 是消息体，msg ['Text'] 用来获取消息内容 # 第一个单引号中的内容是关键词，使用正则匹配，可以自行更改 (我使用.* 表示任意内容), 如果使用中文注意 2.x 版本的 Python 会报错，需要 u 前缀 message = msg ['Text'] print (message) # 增加睡眠机制，随机等待一定的秒数 (1-10 秒) 再回复，更像人类 second = random.randint (1,10) sleep (second) # 为了保证在图灵 apikey 出现问题的时候仍旧可以回复，这里设置一个默认回复 defaultReply = 'I received:' + message # 如果图灵 apikey 出现问题，那么 reply 将会是 None reply = get_response (message) # a or b 的意思是，如果 a 有内容，那么返回 a, 否则返回 b return reply or defaultReply# 热启动，退出一定时间内重新登录不需要扫码 (其实就是把二维码图片存下来，下次接着使用)itchat.auto_login (hotReload=True)# 开启命令行的二维码 itchat.auto_login (enableCmdQR=True)# 运行 itchat.run ()代码截图（使用工具渲染了一下）：接入机器人演示 演示一下，随便聊了几句：备注 1、ItChat 项目 GitHub 地址：https://github.com/littlecodersh/itchat ；2、ItChat 项目说明文档：https://itchat.readthedocs.io/zh/latest ；3、感谢微博科普博主 灵光灯泡 的科普视频 https://weibo.com/6969849160/HeLhjcKtA 以及文档参考 石墨文档 ；4、Python 下载官网：https://www.python.org/downloads/windows ，大家一定要选择与自己当前环境适配的版本（包括操作系统版本、Python 版本），环境变量最好配置一下；5、图灵机器人官网：http://www.tuling123.com ；]]></content>
      <categories>
        <category>ItChat 系列</category>
      </categories>
      <tags>
        <tag>ItChat</tag>
        <tag>微信接口</tag>
        <tag>自定义接口</tag>
        <tag>自动回复</tag>
        <tag>微信机器人</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微博电影文稿备份 2]]></title>
    <url>%2F2019020101.html</url>
    <content type="text"><![CDATA[在 2019 年 1 月 1 日整理过一篇，包含 10 部电影，这是第二篇。武侠 《武侠》是一部 2011 年的动作悬疑片，豆瓣评分 6.6，好于 48% 的动作片，好于 43% 的悬疑片，可见评分不是很好，但是里面的动作戏还是不错的。这部电影由陈可辛导演，甄子丹、金城武、汤唯主演。1917 年，中国西南边陲的刘家村，刘金喜（甄子丹 饰演）和妻子阿玉（汤唯 饰演）共同抚养两个儿子方正和晓天，日子平淡且幸福。直到某一天，两个不速之客打破了刘家村的平静，也摧毁着金喜一家的生活，这二人企图洗劫村中的钱柜，被刚好在此的金喜撞见，一阵混乱打斗，二匪稀里糊涂被金喜打死。由于其中一人是政府通缉的要犯，因此县官大喜过望，村里人也将金喜奉为大英雄。但是，这看似普普通通的盲打误杀却引起一个人的怀疑，他名叫徐百九（金城武 饰演），是县衙的捕快。从蛛丝马迹上来看，二匪系死于武功高强人之手，徐百九由此留在村里，对金喜展开了连番的观察、调查与试探。 在这一过程中，金喜神秘的真实身份渐渐浮出水面，而刘家村也面临着一场空前的危机。 以下截取的 10 分钟片段（00:05:56 到 00:15:51），是电影开头，刘金喜和两个劫匪的打斗，并稀里糊涂把两个劫匪全部杀死，然后县官带队破案，发现死者之一是通缉犯。在这个片段中，主演悉数出场：汤唯、金城武、甄子丹，而且这一段打斗也是后面剧情发展的主要依据。后面会发现这一段打斗被捕快徐百九解释地合情合理，而刘金喜的身份也因此渐渐浮出水面，为最终的结局埋下了伏笔。让子弹飞 《让子弹飞》是一部 2010 年的剧情喜剧片，豆瓣评分 8.7，好于 97% 的喜剧片，好于 95% 的剧情片，又名《让子弹飞一会》、《火烧云》，由姜文导演。 民国年间，花钱捐得县长的马邦德（葛优 饰演）携妻子（刘嘉玲 饰演）及随从走马上任。途经南国某地，遭劫匪张麻子（姜文 饰演）一伙伏击，随从尽死，只夫妻二人侥幸活命，马为保命，谎称自己是县长的汤师爷。为汤师爷许下的财富所动，张麻子摇身一变化身县长，带着手下赶赴鹅城上任。有道是天高皇帝远，鹅城地处偏僻，一方霸主黄四郎（周润发 饰演）只手遮天，全然不将这个新来的县长放在眼里。张麻子痛打了黄的武教头（姜武 饰演），黄则设计害死张的义子小六（张默 饰演）。原本只想赚钱的马邦德，怎么也想不到竟会被卷入这场土匪和恶霸的角力之中。鹅城上空愁云密布，血雨腥风在所难免……截取的片段一，7 分钟（00:00:32 到 00:07:28），是电影开头的介绍，著名的台词：让子弹飞一会儿。截取的片段二，6 分钟（00:25:45 到 00:31:18），是黄四郎陷害小六子的场景，到底吃了几碗粉，小六子切腹致死，陈坤演技无敌。著名的台词：你不是欺负老实人吗。截取的片段三，12 分钟（00:34:15 到 00:46:17），是六子死后黄四郎约见县长，县长带着师爷奔赴鸿门宴，本来准备杀死黄四郎替六子报仇，后来计划有变。整个聊天过程真的令人大呼过瘾，经典台词：够硬吗？够硬！狼牙 《狼牙》是一部 2008 年的电影，距今已经十年有余，它是一部动作、惊悚电影，又名《狼牙之阿布》，由吴京、李忠志导演，由吴京、卢靖姗主演。 截取的片段之一，8 分钟（00:16:39 到 00:24:40），是吴京遭遇台风封岛无法返回内地，在卢靖姗住处附近的小吃店吃面，遇到了一伙贩毒黑社会，进而产生的动作摩擦。截取的片段之二，12 分钟（01:09:53 到 01:22:20），卢靖姗被黑社会绑架，威胁吴京前来救援，吴京只好独自一人前往，以一敌百，这场面宏大刺激。后会无期 《后会无期》是一部 2014 年的电影，由冯绍峰、陈柏霖、钟汉良、王珞丹、袁泉主演，由韩寒导演，同时也是韩寒的电影处女作。 截取的片段之一【00:16:30 到 00:29:00】，12 分钟，是三人刚到旅馆发生的事情。台词：你这种情况，是要加钱的。截取的片段之二【00:33:54 到 00:42:26】，9 分钟，是三人遇到苏米的家长。台词：汽油车，不能加柴油。失恋 33 天 《失恋 33 天》是一部 2011 年的爱情、剧情片，由白百何、文章、张嘉译、王耀庆主演，由滕华涛导演，又名《黄了一个来月》。 截取的片段，3 分钟【00:55:59 到 00:59:27】，是文章协助白百何咒骂前男友的场景，真的是思路清晰、先下手为强，骂得对方狗血喷头。真正失恋要经过三个阶段：第一阶段当然丧尽自尊，痛不欲生，听到他的名字都会跳起来。第二阶段故作忘记，避而不提伤心事，可是内心隐隐作痛。到了最后阶段，他的名字与路人一样，不过是个名字，一点儿特别意义都没有。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>微博电影</tag>
        <tag>文稿备份</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 异常之 Netty 相关]]></title>
    <url>%2F2019011401.html</url>
    <content type="text"><![CDATA[在做项目的时候，需要新引入一个外部依赖，于是很自然地在项目的 pom.xml 文件中加入了依赖坐标，然后进行编译、打包、运行，没想到直接抛出了异常：122019-01-13_17:18:52 [sparkDriverActorSystem-akka.actor.default-dispatcher-5] ERROR actor.ActorSystemImpl:66: Uncaught fatal error from thread [sparkDriverActorSystem-akka.remote.default-remote-dispatcher-7] shutting down ActorSystem [sparkDriverActorSystem]java.lang.VerifyError: (class: org/jboss/netty/channel/socket/nio/NioWorkerPool, method: createWorker signature: (Ljava/util/concurrent/Executor;) Lorg/jboss/netty/channel/socket/nio/AbstractNioWorker;) Wrong return type in function任务运行失败，仔细看日志觉得很莫名奇妙，是一个 java.lang.VerifyError 错误，以前从来没见过类似的。本文记录这个错误的解决过程。问题出现 在上述错误抛出之后，可以看到 SparkContext 初始化失败，然后进程就终止了；完整日志如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677782019-01-13_17:18:52 [sparkDriverActorSystem-akka.actor.default-dispatcher-5] ERROR actor.ActorSystemImpl:66: Uncaught fatal error from thread [sparkDriverActorSystem-akka.remote.default-remote-dispatcher-7] shutting down ActorSystem [sparkDriverActorSystem]java.lang.VerifyError: (class: org/jboss/netty/channel/socket/nio/NioWorkerPool, method: createWorker signature: (Ljava/util/concurrent/Executor;) Lorg/jboss/netty/channel/socket/nio/AbstractNioWorker;) Wrong return type in function at akka.remote.transport.netty.NettyTransport.(NettyTransport.scala:283) at akka.remote.transport.netty.NettyTransport.(NettyTransport.scala:240) at sun.reflect.NativeConstructorAccessorImpl.newInstance0 (Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance (NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance (DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance (Constructor.java:423) at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$2.apply (DynamicAccess.scala:78) at scala.util.Try$.apply (Try.scala:161) at akka.actor.ReflectiveDynamicAccess.createInstanceFor (DynamicAccess.scala:73) at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply (DynamicAccess.scala:84) at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply (DynamicAccess.scala:84) at scala.util.Success.flatMap (Try.scala:200) at akka.actor.ReflectiveDynamicAccess.createInstanceFor (DynamicAccess.scala:84) at akka.remote.EndpointManager$$anonfun$9.apply (Remoting.scala:711) at akka.remote.EndpointManager$$anonfun$9.apply (Remoting.scala:703) at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply (TraversableLike.scala:722) at scala.collection.Iterator$class.foreach (Iterator.scala:727) at scala.collection.AbstractIterator.foreach (Iterator.scala:1157) at scala.collection.IterableLike$class.foreach (IterableLike.scala:72) at scala.collection.AbstractIterable.foreach (Iterable.scala:54) at scala.collection.TraversableLike$WithFilter.map (TraversableLike.scala:721) at akka.remote.EndpointManager.akka$remote$EndpointManager$$listens (Remoting.scala:703) at akka.remote.EndpointManager$$anonfun$receive$2.applyOrElse (Remoting.scala:491) at akka.actor.Actor$class.aroundReceive (Actor.scala:467) at akka.remote.EndpointManager.aroundReceive (Remoting.scala:394) at akka.actor.ActorCell.receiveMessage (ActorCell.scala:516) at akka.actor.ActorCell.invoke (ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox (Mailbox.scala:238) at akka.dispatch.Mailbox.run (Mailbox.scala:220) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec (AbstractDispatcher.scala:397) at scala.concurrent.forkjoin.ForkJoinTask.doExec (ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask (ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker (ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run (ForkJoinWorkerThread.java:107)2019-01-13_17:18:52 [sparkDriverActorSystem-akka.actor.default-dispatcher-6] INFO remote.RemoteActorRefProvider$RemotingTerminator:74: Shutting down remote daemon.2019-01-13_17:18:52 [sparkDriverActorSystem-akka.actor.default-dispatcher-6] INFO remote.RemoteActorRefProvider$RemotingTerminator:74: Remote daemon shut down; proceeding with flushing remote transports.2019-01-13_17:18:52 [sparkDriverActorSystem-akka.actor.default-dispatcher-6] ERROR Remoting:65: Remoting system has been terminated abrubtly. Attempting to shut down transports2019-01-13_17:18:52 [sparkDriverActorSystem-akka.actor.default-dispatcher-6] INFO remote.RemoteActorRefProvider$RemotingTerminator:74: Remoting shut down.2019-01-13_17:19:02 [main] ERROR spark.SparkContext:95: Error initializing SparkContext.java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds] at scala.concurrent.impl.Promise$DefaultPromise.ready (Promise.scala:219) at scala.concurrent.impl.Promise$DefaultPromise.result (Promise.scala:223) at scala.concurrent.Await$$anonfun$result$1.apply (package.scala:107) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn (BlockContext.scala:53) at scala.concurrent.Await$.result (package.scala:107) at akka.remote.Remoting.start (Remoting.scala:179) at akka.remote.RemoteActorRefProvider.init (RemoteActorRefProvider.scala:184) at akka.actor.ActorSystemImpl.liftedTree2$1(ActorSystem.scala:620) at akka.actor.ActorSystemImpl._start$lzycompute (ActorSystem.scala:617) at akka.actor.ActorSystemImpl._start (ActorSystem.scala:617) at akka.actor.ActorSystemImpl.start (ActorSystem.scala:634) at akka.actor.ActorSystem$.apply (ActorSystem.scala:142) at akka.actor.ActorSystem$.apply (ActorSystem.scala:119) at org.apache.spark.util.AkkaUtils$.org$apache$spark$util$AkkaUtils$$doCreateActorSystem (AkkaUtils.scala:121) at org.apache.spark.util.AkkaUtils$$anonfun$1.apply (AkkaUtils.scala:53) at org.apache.spark.util.AkkaUtils$$anonfun$1.apply (AkkaUtils.scala:52) at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp (Utils.scala:2024) at scala.collection.immutable.Range.foreach$mVc$sp (Range.scala:141) at org.apache.spark.util.Utils$.startServiceOnPort (Utils.scala:2015) at org.apache.spark.util.AkkaUtils$.createActorSystem (AkkaUtils.scala:55) at org.apache.spark.SparkEnv$.create (SparkEnv.scala:266) at org.apache.spark.SparkEnv$.createDriverEnv (SparkEnv.scala:193) at org.apache.spark.SparkContext.createSparkEnv (SparkContext.scala:288) at org.apache.spark.SparkContext.(SparkContext.scala:457) at org.apache.spark.api.java.JavaSparkContext.(JavaSparkContext.scala:59) at com.ds.octopus.job.utils.SparkContextUtil.refresh (SparkContextUtil.java:77) at com.ds.octopus.job.utils.SparkContextUtil.getJsc (SparkContextUtil.java:34) at com.ds.octopus.job.executors.impl.WeiboZPZExporter.action (WeiboZPZExporter.java:95) at com.ds.octopus.job.executors.impl.WeiboZPZExporter.action (WeiboZPZExporter.java:41) at com.ds.octopus.job.executors.SimpleExecutor.execute (SimpleExecutor.java:40) at com.ds.octopus.job.client.OctopusClient.run (OctopusClient.java:162) at com.yeezhao.commons.buffalo.job.AbstractBUTaskWorker.runTask (AbstractBUTaskWorker.java:63) at com.ds.octopus.job.client.TaskLocalRunnerCli.start (TaskLocalRunnerCli.java:109) at com.yeezhao.commons.util.AdvCli.initRunner (AdvCli.java:191) at com.ds.octopus.job.client.TaskLocalRunnerCli.main (TaskLocalRunnerCli.java:41)2019-01-13_17:19:02 [main] INFO spark.SparkContext:58: Successfully stopped SparkContext错误日志截图：根据日志没有看出有关 Java 层面的什么问题，只能根据 JNI 字段描述符：1class: org/jboss/netty/channel/socket/nio/NioWorkerPool猜测是某一个类的问题，根据：1method: createWorker signature: (Ljava/util/concurrent/Executor;) Lorg/jboss/netty/channel/socket/nio/AbstractNioWorker;) Wrong return type in function猜测是某个方法的问题，方法的返回类型错误。然后在项目中使用 ctrl+shift+t 快捷键（全局搜索 Java 类，每个人的开发工具设置的可能不一样）搜索类：NioWorkerPool，发现这个类的来源不是新引入的依赖包，而是原本就有的 netty 相关包，所以此时就可以断定这个莫名其妙的错误的原因就在于这个类的 createWorker 方法返回类型上面了。搜索类 NioWorkerPool日志的 JNI 字段描述符显示返回类型是 AbstractNioWorker，但是这个一看就是抽象类，不是我们要找的，去类里面看源码，发现 createWorker 方法返回类型是 NioWorker：类 NioWorkerPool 源码 继续搜索类 NioWorker好，此时发现问题了，这个类有 2 个，居然存在两个相同的包名，但是依赖坐标不一样，所以这个隐藏的原因在于类冲突，但是并不能算是依赖冲突引起的。也就是说，NioWorker 这个类重复了，但是依赖包坐标不一样，类的包路径却是一模一样的，不会引起版本冲突问题，而在实际运行任务的时候会抛出运行时异常，所以我觉得找问题的过程很艰辛。使用依赖树查看依赖关系，是看不到版本冲突问题的，2 个依赖都存在：io.netty 依赖 org.jboss.netty 依赖 于是又在网上搜索了一下，发现果然是 netty 的问题，也就是新引入的依赖包导致的，但是根本原因令人哭笑：netty 的组织结构变化，发布的依赖坐标名称变化，但是所有的类的包名称并没有变化，导致了这个错误。问题解决 问题找到了，解决方法就简单了，移除传递依赖即可，同时也要注意以后再添加新的依赖一定要慎重，不然找问题的过程很是令人崩溃。移除依赖 移除配置示例 1234567&lt;!-- 移除引发冲突的 jar 包 --&gt;&lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.jboss.netty&lt;/groupId&gt; &lt;artifactId&gt;netty&lt;/artifactId&gt; &lt;/exclusion&gt;&lt;/exclusions&gt; 问题总结 1、参考：https://stackoverflow.com/questions/33573587/apache-spark-wrong-akka-remote-netty-version ；2、netty 的组织结构（影响发布的 jar 包坐标名称）变化了，但是所有的类的包名称仍然是一致的，很奇怪，导致我找问题也觉得莫名其妙，因为这不会引发版本冲突问题（但是本质上又是 2 个一模一样的类被同时使用，引发类冲突）；3、这个错误信息挺有意思的，解决过程也很好玩，边找边学习；4、对于这种重名的类【类的包路径名、类名】，竟然对应的 jar 包不一样，这种极其特殊的情况也可以使用插件检测出来：12&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;&lt;artifactId&gt;maven-enforcer-plugin&lt;/artifactId&gt; 使用 enforcer:enforce 命令即可。当然，这个插件还可以用来校验很多地方，例如代码中引用了 @Deprecated 的方法，也会给出提示信息，可以按照需求给插件配置需要校验的方面。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>netty</tag>
        <tag>nio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven 插件异常之 socket write error]]></title>
    <url>%2F2019011101.html</url>
    <content type="text"><![CDATA[今天在整理代码的时候，在本机（自己的电脑）通过 Maven 的 deploy 插件（org.apache.maven.plugins:maven-deploy-plugin:2.7）进行发布，把代码打包成构件发布到远程的 Maven 仓库（公司的私服），这样方便大家调用。可是，其中有一个项目发布不了（其它类似的 2 个项目都可以，排除了环境的原因），总是报错：1Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error以上错误日志中的项目名称、包名称均被替换。本文就记录从发现问题到解决问题的过程。环境所使用的 Maven 版本为：3.5.0。问题出现 对一个公共项目进行打包发布，部署到公司私服（已经排除环境因素），出现异常；使用命令 Maven：1mvn deploy出现异常：1234567891011121314[INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 02:49 min[INFO] Finished at: 2019-01-12T16:17:21+08:00[INFO] Final Memory: 68M/1253M[INFO] ------------------------------------------------------------------------[ERROR] Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy (default-deploy) on project dt-x-y-z: Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error -&gt; [Help 1][ERROR][ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR][ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException如果使用 -X 参数（完整命令：mvn deploy -X），可以稍微看到更详细的 Maven 部署日志信息：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990[INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 02:49 min[INFO] Finished at: 2019-01-12T16:17:21+08:00[INFO] Final Memory: 68M/1253M[INFO] ------------------------------------------------------------------------[ERROR] Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy (default-deploy) on project dt-x-y-z: Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error -&gt; [Help 1]org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy (default-deploy) on project dt-x-y-z: Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:213) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:154) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:146) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:81) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:309) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:194) at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:107) at org.apache.maven.cli.MavenCli.execute(MavenCli.java:993) at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:345) at org.apache.maven.cli.MavenCli.main(MavenCli.java:191) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289) at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415) at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)Caused by: org.apache.maven.plugin.MojoExecutionException: Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error at org.apache.maven.plugin.deploy.DeployMojo.execute(DeployMojo.java:193) at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208) ... 20 moreCaused by: org.apache.maven.artifact.deployer.ArtifactDeploymentException: Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error at org.apache.maven.artifact.deployer.DefaultArtifactDeployer.deploy(DefaultArtifactDeployer.java:143) at org.apache.maven.plugin.deploy.AbstractDeployMojo.deploy(AbstractDeployMojo.java:167) at org.apache.maven.plugin.deploy.DeployMojo.execute(DeployMojo.java:157) ... 22 moreCaused by: org.eclipse.aether.deployment.DeploymentException: Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error at org.eclipse.aether.internal.impl.DefaultDeployer.deploy(DefaultDeployer.java:326) at org.eclipse.aether.internal.impl.DefaultDeployer.deploy(DefaultDeployer.java:254) at org.eclipse.aether.internal.impl.DefaultRepositorySystem.deploy(DefaultRepositorySystem.java:422) at org.apache.maven.artifact.deployer.DefaultArtifactDeployer.deploy(DefaultArtifactDeployer.java:139) ... 24 moreCaused by: org.eclipse.aether.transfer.ArtifactTransferException: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error at org.eclipse.aether.connector.basic.ArtifactTransportListener.transferFailed(ArtifactTransportListener.java:52) at org.eclipse.aether.connector.basic.BasicRepositoryConnector$TaskRunner.run(BasicRepositoryConnector.java:364) at org.eclipse.aether.connector.basic.BasicRepositoryConnector.put(BasicRepositoryConnector.java:283) at org.eclipse.aether.internal.impl.DefaultDeployer.deploy(DefaultDeployer.java:320) ... 27 moreCaused by: org.apache.maven.wagon.TransferFailedException: Connection reset by peer: socket write error at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:650) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:553) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:535) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:529) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:509) at org.eclipse.aether.transport.wagon.WagonTransporter$PutTaskRunner.run(WagonTransporter.java:653) at org.eclipse.aether.transport.wagon.WagonTransporter.execute(WagonTransporter.java:436) at org.eclipse.aether.transport.wagon.WagonTransporter.put(WagonTransporter.java:419) at org.eclipse.aether.connector.basic.BasicRepositoryConnector$PutTaskRunner.runTask(BasicRepositoryConnector.java:519) at org.eclipse.aether.connector.basic.BasicRepositoryConnector$TaskRunner.run(BasicRepositoryConnector.java:359) ... 29 moreCaused by: java.net.SocketException: Connection reset by peer: socket write error at java.net.SocketOutputStream.socketWrite0(Native Method) at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111) at java.net.SocketOutputStream.write(SocketOutputStream.java:155) at org.apache.maven.wagon.providers.http.httpclient.impl.io.SessionOutputBufferImpl.streamWrite(SessionOutputBufferImpl.java:126) at org.apache.maven.wagon.providers.http.httpclient.impl.io.SessionOutputBufferImpl.flushBuffer(SessionOutputBufferImpl.java:138) at org.apache.maven.wagon.providers.http.httpclient.impl.io.SessionOutputBufferImpl.write(SessionOutputBufferImpl.java:169) at org.apache.maven.wagon.providers.http.httpclient.impl.io.ContentLengthOutputStream.write(ContentLengthOutputStream.java:115) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon$RequestEntityImplementation.writeTo(AbstractHttpClientWagon.java:209) at org.apache.maven.wagon.providers.http.httpclient.impl.DefaultBHttpClientConnection.sendRequestEntity(DefaultBHttpClientConnection.java:158) at org.apache.maven.wagon.providers.http.httpclient.impl.conn.CPoolProxy.sendRequestEntity(CPoolProxy.java:162) at org.apache.maven.wagon.providers.http.httpclient.protocol.HttpRequestExecutor.doSendRequest(HttpRequestExecutor.java:237) at org.apache.maven.wagon.providers.http.httpclient.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:122) at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.MainClientExec.execute(MainClientExec.java:271) at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184) at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.RetryExec.execute(RetryExec.java:88) at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.RedirectExec.execute(RedirectExec.java:110) at org.apache.maven.wagon.providers.http.httpclient.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184) at org.apache.maven.wagon.providers.http.httpclient.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.execute(AbstractHttpClientWagon.java:834) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:596) ... 38 more[ERROR][ERROR][ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException由于对 Maven 构件的原理不清楚，通过日志报错也看不出根本原因是什么，根据最后一行日志的链接：http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException ，我看到描述：123Unlike many other errors, this exception is not generated by the Maven core itself but by a plugin. As a rule of thumb, plugins use this error to signal a problem in their configuration or the information they retrieved from the POM.The concrete meaning of the exception depends on the plugin so please have a look at its documentation. The documentation for many common Maven plugins can be reached via our plugin index.大致意思也就是说这种类型的错误一般不是 Maven 的问题，而是所使用的 Maven 构件的问题，在这里我使用了 deploy 构件，我也知道和 deploy 构件有关，但是具体原因是什么也没说明；于是接下来使用 -e 参数（完整命令：mvn deploy -e），除了上述的报错日志外，还可以打印更为详细的错误日志追踪信息，然后发现一直会有下面的错误输入，而且一直重复，没有要停的迹象，我只能手动停掉 mvn 进程：1java.lang.IllegalArgumentException: progressed file size cannot be greater than size: 59156480 &gt; 58029604异常信息截图 换算一下单位：59156480KB=56.42M（正是需要发布的构件的大小）；58029604KB=55.34M；1、通过搜索引擎对异常信息的搜索，大部分结果显示和 Maven 后台的 Web 服务有关，如果使用的是 Nginx，会有一个参数用来限制上传文件的大小，上传文件的大小超过最大限制，就会上传失败，并且抛出异常。我部署其它的小构件没有问题，怀疑是这个原因，于是我询问运维人员公司的 Maven 私服对上传的公共构件有没有大小限制（即可能是 Nginx 服务有没有限制上传文件的大小），运维说不会。但是我还是怀疑，于是想通过 Web 端的界面来手动上传我的构件，发现 Web 端的界面没有开放，无法完成上传操作，接下来我就想看看 Maven 后台服务的对应参数配置的值是多大（也可能使用的是默认值），但是不知道后台采用的是什么服务（Nginx 还是 Netty 不确定），先放弃这条路；2、也有结果显示是 Maven 的版本问题，有些版本有 bug，所以造成了这个问题。问题解决 既然没有分析出来具体的原因，只能尝试每一种解决方案了。1、既然怀疑是 Maven 私服限制了构件的大小，那就想办法减小构件。先在本地 install，然后去本地仓库看一下生成的构件的大小，结果我惊讶地发现构件居然有 330M 之大，吓死人了，这个打包发布构件的配置肯定有问题，肯定把第三方依赖全部打进去了。我查看了以前生成的正常的构件，也就 60M 左右。以下放出对比图 2 个 这一看就是把第三方各种依赖包都一起发布了，才会造成构件有这么大，于是更改 pom.xml 文件，把第三方依赖去除，deploy 的时候是不需要的，同时也删除了一些 resources 资源文件夹里面的文本文件，删除时发现文本文件竟然有几十 M，怪不得以前发布的构件大小有 60M 左右，原来都是文本文件在占用空间；更新了之后，直接重新 deploy，不报错了，直接 deploy 成功，去私服仓库搜索查看，大概 30M 左右，很正常 2、问题使用方法一已经解决了，也就是和 Maven 版本没有关系了，而且，在我的当前 Maven 环境下，我去 deploy 其它构件也是成功的，不会有任务报错，所以也从侧面反映了这个问题和 Mave 版本无关，和 Maven 环境也无关； 问题总结 参考：http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException ；apache 的官方 jira：http://mail-archives.apache.org/mod_mbox/maven-issues/201808.mbox/%3CJIRA.13182024.1535592594000.205524.1535738700211@Atlassian.JIRA%3E ；https://issues.apache.org/jira/browse/MNG-6469?page=com.atlassian.jira.plugin.system.issuetabpanels%3Aall-tabpanel ；这个问题去网上搜索不到资料，很痛苦，问人也没有能帮到我的，只能自己去慢慢摸索试验，整个过程比较艰难；]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Maven</tag>
        <tag>deploy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Gson 将 = 转为 u003d 的问题]]></title>
    <url>%2F2019010601.html</url>
    <content type="text"><![CDATA[今天遇到一个问题，实现 Web 后台接收 http 请求的一个方法，发现前端传过来的参数值，有一些特殊符号总是使用了 unicode 编码，例如等号 =，后台接收到的就是 \u003d，导致使用这个参数做 JSON 变换的时候就会出错。我看了一下这个参数取值，是前端直接填写的，而填写的人是从其它地方复制过来的，人为没有去改变，前端没有验证转换，导致传入后台的已经是这样了，那么后台只好自己想办法转换。问题解决 其实就是字符串还原操作，把 Java 字符串里面的 unicode 编码子串还原为原本的字符，例如把 \u003d 转为 = 这样。自己实现一个工具类，做编码字符串和普通字符串的转换，可以解决这个问题。单个编码转换，公共方法示例：1234567891011121314151617/** * unicode 转字符串 * * @param unicode 全为 Unicode 的字符串 * @return */public static String unicode2String(String unicode) &#123; StringBuffer string = new StringBuffer (); String [] hex = unicode.split ("\\\\u"); for (int i = 1; i &lt; hex.length; i++) &#123; // 转换出每一个代码点 int data = Integer.parseInt (hex [i], 16); // 追加成 string string.append ((char) data); &#125; return string.toString ();&#125;整个字符串转换，公共方法示例：12345678910111213141516171819202122232425262728293031/** * 含有 unicode 的字符串转一般字符串 * * @param unicodeStr 混有 Unicode 的字符串 * @return */public static String unicodeStr2String(String unicodeStr) &#123; int length = unicodeStr.length (); int count = 0; // 正则匹配条件，可匹配 \\u 1 到 4 位，一般是 4 位可直接使用 String regex = "\\\\u [a-f0-9A-F]&#123;4&#125;"; String regex = "\\\\u [a-f0-9A-F]&#123;1,4&#125;"; Pattern pattern = Pattern.compile (regex); Matcher matcher = pattern.matcher (unicodeStr); StringBuffer sb = new StringBuffer (); while (matcher.find ()) &#123; // 原本的 Unicode 字符 String oldChar = matcher.group (); // 转换为普通字符 String newChar = unicode2String (oldChar); int index = matcher.start (); // 添加前面不是 unicode 的字符 sb.append (unicodeStr.substring (count, index)); // 添加转换后的字符 sb.append (newChar); // 统计下标移动的位置 count = index + oldChar.length (); &#125; // 添加末尾不是 Unicode 的字符 sb.append (unicodeStr.substring (count, length)); return sb.toString ();&#125;调用示例：12String str = "ABCDEFG\\u003d";System.out.println ("====unicode2String 工具转换:" + unicodeStr2String (str));输出结果：1====unicode2String 工具转换：ABCDEFG=截图示例：问题后续 后续我又在想，这个字符串到底是怎么来的，为什么填写的人会复制出来这样一个字符串，一般 unicode 编码不会出现在日常生活中的。我接着发现这个字符串是从另外一个系统导出的，导出的时候是一个类似于 Java 实体类的 JSON 格式字符串，从里面复制出来这个值，就是 \u003d 格式的。那我觉得肯定是这个系统有问题，做 JSON 序列化的时候没有控制好序列化的方式，导致对于特殊字符就会自动转为 unicode 编码，给他人带来麻烦，当然，我无法得知系统内部做了什么，但是猜测可能是使用 Gson 工具做序列化的时候没有正确使用 Gson 的对象，只是简单的生成 JSON 字符串而已，例如看我下面的代码示例（等号 = 会被转为 \u003d）。使用普通的 1Gson gson1 = new Gson (); 会导致后续转换 JSON 字符串的时候出现 unicode 编码子串的情况，而正确生成 Gson 对象 1Gson gson2 = new GsonBuilder ().disableHtmlEscaping ().create (); 则不会出现这种情况。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Gson</tag>
        <tag>等号编码转换</tag>
        <tag>u003d</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub Pages 禁止百度蜘蛛爬取的问题]]></title>
    <url>%2F2019010501.html</url>
    <content type="text"><![CDATA[最近才发现我的静态博客站点，大部分的网页没被百度收录，除了少量的网页是我自动提交（主动推动、自动推送）的，或者手动提交的，其它的网页都不被收录（网页全部是利用自动提交的 sitemap 方式提交的，一个都没收录）。我查看百度的站长工具后台，发现通过 sitemap 方式提交链接这种方式不可行，因为百度蜘蛛采集链接信息之前需要访问 baidusitemap.xml 文件，而这个文件是在 GitHub Pages 里面的，但是 GitHub Pages 是禁止百度蜘蛛爬取的，所以百度蜘蛛在获取 baidusitemap.xml 文件这一步骤就被禁止了，GitHub Pages 返回 403 错误（在 http 协议中表示禁止访问），因此抓取失败（哪怕获取到 baidusitemap.xml 文件也不行，因为后续需要采集的静态网页全部是放在 GitHub Pages 中的，全部都会被禁止）。本文就详细描述这种现象，以及寻找可行的解决方案。问题出现 网页收录对比差距大 利用搜索引擎的 site 搜索可以看到百度与谷歌明显的差别 百度搜索结果（只有少量的收录，仅有的还是通过主动推送与自动推送提交的）上面那个图片被封了，再来一张局部截图 谷歌搜索结果（收录很多，而且很全面）首先在百度站长工具（官方主页：https://ziyuan.baidu.com/ ）后台看到 baidusitemap.xml 抓取失败，查看具体原因是抓取失败（http 状态码 403）。抓取失败 抓取失败原因概述 根据抓取失败原因，我还以为是文件不存在，或者根据链接打不开（链接是：https://www.playpi.org/baidusitemap.xml ），我使用浏览器和 curl 命令都尝试过了，链接没有问题，可以正常打开。然后根据 403 错误发现是拒绝访问，那就有可能是百度爬虫的问题了（被 GitHub Pages 禁止爬取了）。使用浏览器打开 这里需要注意一点，百度站长工具里面显示的链接是 http 开头的（如上面抓取失败原因概述截图中红框圈出的，不是 https 开头的，我觉得百度爬虫抓取使用的就是 http 开头的链接），不过没关系，我在域名解析里面已经配置了所有的域名情况，完全可以支持。但是有时候仍然会遇到打不开上面链接的情况（在某些电脑上面或者某些网络环境中），我猜测这可能是电脑的缓存或者当前网络的 DNS 设置问题，不是我的站点的问题。因为，哪怕你在浏览器中输入以 http 开头的链接，也会自动跳转到以 https 开头的链接去。浏览器打不开链接的情况（其实不是链接的问题）使用命令行打开（如下使用 curl 命令）1curl https://www.playpi.org/baidusitemap.xml执行命令结果截图 通过百度反馈寻找原因 于是接下来，我就给官方提交了反馈，官方只是回复我说是链接问题（意思就是链接无法正常打开，其实使用浏览器或者检测工具都是可以打开的，但是使用百度爬虫就不行）。提交反馈（官方主页：https://ziyuan.baidu.com/feedback/apply ）反馈回复 前面我已经证明了链接没问题，那我就要猜想是百度蜘蛛爬虫的问题了，于是按照官方回复的建议，使用诊断工具看看是否可行。诊断工具测试多次都失败 如果抓取 UA 设置为移动端（即模拟手机、平板之类的设别），会有部分成功的，而使用 PC 端全部都是失败的。失败原因仍旧是拒绝访问（http 403 状态码）我又接着查看文档（文档地址：https://ziyuan.baidu.com/college/courseinfo?id=267&amp;page=9#007 ），发现拒绝访问的原因之一就是托管服务供应商阻止百度 Spider 访问我的网站，所以猜测是 GitHub Pages 拒绝了百度 Spider 的爬取请求，接着就想办法验证一下猜测是否正确。文档说明截取片段 接下来我又查找了资料，发现网上确实有很多这种说法，而且大家都遇到了这种问题，但是并没有官方的说明放出来。于是，接着我又回复了百度站长对方的反馈，直接问是不是因为 GitHub Pages 禁止了百度爬虫，所以百度爬取的结果总是 403 错误。等了 2 天多（赶上周末），对方没有明确回复，说的都是废话，可能是不想承认，那我也不管了。通过 GitHub Pages 找原因 另一方面，我尝试给 GitHub 的技术支持发送邮件询问，得到了确认的答复，GitHub 已经禁止了百度蜘蛛爬虫的访问，并且不保证在未来的时间恢复。主要是因为以前百度爬虫爬取太猛了，导致 GitHub Pages 不可用或者访问速度变慢，影响了其他正常的用户浏览使用 GitHub Pages，所以把百度爬虫给禁止了（当然，这是官方说法）。GitHub Pages 的反馈链接（填写姓名、邮箱、内容描述即可）：https://github.com/contact ；我发送了一封邮件过去，当然是借助谷歌翻译完成的，勉强能看 成功发送邮件后的通知页面 内容全文如下，仅供参考：1234567891011121314151617A doubt with GitHub PagesHello,I created my own homepage with GitHub Pages,it is https://github.com/iplaypi/iplaypi.github.io.If you input https://iplaypi.github.io,it jumps to https://www.playpi.org automatically because of CNAME file.The website is https://www.playpi.org,and my site only contains static pages and pictures.But I have a problem,the following is my detailed description:I use Google Search Console to crawl my pages and include them.I only need to provide a site file named website.xml,and it works fine.But when i use Baidu Webmaster Tools (a tool made by a Chinese search engine company),it doesn&apos;t work properly.I only need to provide a site file named baiduwebsite.xml,Baidu Spider will crawl the link in this file .But Baidu cannot include my pages finally,and the reason is Baidu Spider can&apos;t crawl my html pages.So,I am trying to find the real reason,then I succeeded.The real reason is Github Pages forbids the crawling of Baidu Spider.So when Baidu Spider crawls my pages,it will definitely fail.Here I want to know is this phenomenon real?If yes,why Github Pages forbids Baidu Spider?And what should i do?Thanks.Best regards.Perry没隔几个小时，就有回复了 回复的重点内容如下：1I&apos;ve confirmed that we are currently blocking the Baidu user agent from crawling GitHub Pages sites. We took this action in response to this user agent being responsible for an excessive amount of requests, which was causing availability issues for other GitHub customers. This is unlikely to change any time soon, so if you need the Baidu user agent to be able to crawl your site you will need to host it elsewhere.那么，我们再来回看一下百度站长里面爬取失败原因的页面，里面有一个用户代理的配置，其实就是构造 http 请求使用的消息头，可以看到正是 Baiduspider/2.0，所以才会被 GitHub Pages 给禁止了。解决方案 至此，我已经把问题的原因搞清楚了。本来这个问题是很好解决的（更换静态博客存储的主机即可，例如各种项目托管服务：码市、gitcafe、七牛云等，或者自己购买一台云主机），但是我不能抛弃 GitHub，于是问题变得复杂了。此时，我还有 3 个方案可以参考：使用 CDN 加速，把每个静态页面都缓存下来，这样百度爬虫的请求就可能不会到达 GitHub Pages，但是不知道有没有保证，可以试试 放弃 自动提交 方式里面的 sitemap 推送 ，改为 主动推送 ，hexo 里面有插件可以用。但是我是坚持大道至简的原则，不想再引用插件了，而且我看了那个插件，需要配置百度账号的信息，我不能把这些信息放在公共仓库里面，会暴露给别人，不想用 在更新博客的同时再部署一份相同的博客 （可以理解为镜像，需要在其它主机部署一份，可以自己搭建主机或者使用类似于 GitHub 的代码托管工具），把 master 分支的内容复制过去即可，然后利用域名解析服务，把百度爬虫的流量引到这份服务器上面（只是为了让百度收录），其他的流量仍然去访问 GitHub Pages，就可以让百度的爬虫顺利爬取到我的博客内容了。这个方法看起来虽然很绕，但是明白了细节实现起来就很简单，而且可靠，可以用 CDN 加速 我先不选择这种方式了，因为需要收费或者免费的加广告，或者服务不稳定，我还是愿意选择稳妥的方式。可以选择的产品有：七牛云、又拍云、阿里云、腾讯云等。选择镜像方式 既然选择了使用复制博客的方式，再加上域名解析服务转移流量，那接下来就开始动手部署了。我手里正好还有一台翻墙使用的 VPS，每个月的流量用不完，所以也不打算使用第三方托管服务了，直接部署在我自己的 VPS 上面就行了。只不过还需要动动手搭建一下 Web 服务，当然是使用强大的 Nginx 了。更改域名服务器和相关配置 1、在 DNSPod 中添加域名DNSPod 账号自行注册，我使用免费版本，当然会有一些限制，例如解析的域名 A 记录个数限制为 2 个【GitHub Pages 有 4 个 ip，我在 Godaddy 中都是配置 4 个，但是没影响，配置 2 个也。或者直接配置 CNAME 记录就行了，以前我不懂就配置了 ip，多麻烦，ip 还要通过 ping iplaypi.github.io 获取，每次还不一样，一共得到了 4 个，多此一举。当然，如果域名被墙了而 ip 没被墙，还是需要这样配置的】。2、添加域名解析记录 我把 Godaddy 中的解析记录直接抄过来就行，不同的是由于使用的是 DNSPod 免费版本，A 记录会少配置 2 个，基本不会有啥影响 【其实不配置 A 记录最好，直接配置 CNAME 就行了，会根据域名自动寻找 ip，以前我不懂】。另外还有一个就是需要针对百度爬虫专门配置一条 www 的 A 记录，针对百度的线路指向自己服务器的 ip【截图只是演示，其中 CNAME 记录应该配置域名，A 记录才是配置 ip】，如果使用的是第三方托管服务，直接添加 CNAME 记录，配置域名就行【例如 yoursite.gitcafe.io】。不使用 A 记录的配置方式 3、在 Godaddy 中绑定自定义域名服务器 第 2 个步骤完成，我们回到 DNSPod 的域名界面，可以看到提示我们修改 NS 地址，如果不知道是什么意思，可以点击提示链接查看帮助手册【其实就是去购买域名的服务商那里绑定 DNSPod 的域名服务器】。提示我们修改 NS 地址 帮助手册 我是在 Godaddy 中购买的域名【不需要备案】，所以需要在 Godaddy 中取消默认的 DNS 域名服务器，然后把 DNSPod 分配的域名服务器配置在 Godaddy 中。这里需要注意，在配置了新的域名服务器的时候，以前的配置的解析记录都没用了，因为 Godaddy 直接把域名解析的工作转给了我配置的 DNSPod 域名服务器【配置信息都转到了 DNSPod 中，也就是步骤 1、步骤 2 中的工作】。原有的解析记录与原有的域名服务器 配置完成新的域名服务器【以前的解析记录都消失了】配置完成后使用 域名设置 里面的 自助诊断 功能，可以看到域名存在异常，主要是因为更改配置后的时间太少了，要耐心等待全球递归 DNS 服务器刷新【最多 72 小时】，不过一般 10 分钟就可以访问主页了。设置镜像服务器 我没有使用第三方托管服务器，例如：gitcafe、码市、coding，而是直接使用自己的 VPS，然后搭配 Nginx 使用。安装 Nginx（基于 CentOS 7 X64）CentOS 的安装过程参考：https://gist.github.com/ifels/c8cfdfe249e27ffa9ba1 。但是，不是全部可信，抽取有用的即可。而且这种方式安装的是已经规划好的一个庞大的包，里面包含了一些常用的模块，可能有一些模块没用，而且如果自己想再安装一些新的模块，就不支持了，必须重新下源码编译安装。总而言之，这种安装方式就是给入门级别的人使用的，不能自定义。1、由于 Nginx 的源头问题，先创建配置文件 12cd /etc/yum.repos.d/vim nginx.repo 填写内容 12345[nginx]name=nginx repobaseurl=http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck=0enabled=12、安装配置 Nginx1234# 安装 yum install nginx -y# 配置 vi /etc/nginx/nginx.conf 填写配置内容 1234567891011121314151617181920212223242526272829303132333435363738user nginx;worker_processes 1;error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; access_log /site/nginx.access.log main; server &#123; listen 80; server_name blog.playpi.org www.playpi.org; access_log /site/iplaypi.github.io.access.log main; root /site/iplaypi.github.io; &#125; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf;&#125;3、开启 80 端口（不开启不行），启动 Nginx1234567891011# 查看已经开启的端口 firewall-cmd --list-ports# 开启端口 firewall-cmd --permanent --zone=public --add-port=80/tcp# 重载更新的端口信息 firewall-cmd --reload# 启动 Nginx# 这种方式不行，找不到目录 /etc/init.d/nginx start# 这种方式可以 service nginx start 额外考虑情况 1、关于 https 认证 要不要考虑 https 的情况，如果百度爬虫没用到 https 抓取（除了 sitemap.xml 文件还要考虑文件里面的所有链接格式，也是 https 的），就不考虑。其实一定要考虑，因为百度爬虫用到了 https 链接去抓取，所以还要想办法开启 Nginx 的 https。此外，在百度的 https 认证里面，也是需要开启 https 的，否则申请不通过。我的域名不知道什么时候验证失败了，但是一开始的时候是验证成功的（可能是 GitHub Pages 禁止百度爬虫的原因，因为以前全部都是 GitHub Pages 提供站点支持）我想重新验证一下，没想到有次数限制，还是先把 Nginx 的 https 开启之后再验证吧 开启 Nginx 的 https，并且保证站点全部的链接都是 https 的，但是同时也要支持 http，使用 301 重定向到 https。1-1、查看 Nginx 的 https 模块 先查看我安装的小白版本的 Nginx 里面有没有关于 https 的模块，使用命令 nginx -V，可以看到是有的，这个模块就是 –with-http_ssl_module。1-2、申请证书 可以购买或者从阿里云、腾讯云里面申请免费的，但是我还是觉得使用 OpenSSL 工具自己生成方便，先查看机器有没有安装 OpenSSL 工具，使用 openssl version 命令，如果没有则需要安装 yum install -y openssl openssl-devel，安装完成后开始生成证书。生成证书的命令：1openssl req -x509 -nodes -days 36500 -newkey rsa:2048 -keyout /site/ssl-nginx.key -out /site/ssl-nginx.crt在生成的过程中还需要填写一些参数信息：国家、城市、机构名称、机构单位名称、域名、邮箱等，这里特别注意我为了能让多个子域名公用一个证书，采用了泛域名的方式（星号的模糊匹配：*.playpi.org）。这种生成证书的方式只是为了测试使用，最终的证书肯定是不可信的，浏览器会提示此证书不受信任，所以还是通过其它方式获取证书比较好（后续我会通过阿里云或者 letsencrypt 获取免费的证书，具体博客参考可以使用相关关键词在站内搜索）。完整信息填写 12345678910111213141516171819Generating a 2048 bit RSA private key........+++..............+++writing new private key to &apos;/site/ssl-nginx.key&apos;-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter &apos;.&apos;, the field will be left blank.-----Country Name (2 letter code) [XX]:CNState or Province Name (full name) []:GuangdongLocality Name (eg, city) [Default City]:GuangzhouOrganization Name (eg, company) [Default Company Ltd]:playpiOrganizational Unit Name (eg, section) []:playpiCommon Name (eg, your name or your server&apos;s hostname) []:*.playpi.orgEmail Address []:playpi@qq.com1-3、更改配置并重启 Nginx 重新配置 http 与 https 的参数（只列出 server 的主要部分，blog 二级域名主要是为了测试使用的，blog 的流量全部导入我的 VPS 中），特别注意 rewrite 的正则表达式，只替换域名部分，链接部分不能替换，否则都跳转到主页去了 123456789101112131415161718192021222324252627282930313233server &#123; listen 80; server_name www.playpi.org; access_log /site/iplaypi.github.io.http-www-access.log main; rewrite ^/(.*)$ https://www.playpi.org/$1 permanent; &#125; server &#123; listen 80; server_name blog.playpi.org; access_log /site/iplaypi.github.io.http-blog-access.log main; rewrite ^/(.*)$ https://blog.playpi.org/$1 permanent; &#125; server &#123; listen 443 ssl;# 监听端口 server_name www.playpi.org blog.playpi.org;# 域名 access_log /site/iplaypi.github.io.https-access.log main; root /site/iplaypi.github.io; ssl_certificate /site/ssl-nginx.crt;# 证书路径 ssl_certificate_key /site/ssl-nginx.key;#key 路径 ssl_session_cache shared:SSL:1m;# 储存 SSL 会话的缓存类型和大小 ssl_session_timeout 5m;# 配置会话超时时间 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;# 为建立安全连接，服务器所允许的密码格式列表 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on;# 依赖 SSLv3 和 TLSv1 协议的服务器密码将优先于客户端密码 #减少点击劫持 add_header X-Frame-Options DENY; #禁止服务器自动解析资源类型 add_header X-Content-Type-Options nosniff; #防 XSS 攻击 add_header X-Xss-Protection 1; &#125; 开启 443 端口，重启 Nginx12345678910# 查看已经开启的端口 firewall-cmd --list-ports# 开启端口 firewall-cmd --permanent --zone=public --add-port=443/tcp# 重载更新的端口信息 firewall-cmd --reload# 验证 Nginx 配置是否准确 nginx -t# 重新启动 Nginxnginx -s reload1-4、打开链接查看 使用 blog 二级域名测试（也需要在 DNSPod 中配置一条 A 记录解析规则）或者使用 curl 命令模拟请求，由于有重定向的问题，所以失败 既然开启了 https，可以使用 curl 关闭失效证书的方式（-k 参数）访问 https 链接 去百度站长里面重新提交 https 认证（使用上面的测试证书是认证失败的，我去阿里云重新申请了证书，认证成功了，申请证书的教程可以在本站搜索，为了给 2 个二级域名不同的证书，nginx 还需要重新配置 server 信息）2、端口的问题 为什么在上面配置域名解析记录的时候，百度的 A 记录配置 VPS 的 ip 就行了呢，这是因为在 VPS 上面只有 Nginx 这一种 Web 服务，机器会分配给它一个端口（默认 80，也是 http 的默认端口，可以配置），然后 www 的访问就使用这个端口（在 Nginx 的配置里面有，还有另外一个 blog 的），所以可以忽略端口的信息。但是如果一台机器上面有各种 Web 服务，切记确保端口不要冲突（例如 Tomcat 和 Nginx 同时存在的情况），并且给 Nginx 的就是 80 端口，然后如果有其它服务，可以使用 Nginx 做代理转发（例如把 email 二级域名转到一个端口，blog 二级域名转到另一个端口）。完善自动获取更新脚本，拉取 mater 分支的静态页面 1、先用简单的方式 使用 git 把项目克隆到：/site/iplaypi.github.io 即可。2、利用钩子自动拉取 master 分支内容到指定目录 本来最简单的方式就是在 travis 自动构建的时候，把生成的静态页面直接拷贝到目标主机就行了。也就是把 public 目录里面的内容使用类似 scp 的命令拷贝到我的服务器即可。但是，我觉得这种方式太简易，我还是想利用起来 GitHub 的钩子功能，在项目有 push 发生的时候，自动触发我服务器上面的脚本，然后脚本就会执行 pull 的操作。详情见我的另外一篇博客：使用 Github 的 WebHooks 实现代码自动更新 。 验证结果 以下验证都是在没有开启 https 的情况下，即没有对 http 进行 301 重定向，如果做了 301 重定向截图内容会有一点不一样，curl 也会直接失败（需要访问 https 格式的链接）。使用最简单的方式验证就是在百度站长工具里面使用 抓取诊断 来进行模拟抓取多次，看看成功率是否是 100%。通过测试，可以看到，每次抓取都会成功，那么接下来就等待百度自己抓取了（百度爬虫抓取 sitemap.xml 文件的频率很低，可能要等一周）。使用抓取诊断方式来验证，这个过程有一个插曲，就是无论怎么验证都是失败的，但是使用 curl 模拟请求却是成功的。我看了失败原因概述里面，抓取的 ip 地址仍旧是 GitHub Pages 的，说明百度爬虫的流量没有到我自己的 VPS 上面。我一开始还以为是 DNSPod 配置没生效，但是通过 curl 模拟请求却可以，说明 DNSPod 配置没问题，那就是百度的问题了，应该是缓存。后来，我在移动端 UA 与 PC 端 UA 切换了一下，然后就行了。此外，既然我们知道了百度爬虫设置的用户代理，那么就可以直接使用 curl 命令来模拟百度爬虫的请求，观察返回的 http 结果是否正常。模拟命令如下：1curl -A "Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)" http://blog.playpi.org/baidusitemap.xml模拟请求的结果，可以看到也是正常的（下面的截图在没有开启 https 的情况下，如果开启 301 重定向就不行了，需要直接访问 https 链接）如果开启了 https，即对 http 请求进行 301 重定向，则可以直接访问 https 链接（如果证书是无效的，像我截图中的，则可以使用 curl 关闭无效证书的方式，加一个 -k 参数）我也去看了 VPS 上面的 Nginx 日志，确实百度爬虫的流量都被引入到这里来了，皆大欢喜 后续还需要观察看看百度的收录结果（等待 3 天后更新了，结果如下）问题总结1、这篇博客耗费了我一个多月才完成，当然不是写了一个多月，而是从发现问题到解决问题，最终写成这篇博客，前后经历了一个多月。在这一个多月里，我看了很多别人的博客，问了一些人，也看了一些技术资料，学到了很多以前不了解的知识，而且通过动手去解决问题，整个过程收获颇丰。2、写 Markdown 文档，使用代码块标记的时候，使用 3 个反单引号来标记，如果不熟悉代码块里面的编程语言，可以省略类型（例如 java、bash、javascript），不要填写，否则填错了生成的 html 静态文件是空白的。还有就是如果代码块里面放的是一段英文文本，和编程语言无关，也不要填写类型，否则生成的 html 静态文件也是空白的。3、通过实战学习了一些网络知识，例如：CNAME、A 记录、域名服务器、二级域名等、https 证书，也学习了一些关于 Nginx 的知识。4、关于访问速度的问题，GitHub Pages 的 CDN 还是很强大的，不会出现卡顿的情况。但是有时候貌似 GitHub 会被墙，打不开。此外，我搞这么久就是为了让百度爬虫能收录我的站点文章，所以自己搭建的 VPS 只是为了给百度爬虫爬取用的，其它正常人或者爬虫仍旧是访问 GitHub Pages 的链接。5、关于 https，使用 GitHub Pages 的时候，服务全部是 GitHub Pages 提供的，我无需关心。但是，自己使用 VPS 做了一个镜像，就需要配置一模一样的环境给百度爬虫使用，否则会导致一些失败的现象，例如 htps 认证失败、链接抓取失败。因此，一定要开启 https，并且同时也支持 http。以下是整理的网络请求流程图，清晰明了。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>建站</tag>
        <tag>GitHub Pages</tag>
        <tag>SEO</tag>
        <tag>百度蜘蛛</tag>
        <tag>Baiduspider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微博电影文稿备份]]></title>
    <url>%2F2019010101.html</url>
    <content type="text"><![CDATA[元旦是个好日子，在一年之始，我整理硬盘文件夹的过程中，发现了很多电影的资料，都是以前下载的，有的看过有的没看过。突发奇想，有些觉得好看的电影可以保留下来，剪辑部分片段发到微博，也可以用来记录自己曾经看过那些电影，本文记录随微博发送的文稿，防止被删。双旗镇刀客 《双旗镇刀客》，是一部 1991 年的武侠动作片，获奖很多，豆瓣评分 8.1，好于 95% 的武侠片，好于 92% 的动作片。但是，这部电影里面几乎没有打斗的场面，也没有动作大场面，但依然削弱不了它的武侠内涵。 仅有的几个决斗场面全部都是一对一，看不到出招、闪躲、反杀，细节全部使用蒙太奇的手法表现，只能听到一声风声、一声刀声、一声剑声，然后胜败已分，没有嘶喊、没有大动作，非常干净利落。视频截取的片段（01:20:07 到 01:23:03）是最后一刀仙和孩哥决斗的场面。对了，你们能认出饰演这一刀仙的演员是谁吗？–《双旗镇刀客》：粗粝、苦难、鲁莽的平民式的武侠片。扫毒 扫毒片段之一（01:22:06 到 01:32:16）：泰国一战回来，过了很久，马昊天【刘青云 饰演】、张子伟【张家辉 饰演】、苏建秋【古天乐 饰演】第一次见面，他们并不知道张子伟还活着，看一下大家的反映，三大影帝同场飙戏。扫毒片段之二（01:36:30 到 01:46:42）：三兄弟在天台见面，交换人质，然后引发枪战。这一段也是张家辉的鬼畜视频的素材，台词是：五年，五年，你知道这五年我怎么过的吗？你知道吗？【见视频 3 分 38 秒处】那山那人那狗 那山那人那狗，是一部 1999 年的电影，讲述的是儿子（刘烨 饰演）高考落榜不得已回到大山中的家后，做了大半辈子山村邮递员的父亲（滕汝骏 饰演）提前退休，安排儿子接下自己的工作。儿子上班第一天，父亲千叮咛万嘱咐之后仍不放心，带上长年在其左右的忠实老狗（老二）决定陪儿子再走一趟送信之旅。在这趟旅途之中，父子由于长期隔膜一开始默默不语，后来渐渐打开了话匣子，对彼此有了更深一层的认识和了解，故事自然真实、扣人心弦。以下截取的 11 分钟片段（00:51:52 到 01:03:04）是很重要的一场戏，父子两人一同过河，然后共同休息聊天，在这个过程中，父子的情感逐渐加深了。倚天屠龙记之魔教教主 这是一部 1993 年的武侠片：《倚天屠龙记之魔教教主》，看看这武打场面，动作发挥，虽然当时科技不发达，资源不足，但是拍出来的电影仍然很经典。再看看现在，科技发达了，资金充足，但是动作戏只是做做样子，实在看不下去。截取的 7 分钟片段（01:04:22 到 01:11:24）是围攻光明顶的场面，张无忌出场以及打戏过程，动作非常流畅、到位。叶问外传：张天志 作为《叶问》系列影片，电影《叶问外传：张天志》延续了《叶问 3》的故事，讲述了同为咏春传人的张天志在比武惜败叶问后，决意放下功夫，远离江湖纷争。但面对接踵而至的连番挑衅，面对家国大义遭受的恶意侵犯，决定重拾咏春惩戒毒贩，「以武之道」捍卫民族道义尊严的故事。电影虽然总体评分不高，但是里面的打戏还是值得看的。以下截取的 3 分钟片段（00:08:40 到 00:11:54）是电影开场的矛盾冲突过程以及打戏场面，动作流利，拳拳到肉。屏住呼吸 《屏住呼吸》是一部 2016 年的电影，豆瓣评分 7.0，好于 63% 的惊悚片，好于 82% 的恐怖片。讲述的是三位惯偷（洛奇、艾利克斯、摩尼）潜入一位退伍老兵的家里，老兵因为女儿的车祸获取了巨额赔偿金，他们想偷取这笔钱，从此金盆洗手。 三人潜入了老兵位于底特律的如同鬼宅一般的老房子中，整个社区只有他一个人居住，没想到却就此打开了地狱的大门。老兵拥有着高于常人的嗅觉和听力，以及极为敏捷的行动力，很快就发现了三名不速之客的行踪。在他的枪口之下，一行人犹如瓮中之鳖，无处可逃。在紧张而又激烈的追逃之中，洛奇和艾利克斯误打误撞跌落到了地下室中，却因此发现了一个可怕的秘密。以下截取的 14 分钟视频片段（00:24:51 到 00:39:15）是三人刚刚潜入住宅，本以为被迷晕的老兵却突然出现，及其凶狠地打死小偷一人，去检查保险箱又恰巧被洛奇看到密码，接着他们小偷二人偷取巨额现金准备逃跑。整个过程真的让人屏住呼吸，时刻感觉到危险，此时已经不是偷东西那么简单了，自己的生命已经快保不住了。无间道 《无间道》是一部 2003 年的悬疑犯罪剧情片，获得金马奖最佳剧情片，金像奖最佳电影，豆瓣评分 9.1，好于 99% 的犯罪片，好于 98% 的剧情片。1991 年，香港黑帮三合会会员刘健明（刘德华 饰演）听从老大韩琛（曾志伟 饰演）的吩咐，加入警察部队成为黑帮卧底，韩琛许诺刘健明会帮其在七年后晋升为见习督察。1992 年，警察训练学校优秀学员陈永仁（梁朝伟 饰演）被上级要求深入到三合会做卧底，终极目标是成为韩琛身边的红人。2002 年，两人都不负重望，也都身背重压，但是刘健明渐想成为一个真正的好人，而陈永仁则盼着尽快回归警察的身份。 在后续的较量中，逐渐暴露出双方均有卧底的事实，引发双方高层清除内鬼的决心。命运迥异又相似的刘健明和陈永仁开始在无间道的旅程中接受严峻考验。 以下截取的 7 分钟片段（01:29:28 到 01:36:04），是结局的天台见面，台词、配乐、镜头都很经典，最后的 2 个卧底双双命丧枪口，令人唏嘘。经典台词对话如下：刘建明：给我一个机会。陈永仁：怎么给你机会。刘建明：我以前没得选，我现在想做一个好人。陈永仁：好啊，跟法官说，看给不给你做好人。刘建明：那就是要我死。陈永仁：对不起，我是警察。刘建明：谁知道？毒液：致命守护者 《毒液：致命守护者》是一部 2018 年的科幻、动作、惊悚电影，又名毒魔、猛毒，豆瓣评分 7.2，好于 69% 的科幻片，好于 69% 的动作片。 剧情简介：艾迪是一位深受观众喜爱的新闻记者，和女友安妮相恋多年，彼此之间感情十分要好。安妮是一名律师，接手了生命基金会的案件，在女友的邮箱里，艾迪碰巧发现了基金会老板德雷克不为人知的秘密。为此，艾迪不仅丢了工作，女友也离他而去。之后，生命基金会的朵拉博士找到了艾迪，希望艾迪能够帮助她阻止德雷克疯狂的罪行。在生命基金会的实验室里，艾迪发现了德雷克进行人体实验的证据，并且在误打误撞之中被外星生命体毒液附身。回到家后，艾迪和毒液之间形成了共生关系，他们要应对的是德雷克派出的一波又一波杀手。以下截取的 10 分钟片段（00:50:25 到 01:00:55），是艾迪和毒液共生时第一次遇到德雷克派出的杀手的情景，双方产生了激烈的打斗，此时可以看到艾迪和毒液完美的共生关系。剑雨 《剑雨》是一部 2010 年的武侠、动作电影，豆瓣评分 7.1，好于 58% 的武侠片，好于 66% 的动作片。这部电影由吴宇森监制，有人称它是《卧虎藏龙》之后最好的武侠片。 剧情简介：八百年前，竺人达摩来至中原弘法，其死后尸体被人盗取并分为两部分。传说拿到达摩尸体的人能练成绝世武功，因此江湖上风波骤起。时有转轮王（王学圻 饰演）操控的黑石杀手集团，转轮王率徒众夜袭藏匿半具达摩尸首的首辅张海端（李庆祥 饰演）宅邸，但是他的手下细雨（林熙蕾 饰演）却带着残尸绝走江湖，致令转轮王发出江湖追杀令，引出一阵血雨腥风。为避追杀，细雨易容，更名曾静（杨紫琼 饰演），逃亡期间结识了木讷善良的江阿生（郑宇成 饰演）。一段时间相处，二人渐渐萌生感情，更喜结连理。但是江湖恩怨，怎可轻易了结，后续引发了一系列复仇的情节。截取的 9 分钟片段（01:19:06 到 01:28:15），是曾静被黑石集团追杀，受伤后归家昏厥，而江阿生此时不得不重新拿起剑与敌人厮杀的场景。这一段打斗堪称精彩，使用兵器剑，动作流畅，很符合吴宇森的暴力美学。精彩对白：1、我愿是你杀的最后一人。2、我愿化身石桥，受那五百年风吹，五百年日晒，五百年雨淋，只求她从桥上走过。网络谜踪 《网络谜踪》是一部 2018 年的悬疑、惊悚、犯罪电影，豆瓣评分高达 8.6，好于 96% 的悬疑片，好于 96% 的犯罪片，又名人肉搜寻、人肉搜索、搜索、屏幕搜索。 影片讲述的是工程师大卫・金一直引以为傲的 16 岁乖女玛戈特突然失踪，前来调查此案的警探怀疑女儿离家出走。不满这一结论的父亲为了寻找真相，独自展开调查，他打开了女儿的笔记本电脑，用社交软件开始寻找破案线索，大卫必须在女儿消失之前，沿着她在虚拟世界的足迹找到她。最后经历了各种挫折，总算找到了真相，可惜女儿已经被害离世。没想到，最终在女儿的遗体告别仪式上，父亲又发现了不可告人的秘密，事情得到大反转。整个电影的镜头大部分时间都是由录像、监控、视频、新闻画面组成，反正全部是电子屏幕，很少有摄像机直接拍摄的场景，但是正是由于这样，才体现出整个过程的悬疑、惊悚的效果。看似牢不可破的密码，跳转几次邮箱就可破译；看似无比相熟的女儿，连吸食大麻都未曾知。本来代表身份的头像，竟是无需版权的图库模特；本来孤立寡言的女孩，却在直播软件里袒露心扉。最亲近的女儿却比密码还难破译，最沉默的女孩却比模特还健谈，面前提供海量信息的屏幕，当你在凝视它，它也在吞食你。以下截取的 15 分钟片段（00:42:09 到 00:57:26），是父亲根据女儿在社交网站上的图片、视频发现了女儿的可能去处，并没有一味地听从被 “分配” 过来的警探的建议，没想到真的找到了女儿的遗物和沉入湖底的汽车，这也给警察的搜索带来了方向。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>微博电影</tag>
        <tag>文稿备份</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[蒸水蛋做法总结]]></title>
    <url>%2F2018122901.html</url>
    <content type="text"><![CDATA[蒸水蛋是一道小吃，有时候就简称为水蛋，可以当菜配饭吃，也可以配包子当做早餐，或者晚上蒸一碗当做宵夜，都非常好。吃起来嫩滑爽口，而且营养也丰富，做法非常简单，本文就记录蒸水蛋的过程。食材准备 2 人份的材料（1 人份减半即可，但是我觉得 1 人份的太少了，做起来浪费，不值当）： 鸡蛋 2 只（1 只做成 1 碗）葱花少许 生抽少许 食用盐少许 香油少许 制作步骤 1、准备 2 只小碗（有条件的可以使用带盖子的蒸盅，也就是平时吃快餐盛汤的那种带盖子的小碗），普通的小饭碗即可，一定要是耐热的材料，不要用秸秆环保碗、塑料饭盒、普通玻璃碗等（蒸的时候温度很高，虽然水的沸点是 100 度，但是锅内因为有水蒸气存在，压强变大，同时水蒸气转为液态会放热，锅内实际温度大于 100 度），分别打入 1 只鸡蛋，加少许食用盐，搅拌均匀（下面过程就以 1 份为准，另外 1 份是同样的操作）；2、搅拌均匀后开始加温水（最好是温水），温水的量大概是鸡蛋液的 2 倍，即鸡蛋液比温水等于 1:2，注意加温水的量，少了多了都不好（2-3 倍都行，如果碗大一点可能 1 份水蛋就需要放 2 只鸡蛋，不然显得太少了），继续搅拌，此时搅拌完成后表面应该会有一层小泡沫，可以用勺子把小泡沫都盛出去，保证蒸出来的水蛋表面光滑（怕浪费保留也行）； 搅拌均匀 3、蒸鸡蛋的时候使用保鲜膜封住碗口（有条件的使用整蛊更方便，盖子一盖即可），或者使用小盘子反盖在碗口，这样做是为了保证密封，一方面为了保证蒸出来的水蛋嫩滑，另一方面为了避免液化的水蒸气滴进去，影响水蛋的质量，先大火烧水，等水开后转为小火（火力很重要），再蒸 8 分钟即可出锅（这个时间很重要，太久了鸡蛋就老了）； 开始蒸，我为了省事就不撇小泡沫，也不盖保鲜膜了，所以做出来的成品会有点难看 4、取出后，滴入少许香油、生抽（不加也行，直接吃），撒入一点点葱花，即可食用，入口即化，滑嫩可口； 成品 以下这个我认为是做失败的，加太多水，蒸的过程中不断滴入液化的水蒸气，破坏了美感，也没葱花，但是吃起来绝对美味。注意事项1、食用盐是搅拌鸡蛋的时候就加入的，不是蒸好后再放的，这样才能入味而且分布均匀；2、加水时一定要加温水，不是冷水，也不是热水，温水才能让蒸出来的水蛋保持嫩滑；3、如果使用保鲜膜，一定要用可以蒸的材料，不是随便能用的。如果是 PVC 材料（聚氯乙烯），坚决不行，含有塑化剂释放有毒物质影响健康，如果是 PE 材料（聚乙烯），无毒，但是耐热温度不够，也不行，如果是 PVDC 材料（聚偏氯乙烯），安全温度在 140 度，可以使用。所以还是使用盘子比较好。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>蒸水蛋</tag>
        <tag>水蛋</tag>
        <tag>蒸鸡蛋</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 异常之 java.net.BindException: 地址已在使用]]></title>
    <url>%2F2018122801.html</url>
    <content type="text"><![CDATA[今天查看日志发现，所有的 Spark 程序提交时会抛出异常：1java.net.BindException: 地址已在使用 而且不止一次，会连续有多个这种异常，但是 Spark 程序又能正常运行，不会影响到对应的功能。本文就记录发现问题、分析问题的过程。问题出现 在 Driver 端查看日志，发现连续多次相同的异常（省略了业务相关类信息）：异常截图 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879// 第一次异常 2018-12-28_12:50:56 [main] WARN component.AbstractLifeCycle:204: FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: 地址已在使用 java.net.BindException: 地址已在使用 at sun.nio.ch.Net.bind0 (Native Method) at sun.nio.ch.Net.bind (Net.java:433) at sun.nio.ch.Net.bind (Net.java:425) at sun.nio.ch.ServerSocketChannelImpl.bind (ServerSocketChannelImpl.java:223) at sun.nio.ch.ServerSocketAdaptor.bind (ServerSocketAdaptor.java:74) at org.spark-project.jetty.server.nio.SelectChannelConnector.open (SelectChannelConnector.java:187) at org.spark-project.jetty.server.AbstractConnector.doStart (AbstractConnector.java:316) at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart (SelectChannelConnector.java:265) at org.spark-project.jetty.util.component.AbstractLifeCycle.start (AbstractLifeCycle.java:64) at org.spark-project.jetty.server.Server.doStart (Server.java:293) at org.spark-project.jetty.util.component.AbstractLifeCycle.start (AbstractLifeCycle.java:64) at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:252) at org.apache.spark.ui.JettyUtils$$anonfun$5.apply (JettyUtils.scala:262) at org.apache.spark.ui.JettyUtils$$anonfun$5.apply (JettyUtils.scala:262) at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp (Utils.scala:2024) at scala.collection.immutable.Range.foreach$mVc$sp (Range.scala:141) at org.apache.spark.util.Utils$.startServiceOnPort (Utils.scala:2015) at org.apache.spark.ui.JettyUtils$.startJettyServer (JettyUtils.scala:262) at org.apache.spark.ui.WebUI.bind (WebUI.scala:136) at org.apache.spark.SparkContext$$anonfun$13.apply (SparkContext.scala:481) at org.apache.spark.SparkContext$$anonfun$13.apply (SparkContext.scala:481) at scala.Option.foreach (Option.scala:236) at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:481) at org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:59)......2018-12-28_12:50:56 [main] WARN component.AbstractLifeCycle:204: FAILED org.spark-project.jetty.server.Server@33e434c8: java.net.BindException: 地址已在使用 java.net.BindException: 地址已在使用 at sun.nio.ch.Net.bind0 (Native Method) at sun.nio.ch.Net.bind (Net.java:433) at sun.nio.ch.Net.bind (Net.java:425) at sun.nio.ch.ServerSocketChannelImpl.bind (ServerSocketChannelImpl.java:223) at sun.nio.ch.ServerSocketAdaptor.bind (ServerSocketAdaptor.java:74) at org.spark-project.jetty.server.nio.SelectChannelConnector.open (SelectChannelConnector.java:187) at org.spark-project.jetty.server.AbstractConnector.doStart (AbstractConnector.java:316) at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart (SelectChannelConnector.java:265) at org.spark-project.jetty.util.component.AbstractLifeCycle.start (AbstractLifeCycle.java:64) at org.spark-project.jetty.server.Server.doStart (Server.java:293) at org.spark-project.jetty.util.component.AbstractLifeCycle.start (AbstractLifeCycle.java:64) at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:252) at org.apache.spark.ui.JettyUtils$$anonfun$5.apply (JettyUtils.scala:262) at org.apache.spark.ui.JettyUtils$$anonfun$5.apply (JettyUtils.scala:262) at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp (Utils.scala:2024) at scala.collection.immutable.Range.foreach$mVc$sp (Range.scala:141) at org.apache.spark.util.Utils$.startServiceOnPort (Utils.scala:2015) at org.apache.spark.ui.JettyUtils$.startJettyServer (JettyUtils.scala:262) at org.apache.spark.ui.WebUI.bind (WebUI.scala:136) at org.apache.spark.SparkContext$$anonfun$13.apply (SparkContext.scala:481) at org.apache.spark.SparkContext$$anonfun$13.apply (SparkContext.scala:481) at scala.Option.foreach (Option.scala:236) at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:481) at org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:59)......// 第二次异常 2018-12-28_12:50:56 [main] WARN component.AbstractLifeCycle:204: FAILED SelectChannelConnector@0.0.0.0:4041: java.net.BindException: 地址已在使用 java.net.BindException: 地址已在使用 at sun.nio.ch.Net.bind0 (Native Method) at sun.nio.ch.Net.bind (Net.java:433) at sun.nio.ch.Net.bind (Net.java:425)...... 其它信息都一样 // 第三次异常 2018-12-28_12:50:56 [main] WARN component.AbstractLifeCycle:204: FAILED SelectChannelConnector@0.0.0.0:4042: java.net.BindException: 地址已在使用 java.net.BindException: 地址已在使用 at sun.nio.ch.Net.bind0 (Native Method) at sun.nio.ch.Net.bind (Net.java:433) at sun.nio.ch.Net.bind (Net.java:425)....... 其它信息都一样 // 第一次异常 2018-12-28_12:50:56 [main] WARN component.AbstractLifeCycle:204: FAILED SelectChannelConnector@0.0.0.0:4043: java.net.BindException: 地址已在使用 java.net.BindException: 地址已在使用 at sun.nio.ch.Net.bind0 (Native Method) at sun.nio.ch.Net.bind (Net.java:433) at sun.nio.ch.Net.bind (Net.java:425)....... 其它信息都一样 可以轻易发现核心的地方在于：1FAILED SelectChannelConnector@0.0.0.0: 端口号: java.net.BindException: 地址已在使用 端口号在不断变化，从 4040 一直到 4043，才停止了异常的抛出。问题分析 在 Spark 创建 context 的时候，会使用 4040 端口作为默认的 SparkUI 端口，如果遇到 4040 端口被占用，则会抛出异常。接着会尝试下一个可用的端口，采用累加的方式，则使用 4041 端口，很不巧，这个端口也被占用了，也会抛出异常。接着就是重复上面的过程，直到找到空闲的端口。这个异常其实没什么问题，是正常的，原因可能就是在一台机器上面有多个进程都在使用 Spark，创建 context，有的 Spark 任务正在运行着，占用了 4040 端口；或者就是单纯的端口被某些应用程序占用了而已。此时是不能简单地把这些进程杀掉的，会影响别人的业务。问题解决 既然找到了问题，解决办法就很简单了：1、这本来就不是问题，直接忽略即可，不会影响 Spark 任务的正常运行；2、如果非要不想看到异常日志，那么可以检查机器的 4040 端口被什么进程占用了，看看能不能杀掉，当然这种方法不好了；3、可以自己指定端口（使用 spark.ui.port 配置项），确保使用空闲的端口即可（不建议，因为要确认空闲的端口，如果端口不空闲，Spark 的 context 会创建失败，更麻烦，还不如让 Spark 自己去重试）。参考：hortonworks原文：When a spark context is created, it starts an application UI on port 4040 by default. When the UI starts, it checks to see if the port is in use, if so it should increment to 4041. Looks like you have something running on port 4040 there. The application should show you the warning, then try to start the UI on 4041.This should not stop your application from running. If you really want to get around the WARNING, you can manually specify which port for the UI to start on, but I would strongly advise against doing so.To manually specify the port, add this to your spark-submit:–conf spark.ui.port=your_port]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>BindException</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS 异常之 READ is not supported in state standby]]></title>
    <url>%2F2018122702.html</url>
    <content type="text"><![CDATA[今天查看日志发现，以前正常运行的 Spark 程序会不断抛出异常：1org.apache.hadoop.ipc.RemoteException (org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby但是却没有影响到功能的正常运行，只不过是抛出了大量的上述异常，而且内容都一样，也都是操作 HDFS 产生的，所以猜测与 HDFS 集群（或者配置）有关系。本文就记录发现问题、解决问题的过程。问题出现 按照日常操作，查看 Spark 任务的 Driver 端的日志，结果发现了大量的重复异常，又看了一下对功能的影响，结果发现没有影响，所有功能均正常运行，产生的结果也是期望的。问题分析 详细来看一下 Driver 端的日志异常信息：123456789101112131415161718192021222324252627282930313233342018-12-26_23:25:40 [main] INFO retry.RetryInvocationHandler:140: Exception while invoking getFileInfo of class ClientNamenodeProtocolTranslatorPB over hadoop1/192.168.10.162:8020. Trying to fail over immediately.org.apache.hadoop.ipc.RemoteException (org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation (StandbyState.java:87) at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation (NameNode.java:1722) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation (FSNamesystem.java:1362) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo (FSNamesystem.java:4414) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo (NameNodeRpcServer.java:893) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo (ClientNamenodeProtocolServerSideTranslatorPB.java:835) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod (ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call (ProtobufRpcEngine.java:619) at org.apache.hadoop.ipc.RPC$Server.call (RPC.java:962) at org.apache.hadoop.ipc.Server$Handler$1.run (Server.java:2039) at org.apache.hadoop.ipc.Server$Handler$1.run (Server.java:2035) at java.security.AccessController.doPrivileged (Native Method) at javax.security.auth.Subject.doAs (Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs (UserGroupInformation.java:1628) at org.apache.hadoop.ipc.Server$Handler.run (Server.java:2033) at org.apache.hadoop.ipc.Client.call (Client.java:1468) at org.apache.hadoop.ipc.Client.call (Client.java:1399) at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke (ProtobufRpcEngine.java:232) at com.sun.proxy.$Proxy30.getFileInfo (Unknown Source) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo (ClientNamenodeProtocolTranslatorPB.java:768) at sun.reflect.GeneratedMethodAccessor34.invoke (Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke (Method.java:498) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod (RetryInvocationHandler.java:187) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke (RetryInvocationHandler.java:102) at com.sun.proxy.$Proxy31.getFileInfo (Unknown Source) at org.apache.hadoop.hdfs.DFSClient.getFileInfo (DFSClient.java:2007) at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall (DistributedFileSystem.java:1136) at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall (DistributedFileSystem.java:1132) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve (FileSystemLinkResolver.java:81) at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus (DistributedFileSystem.java:1132) at org.apache.hadoop.fs.FileSystem.isFile (FileSystem.java:1426)注意一下核心异常所在：12Exception while invoking getFileInfo of class ClientNamenodeProtocolTranslatorPB over hadoop1/192.168.10.162:8020. Trying to fail over immediately.org.apache.hadoop.ipc.RemoteException (org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby当去从 hadoop1/192.168.10.162:8020 这里 getFileInfo 的时候，抛出了异常，而且明确告诉我们这台机器处于 standby 状态，不支持读取操作。此时，可以想到，肯定是 hadoop1/192.168.10.162:8020 这台机器已经处于 standby 状态了，无法提供服务，所以抛出此异常。既然问题找到了，那么问题产生的原因是什么呢，以及为什么对功能没有影响，接下来一一分析。首先查看 hdfs-site.xml 配置文件，看看 namenode 相关的配置项：12345678910111213141516171819&lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;r-cluster&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.namenodes.r-cluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.r-cluster.nn1&lt;/name&gt; &lt;value&gt;hadoop1:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.r-cluster.nn2&lt;/name&gt; &lt;value&gt;rocket15:8020&lt;/value&gt;&lt;/property&gt;可以看到，namenode 相关配置有 2 台机器：nn1、nn2，而上述产生异常的信息表明连接 nn1 被拒绝，那么我去看一下 HDFS 集群的状态，发现 nn1 果然是 standby 状态的，而 nn2（rocket15） 才是 active 状态。再仔细查看日志，没有发现连接 nn2 的异常，那就说明是第一次连接 nn1 抛出异常，然后试图连接 nn2，成功连接，没有抛出异常，接下来程序就正常处理数据了，对功能没有任何影响。到这里，我们已经分析出了整个过程，现象表明这个异常只是连接了 standby 状态的 namenode，是正常抛出的。然后会再次连接另外一台 active 状态的 namenode，连接成功。抛异常的流程细节 1、客户端在连接 HDFS 的时候，会从配置文件 hdfs-site.xml 中，读取 nameservices 的配置，获取机器编号，我这里是 nn1 和 nn2，分别对应着 2 台 namenode 机器；2、客户端会首先选择编号较小的 namenode（我这里是 nn1，对应着 hadoop1），试图连接；3、如果这台 namenode 是 active 状态，则客户端可以正常处理请求；但是如果这台 namenode 是 standby 状态，则客户端抛出由服务端返回的异常：Operation category READ is not supported in state standby，同时打印 ip 信息，接着会尝试连接另外一台编号较大的 namenode（我这里是 nn2，即 rocket15）；4、如果连接成功，则客户端可以正常处理请求；如果 nn2 仍然像 nn1 一样，客户端会抛出一样的异常，此时会继续反复重试 nn1 与 nn2（重试次数有配置项，间隔时间有配置项）；如果有成功的，则客户端可以正常处理请求，如果全部失败，则客户端无法正常处理请求，此时应该要关注解决 namenode 为什么全部都处在 standby 状态。 配置参数如下（参考 Hadoop 官方文档 ）：1234567891011121314151617181920212223242526272829&lt;!-- 客户端重试次数，默认 15 --&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.max.attempts&lt;/name&gt; &lt;value&gt;15&lt;/value&gt;&lt;/property&gt;&lt;!-- 客户端 2 次重试间隔时间，默认 500 毫秒 --&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.sleep.base.millis&lt;/name&gt; &lt;value&gt;500&lt;/value&gt;&lt;/property&gt;&lt;!-- 客户端 2 次重试间隔时间，默认 1500 毫秒 --&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.sleep.max.millis&lt;/name&gt; &lt;value&gt;1500&lt;/value&gt;&lt;/property&gt;&lt;!-- 客户端 1 次连接中重试次数，默认 0, 在网络不稳定时建议加大此值 --&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.connection.retries&lt;/name&gt; &lt;value&gt;0&lt;/value&gt;&lt;/property&gt;&lt;!-- 客户端 1 次连接中超时重试次数，仅是指超时重试，默认 0, 在网络不稳定时建议加大此值 --&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.connection.retries.on.timeouts&lt;/name&gt; &lt;value&gt;0&lt;/value&gt;&lt;/property&gt; 问题解决 既然明确了问题，并且分析出了具体原因，解决起来就简单了，对于我这种情况，有 2 种方法：1、不用解决，也无需关心，这个异常没有任何影响，会自动重连另外一台 active 状态的 namenode 机器的；2、如果就是一心想把异常消除掉，那就更改 hdfs-site.xml 配置文件里面的 nameservices 配置项对应的机器，把编号最小的机器设置成状态为 active 的 namenode（例如我这里把 nn1、nn2 的对应的机器 ip 地址交换一下即可，确保 nn1 是 active 状态的），那么连接 HDFS 的时候第一次就会直接连接这台机器，就不会抛出异常了（但是要注意 namenode 以后可能是会挂的，挂了会自动切换，那么到那个时候还要更改这个配置项）。123456789&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.r-cluster.nn1&lt;/name&gt; &lt;value&gt;rocket15:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.r-cluster.nn2&lt;/name&gt; &lt;value&gt;hadoop1:8020&lt;/value&gt;&lt;/property&gt;问题总结1、参考：http://support-it.huawei.com/docs/zh-cn/fusioninsight-all/maintenance-guide/zh-cn_topic_0062904132.html2、这个问题其实不是问题，只不过抛出了异常，我看到有点担心而已，但是如果连接所有的机器都抛出这种异常，并且重试了很多次就有影响了，说明所有的 namenode 都挂了，根本无法正常操作 HDFS 系统；3、根据 2 进行总结：如果只是在操作 HDFS 的时候打印一次（每次操作都会打印一次），说明第一次连接到了 standby 状态的 namenode，是正常的，不用关心；但是，如果出现了大量的异常（比如连续 10 次，连续 20 次），说明 namenode 出问题了，此时应该关心 namenode 的状态，确保正常服务。]]></content>
      <categories>
        <category>Hadoop 从零基础到入门系列</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Spark</tag>
        <tag>HDFS</tag>
        <tag>nameNode</tag>
        <tag>standby</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS 异常之 Filesystem closed]]></title>
    <url>%2F2018122701.html</url>
    <content type="text"><![CDATA[今天通过 Hadoop 的 api 去操作 HDFS 里面的文件，读取文本内容，但是在代码里面总是抛出以下异常：1Caused by: java.io.IOException: Filesystem closed然而文本内容又是正常读取出来的，但是我隐隐觉得读取的文本内容可能不全，应该只是所有文本内容的一部分。本文就记录这个问题的原因、影响以及解决方法。问题出现 通过查看日志发现，有大量的异常日志打印出来，全部都是操作 HDFS 的时候产生的，有的是使用 Spark 连接 HDFS 读取文本数据，有的是使用 Hadoop 的 Java api 通过文件流来读取数据，每次读取操作都会产生一个如下异常信息（会影响实际读取的内容，多个 DataNode 的内容会漏掉）：123456789101112131415161718192021222324252627282018-12-26_23:25:46 [SparkListenerBus] ERROR scheduler.LiveListenerBus:95: Listener EventLoggingListener threw an exceptionjava.lang.reflect.InvocationTargetException at sun.reflect.GeneratedMethodAccessor33.invoke (Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke (Method.java:498) at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply (EventLoggingListener.scala:150) at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply (EventLoggingListener.scala:150) at scala.Option.foreach (Option.scala:236) at org.apache.spark.scheduler.EventLoggingListener.logEvent (EventLoggingListener.scala:150) at org.apache.spark.scheduler.EventLoggingListener.onJobStart (EventLoggingListener.scala:173) at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent (SparkListenerBus.scala:34) at org.apache.spark.scheduler.LiveListenerBus.onPostEvent (LiveListenerBus.scala:31) at org.apache.spark.scheduler.LiveListenerBus.onPostEvent (LiveListenerBus.scala:31) at org.apache.spark.util.ListenerBus$class.postToAll (ListenerBus.scala:55) at org.apache.spark.util.AsynchronousListenerBus.postToAll (AsynchronousListenerBus.scala:37) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp (AsynchronousListenerBus.scala:80) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply (AsynchronousListenerBus.scala:65) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply (AsynchronousListenerBus.scala:65) at scala.util.DynamicVariable.withValue (DynamicVariable.scala:57) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp (AsynchronousListenerBus.scala:64) at org.apache.spark.util.Utils$.tryOrStopSparkContext (Utils.scala:1181) at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run (AsynchronousListenerBus.scala:63)Caused by: java.io.IOException: Filesystem closed at org.apache.hadoop.hdfs.DFSClient.checkOpen (DFSClient.java:795) at org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync (DFSOutputStream.java:1986) at org.apache.hadoop.hdfs.DFSOutputStream.hflush (DFSOutputStream.java:1947) at org.apache.hadoop.fs.FSDataOutputStream.hflush (FSDataOutputStream.java:130) ... 20 more最直接清晰的描述就是：1Caused by: java.io.IOException: Filesystem closed上述异常信息表明 HDFS 的 Filesystem 被关闭了，但是代码仍旧试图打开文件流读取内容。问题解决 分析一下 根据上述信息，查看代码，每次操作 HDFS 都是独立的，会先根据统一的 conf 创建 Filesystem，然后根据文件路径创建 Path，打开输入流，读取内容，读取完成后关闭 Filesystem，没有什么异常的地方。同时，根据异常信息可以发现，异常的抛出点并不是业务逻辑代码，更像是已经开始开启文件流读取文件，读着读着 Filesystem 就被关闭了，然后引发了异常，而业务逻辑中并没有突然关闭 Filesystem 的地方，也没有多线程操作 Filesystem 的地方。1234567891011121314151617181920212223242526272829303132333435363738/** * 获取文件内容 * 纯文本，不做转换 * 如果传入目录，返回空内容 * * @param hdfsFile * @return */public static Set&lt;String&gt; getFileContent(String hdfsFile) &#123; Set&lt;String&gt; dataResult = new HashSet&lt;&gt;(); FileSystem fs = null; try &#123; // 连接 hdfs fs = FileSystem.get (CONF); Path path = new Path (hdfsFile); if (fs.isFile (path)) &#123; FSDataInputStream fsDataInputStream = fs.open (path); BufferedReader bufferedReader = new BufferedReader (new InputStreamReader (fsDataInputStream)); String line = null; while (null != (line = bufferedReader.readLine ())) &#123; dataResult.add (line); &#125; &#125; else &#123; LOGGER.error ("!!!! 当前输入参数为目录，不读取内容:&#123;&#125;", hdfsFile); &#125; &#125; catch (Exception e) &#123; LOGGER.error ("!!!! 处理 hdfs 出错:" + e.getMessage (), e); &#125; finally &#123; if (null != fs) &#123; try &#123; fs.close (); &#125; catch (IOException e) &#123; LOGGER.error ("!!!! 关闭文件流出错:" + e.getMessage (), e); &#125; &#125; &#125; return dataResult;&#125;通过查找文档发现，这个异常是 Filesystem 的缓存导致的。当任务提交到集群上面以后，多个 datanode 在 getFileSystem 过程中，由于 Configuration 一样，会得到同一个 FileSystem。如果有一个 datanode 在使用完关闭连接，其它的 datanode 在访问时就会出现上述异常，导致数据缺失（如果数据恰好只存在一个 datanode 上面，可能没问题）。找到方法 通过上面的分析，找到了原因所在，那么解决方法有 2 种：1、可以在 HDFS 的 core-site.xml 配置文件里面把 fs.hdfs.impl.disable.cache 设置为 true，这样设置会全局生效，所有使用这个配置文件的连接都会使用这种方式，有时候可能不想这样更改，那就使用第 2 种方式；1234&lt;property&gt; &lt;name&gt;fs.hdfs.impl.disable.cache&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;2、在 HDFS 提供的 Java api 里面更改配置信息，则会只针对使用当前 conf 的连接有效，相当于临时参数。12// 缓存 fs, 避免多 datanode 异常：Caused by: java.io.IOException: Filesystem closedCONF.setBoolean ("fs.hdfs.impl.disable.cache", true);上面 2 种方法的目的都是为了关闭缓存 Filesyetem 实例，这样每次获得的 Filesystem 实例都是独立的，不会产生上述的异常，但是缺点就是会增加网络的 I/O，频繁开启、关闭文件流。问题总结1、参考：https://stackoverflow.com/questions/23779186/ioexception-filesystem-closed-exception-when-running-oozie-workflow ；2、保留日志，查看日志很重要；3、FileSytem 类内部有一个 static CACHE，用来保存每种文件系统的实例集合，FileSystem 类中可以通过参数 fs.% s.impl.disable.cache 来指定是否禁用缓存 FileSystem 实例（其中 % s 替换为相应的 scheme，比如 hdfs、local、s3、s3n 等）。如果没禁用，一旦创建了相应的 FileSystem 实例，这个实例将会保存在缓存中，此后每次 get 都会获取同一个实例，但是如果被关闭了，则再次用到就会无法获取（多 datanode 读取数据的时候）；4、源码分析放在以后，留坑。]]></content>
      <categories>
        <category>Hadoop 从零基础到入门系列</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Spark</tag>
        <tag>Filesystem</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[可乐鸡翅做法总结]]></title>
    <url>%2F2018122501.html</url>
    <content type="text"><![CDATA[可乐鸡翅，是一道做法很简单的菜，很巧妙地将饮料和鸡翅结合在一起，做出来的可乐鸡翅既好看又好吃。本文简单介绍可乐鸡翅的做法总结，这是一种偏甜的做法。食材准备 3 人份的材料（8-10 个鸡翅），吃多了也不好吃1、鸡翅 9 个，最好是鸡中翅（一般 2-3 元一个）；2、可乐 1 罐（330 毫升的，如果鸡翅多的话，适当增加可乐）； 选择百事可乐 3、姜片、八角、桂皮（也可以不用）；4、料酒、生抽、老抽、食用盐； 制作步骤 1、在鸡翅背面划几刀（正面保留完整为了摆盘好看而已），更容易入味，用食用盐、料酒、老抽腌制 10 分钟，备用； 划刀腌制 2、锅中加水，放入姜片、少量料酒，鸡翅也下锅（冷水下锅），煮开即可，不用煮透（煮透鸡翅就老了），看到浮沫很多就可以捞出，用温水清洗一下，晾干（晾不干就用厨房纸擦一下，防止煎的时候溅油），此时如果发现有不干净的鸡毛可以拔干净； 鸡翅冷水下锅 鸡翅焯水出浮沫 3、锅中放入少量油（不放也行，鸡翅会自己出油的），放入姜片，开始煎鸡翅，开小火，煎至两面金黄即可，不可以煎太久，否则鸡翅老了不好吃； 小火煎 4、加一罐可乐，适量料酒、生抽、老抽，适量桂皮、八角，开始小火炖煮，炖至可乐还有一小碗水的量的时候，尝尝味道，适量加盐； 加入可乐、配料 小火炖煮 5、炖至汤浓收汁，基本所有的汁都覆盖在鸡翅上面了，鸡翅也有味道，装盘，正面朝上，把锅中剩余的汤汁淋入鸡翅中（大概 1-2 饭勺的量），再撒上少许白芝麻，既好看又好吃。 可以收汁 收汁之前补充食用盐、老抽 收汁完成 装盘 注意事项1、不要再放糖了，一罐可乐里面含糖大概 35 克；2、如果放了那种本身是咸味的生抽，也不用放盐了，或者少量放一点点（放盐之前先尝尝汤水的味道，不容易出差错）；3、焯水的时候冷水下锅，防止肉老了，并且放一点姜片和料酒，去腥味；4、鸡翅焯水后晾干很有必要，否则下一步骤煎的时候水和热油混合一起会溅出油的；5、甜味和咸味的控制依据个人口味调整，此外，可口可乐比百事可乐更甜，即含糖量更高。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>可乐鸡翅</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 错误之 JavaSparkContext not serializable]]></title>
    <url>%2F2018122101.html</url>
    <content type="text"><![CDATA[今天更新代码，对 Spark 里面的 RDD 随便增加了一个 Function，结果遇到了序列化（Serializable）的问题，这个不是普通的自定义类不能序列化问题，而是 JavaSparkContext 的用法问题，由于小看了这个问题，多花了一点时间解决问题，本文就记录下这一过程。问题出现 针对已有的项目改动了一点点，结果直接出现了这个错误：一开始疏忽大意了，以为像往常一样，是某些需要传递的对象对应的类没有序列化，由于对代码不敢改动太大，就想着用最简单的方法，把几个自定义类都序列化了，以为就应该可以了。结果，还是不行，此时虽然不会有自定义类的序列化问题了，但是却出现了终极错误：JavaSparkContext not serializable，这是什么意思呢，是说 JavaSparkContext 不能序列化，总不能把 JavaSparkContext 序列化吧，Spark 是不允许这么干的。那么问题是什么呢？我首先猜测肯定是 Function 里面用到了 JavaSparkContext 对象，导致启动 Spark 任务的时候，需要序列化 Function 用到的所有对象（当然也需要序列化对象所属类里面的所有属性），而这些 Function 所用到的所有对象里面，就有 JavaSparkContext 对象。于是，我耐心看了一下代码，果然，在创建 Function 对象的时候，竟然把 JavaSparkContext 对象作为参数传进去了，还是因为 JavaSparkContext 不能乱用。其实，报错日志里面都已经明显指向说明了，除了自定义的类，错误归结于 1at org.apache.spark.api.java.AbstractJavaRDDLike.mapPartitions (JavaRDDLike.scala:46) 而这里的代码，正是我增加的一部分，为了贪图简单方便，直接把 JavaSparkContext 对象传递给了 mapPartitions 对应的 Function。解决问题 既然找到了问题，接下来就好办了。既然 JavaSparkContext 不能乱用，那就不用，把这个传递参数去掉，即可正常运行，但是这样做太简单粗暴，不是解决问题的思路。仔细分析一下，可以有 2 种解决办法（思路就是避免序列化）：1、如果在 Function 里面非要用到 JavaSparkContext 对象，那就把 JavaSparkContext 对象设置为全局静态的 Java 属性（使用 static 关键字），那么在哪里都可以调用它了，而无需担心序列化的问题（静态属性可以避免从 Driver 端发送到 Executor 端，从而避免了序列化过程）；2、对于 Function 不要使用内部匿名类，这样必然需要序列化 Function 对象，同时也必然需要序列化 Function 对象用到的 JavaSparkContext 对象，其实可以把 Function 类定义为内部静态类，就可以避免序列化了。问题总结 1、出现这种错误，不要想当然地认为就是某种原因造成的，而要先看详细日志，否则会走弯路，浪费一些时间（虽然最终也能解决问题）；2、有时候状态不好，晕乎乎的，找问题又慢又低效，此时应该休息一下，等头脑清醒了再继续找问题，否则可能事倍功半，而且影响心情。 参考：https://stackoverflow.com/questions/27706813/javasparkcontext-not-serializable]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark序列化</tag>
        <tag>serializable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微博 url mid 相互转换算法实现 - Java 版本]]></title>
    <url>%2F2018122001.html</url>
    <content type="text"><![CDATA[对微博数据有了解的人都知道，一条微博内容对应有唯一的微博 url，同时对微博官方来说，又会生成一个 mid，mid 就是一条微博的唯一标识（就像 uid 是微博用户的唯一标识一样），也类似于人的身份证号。其实，微博 url 里面有一串看起来无意义的字符（由字母、数字组成，6-8 个字符长度），可以和 mid 互相转换，本文就根据理论以及 Java 版本的实现，讲解微博 url 与 mid 的互相转换过程。 待整理。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>微博url</tag>
        <tag>微博mid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript 中字符串截取方法总结]]></title>
    <url>%2F2018121901.html</url>
    <content type="text"><![CDATA[最近在处理数据的时候，用到了 JavaScript 编程语言，通过绕弯路来解决 ETL 处理的逻辑，其中就用到了字符串的截取方法，查 JavaScript 的文档看到了 3 个方法，被绕的有点晕，本文就总结一下 JavaScript 中字符串截取的方法。开篇 首先声明，JavaScript 中对方法名字的大小写是敏感的，该是小写就是小写，该是大写就是大写。substring () 方法 定义和用法 substring () 方法用于截取字符串中介于两个指定下标之间的字符 语法 stringObject.substring (start, stop) 上述参数解释：参数名 解释说明 start 必须，一个整数（是负数则被自动置为 0），要截取的子串的第一个字符在 stringObject 中的位置 end 可选（如果省略该参数，则被默认为字符串长度），一个整数（是负数则被自动置为 0），比要截取的子串的最后一个字符在 stringObject 中的位置多 1返回值 一个全新的字符串，其实就是 stringObject 的一个子字符串，其内容是从 start 到 stop-1 的所有字符，其长度为 stop 减 start。注意事项 1、substring () 方法返回的子字符串包括 start 处的字符，但是不包括 stop 处的字符，这一点可能很多人会迷惑，其实很多编程语言都是这个逻辑；2、如果参数 start 与 stop 相等，那么该方法返回的就是一个空串（即长度为 0 的字符串，不是 null，也不是 undefined）；3、如果 start 比 stop 大，那么该方法在截取子串之前会先交换这两个参数，这就会导致参数的顺序不影响截取的结果了；4、参数理论上不能出现负数（在本方法中无特殊意义，在其它方法中就有特殊意义了），如果有，那么在截取子串之前会被置为 0。 举例说明 例子 1（从下标 3 截取到字符串最后）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substring (3))&lt;/script&gt;输出（长度为 10 的子串）：1lo-world!例子 2（从下标 3 截取到下标 8）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substring (3, 8))&lt;/script&gt;输出（长度为 5 的子串）：1lo-wo例子 3（从下标 3 截取到下标 8，但是参数位置反了）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substring (8, 3))&lt;/script&gt;输出（长度为 5 的子串）：1lo-wo例子 4（参数为负数，从下标 0 截取到下标 3）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substring (-1, 3))&lt;/script&gt;输出（长度为 3 的子串）：1Helsubstr () 方法 定义和用法 substr () 方法可在字符串中截取从 start 下标开始的指定长度的子串 语法 stringObject.substr (start, length) 上述参数解释：参数名 解释说明 start 必须，必须是数值（0、正数、负数都可以），表示要截取的子串的起始下标。如果是负数，那么该参数声明的是从字符串的尾部开始计算的位置。也就是说，-1 指字符串中最后一个字符，-2 指倒数第二个字符，以此类推。（参数为负数也可以理解成字符串长度加负数之和即为起始下标）length可选（如果省略该参数，那么默认为从 start 开始一直到 stringObject 的结尾对应的长度），必须是数值（0、正数、负数都可以）。返回值 一个全新的字符串，包含从 stringObject 的 start（包括 start 所指的字符）下标开始的 length 个字符。如果没有指定 length，那么返回的字符串包含从 start 到 stringObject 的结尾的字符。如果 length 指定为负数或者 0，那么返回空串。如果 length 指定为远远大于 stringObject 长度的正数，那么返回的字符串包含从 start 到 stringObject 的结尾的字符。注意事项 1、start 参数为负数是有特殊含义的；2、如果 length 指定为负数或者 0，那么返回空串（即长度为 0 的字符串，不是 null，也不是 undefined）；3、ECMAscript 没有对该方法进行标准化，因此不建议使用它。 举例说明 例子 1（从下标 3 截取到字符串最后）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substr (3))&lt;/script&gt;输出（长度为 9 的子串）：1lo-world!例子 2（从下标 3 截取长度为 5 的子串）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substr (3, 5))&lt;/script&gt;输出（长度为 5 的子串）：1lo-wo例子 3（从下标 3 截取长度为 - 5 的子串，返回空串）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substr (3, -5))&lt;/script&gt;输出（返回空串）：12例子 4（start 参数为负数，即从字符串倒数第 5 个位置截取长度为 3 的子串）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substr (-5, 3))&lt;/script&gt;输出（长度为 3 的子串）：1orlslice () 方法 定义和用法 slice () 方法用于截取字符串中介于两个指定下标之间的字符，与 substring () 方法的功能类似 语法 stringObject.slice (start, end) 上述参数解释：参数名 解释说明 start 必须，一个整数（0、正数、负数，负数有特殊含义），要截取的子串的第一个字符在 stringObject 中的位置。如果是负数，那么该参数声明的是从字符串的尾部开始计算的位置。也就是说，-1 指字符串中最后一个字符，-2 指倒数第二个字符，以此类推。（参数为负数也可以理解成字符串长度加负数之和即为起始下标）end可选（如果省略该参数，则被默认为字符串长度），一个整数（负数含义与 start 相同），比要截取的子串的最后一个字符在 stringObject 中的位置多 1返回值 一个全新的字符串，其实就是 stringObject 的一个子字符串，其内容是从 start 到 stop-1 的所有字符，其长度为 stop 减 start。注意事项 1、slice () 方法返回的子字符串包括 start 处的字符，但是不包括 stop 处的字符，这一点可能很多人会迷惑，其实很多编程语言都是这个逻辑；2、如果参数 start 与 stop 相等，那么该方法返回的就是一个空串（即长度为 0 的字符串，不是 null，也不是 undefined）；3、参数可以出现负数（比 substring () 方法灵活多了）。 举例说明 例子 1（从下标 3 截取到字符串最后）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.slice (3))&lt;/script&gt;输出（长度为 9 的子串）：1lo-world!例子 2（从下标 3 截取到下标 8）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.slice (3, 8))&lt;/script&gt;输出（长度为 5 的子串）：1lo-wo例子 3（从下标 3 截取到下标 8，但是参数使用负数，从下标 - 9 截取到下标 - 4）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.slice (-9, -4))&lt;/script&gt;输出（长度为 5 的子串，（-4）-（-9）=5）：1lo-wo例子 4（从下标 3 截取到下标 2）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.slice (3, 2))&lt;/script&gt;输出返回空串）：12]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>字符串截取</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[西红柿疙瘩汤做法总结]]></title>
    <url>%2F2018121601.html</url>
    <content type="text"><![CDATA[西红柿疙瘩汤，是一道做法非常简单的主食与配菜混为一起的菜肴，适合在寒冷的冬天食用，吃一碗热乎乎的，非常暖胃，我知道在中原地区（河南、安徽北部）都有这个做法。本文就讲述西红柿疙瘩汤的做法总结。食材准备 以下的食材份量大约 2 人份：黄心乌菜一颗（实在没有使用其它青菜也可以）西红柿一颗（粉的最好，与脆的对立）鸡蛋 2 颗 面粉 100 克 小葱、香菜各 2 棵 调味料（食用盐、芝麻油）制作步骤 从开火到关火预计耗时 15-20 分钟。0、葱花香菜段。1、西红柿去皮，划十字刀花，放入热水中烫 1 分钟左右，取出直接去皮，不去皮也行，但是会影响口感，去皮后切丁，切小一点，放入碗中备用。粉粉的西红柿 十字花刀 开水烫 1 分钟（30 秒翻身一次），轻易去皮 西红柿去皮 西红柿切丁 2、准备黄心乌菜，洗干净，随便切（手撕也行，无所谓），切成条状或者小块状，别太大就行。3、面粉放入大碗中，放在水龙头下，让水一滴一滴滴下来，迅速搅拌面粉，很快就可以做成面粒。4、鸡蛋打入碗中，搅拌均匀备用。5、锅烧热，倒入油，炒制西红柿丁，中小火炒制 3-5 分钟，此时西红柿的状态就是一半是糊状，一半是小颗粒，混合在一起，倒入开水（注意量的控制，比想象的多倒一点，面粒会吸收大量水分的），大火烧开。 西红柿丁炒制 加开水，煮开 6、烧开后放入面粒，大火煮 5 分钟，面粒基本熟透，汤变得浓稠，放入青菜，中火继续煮 1 分钟左右，鸡蛋液慢慢淋入锅中，搅拌，放入食用盐，中火继续煮 2 分钟。 放入面粒，继续煮 5 分钟 7、开锅，放入芝麻油、香菜段，葱花，搅拌十几秒，关火。 一锅 一碗 做完顺便又加了 2 个菜：花菜回锅肉 辣椒回锅肉 注意事项 1、青菜最好选择黄心乌，因为我一直吃的都是这种，黄心乌这种青菜一般在沿淮地区才播种，因为它比较耐寒，在秋季播种，在冬天收割，一般北方的冬天也看不到其它青菜可以生长了。2、条件允许的话，可以放一点酱肉之类的肉粒进去，更能增加食欲。3、做面粒的时候切记不要直接倒水搅拌，这样是做不成的一粒一粒的效果的，只能用水滴进去然后迅速搅拌，使水滴周围裹上面粉形成一粒，很快就全部都是面粒了，而且很均匀，另外，做好面粒后要立马使用，不要提前做好放那里，因为放久了（10 分钟都不行）面粒会粘连在一起，实在要放的话再多加点面粉进去，让面粒之间隔开。4、西红柿最好选择粉的，就是那种吃起来很柔绵的，更容易做成均匀的汤。5、如果在煮的过程中发现有点粘锅，那是因为水少了，面太多了，这时候用汤勺试着加 1-2 勺水进去，再搅拌一下，如果还是粘锅再加 1-2 勺，千万不要一下子加很多水，面汤最好的状态就是不粘锅但是又很浓稠。6、做回锅肉，肉要煮到什么程度才能回锅，简单的判断方法就是筷子可以轻易穿透肉，一般要煮 20 分钟以上。 补充说明2018 年 12 月 23 日，广州突然降温，降到 17 度左右（前一天的冬至还 25 度呢，短袖都穿起来了），天气冷了，于是又煮了一锅。可惜这次没买到香菜，没买到黄心乌菜，也没买到酱肉，凑活着吃。2018 年 12 月 30 日，广州的温度降到了个位数，最低 5 度，实在是冷。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>西红柿疙瘩汤</tag>
        <tag>疙瘩汤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用正则表达式列表]]></title>
    <url>%2F2018121401.html</url>
    <content type="text"><![CDATA[正则表达式是一种匹配模式，在日常学习或者工作中，如果善于使用正则表达式，可以大大提高效率。特别是在文本处理方面，使用正则表达式与不使用的效率可谓天壤之别。当然，如果确实没有使用的场景，了解一下正则表达式的知识点也是有好处的，可以从归纳总结的角度思考问题。本文首先概述一下正则表达式的基本知识点，再记录一些常用的正则表达式，以便查用。正则表达式的概念 正则表达式【regular expression】描述了一种匹配字符串的 模式 【pattern】，其本身是一个 字符串 ，由 普通字符 、 元字符 组成。通俗点说，正则表达式就是一种人为定义的规则，用来匹配字符串，一般有三种作用：可以检查一个字符串中是否含有某种子串，例如判断微博内容中是否包含微博话题 将字符串中某种子串替换掉，例如替换微博内容中的话题为空白 从某个字符串中抽取某种子串，例如从微博内容中抽取话题 正则表达式是非常简单而强大的，学会之后，除了能提高效率，还会给你带来绝对的成就感。所以，在此，要了解上面的概念并记住这几个名词： 模式 、 字符串 、 普通字符 、 元字符 。正则表达式的入门语法 正则表达式的语法很简单，本质就是一个字符串，所以任意一个字符串就是一个正则表达式，但是为了保证这个字符串有意义、有实际用处，还是要遵循正则表达式的语法规范，下面就列举一些常见的语法规范。基本字符 []：中括号表达式，表示集合()：子表达式{}：限定符表达式，用来限定次数|：多个选项中的一个，即或的逻辑关系，满足其一即可.：点号，表示除去换行符【\n】之外的任意字符\：转义字符，用来转义特殊字符 定位符 ^：开始位置$：结束位置 限定符 *：匹配 0 次或多次+：匹配 1 次或多次?：匹配 0 次或 1 次{n}：匹配确定的 n 次{n,}：至少匹配 n 次{n,m}：最少匹配 n 次且最多匹配 m 次 转义字符 除了表示特殊的字符，匹配元字符都需要使用 \r：回车符\n：换行符\s：匹配任何空白字符，包括空格、制表符、换页符，等价于 [\f\n\r\t\v]\d：匹配一个数字字符，等价于 [0-9]\w：匹配字母、数字、下划线，等价于 [A-Za-z0-9_] 普通字符 元字符之外的字符都是普通字符，即 1-4 之外的所有字符【当然，1-4 中没有列出完整的元字符】，例如标点符号、中文字、英文字母、数字等都是普通字符。其实，平时大家都会用到的一个功能就涉及到正则表达式：在文件管理器中根据关键词搜索文件，虽然输入的只是一个普通的字符串，但是文件管理器会把它作为正则表达式来搜索，只要文件名称包含指定的字符串，就会命中【而不是说文件名一定要等于指定的字符串】。常用模式举例 这里所说的模式，其实就是一条规范的正则表达式，具有实际的用处。座机号码：\d {3}-\d {8}|\d {4}-\d {7,8}网址链接：(https?|ftp|file)://[-A-Za-z0-9+&amp;@#/%?=~_|!:,.;]+[-A-Za-z0-9+&amp;@#/%=~_|]微博表情【[] 里包含 1 到 8 个中文、字母】：\[[\u4e00-\u9fa5A-Za-z]{1,8}\]微博话题：#[^@&lt;&gt;#&quot;&amp;&#39;\r\n\t]{1,49}#，不考虑复杂情况使用 #[^#]{1,49}# 也可以 微博用户昵称【中文、数字、字母、横线、下划线的组合，2-30 个字符】：@[\u4e00-\u9fa5A-Z0-9a-z_-]{2,30}所有的小写字母：[a-z]所有的大写字母：[A-Z]所有的字母：[a-zA-Z]所有的数字、句号、减号：[0-9\.\-]中文：[\u4e00-\u9fa5]，利用了转义字符与 Unicode 编码 除了小写字母以外的所有字符：[^a-z]除了双引号【“】和单引号【’】之外的所有字符：[^\”\&#39;]整数：^\-?[0-9]+$小数：^[-]?[0-9]+(\.[0-9]+)?$一个数字：\d，或者 [0-9]单词 yeah 连续出现一次以上：(yeah)+其它正则表达式可以参考菜鸟工具里面的列表：https://c.runoob.com/front-end/854 。在线学习网站以及使用演示 看的再多，懂得再多，不使用很快也就忘记了，所以切记：熟能生巧。下面就使用微博内容的例子来演示正则表达式的入门级别使用。使用在线工具 1、https://tool.oschina.net/regex2、https://regex101.com 【需要翻墙，注意程序语言的选择，选择 ECMAScript，如果使用过程中有红色填充就表示出错警告】 假如有一篇微博，内容为：1 我分享了一张图片，来自 #腾讯动漫# 我分享了一张图片，来自 #腾讯动漫# http://t.cn/RDFUnja http://t.cn/zT8RAls //@狮鸢 LionGlede2018:[笑 cry] 哈哈哈哈哈换肚子疼 //@果子狸爬大树: [允悲]//@飞船叔叔：呵呵哈哈哈太乖了 //@Suhero-D-Ace: 笑昏 //@土豆动漫：真・睡蒙了 [允悲]//@妖妖小精: [允悲]//@M 大王叫我来巡山：这书包咋这么硬呢 [允悲][good]演示几个匹配操作：1、抽取微博内容中的话题：#[^@&lt;&gt;#&quot;&amp;&#39;\r\n\t]{1,49}#2、抽取微博内容中的表情：\[[\u4e00-\u9fa5A-Za-z]{1,8}\]3、判断是否以【我分享了一张图片，来自】开头：^ 我分享了一张图片，来自.*当然，使用 ^(我分享了一张图片，来自).* 更为准确 4、抽取微博内容中的链接：(https?|ftp|file):\/\/[-A-Za-z0-9+&amp;@#/%?=~_|!:,.;]+[-A-Za-z0-9+&amp;@#/%=~_|]5、抽取微博内容中的用户昵称：@[\u4e00-\u9fa5A-Z0-9a-z_-]{2,30} 总结 使用正则表达式之前需要先归纳总结，找到需要匹配的内容的规律，然后才能着手写正则表达式【即模式】，接着才能测试验证正则表达式是否准确。还需要注意一点，在不同的编程语言中，对于某些特殊的字符需要转义，例如双引号、单引号、反斜杠等，还有些符号需要额外特殊处理，例如在 PHP 中，中文编码需要使用形如 \x {4e00}-\x {9fa5A} 的格式。在文本编辑器中使用 平时处理文本文件时，有时候需要查找替换指定的内容、删除指定的内容、统计指定的内容出现的次数等等。如果这里指定的内容只是具体的字符串【例如在内容中查找：爱国敬业】，用不到正则表达式，但是如果指定的内容是一种规则，无法给出具体的表示【例如在内容中查找网址链接，并不是指某一条具体的网址链接】，那就需要正则表达式出场了。文本编辑器推荐 1、不要使用 Windows 自带的记事本，原因：编码支持差、效率低、扩展功能弱。 举例：【联通】乱码问题、打开 11MB 大小的日志文件会卡住 2、推荐 Notepad++、Sublimetext、EmEditor 等文本编辑器Notepad++ 官网：https://notepad-plus-plus.orgSublimetext 官网：https://www.sublimetext.com【打不开进镜像网站：http://www.sublimetextcn.com 】3、此外还有一些收费的工具，例如：Ultraedit、EditPlus 等，不再介绍 实际演示 使用 Sublimetext 工具，继续使用上述的微博内容，操作演示，更多功能请自行探索：1、查找替换表情，批量操作【需要开启正则模式】查找【Ctrl + F】，或者在 查找 下拉列表中选择 查找匹配值 查找替换【Ctrl + H】，或者在 查找 下拉列表中选择 替换匹配值 2、统计网址链接个数，在左下角可以看到命中个数【需要开启正则模式】 查找【Ctrl + F】，或者在 查找 下拉列表中选择 查找匹配值 3、打开大文件，操作流畅【打开 11MB 的 log 文件】4、查看设置文件编码 在 文件 -&gt; 保存 编码中选择 5、同时修改多处内容，Multiple Selections【Ctrl+D】6、列模式编辑【Ctrl + Shift + L，配合 Shift 多选内容】 此外，有的版本显示文件名会乱码，显示方格，需要在 首选项 -&gt; 设置 - 用户 中添加配置项：&quot;dpi_scale&quot;: 1.0 ，即可正常显示。使用 EmEditor 工具：使用 EmEditor 打开 10 万数据量的大文件，这是一款针对 csv 文件专门设计的工具，几十万数据量不在话下，只要电脑内存够用，打开几个 GB 的 csv 文件也很轻松，而 Excel 遇到几万的数据量基本就卡住，无法操作了。1、打开大文件，内容搜索、文件切分、编码设置 2、缺点，不能像 Excel 那么灵活进行筛选统计分析 后记 有一次遇到一个需求，使用正则表达式处理，逻辑很简单，一开始我处理时，直接写了一个嵌套很多层 .、+、* 等符号的正则表达式，结果出现了无限循环，电脑主机的 CPU 持续保持在 100%，十几个小时仍在匹配，这种肯定不能使用，我在另外一篇博客中有介绍：一条正则表达式引发的惨案 ，有兴趣的可以看一下。 接着我在下面列出一个看似很简单的正则表达式，但是却可以进入无限循环。正则表达式：^([hH][tT]{2}[pP]:\/\/|[hH][tT]{2}[pP][sS]:\/\/)(([A-Za-z0-9-~]+).)+([A-Za-z0-9-~\\\\/])+$匹配内容：http://www.fapiao.com/dddp-web/pdf/download?request=6e7JGxxxxx4ILd-kExxxxxxxqJ4-CHLmqVnenXC692m74H38sdfdsazxcUmfcOH2fAfY1Vw__%5EDadIfJgiEf]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[与微博内容分析相关的正则表达式]]></title>
    <url>%2F2018121101.html</url>
    <content type="text"><![CDATA[在分析微博内容时，常常需要进行特殊内容去除与抽取，例如抽取微博话题、微博昵称、微博表情、微博短链接、网址长链接等等。本文依据实际使用情况，记录下了与微博内容分析相关的正则表达式，以便查用。微博表情 表情是使用左右中括号包含的文本（在实际使用时，显示的是 emoji 表情，不是单纯的字符），例如：[爱心]、[微笑]、[笑哭]，分别表示：:heart:、❤️、:smile:、😊、:joy:、😂参考：emoji 百科 。 如果在微博内容中抽取表情，使用正则表达式（1-7 个字符，中文和字母，不排除有的新的表情出现，导致字符更长）：1\[[\u4e00-\u9fa5A-Za-z]&#123;1,7&#125;\]不同字符长度的表情举例（我用了 10 分钟把微博表情全部浏览了一遍，发现 [小黄人] 系列、[文明遛狗] 是最近刚刚发布出来的）：[耶]、[来]、[跪了]、[ok]、[中国赞]、[紫金草]、[doge]、[文明遛狗]、[给你小心心]、[小黄人微笑]、[弗莱见钱眼开]、[小黄人剪刀手]、[哆啦 A 梦害怕]、[带着微博去旅行]。注意，在 2019 年 3 月 21 日，发现微博新增了表情：[大侦探皮卡丘微笑]，这个表情有 8 个字符，所以表情的正则表达式也要做相应的更新。1\[[\u4e00-\u9fa5A-Za-z]&#123;1,8&#125;\]微博昵称 微博昵称是用户填写的昵称，并且在转发或者提到时，会增加 @ 前缀，例如有一个 playpi 微博用户，在实际微博内容中，会以 @playpi 的形式出现，当然，微博昵称的可用字符是有限制的，不是任意字符都行，长度也是有限制的，最少 4 个字符，最多 30 个字符。以及微博客服的回答：微博客服微博 。 但是这个规则是针对修改昵称的限制，如果有些帐号是以前注册的，并且昵称在微博官方限制以前没有修改过，那么就有可能是 2 个字符，3 个字符，例如各个明星、作家、自媒体的个人微博：@阑夕、@王力宏、@韩寒 等等。如果在微博内容中抽取昵称，使用正则表达式（中文、数字、字母、横线、下划线的组合，2-30 个字符）：1@[\u4e00-\u9fa5A-Z0-9a-z_-]&#123;2,30&#125;微博话题 话题是微博定义的一种概念，可以用来标识热门事件、重大新闻、明星、综艺节目等等，发布规则就是使用 2 个 #符号包含话题内容（例如：# 创造 101#），话题即生成，微博还专门有一个实时话题榜单。如果在微博内容中抽取话题，使用正则表达式（2 个 #号之间，非指定的符号，长度在 1-49 之间）：1#[^@&lt;&gt;#"&amp;'\r\n\t]&#123;1,49&#125;#注意，我找到 2014 年的 一篇旧帖子 ，微博小秘书评论说话题不能包含指定的几个特殊字符，还有内容长度限制，但是我在微博页面试了一下，这些特殊字符都可以使用（但是生成的话题页面，&lt; 字符、&gt; 字符被转成了 html 字符实体，换行符后的内容被截断，@符号、’ 单引号、” 双引号被自动替换掉，# 符号根本无法发布，空格符可以正常使用），而且长度限制是 1-49 个字符（中英文、标点都算 1 个字符）。但是为了话题内容的传播，还是使用通俗易懂的中文或者字母比较好。 微博短链接 微博短链接是微博官方提供的网址压缩功能产生的一种只包含少量字符的短网址，例如：http://finance.sina.com.cn ，压缩后为：http://t.cn/RnM1Uti 。这样的话，发微博时链接占用更少的字符长度。如果发微博时，内容中带了链接，例如视频地址、淘宝店地址，会被自动压缩为短链接。微博短链接可以直接在浏览器中访问，会被微博的网址解析服务器转换为原来的正常链接再访问。如果在微博内容中抽取短链接，使用正则表达式（我这里只是抽取 t.cn 域名的，6-8 个字母、数字）：1#https&#123;0,1&#125;://t.cn/[A-Z0-9a-z]&#123;6,8&#125;[/]&#123;0,1&#125;#参考：微博开放平台说明：http://open.weibo.com/wiki/2/short_url/shorten ；免费在线短链接转换工具：http://dwz.wailian.work 。网址长链接 网址长链接也就是普通的网址，有多种可能性。如果在微博内容中抽取网址长链接，使用正则表达式（我这里只考虑 http、https、ftp、file 协议）：1(https?|ftp|file)://[-A-Za-z0-9+&amp;@#/%?=~_|!:,.;]+[-A-Za-z0-9+&amp;@#/%=~_|]]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
        <tag>微博内容</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10 默认程序设置无效]]></title>
    <url>%2F2018120901.html</url>
    <content type="text"><![CDATA[装了 Windows 10 系统（教育版本），用了将近 3 个月了，最近发现一个诡异的现象，我的默认程序设置每次都只是设置后生效一段时间，例如视频播放器、音乐播放器，我分别设置成了迅雷看看、网易云音乐，用了半天之后，发现又变成了 Window 10 系统自带的视频播放器。这个现象也不是重启之后才出现的，而是平时用着用着就会出现，很莫名其妙。后来查阅资料发现这是一个普遍的现象，这个问题的根本原因是 Windows 10 自带的 bug，通常导致这个 bug 出现的原因是开启了系统的自动更新。现象 在 Windows 10 系统（没有打对应补丁的）中，如果开启了系统自动更新，就会触发相应的 bug：默认程序会被系统更改回系统自带的程序，例如视频播放器、音乐播放器等等。这个问题的原因用官方标识来指定就是由于 KB3135173 所致，同时这个 bug 已经有对应的补丁了。按照系统设置，把某些默认程序改为自己需要的，我这里把视频播放器改为迅雷影音，设置特定格式的文件（.mkv，.mp4 等等）使用迅雷影音打开。在桌面右下角打开 所有设置 选项 在 Windows 设置中，选择 应用 选项 选择默认应用，设置视频播放器为 迅雷影音 上述的设置步骤实际上还不够，因为视频类型有很多种，还需要进一步指定每种类型的默认播放器，在默认应用下方有一个 按文件类型指定默认应用 选项 我这里特别关注 .mkv、.mp4 这 2 种格式的文件，默认应用设置为 迅雷影音 上述内容设置完成，就可以使用了，但是用不了多久，系统时不时就弹出提示框，通知默认程序重置，然后又被设置为系统内置的应用了 解决方案 不推荐方案 更改注册表、使用命令行卸载系统默认程序，这些方案是可行的，但是对于普通用户来说太麻烦了一点，根本不懂得如何操作，而且解决方法太粗暴了，当然喜欢折腾的人是可以选择的。以下给出几个命令行示例（需要在管理员模式下执行，打开 Windows PowerShell 的时候选择有管理员的那个）：卸载 “电影和电视” 应用（星号表示通配符，下同）1get-appxpackage *zunevideo* | remove-appxpackage卸载 “Groove 音乐” 应用 1get-appxpackage *zunemusic* | remove-appxpackage 卸载 “照片” 应用 1get-appxpackage *photos* | remove-appxpackage 如果还想恢复已经卸载的系统自带应用，可以使用以下命令（重装所有系统内置的应用）1Get-AppxPacKage -allusers | foreach &#123;Add-AppxPacKage -register "$($_.InstallLocation)appxmanifest.xml" -DisableDevelopmentMode&#125;推荐直接打补丁（更新系统）这个方法很简单，容易操作，直接在系统更新里面更新即可，确保要能更新到 KB3135173 这个补丁才行（或者更高版本的补丁）。我这里是已经更新完成的，等待重启，补丁标识是 KB4469342。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>Win10</tag>
        <tag>默认程序设置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark on Yarn 查看日志]]></title>
    <url>%2F2018120702.html</url>
    <content type="text"><![CDATA[一直一来都是直接在 Yarn 的 UI 界面上面查看 Spark 任务的日志的，感觉看少量的内容还勉强可以，但是如果内容很多，浏览器就没法看了，更没法分析。本文讲述如何使用 Yarn 自带的命令在终端查看 Spark 任务的日志，也可以拷贝出日志文件，便于分析。1、查看某个 Spark 任务的日志，使用 logs 入口：1yarn logs -applicationId application_1542870632001_26426如果日志非常多，直接看会导致刷屏，看不到有用的信息，所以可以重定向到文件中，再查看文件：1yarn logs -applicationId application_1542870632001_26426 &gt; ./application.log2、查看某个 Spark 任务的状态，使用 application 入口：1yarn application -status application_1542870632001_26426同时也可以看到队列、任务类型、日志链接等详细信息 3、kill 掉某个 Spark 任务，有时候是直接在 Driver 端 kill 掉进程，然后 Yarn 的 Spark 任务也会随之失败，但是这种做法是不妥的。其实 kill 掉 Spark 任务有自己的命令：1yarn application -kill application_1542870632001_264264、需要注意的是，步骤 1 中去查看日志，要确保当前 HADOOP_USER_NAME 用户是提交 Spark 任务的用户，否则是看不到日志的，因为日志是放在 HDFS 对应的目录中的，其中路径中会有用户名。此外，步骤 1 中的日志要等 Spark 任务运行完了才能看到，否则日志文件不存在（还没收集到 HDFS 中）。 在 Linux 环境中可以使用 export HADOOP_USER_NAME=xxx 临时伪装用户。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Yarn</tag>
        <tag>日志查看</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[煮鸡蛋做法总结]]></title>
    <url>%2F2018120301.html</url>
    <content type="text"><![CDATA[本文记录水煮鸡蛋的做法总结。介绍 水煮鸡蛋是最常见的吃法之一，做法非常简单，直接将鸡蛋放入开水中煮熟即可。煮熟的鸡蛋营养丰富，水煮鸡蛋的营养可以 100% 被保留，是所有的鸡蛋做法中营养被保留的最好的一种。建议每天食用 1-2 个，因为过量的食用可能会导致营养不良，同时鸡蛋的营养并没有被身体吸收，相当于浪费了。在生活当中，大家几乎每天早上都会吃煮鸡蛋，或者茶叶蛋，但是有一些人卖的煮鸡蛋不算成功的煮鸡蛋，因为剥皮的时候发现不好剥，蛋壳与蛋白紧紧粘在一起，吃起来可麻烦了，这是因为煮鸡蛋的做法错误，遗漏了重要的步骤。做法步骤 1、简单地清洗一下鸡蛋，因为鸡蛋的表面可能会有一些茅草、粪便之类的污垢，这是因为鸡蛋必须是原生的，存储、运输、销售过程都不能清洗，如果非要清洗，水会破坏表面的保护膜，放不了两天鸡蛋就坏了；2、放在冷水中浸泡一会儿，1-2 分钟，这样做的目的是防止沸水煮的时候蛋壳破裂；3、放入锅中，水的高度稍微没过鸡蛋，使用中火煮开水，不要使用大火，大火煮的速度太快，鸡蛋容易裂开，另外中火使水沸腾的时间会长一些，预热了鸡蛋，味道更香；4、水沸腾后，改为小火，煮 7-8 分钟（如果继续使用中火，5 分钟左右即可）；5、如果需要溏心蛋（蛋清凝固，蛋黄成稠液状，软嫩滑润），煮 5 分钟即可；6、煮熟后不要立即捞出，等 1-2 分钟，然后才捞出，切记此时需要放入冷水中，浸泡 1-3 分钟，这一步骤的目的是保证鸡蛋容易剥开，避免蛋白和蛋壳粘在一起。 煮鸡蛋成品 注意事项1、煮鸡蛋前最好放入冷水中浸泡 1-2 分钟，防止煮的过程开裂；2、注意控制火力和时间，鸡蛋不能煮太久，超过 10 分钟会有化学反应，导致营养流失；3、煮熟后不要立即捞出，捞出后也要放在冷水中浸泡，防止蛋白和蛋壳粘在一起；4、每天不要吃太多，1-2 个就够了；5、如果想要保持蛋黄在中间，煮鸡蛋的过程中要适当搅拌让鸡蛋旋转。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>煮鸡蛋</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一条正则表达式引发的惨案]]></title>
    <url>%2F2018120201.html</url>
    <content type="text"><![CDATA[本文讲述由于正则表达式引发的性能惨案，背景就是使用 Java 编程语言进行正则表达式匹配，由于正则表达式很复杂，再加上 Java 使用的是 NFA（非确定型有穷自动机）匹配引擎，导致匹配一条文本内容使用了十几个小时还没完成，一直卡住，同时线上环境的主机 CPU 使用率也居高不下（我猜的，因为我没有权限看）。 整理中。 参考：http://www.cnblogs.com/study-everyday/p/7426862.htmlhttps://www.jianshu.com/p/5c2e893b8d5d]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
        <tag>Java NFA</tag>
        <tag>非确定型有穷自动机</tag>
        <tag>正则无限回溯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jackson 包版本低导致 NoSuchMethodError]]></title>
    <url>%2F2018120101.html</url>
    <content type="text"><![CDATA[本文讲述 Java 项目由 Maven 包冲突或者版本不合适导致的运行时错误：1java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.JavaType.isReferenceType () Z起因 今天在升级 Web 项目的相关接口，更新了所依赖的 SDk 版本，删除了一些旧代码，测试时发现某个功能不可用，直接抛出异常，异常是在运行时抛出的，编译、打包、部署都没有任何问题。我看到第一眼，就知道肯定是 Maven 依赖问题，要么是版本冲突（存在不同版本的 2 个相同依赖），要么是依赖版本不对（太高或者太低），但为了保险起见，我还是先检查了一下 Git 的提交记录，看看有没有对 pom.xml 配置文件做相关改动。检查后发现，除了一些业务逻辑的变动，以及无关 jackson 依赖的版本升级，没有其它对 pom.xml 文件的改动，由此可以断定，某个依赖的升级导致了此问题，问题原因找到了，接下来就是解决问题。解决办法 查看项目的 Maven 依赖树 由于依赖太多，使用可视化的插件查看太繁杂，所以选择直接使用 maven 的 dependency 构件来生成文本，然后再搜索查看：1mvn dependency:tree &gt; tree.txt在 tree.txt 文件中搜索 jackson，可以找到 jackson-databind 相关的依赖包，还有 jackson-annotations、jackson-core 这 2 个依赖包。jackson-databind 的版本为 2.9.3确定了使用的版本，接下来可以在 IDEA 里面搜索一下这个类，然后再找调用的方法，直接去查看源码，看看到底有没有这个方法。搜索 JavaType Java 类，注意包的路径，可能会有很多重名的类出现，我是用 Ctrl + Shift + T 的快捷键搜索，各位根据自己的快捷键设置进行搜索。然后进入类的源代码，搜索方法 isReferenceType，报错信息后面的大写的 Z，是 JNI 字段描述符，表示这个方法的返回值类型，Z 表示 Boolean 类型，我们搜索看看有没有这个方法。我们发现连同名的方法都没有，更不用看返回值类型了，但是注意还是要去父类还有接口里面去搜索一下，保证都没有才是最终的没有。经过查找，没发现这个方法（主要原因是父类 ResolvedType 的版本太低，父类所在的 jackson-core 的版本只有 2.3.3，所以找不到这个方法），到这里就要准备升级 jackson-core 或者降级 jackson-databind 依赖了。去除多余依赖 如果是检查到存在依赖冲突的情况，一般是高低版本之间的冲突（最多的情况是多级传递依赖引起的），然后 Maven 编译打包时会全部打进业务的包。1、导致运行时程序不知道选择哪一个，于是抛出 NoSuchMethodError 异常，此时根据需要，移除多余的依赖包即可；2、步骤 1 操作后，还是一种可能是虽然只存在一个版本，但是由于版本太新或者太旧，无法兼容所有的调用，导致多处需要调用这个依赖包的地方总会有某个地方出现 NoSuchMethodError 异常。此时就比较麻烦，如果能找到一个合适版本的依赖包，兼容所有的调用，当然是好的；或者升级调用处对应的接口版本；如果还是无法解决，就只能通过 Shade 构件解决问题了，此处就不赘述了。经过检查，我这里遇到的就是步骤 2 的情况，虽然只剩下一个依赖包，但是版本太低或者太高，导致调用时找不到 isReferenceType 方法，类其实是存在的，所以要采用升级或者降级的方式。升级降级依赖 如果是检查到只有一个依赖，并没有冲突的情况，就容易了，直接找到最稳定的版本或者适合使用的旧版本，提取依赖的坐标，配置到 pom.xml 文件中即可。经过检查，我这里遇到的就是这种情况，去 Maven 私服中搜索 jackson，找到合适的版本（自己根据需要选择，我这里选择 jackson-databind 的 2.9.7 版本，然后 jackson-core 也指定 2.9.7 版本，就可以了，然后又查资料也发现这个方法是 2.6.0 版本之后才开始加上的），配置到 pom.xml 文件中即可。私服搜索 配置到 pom.xml我这里使用了常量，在 pom.xml 文件的 properties 属性下面配置即可。踩坑总结1、其实 jackson 这个依赖我并没有使用，而是引用的一个第三方依赖内部使用的，但是这个第三方依赖并没有一同打进来，也没有说明需要什么版本的，所以导致我自己在实验，最终找到到底哪一个版本合适。2、为了统一，jackson-core 的版本要与 jackson-databind 的版本一致，jackson-databind 里面是已经自带了 jackson-annotations 的，由于 jackson-databind 里面的类继承了 jackson-core 里面的，所以才都要升级并且保持版本一致。3、搜索类方法时，注意留意父类和接口里面，不一定非要在当前类里面出现。更改版本后同样也去类里面搜索一下，看看有没有需要调用的方法出现，确定版本用对了再继续做测试。4、这种错误在编译、打包、部署阶段是检查不出来的，因为代码并没有实际调用到，属于运行时错误，只有跑起来程序，执行到需要使用该方法的时候，才会报错。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>NoSuchMethodError</tag>
        <tag>jackson</tag>
        <tag>Maven</tag>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub 个人站点绑定独立的域名]]></title>
    <url>%2F2018112701.html</url>
    <content type="text"><![CDATA[随着越来越多的人使用 GitHub，都在里面创建了自己的仓库，或者 clone 了别人的优秀项目，也有很多人想利用 GitHub 自带的 GitHub Pages 来搭建个人博客，此时就可以使用独立的域名 https://www.username.github.io 访问自己的博客，全部的资源都来自于 GitHub，并且是免费的，不需要其它任何配置或者购买，这里面包含域名、流量、带宽、存储空间、Htpps 认证等服务。但是，有的人可能购买了自己的独立域名，例如： https://www.abc.com ，并且想把域名直接绑定到 GitHub 免费的域名上面，这样以后访问博客的时候更容易辨识，本文就描述 GitHub Pages 绑定独立域名的操作过程，前提是 GitHub Pages 已经创建完成。我在 Godaddy 上面购买了域名：playpi.org，选择 Godaddy 主要是不想备案，国内的域名服务商都要求备案，我以前在阿里云上面买过一个，后来没按照要求备案就不能用了，我也放弃了。购买域名 当然，大家可以选择自己喜欢的域名服务商，例如腾讯云、阿里云等，但是这些域名服务商需要给域名备案，有点麻烦【当然不是所有的域名都需要备案】。所以我选择的域名服务商是 Godaddy，主页地址：https://sg.godaddy.com/zh ，在主页中点击左上角的 域名 ，开始搜索域名。我这里输入域名 playpi.org，可以看到被占用了，已经已经被我购买了，可以看到右侧显示出了可以购买的域名列表，并且带有报价。在左侧，可以添加筛选条件，过滤掉自己不想要的域名。如果找到了满意的域名，加入购物车购买就行了。选择域名服务器 有了域名，还没有用，因为还没有把域名用起来，所以接下来需要找域名服务器，把你的域名解析到 GitHub Pages 去。这样，才能保证访问你的域名，自动跳转到 GitHub Pages 去。我一开始选择的 Godaddy 自己的域名服务器，只需要在 我的产品 -&gt; 域名 -&gt;DNS，设置一些解析记录即可。Godaddy 的配置解析规则可以参考下图【可以先忽略解析规则，后面会讲到的】：后来由于 GitHub Pages 屏蔽百度爬虫的问题，我必须设置一条专门的解析规则去解析百度爬虫的请求，引入到我自己的 Web 服务器上面，但是 Godaddy 不支持线路的自定义，比较笼统，所以我就放弃了。转而选择了腾讯的 DNSPod，还是比较好用的，虽然前不久刚出过问题，大量的网络瘫痪，但是解析速度还是挺快的。先在 DNSPod 中添加域名，也就是在 Godaddy 中购买的域名【如果是直接在腾讯云中购买的，就不用配置了，默认就有】。添加完成后，可以看到提示我 NS 地址还未修改，也就是目前仍旧是 Godaddy 负责解析这个域名，所以要把域名服务器给切换过来。如果不知道是什么意思，可以点击提示链接查看帮助手册【其实就是去购买域名的服务商那里绑定 DNSPod 的域名服务器】提示我们修改 NS 地址 帮助手册 我在这就直接查看当前的域名的 NS 地址，选择域名，进入配置页面查看。去 Godaddy 中配置域名服务器，替换掉原本默认的。在 我的产品 -&gt; 域名 -&gt;DNS：我把它修改为我在 DNSPod 中查到的属于我的域名的域名服务器，一般都会有 2 个，保证可靠性。配置完成新的域名服务器【以前的解析记录都消失了】：配置完成，域名解析的工作就完全交给 DNSPod 了，我们可以退出 Godaddy 了【只是在这里买了一个域名】，接下来全程都要在 DNSPod 中配置其它信息。配置域名的解析规则 上一步骤我已经配置完成了域名的基本信息，接下来需要配置的就非常关键了，是域名的解析规则，它会指引着访问域名的请求怎么跳转。这里先提前说一下配置规则：主机记录 为 @表示直接访问域名，例如访问 playpi.org主机记录 为其它字符表示访问二级域名，例如访问 www.playpi.org 、blog.playpi.org记录类型 为 A 表示跳转到 ip 地址，后面的 记录值 就需要填 ip，例如 66.32.122.18记录类型 为 CNAME 表示跳转到域名，后面的 记录值 就需要填域名，例如 blog.playpi.org线路类型 是 DNSPod 自定义的逻辑分类，给访问的请求分类，例如百度爬虫、搜狗爬虫，这个选项对于我来说很有用，可以解决 GitHub Pages 屏蔽爬虫的问题 我把 Godaddy 中的解析记录直接抄过来就行，不同的是由于使用的是 DNSPod 免费版本，A 记录会少配置 2 个，基本不会有啥影响 【其实不配置 A 记录最好，直接配置 CNAME 就行了，会根据域名自动寻找 ip，以前我不懂】。另外还有一个就是需要针对百度爬虫专门配置一条 www 的 A 记录，针对百度的线路指向自己服务器的 ip【截图只是演示，其中 CNAME 记录应该配置域名，A 记录才是配置 ip】。如果使用的是第三方托管服务，直接添加 CNAME 记录，配置域名就行【例如 yoursite.gitcafe.io】。上面的配置里面的 A 记录明显是多余的，而且还要通过 ping 去寻找那几个 ip【我这里是 ping iplaypi.github.io 得到，大家换为自己的 GitHub 用户名即可，每个用户之间的 ip 应该有差别，不会完全一样】。所以建议大家不使用 A 记录的配置方式，直接使用 CNAME 配置。配置完成后使用 域名设置 里面的 自助诊断 功能，可以看到域名存在异常，主要是因为更改配置后的时间太少了，要耐心等待全球递归 DNS 服务器刷新【最多 72 小时】，不过一般 10 分钟就可以访问主页了。但是后来我发现，那个 GitHub 的域名【iplaypi.github.io】被墙了而 ip 没被墙，表现为每天总会有一段时间访问不了【DNSPod 也会给我发告警邮件，说宕机了，当然是他们的域名测试服务器连不上这个域名】，而且我用自己的浏览器也访问不了。而 blog 那个二级域名却可以正常访问，这就说明 GitHub 的那个域名不好使，而我自己给 blog 专门部署 Web 服务是正常的。因此，在主机记录为 @ 的解析规则里面还是配置 A 记录吧，把几个 ip 都配置上去【免费版本的 DNSPod 只能添加 2 条，可怜】。这样做还会引起 GitHub 的警告，因为这个 ip 地址可能会变化，所以 GitHub 建议配置域名。如果想知道域名对应的 ip 地址，除了使用 ping 之外，还有更快捷的方法：dig 命令。在 GitHub 中设置 CNAME关于域名的配置都完成了，最后还有一个重要的步骤，需要在 GitHub 的项目中添加一个文件，文件名称是 CNAME，文件内容就是域名【我这里使用的是二级域名，也可以，就是在直接访问域名的时候多了一次转换】。那这个文件的作用是什么呢，为什么要这么配置呢？其实，CNAME 是一个别名记录，它允许你将多个名字映射到同一台计算机，还决定着主页的链接最终展示的样子，直接是域名【https://playpi.org 】还是带二级域名【https://www.playpi.org 】。这里有 GitHub 的官方说明：https://help.github.com/en/articles/using-a-custom-domain-with-github-pages 。此外，在 GitHub 中还可以开启 https 认证，这样你的每一个文档链接都会有把小绿锁了，GitHub 使用的是 Lets Encrypt 的证书，有效期 3 个月，不过别担心过期问题，GitHub 会自动更新的。开启了 https 认证后，哪怕使用 http 的链接访问，也会自动跳转的。那如果有人想把我的域名访问指向自己的 GitHub，是不是他在自己的仓库里面新建一个 CNAME 文件，并且填上我的域名就行了呢？其实不行，GitHub 是禁止这样做的。即使有人真的在自己的仓库里面新建了 CNAME 文件并且填写了我的域名，GitHub 是不认可的并且会给出警告。当然，如果我自己在 GitHub 中没有使用这个域名，别人当然可以使用。现在突然想到一个问题，我把自己的域名和域名服务器都暴漏了，会不会有人在 DNSPod 中把我的域名解析到其它地方去了【看起来所有的 DNSPod 的域名服务器都是一样的 2 台机器】，然后我就访问不了自己网站了，或者说流量变小了。我觉得 DNSPod 一定会禁止这种行为，否则岂不是乱套了，所以不同担心。经过实际测试，DNSPod 是不会允许这个现象发生的，如果别人要配置你的域名，DNSPod 检测到有第二个人配置同一个域名，会拒绝。如果你想主动给别人使用，需要第一个人解除绑定，然后第二个人才能继续使用，所以可以放心操作。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>个人站点</tag>
        <tag>绑定域名</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10 输入法简繁体快捷键与 IDEA 冲突]]></title>
    <url>%2F2018112301.html</url>
    <content type="text"><![CDATA[用了 2 个月的 Windows 10 系统（教育版），又安装了 IDEA 代码集成工具，开发的时候，发现每一次只要我使用快捷键 Ctrl + Shift + F 格式化代码后（主要作用就是代码对齐），不起作用，而且写中文注释时发现输入法的中文就被切换为了繁体，再来一次就被切换为了简体。到这里，我知道 IDEA 的快捷键与输入法的快捷键冲突了。解决方案 1、如前文描述，在写代码的过程中发现这个问题，并且看出是快捷键冲突的问题，接下来就要解决它。作为一名工程师，IDEA 的快捷键是因为使用习惯设置的，是写代码效率的保证，不可能更改的，任何与它有冲突的快捷键都要让步，那肯定是要更改输入法的快捷键的；2、信心满满，打开 搜狗输入法 的 属性设置 界面，找到 高级 选项，选择，可以看到里面有 快捷键 的相关配置；配置所有的快捷键 3、看了半天，也就这么几个快捷键配置，里面根本没有 简体 / 繁体 切换这一个配置选择，去搜索了一下其它资料，发现 简体 / 繁体 切换这一个快捷键是 Windows 10 系统内置的，默认就是 Ctrl + Shift + F，默认是给微软输入法使用的，某些版本的 Windows 10 系统有 bug，无法更改，哪怕卸载微软输入法，安装其它输入法也无效；4、我看了我的 Windows 10 系统版本，已经是新版本了，不会有那个 bug 出现了，所以要从系统设置入手了，应该有地方设置才对，查看了语言里面的设置信息，没找到，只能又返回到搜狗输入法里面，这时突然看到里面有一个 系统功能快捷键 选项；5、就是这里了，点进去，把 简繁切换 关闭（如果需要保留的话，更改快捷键即可），解决问题。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>Win10</tag>
        <tag>输入法</tag>
        <tag>快捷键冲突</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 博客静态资源压缩优化]]></title>
    <url>%2F2018112101.html</url>
    <content type="text"><![CDATA[使用 hexo-cli 生成的静态网页 html 文件，使用文本编辑器打开，可以看到内容中有大量的回车换行等空白符。尽管是空白符，但是也占据着空间大小，而且那么多，导致 html 文件偏大，网页加载时不仅浪费流量，而且还影响速度。同时，最重要的是对于手机端来说，静态页面 html 文件太大了的确不友好。所以要做优化，用术语说是压缩，其实目的就是在生成 html 文件时，尽量去除内容中多余的空白符，减小 html 文件的大小。此外，顺便也把 css 文件、js 文件一起压缩了。当前现象 为了简单起见，只是列举 html 文件来看现象，目前查看生成的 8 个 html 静态页面（为了具有对比性，不包含当前页面），大小为 314 K。打开其中一个 html 文件查看内容，可以看到很多回车换行符。接下来就是要想办法消除这些空白符。压缩方式选择 通过查看 hexo 官网（附上插件库：hexo 插件库 ），搜索资料了解别人的例子，发现有两种方式： 一种是先全局（-g 参数）安装 gulp 模块，根据压缩需求再安装需要的模块，例如 gulp-htmlclean、gulp-htmlmin、gulp-imagemin、gulp-minify-css、gulp-uglify，每个模块都有自己的功能，另外需要单独配置一个 js 脚本（放在站点根目录下），指明使用的模块，文件所在目录或者通配符文件名，然后每次使用 hexo generate 之后再使用 gulp 就可以压缩文件了。这种方式灵活度高，可以自定义，而且 gulp 的功能与 hexo 解耦，如果有其它静态文件，也可以使用 gulp 进行压缩。但是缺点也显而易见，门槛太高了，根据我的折腾经验，如果出了问题肯定要捣鼓半天，对于我这种零基础的人来说不够友好，我不选择；另一种是类似于 hexo 的一个插件，像其它插件或者主题一样，直接安装一个模块，在配置文件中配置你想要的压缩内容，在 hexo generate 的时候就可以实现压缩，无需关心具体流程，也不用配置什么脚本，非常容易，我选择这个，目前我看到有两个类似的插件：hexo-neat、hexo-filter-cleanup，用法都差不多，我选择前者，其实这些插件也是依赖于其它插件，把多种插件的功能整合在一起而已。安装配置 hexo-neat 插件其实是使用 HTMLMinifier、clean-css、UglifyJS 插件实现。 安装（由于网络不稳定因素，可能不是一次就成功，可以多试几次）1npm install hexo-neat --save站点配置 编辑站点的配置文件 _config.yml，开启对应的属性 12345678910111213141516171819202122# 文件压缩，设置一些需要跳过的文件 # hexo-neatneat_enable: true# 压缩 htmlneat_html: enable: true exclude:# 压缩 cssneat_css: enable: true exclude: - '**/*.min.css'# 压缩 jsneat_js: enable: true mangle: true output: compress: exclude: - '**/*.min.js' - '**/jquery.fancybox.pack.js' - '**/index.js' 查看效果 在执行 hexo generate 的命令行中就可以看到压缩率输出。8 个 html 文件被压缩后，大小只有 206 K，和之前的 314 K 比少了 108 K，虽然只是简单的数字，也可以看到压缩效果不错。继续打开先前打开的那个 html 文件，可以看到整个 html 文档被合并成为了一行文本内容，不影响浏览器对 html 文件的解析展示，回车换行的空白符内容肯定没有了。但是这样对于 html 文件的可读性变差了，最好还是使用一些回车换行符的，还好这些 html 文件我不会去看，能接受目前的效果。踩坑记录 1、由于牵涉到压缩文件，所以 hexo 生成静态文件的速度会比以前慢一点，但是可以接受。2、不要跳过 .md 文件，也不要跳过 .swig 文件，因为是在 hexo generate 阶段进行压缩的，所以这些文件必须交给 hexo-neat 插件处理，才能保证生成的 html 文件纯净。3、参考博客： 个人博客 CSDN 博客 个人博客 掘金博客]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>建站</tag>
        <tag>代码压缩</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google 账号开启两步验证与应用专用密码]]></title>
    <url>%2F2018111901.html</url>
    <content type="text"><![CDATA[使用 Google 账号的都知道，带来了很多方便，不仅有强大的免费搜索服务，还有 Google 文档、云主机、云存储等各种服务，但是唯一的缺点是需要翻墙，让一些人望而却步，把很多人挡在了便利门外。本文是针对已经实现翻墙愿望，并在日常工作中会使用到 Google 账号的人，说不定可以给你带来一些冷知识，解决一些小问题。Google 账号的便利性 目前在日常工作与生活中，查找资料时，基本使用的都是 Google 搜索，并且使用非常好用的 Chrome 浏览器。其中我用的最多就是标签收藏，平时偶尔搜到什么有用的知识点或者需要反复查看的网页，来不及看完整理，就先把网页分类收藏了，以便日后查漏补缺。此时，利用 Chrome 浏览器的标签收藏功能，可以很方便地把一切网页收藏起来，并且可以很好地分类存放，清晰明了。可能有人说也有很多其它的工具可以做到这一点，不久收藏吗？但是我觉得还是利用 Chrome 浏览器自带的这个功能比较好，再配合 Google 账号，就可以达到同步更新的效果了，公司的电脑、家里的电脑，只要都登录了 Google 账号，所有收藏的标签都可以实时同步。而且，所有的浏览记录、搜索历史、记住的账号密码等等，都可以同步，跨机器使用也很方便。再配合 Chrome 浏览器的插件，对收藏的网页搜索起来非常方便。Google 账号开启两步验证 为了安全起见，最好给 Google 账号开启两步验证，可以选择绑定手机号、启用身份验证器、安全密钥等方式，为了方便，我选择了绑定手机号。开启两步验证后，在陌生的设备上登录 Google 账号（包括 Google 自家的各种应用，例如邮件、YouTube 等）需要验证码的二次验证，当然，如果把设备设置为可信任的设备，则不需要每次都重复输入验证码。开启的方式非常简单，登录 Google 账号，在” 登录与安全 “中有” 两步验证 “的开启选项，选择自己需要的方式，继续即可。启两步验证 1启两步验证 2如果使用” 身份验证器 “的方式，还需要在手机上安装一个” 身份验证器 “应用，校准时间后，每隔 30 秒更新验证码，登录账号时需要使用当前的验证码，并且在有效期内完成登录的操作，否则验证码过期，需要使用新的验证码，类似于手机收到的验证码只有 1 分钟一样。同时，如果使用 Google 邮箱账号注册了其它平台的账号，例如注册了 Twitter，注册了 Facebook，为了安全起见也可以使用” 身份验证器 “的方式，一种验证方式管理着多种账号的安全。开启两步验证后带来的问题 我遇到的问题之一就是自己手机的邮件客户端无法登录 Google 邮箱了，我使用的时第三方邮件客户端，总是提示我密码错误，其实密码没有错误，是因为 Google 账号开启两步验证后，邮箱的登录也需要对应方式的验证，但是第三方邮件应用并没有做这个验证，所以无法登录。本来是想着单独把 Google 邮箱的两步验证关闭，但是找了半天设置选项也没有找到，看来 Google 账号已经是一个大统一的账号，不允许单独设置涉及安全性的信息，可以理解。同理，使用其它应用客户端也会遇到相同的问题，当然，Google 官方解释说明也解释了有部分设备不需要关注这个问题，其它大部分设备或者应用还是要受到影响的。见：使用应用专用密码登录 此时，需要使用” 应用专用密码 “或者在手机上开发一个” 具有账号访问权限的应用 “用来代理整个 Google 的账号访问。问题的解决方法 应用专用密码方式的使用 1、在 Google 账号的登录和安全中，可以找到” 应用专用密码 “这个选项：2、点击进入后，可以看到选择应用与选择设备，由于我使用的是一种不知名的 Android 手机，所以官方选项中没有可以选择的，只好自定义一种，随便起一个名字标识即可。3、选择完成后，会生成一串 16 位的密码，这个密码就可以在其它设备上登录的时候使用，不需要使用原来的密码，也不需要使用 Google 验证码。4、在使用过程中还可以看到设备的情况。 具有账号访问权限的应用的使用 这种方式就是手机本身有一个后台应用，代理了 Google 账号的一切请求，把信息转发到本地应用（比如 Chrome 浏览器就是这样一个应用，只不过是官方开发的，只要登录了 Google 账号，邮件、YouTube、搜索、Play、相册、日历等等这些应用同步一起使用，不需要额外再登录，这也是我使用 Chrome 浏览器的原因。），所以后台应用如果知道了 Google 账号的用户名、密码，就可以代理所有 Google 应用的请求，无需关心 应用专用密码了。我发现锤子手机的 Smartisan OS 系统（v6.0.3，Android 版本 7.1.1）对邮件就做了这个后台应用 Smartisan Mail，所以在使用内置的邮件客户端时，即使开启了两步验证，也无需关心验证码的问题（第一次登录还是需要验证的）。下面截图则是一步一步设置：1、在邮件客户端设置中添加 Google 邮箱2、输入 Google 账号密码（也是邮箱密码）3、输入验证码（由于开启了两步验证，一定需要），此时切记勾选” 在此计算机上不再询问 “，才能保证邮件客户端正常收发 Goole 邮件，否则不行。4、允许，可以看到 Smartisan Mail 想要访问 Google 账号5、点开 Smartisan Mail，可以看到开发者信息，里面其实设置了代理转发6、此外，在登录成功后，在 Google 账号的登录和安全中，可以看到具有账号访问权限的应用：]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>Google 账号</tag>
        <tag>两步验证</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Vultr 搭建 Shadowsocks（VPS 搭建 SS）]]></title>
    <url>%2F2018111601.html</url>
    <content type="text"><![CDATA[本文讲述通过 Vultr 云主机搭建 Shadowsocks 的过程，图文并茂，非常详细。当然，关于云主机有很多 VPS 都可以选择，例如 搬瓦工 、Godaddy、Vultr，读者可以根据价格、配置、地区、操作系统等因素自由选择，但我还是推荐 Vultr，因为它扣费灵活、主机管理灵活、价格优惠，并且还支持支付宝和微信支付，简直太方便了。声明 ：2019 年 08 月 09 日发现 Vultr 官方不会再赠送 $10 的代金券给新注册用户，只会给我发放代金券，但是 $25 的代金券仍然有效，请读者选择 $25 对应的链接打开注册，以免错失了代金券。主机购买 使用 Vultr 的云主机，最好选择洛杉矶地区的或者日本的服务器，我亲自测试这两个地区的服务器最稳定，已经推荐给很多人，而且网速相对来说较好，我的推广链接【可以获取 10 美元的代金券，只要充值 10 美元就能使用】：我的 10 美元推广链接 ，官网链接也在这里：Vultr 。 这里再多说点，如果使用上面的推广链接注册 Vultr 帐号，可以获取 10 美元的代金券，需要在 30 天之内使用，使用的条件就是充值 10 美元以上的钱。例如充值 10 美元就会获取 20 美元的帐号余额，这些钱如果购买 3.5 美元的主机可以使用半年了，挺划算的。此外还有一个限时的大优惠，如果准备长期使用 Vultr，肯定要充值多一点，我这里有一个限时的推广链接：我的 25 美元推广链接 ，可以获取 25 美元的代金券，使用条件就是充值 25 美元以上的金额。假如充值了 25 美元，总共获取 50 美元入账，购买 3.5 美元的主机可以使用 14 个多月，适合长期使用 Vultr 的。 以下列举 Vultr 的五大好处：扣费灵活 ，Vultr 有一个好处就是主机的费用并不是按照月份扣除的，而是按照天扣除的，每天扣除的费用是 月租 / 30。例如你的主机只用了 10 天，然后销毁不用了，实际只会扣除月租 1/3 的钱，这种方式很是灵活，哪怕主机的 IP 地址被屏蔽了也可以销毁重新生成一个，并不会浪费钱。它不像国内的云服务商，一般是按照月份扣费的。主机管理灵活 ，它不像国内的云服务商，购买一台云主机后，直接先扣费，然后分配一台主机，IP 地址是固定的，如果有问题只能重启。而在 Vultr 中是可以随意创建、销毁虚拟主机的，根据你自己的需求，选择配置、主机机房位置、操作系统，几分钟就可以生成一台主机，如果用了几天觉得不好，或者 IP 地址被封，再销毁重新创建即可，Vultr 只会扣除你几天的费用，非常人性化。价格优惠 ，根据配置的不同，价格有多个档次，有 $2.5 / 月 （只有 IP6 地址）、$3.5 / 月、$5 / 月 等等，更贵的也有，一般个人使用选择这三个中的一个就够用了，但是要注意便宜的经常售罄，而且最便宜的只支持 IP6，慎用。大家如果看到没有便宜的主机了不用着急，可以先买了贵的用着，反正费用是按照天数扣除的，等后续发现便宜的套餐赶紧购买，同时把贵的主机给销毁，不会亏钱的。 付费方式灵活 ，付费方式除了支持常见的 Paypal、 信用卡 等方式，它还支持 比特比 、 支付宝 、 微信 等方式。就问你是不是很人性化，作为一家国外的公司，还特意支持 支付宝 、 微信 的方式支付，也从侧面反映了随着中国的日益强大，中国的电子支付方式正在走向全球，越来越流行。机房分布全球 ，它的机房位置遍布全球，例如 日本 、 新加坡 、 澳大利亚 、 美国 、 德国 、 英国 、 加拿大 ，读者根据网络的需求可以灵活选择。关于 Vultr 云主机的生成以及 Vultr 系统的常用功能使用，由于不在这篇博客的介绍内容中，所以在此不再赘述。但是，我会在以后的某一天把它补充完整，并放上链接：使用 Vultr 创建云主机详细步骤 。Shadowsocks 服务安装 云主机选择 CentOS 7 x64 版本，全程操作使用 Linux 命令（注意，如果选择其它系统命令会不一致，请自己查询，例如：Debian/Ubuntu 系统的安装命令更简洁，先 apt-get install python-pip，再 pip install shadowsocks 即可）。注意如果安装了防火墙（更安全），需要的端口一定要开启，否则启动 Shandowsocks 会失败。安装组件：123yum install m2crypto python-setuptoolseasy_install pippip install shadowsocks过程如图：配置服务器参数：1vi /etc/shadowsocks.json如下列出主要参数解释说明 参数名称 解释说明 server 服务器地址，填 ip 或域名 local_address 本地地址 local_port 本地端口，一般 1080，可任意 server_port 服务器对外开的端口 password 密码，每个端口可以设置不同的密码 port_passwordserver_port + password ，服务器端口加密码的组合timeout 超时重连 method 加密方法，默认：“aes-256-cfb”fast_open开启或关闭 TCP_FASTOPEN，填 true /false，需要服务端支持 配置多端口信息（多个帐号，多人也可用）：12345678910111213&#123; "server": "你的 IP 地址"（例如：192.168.0.1）, "local_address": "127.0.0.1"（默认值）, "local_port":1080（默认值）, "port_password"（开启的端口和密码，自己按需配置，确保端口打开并不被其它程序占用）: &#123; "1227": "pengfeivpn1227", "1226": "pengfeivpn1226", "1225": "pengfeivpn" &#125;, "timeout":300（超时时间，默认值）, "method":"aes-256-cfb"（加密方法，默认值）, "fast_open": false&#125;配置多端口信息（纯净版本，更改 ip、端口等信息直接复制使用）：12345678910111213&#123; "server": "x.x.x.x", "local_address": "127.0.0.1", "local_port":1080, "port_password": &#123; "1227": "vpn1227", "1226": "vpn1226", "1225": "vpn" &#125;, "timeout":300, "method":"aes-256-cfb", "fast_open": false&#125;配置一个端口信息（只有一个帐号，多人也可用）：12345678910&#123; "server":"你的 IP 地址"（例如：192.168.0.1）, "server_port":1225（唯一的端口）, "local_address":"127.0.0.1", "local_port":1080, "password":"pengfeivpn"（唯一的密码）, "timeout":300, "method":"aes-256-cfb", "fast_open":false&#125;配置一个端口信息（纯净版本，更改 ip、端口等信息直接复制使用）：12345678910&#123; "server":"x.x.x.x", "server_port":1225, "local_address":"127.0.0.1", "local_port":1080, "password":"vpn", "timeout":300, "method":"aes-256-cfb", "fast_open":false&#125;Shadowsocks 性能优化：另外还有很多参数可以优化性能，例如设置连接数、字节大小等，比较复杂，在此略过。防火墙安装：123456789101112# 安装防火墙 yum install firewalld# 启动防火墙 systemctl start firewalld# 查看目前已经开启的端口号 firewall-cmd --list-ports# 端口号是你自己设置的端口 firewall-cmd --permanent --zone=public --add-port=1225/tcpfirewall-cmd --permanent --zone=public --add-port=1226/tcpfirewall-cmd --permanent --zone=public --add-port=1227/tcp# 重载更新的端口信息 firewall-cmd --reload过程如图：这里需要注意，如果没安装防火墙，或者安装防火墙但是没有开启端口，启动 Shadowsocks 时会报错：socket.error: [Errno 98] Address already in use，启动失败，无法提供翻墙服务，而且不要被错误信息误导，不是端口被占用，是端口没有开启。启动 Shadowsocks：123456# 后台运行 ssserver -c /etc/shadowsocks.json -d start# 调试时使用下面命令，实时查看日志 ssserver -c /etc/shadowsocks.json# 停止运行 ssserver -c /etc/shadowsocks.json -d stop过程如图：在使用 ssserver -c /etc/shadowsocks.json 查看启动日志时，发现除了正常的启动信息，还会有错误信息：12345678910111213INFO: loading config from /etc/shadowsocks.json2019-05-06 15:46:11 INFO loading libcrypto from libcrypto.so.102019-05-06 15:46:11 INFO starting server at 66.42.105.87:1227Traceback (most recent call last): File &quot;/usr/bin/ssserver&quot;, line 9, in &lt;module&gt; load_entry_point (&apos;shadowsocks==2.8.2&apos;, &apos;console_scripts&apos;, &apos;ssserver&apos;)() File &quot;/usr/lib/python2.7/site-packages/shadowsocks/server.py&quot;, line 68, in main tcp_servers.append (tcprelay.TCPRelay (a_config, dns_resolver, False)) File &quot;/usr/lib/python2.7/site-packages/shadowsocks/tcprelay.py&quot;, line 582, in __init__ server_socket.bind (sa) File &quot;/usr/lib64/python2.7/socket.py&quot;, line 224, in meth return getattr (self._sock,name)(*args)socket.error: [Errno 98] Address already in use错误信息截图 根据关键词 socket.error: [Errno 98] Address already in use 查阅资料，发现是主机的配置问题，停止的 ss 服务没有及时释放端口，导致启动时报错。但是我发现这种报错信息并没有影响到后台 ss 服务的正常启动，也就是说端口正常提供服务，可以顺利翻墙。同时，我按照其他人的解释说明，在 /etc/sysctl.conf 配置文件中增加了 ip 的配置：1234net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_fin_timeout = 5然后使用 /sbin/sysctl -p 命令让内核参数生效，结果在重启 ss 服务时看到日志里面还会有上面那种错误信息。后续考虑到这种现象并没有影响正常的服务，也就先不放在心上，专心做其它的工作。客户端使用 Windows 平台使用 下载 Windows 平台的客户端，下载地址：shadowsocks-windows GitHub，shadowsocks 官网 ，直接解压放入文件夹即可使用，不需要安装。 但是注意配置内容（端口、密码、加密协议等等），另外注意有些 Windows 系统缺失 Shadowsocks 必要的组件（.NET Framework），需要安装，官网也有说明。配置示例：实际上下载程序后，无需安装，直接解压即可，解压后只有一个 exe 文件，双击即可运行（最好放入指定文件夹中，便于程序管理和升级）。第一次启动，需要设置参数，如上图所示，至少配置一台机器，另外还可以设置开机启动，以后不用重新打开。此外，如果有更新版本的程序，会放在 ss_win_temp 文件夹下，直接解压后复制替换掉当前的 exe 文件即可；如果文件夹中有 gui-config.json、statistics-config.json 这 2 个文本文件，它们是程序的配置以及前面设置的翻墙配置，不能删掉；如果使用系统代理的 PAC 模式（推荐使用），会生成 pac.txt 文本文件，存放从 GFWList 获取的被墙的网址，必要时才会通过翻墙代理访问，其它正常的网址则直接访问，这样可以节约流量。如果有切换代理的需求，搭配浏览器的插件来完成，例如 Proxy SwitchyOmega 就可以。关于启动系统代理并使用 PAC 模式（根据条件过滤，不满足的直连），如果是入门级别使用，直接设置完就可以用了，不用再管其它设置，切记要定时更新 GFWList 列表，因为如果某些网站最近刚刚被屏蔽，不在以前的 HFWList 列表里面，就会导致无法连接，只有及时更新才能正常连接。但是还有一种极端情况，就是某些网站 GFWList 迟迟没有收录，怎么更新都不会起作用，别着急，此时可以使用用户自定义规则，模仿 GFWList 填写自己的过滤规则，即可实现灵活的切换，使用用户自定义规则后会在安装文件夹中生成 user-rule.txt 文本文件。其实，PAC 模式的原理就是根据公共的过滤规则（收集被屏蔽的网站列表），自动生成了一个脚本文件，把脚本文件绑定到浏览器的代理设置中，使浏览器访问网站前都会运行这个脚本，根据脚本的结果决定是直接访问还是通过本地代理访问，脚本在 Shadowsocks 的 PAC 设置中可以看到，浏览器的设置信息可以在代理设置中看到（浏览器在 Shadowsocks 开启系统代理的时候会自动设置代理，无需人工干预）。由此可以得知，通过本机访问网络，决定是直接连接还是通过 Shadowsocks 代理连接的是 PAC 脚本，并不是 Shadowsocks 本身，所以如果使用系统的 Ping 命令访问 www.google.com 仍然是不能访问的，因为直接 Ping 没有经过 PAC 脚本，还是直接连接了，不可能访问成功。除了浏览器之外，如果其它程序也想访问被屏蔽的网站（例如 Git、Maven 仓库），只能通过程序自己的代理设置进行配置，完成访问的目的。（如果放弃 PAC 模式，直接使用全局模式，则不需要配置任何信息，本机所有的网络请求会全部经过翻墙代理，当然这样做会导致流量消耗过大，并且国内的正常网站访问速度也会很慢）获取到的 PAC 脚本地址为：http://127.0.0.1:1080/pac?t=20181118030355597&amp;secret=qZKsW49fDFezR4jJQtRDhUVPRqnFu6JC3Nc+vtXDb0g=以上是查看 Chrome 浏览器和 IE 浏览器的代理设置信息，对于 Microsoft Edge（Windows 10 自带）浏览器来说，界面有点不一样，在设置 -&gt; 高级 -&gt; 代理设置里面。此外，如果在浏览器中有更灵活的需求应用， 例如在设置多个代理的情况下，针对公司内网是一套，针对指定的几个网站是一套，针对被屏蔽的网站是一套，剩余的直接连接。在这种情况下仅仅使用代理脚本就不能完成需求了，显得场景很单一，当然也可以把脚本写的复杂一点，但是成本太高，而且不方便维护更新。这个时候就需要浏览器的插件出场了，例如在 Chrome 下我选择了 SwitchyOmega 这个插件，可以设置多种情景模式，根据实际情况自由切换，非常方便。我设置了三种情景模式：hdpProxy（公司内网）、shadowSocks（翻墙代理）、auto switch（根据条件自动切换），前面两种情景模式直接设置完成即可，最后的 auto switch 需要配置得复杂一点，根据正则表达式或者通配符指定某些网站的访问方式必须使用 hdpProxy 代理，另外其它的根据规则列表 （https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt ，和 Shadowsocks 的 GFWList 列表类似）必须通过翻墙代理，剩余的才是直接连接。当然，此时就不需要把 Shadowsocks 设置为系统代理了，保持 Shadowsocks 后台运行就可以了。Android 平台使用Android 平台的安装使用方法就非常简单了，分为 安装、配置、启动 这 3 个步骤，没有其它多余的操作。安装 下载 Android 平台的客户端，一般我们都称之为 影梭 ，在应用商城是找不到的，因为通不过审核，所以只能去官网下载，下载地址：shadowsocks 官网 。切记，千万不要去第三方网站下载，因为下载的安装包可能带有其它的应用，导致给你的手机安装了一堆软件。当然，如果你连官网都不信，可以自己下载源代码，自己打包 apk 文件，也是可以的，懂一点点 Android 开发就行了，源代码全部是开源的，放在了 GitHub 上面：https://github.com/Jigsaw-Code/outline-client/ 。 下载完 apk 文件，安装也就是和安装普通的应用一样，需要注意的是有些 Android 手机会禁止外部来源的 app（不是从应用商店下载安装的）安装，所以需要同意一下，也就是 信任此应用 ，才能顺利完成安装。配置 需要配置的内容和 Windows 平台的一样，把那些必要的参数填进去就行了，其它内容不需要关心。例如我这里配置了 ip、端口、密码、加密方式等。启动 启动只要点击右上角的灰色圆形按钮，里面有一个小飞机，大概等待几秒钟，就会变绿，表示已经连接上 VPN 了，此时手机就可以连接被屏蔽的网站了。唯一的缺点就是，不支持设置类似于 PAC 规则的站点切换（ 路由 默认设置的是绕过中国大陆地址），因为只要一连上 VPN，手机上所有的国外连接都是走 VPN，会导致连某些正常的国外的网站也会慢一点，还浪费 VPS 的流量。当然，如果是在 WIFI 的环境下，通过 Android 系统的网络代理设置也可以设置一些类似于 PAC 的规则，就不细说了。启动后，还可以看到流量发送接收统计信息。在手机的设置里面也可以看到 VPN 的开启 踩坑记录 1、在云主机安装服务端后，又安装了防火墙，但是没有开启 Shadowsocks 需要的端口，导致启动 Shadowsocks 总是失败，但是报错信息又是 Python 和 Linux 的，看不懂，搜索资料也搜不到，后来重装，并且想清楚每一步骤是干什么的，会造成什么影响，通过排除法找到了根本原因。2、在 Windows 平台使用的时候，安装了客户端，也安装了 .NET Framework 组件，配置信息确认无误，但是就是上不了外网，同样的操作使用 Android 客户端却可以，所以有理由怀疑是自己的主机问题。后来，重启系统，检查网络，关闭杀毒软件，还是不行，后来，依靠搜索，找到了是杀毒软件 Avast 的问题，扫描 SSL 连接被开启了，大坑，关闭即可。3、参考： 梯子搭建4、本来以为 Shadowsocks 的系统代理中的 PAC 模式会在接收到网络请求的基础上进行过滤，即 Shadowsocks 能控制所有的网络请求进行过滤判断，然后该翻墙的翻墙，该直连的直连，后来发现不是的，浏览器插件 SwitchyOmega 设置代理规则后，PAC 脚本就不会生效了，全部使用 Shadowsocks 代理的网站都直接翻墙，不会有任何判断了，导致优酷视频消耗了大量的流量，而且速度还很慢。另外，为了保证国内的网站不是经过翻墙代理，能直接连接，就不能使用全局模式。5、使用插件 SwitchyOmega 的过程中，一开始是自己整理一些规则，而没有使用https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt 列表规则，导致配置信息很多，而且自己看着头都大，不好维护与查看，后来就发现了列表规则，解放了劳动力。6、解决了 Chrome 浏览器的收藏跨平台自动更新同步的问题，以前在三台电脑之间添加取消收藏，总是不能更新同步，需要手动开启系统代理设置全局模式（Chrome 浏览器的收藏同步功能被屏蔽了，我又不知道 url 是什么），等一会更新同步之后再关闭（防止其它场景也翻墙了）。目前使用规则列表，收藏可以自动更新同步了，不需要手动来回切换了，也不用担忘记同步的情况了。]]></content>
      <tags>
        <tag>Vultr</tag>
        <tag>VPS</tag>
        <tag>Shadowsocks</tag>
        <tag>Avast</tag>
        <tag>影梭</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Aria2 Web 管理面板使用]]></title>
    <url>%2F2018110902.html</url>
    <content type="text"><![CDATA[如果使用浏览器默认的下载器，下载百度云的文件速度大多数情况下不理想，而且大文件不能下载，同时非客户端下载文件又会严重限速。如果使用插件和脚本配合，突破了百度云的限制，可以下载大文件，但是遇到下载中途失败的情况，有时候不能继续下载，还要重头来，会让人崩溃，体验非常不友好。那么我前面一篇文章描述了这个过程，同时给出了几个解决方案，本文就记录一下其中涉及到 Aria2 的使用以及其中一种管理面板【YAAW for Chrome】的使用。插件的安装 这款插件的全名是 Yet Another Aria2 Web Frontend，简称 YAAW，可以去谷歌的插件商店获取：YAAW 。当然，如果你没有翻墙，是打不开这个链接的，你可以选择去国内的镜像站点下载，例如：chrome-extension-downloader ，或者 getcrx ，至于怎么使用可以参考本博客的 关于页面 。为了方便你们，我把这款插件的插件 id 也放出来：dennnbdlpgjgbcjfgaohdahloollfgoc。 详细的安装过程就不赘述了，不是重点，一般也就是在线安装或者下载 crx 离线文件安装，都不是困难的事情。插件的使用 为了使用这款插件，还需要 Aria2 、baiduexporter 这 2 款工具的协助。Aria2 在前文已经描述过了，是用来下载文件的后台程序，本来直接使用它下载文件就可以了，但是奈何不方便批量下载以及任务管理，所以需要搭配 YAAW 来使用。baiduexporter 这款插件是用来转换百度云盘文件的链接的，转为 Aria2 可以直接使用的方式。这 2 款工具的安装可以参考上一篇博客，在这里仅给出对应的链接和插件 id：Aria2 安装包 、Aria2 介绍 、baiduexporter 插件 、baiduexporter 插件的 id【jgebcefbdjhkhapijgbhkidaegoocbjj】。Aria2 一切准备就绪后，我先在后台启动 Aria2 进程。切记要开启 RPC 模式，否则 YAAW 插件无法监控后台的下载任务，也就无法进行管理了。还要开启断点续传，这样才能在下载出现异常中断之后，还能接着上次的进度继续下载，节约时间。启动 Aria2 进程 YAAW 如果一开始直接打开 YAAW 插件，会显示错误：Error: Internal server error，其实就是没有找到 Aria2 进程。那它们是怎么通信的呢，其实就是依靠一个端口，这个端口我们使用默认的就行了【默认就是 6800】，否则要在 YAAW 和 Aria2 两边都要设置，并且保持一致。YAAW 插件设置端口 Aria2 设置端口 当然，从上面的截图中我们可以看到，还可以设置一些其它参数，例如：自动刷新时间、限速大小、用户代理、基础目录。然而，这些参数都是全局性的，我们没有必要设置，因为等到真正需要下载文件的时候，还可以重新设置，实际应用中不一定每次下载的设置都一致，所以放在每次下载文件的时候重新设置显得更灵活。baiduexporterbaiduexporter 的安装就比较简单了，就是一个浏览器插件而已，安装后打开即可。三者结合协同工作 打开我的百度云网盘，随意找一个文件测试 细心的人可以发现，在选中一个文件后，在本来的 下载 旁边多了一个选择项 导出下载 ，如果移动鼠标到上面， 导出下载 会展开下拉列表，出来 3 个选项：Aria2 RPC、文本导出、设置。如果我选择第一个 Aria2 RPC，则会直接调用后台的 Aria2 进程，直接帮我下载文件了，不需要我使用 Aria2 的原生命令加上必要的参数去启动一个下载任务了。而第二个文本导出，其实就是导出这个百度云文件的 Aria2 完整的命令，这样我们就可以复制使用，在后台起一个 Aria2 的下载任务了【有了第一种的 RPC 方式，更为方便快捷，肯定不用这种方式】。而设置则是导出的参数设置，这个没什么好说的，一般使用默认的就行了。好，接下来重点来了，必将能让你感受到什么叫做方便快捷。前面说那么多操作步骤，是不是发现还没 YAAW 插件什么事情，别着急，接下来的描述都是它的。直接打开插件：有没有发现什么，刚才下载的任务已经在这里可以看到了。不仅如此，还可以在这个控制面板中随意操管理任务：暂定、开始、删除，还可以看到下载的网速和进度百分比，多方便，这已经近似于一个下载管理软件了【虽然很简陋，但是比直接操作 Aria2 后台方便多了】。刚才使用百度云下载文件的时候，是直接在 导出下载 中一键勾选的，很方便，但是如果是别人发给你一个 Aria2 能下载的链接，你该怎么办呢？是使用 Aria2 后台起一个下载任务，还是怎么样，因为此时没有像下载百度云盘文件那么方便的按钮给你选择。别担心，此时又要使用 YAAW 的另外一个功能了：创建任务，也就是相当于在迅雷中创建一个下载任务一样。在 YAAW 插件中有一个 ADD 按钮。点击后弹出一个配置的对话框，在里面填上对应的参数就行了。不知道大家有没有发现，这里面的参数和使用 baiduexporter 插件的 导出下载 里面的 文本导出 导出的文本里面的一些内容很是相似，另外需要自己设置一下文件名字、文件下载目录、用户代理等信息就行了。这里就不再具体演示了。当然，以上只是使用百度云网盘作为示例，大家比较容易理解，演示给大家，纯粹是为了抛砖引玉，其实 Aria2 还支持更多的协议，大家可以自行参考 Aria2 的官方网站。如果下载磁力链接的东西，有时候迅雷更快，因为毕竟它是 p2p 的，可以加速，大家还是要看情况使用。温馨提示 除了突破限速的场景，我还发现一个场景，那就是资源文件被禁用的时候，也可以突破禁用，自由下载资源文件，例如盗版电影的下载。在不久后的 2019 春节档，竟然出现了史无前例的电影高清资源泄漏的事件。重点包括《新喜剧之王》、《疯狂的外星人》、《流浪地球》、《飞驰人生》这几部，电影刚上映 3 天，就有高清资源【画质比较好，不是一般的枪版】流出来了，大家可以下载。接着电影官网就鼓励大家进行举报封禁，导致各大下载软件都屏蔽了资源，显示由于版权问题而禁止下载。这时候各种开源的下载器就派上用场了，例如 Aria2 就是一个，但是缺点就是网速可能没有那么快，因为没有 p2p 加速机制。但是我仅仅是为了学习，测试一下也无妨。可以看到，使用迅雷下载是被禁止的。把 torrent 文件保存下来，转而使用 Aria2 下载，为了方便直接在 YAAW 插件上面建任务，直接上传 torrent 文件即可，其它参数则使用默认的。可以看到，下载速度可以达到 2M 以上。以上下载电影实践中，当然目的只是为了学习使用，给你们演示一下而已，不是提倡下载盗版电影，在下载一些被屏蔽的资源或者被限速的情况下，可以使用这种方式试试。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>aria2</tag>
        <tag>百度云下载</tag>
        <tag>百度云限速</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Aria2 加速百度网盘下载]]></title>
    <url>%2F2018110901.html</url>
    <content type="text"><![CDATA[在日常工作和生活当中，应该有不少人自愿或者被迫使用百度网盘，一是因为其它厂商基本都关停了网盘服务；二是在获取互联网资料的时候，基本都是先获取到百度网盘链接，然后自己再去下载；三是有时候想备份一些文件，也只能想起来有百度网盘可以使用。这样的话，慢慢地总是会碰到需要百度网盘的时候，我们暂且不考虑这家企业的口碑怎么样，百度网盘这个产品本身还是不错的：有免费的大量空间，使用人群多，分享获取资料方便。但是，产品让人诟病的地方也有几个，而且由此造成的用户体验非常差，大家骂声一片。本文就详细讲述百度网盘这个产品让人诟病的地方以及可以使用技术方式绕过它，从而提升自己的体验。当然，如果你的钱到位的话，直接充值会员吧，可以消除一切不好的使用体验，同时也免去了阅读本文的时间。使用中遇到的问题 本文是针对不充会员的免费用户群体的，在 Windows 平台安装，在 Chrome 浏览器中使用。下载速度太慢，慢到反人类 让人诟病的问题之一是下载速度太慢了，对于免费用户基本维持在几 KB/s 到十几 KB/s 之间，也就是说如果你想下载一部 1 G 大小的电影，按照 1000 M 计算，下载速度按照 10 KB/s 算（取这样的数值方便后续计算），下载完需要 1000 个 100 秒，也就是约等于 27.78 个小时（10 万秒），所以在下载列表中经常看到下载任务还需要大于一天才能完成，这怎么让人受得了，不骂才怪呢！但是只要充值会员，下载速度基本就暴增，可以完全利用宽带的带宽，例如 100 M 的宽带，下载速度可达 12.8 MB/s，哪怕只是 10 M 的宽带，下载速度也能到 1.28 MB/s。因此，百度网盘客户端对于免费用户限制速度限制得太严重了，不充值会员根本没法使用。而且，有时候勉强能使用的时候，经常会弹出会员试用 300 秒的提示，只要选择了，下载速度立马飞速提升，300 秒后又急速下降，经常下降到只有 3.14 KB/s，让人抓狂。网页版限制下载大文件，强迫安装百度网盘客户端 既然百度网盘客户端做了下载速度限制，那么大多数人会想到选择使用浏览器直接下载，同时又可以免去安装百度网盘客户端的麻烦，浏览器的下载速度通常在几百 KB/s，不会像百度网盘客户端那样特别地慢。但是，直接使用网页版的百度网盘下载文件，对文件大小有限制，太大的文件会被网页拦截，下载不了，而是弹出安装百度网盘客户端的提示，这样又回到了原点，因为如果用百度网盘客户端下载速度被限制了。解决问题 使用 aria2 突破线程数限制、下载速度限制 简介 Aria2 是一个多平台轻量级的下载工具，支持 Http、Ftp、BitTorrent、Web 资源等多种格式，使用命令行启动任务，更多具体信息查看官网说明：Aria2 介绍。这种工具可以最大程度利用你的网络带宽，实际上你可以自由配置，包括线程数、网络传输速度、RPC 端口、断点续传是否开启等。 安装 去官网下载安装包：Aria2 安装包 ，我的 Widows 系统 64 位，选择对应的安装包下载。 下载完成后，得到一个 zip 格式的文件，其实直接解压即可，不需要安装，解压后会得到一系列文件，为了方便管理，都放在 aria2 文件夹下面，再复制到程序对应的目录。其中，有一个 .exe 文件，就是运行任务时需要的文件。此外，为了方便起见，把 .exe 文件的路径配置到系统的环境变量中去，这样在任何目录都可以执行 aria2 命令了；如果不配置则只能在 aria2 目录中执行相关命令，否则会找不到程序。配置 1、如果单纯使用命令行启动下载任务，可以把参数信息直接跟在命令后面，临时生效，也就是参数只对当前下载任务有效。显然，这样做很麻烦，每次都是一长串的命令，而且当任务非常多的时候也无法管理，所以不建议使用这种方式。当然，如果只是测试折腾一下，或者也不经常使用，只是偶尔下载一个东西，还是用这种方式比较简介，不用管其它复杂的配置，不用管插件的安装。 单命令行启动任务示例，从电影天堂下载《一出好戏》这部电影。如果下载百度网盘的文件，需要使用 baiduexporter 插件生成 url，生成方式见后续步骤。1234567aria2c.exe -c -s32 -k32M -x16 -t1 -m0 --enable-rpc=true 下载 url 取值 -t1 表示的是每隔 1 秒重试一次 -m0 表示的是重试设置 此外，下载 url 中会包含 --header 的信息：User-Agent、Referer、Cookie、url 理论上 User-Agent、Referer 应该时固定的，Cookie、url 每次会生成不一样的 User-Agent: netdisk;5.3.4.5;PC;PC-Windows;5.1.2600;WindowsBaiduYunGuanJiaReferer: http://pan.baidu.com/disk/home这里再给一个完整的下载命令示例：1aria2c -c -s256 -k2M -x256 -t1 -m0 --enable-rpc=true -o &quot;pyspark-part1.zip&quot; --header &quot;User-Agent: netdisk;5.3.4.5;PC;PC-Windows;5.1.2600;WindowsBaiduYunGuanJia&quot; --header &quot;Referer: http://pan.baidu.com/disk/home&quot; --header &quot;Cookie: BDUSS=FFzb2s3Z2NRcnlTRE00WkxLYn5jTzhLdXktflVYbWprdXRpZm5EQ1FnYXlyTzFaSVFBQUFBJCQAAAAAAAAAAAEAAADYoS0veWVhckxQRjEzMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALIfxlmyH8ZZb; pcsett=1506245668-3f7c157ceb2130e195638efdf62944aa&quot; &quot;https://pcs.baidu.com/rest/2.0/pcs/file?method=download&amp;app_id=250528&amp;path=%2FQQ% E7% BE% A4% E5%90%88% E4% B9% B0% E5% A4% A7% E6%95% B0% E6%8D% AE% E8% A7%86% E9% A2%91%2Fxtwy% E4% B9%8Bpyspark% E8% A7%86% E9% A2%91%2F% E5% AD% A6% E5% BE%92% E6%97% A0% E5% BF% A7pyspark% E8% AF% BE% E7% A8%8Bpart1.zip&quot;2、如果是后台启动，通过其它管理插件来创建下载任务，则直接使用配置文件，文件名称为 aria2.conf，并在启动 aria2 时指定配置文件的位置。这样做的好处是使用一个配置文件就可以指定常用的参数配置，不用更改，每次下载文件前启动 aria2 即可。配置文件可选项如下，例如下载文件存放位置、是否开启 RPC、是否开启断点续传，具体更为详细的内容请参考文档：Aria2 配置信息文档 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394## '#' 开头为注释内容，选项都有相应的注释说明，根据需要修改 #### 被注释的选项填写的是默认值，建议在需要修改时再取消注释 #### 文件保存相关 ### 文件的保存路径 (可使用绝对路径或相对路径), 默认：当前启动位置 dir=E:\\aria2download\\# 启用磁盘缓存，0 为禁用缓存，需 1.16 以上版本，默认：16Mdisk-cache=32M# 文件预分配方式，能有效降低磁盘碎片，默认:prealloc# 预分配所需时间: none &lt; falloc &lt; trunc &lt; prealloc# NTFS 建议使用 fallocfile-allocation=none# 断点续传 continue=true## 下载连接相关 ### 最大同时下载任务数，运行时可修改，默认：5max-concurrent-downloads=32# 同一服务器连接数，添加时可指定，默认：1max-connection-per-server=5# 最小文件分片大小，添加时可指定，取值范围 1M -1024M, 默认：20M# 假定 size=10M, 文件为 20MiB 则使用两个来源下载；文件为 15MiB 则使用一个来源下载 min-split-size=16M# 单个任务最大线程数，添加时可指定，默认：5split=32# 整体下载速度限制，运行时可修改，默认：0#max-overall-download-limit=0# 单个任务下载速度限制，默认：0#max-download-limit=0# 整体上传速度限制，运行时可修改，默认：0max-overall-upload-limit=1M# 单个任务上传速度限制，默认：0#max-upload-limit=1000# 禁用 IPv6, 默认:falsedisable-ipv6=false## 进度保存相关 ### 从会话文件中读取下载任务 input-file=aria2.session# 在 Aria2 退出时保存 ` 错误 / 未完成 ` 的下载任务到会话文件 save-session=aria2.session# 定时保存会话，0 为退出时才保存，需 1.16.1 以上版本，默认：0#save-session-interval=60## RPC 相关设置 ### 启用 RPC, 默认:falseenable-rpc=true# 允许所有来源，默认:falserpc-allow-origin-all=true# 允许非外部访问，默认:falserpc-listen-all=true# 事件轮询方式，取值:[epoll, kqueue, port, poll, select], 不同系统默认值不同 #event-poll=select# RPC 监听端口，端口被占用时可以修改，默认：6800#rpc-listen-port=6800# 设置的 RPC 授权令牌，v1.18.4 新增功能，取代 --rpc-user 和 --rpc-passwd 选项 #rpc-secret=mivm.cn# 设置的 RPC 访问用户名，此选项新版已废弃，建议改用 --rpc-secret 选项 #rpc-user=&lt;USER&gt;# 设置的 RPC 访问密码，此选项新版已废弃，建议改用 --rpc-secret 选项 #rpc-passwd=&lt;PASSWD&gt;## BT/PT 下载相关 ### 当下载的是一个种子 (以.torrent 结尾) 时，自动开始 BT 任务，默认:truefollow-torrent=true# BT 监听端口，当端口被屏蔽时使用，默认：6881-6999listen-port=51413# 单个种子最大连接数，默认：55#bt-max-peers=55# 打开 DHT 功能，PT 需要禁用，默认:trueenable-dht=true# 打开 IPv6 DHT 功能，PT 需要禁用 #enable-dht6=false# DHT 网络监听端口，默认：6881-6999#dht-listen-port=6881-6999# 本地节点查找，PT 需要禁用，默认:false#bt-enable-lpd=true# 种子交换，PT 需要禁用，默认:trueenable-peer-exchange=true# 每个种子限速，对少种的 PT 很有用，默认：50K#bt-request-peer-speed-limit=50K# 客户端伪装，PT 需要 peer-id-prefix=-TR2770-user-agent=Transmission/2.77# 当种子的分享率达到这个数时，自动停止做种，0 为一直做种，默认：1.0seed-ratio=0.1# 强制保存会话，即使任务已经完成，默认:false# 较新的版本开启后会在任务完成后依然保留.aria2 文件 #force-save=false# BT 校验相关，默认:true#bt-hash-check-seed=true# 继续之前的 BT 任务时，无需再次校验，默认:falsebt-seed-unverified=true# 保存磁力链接元数据为种子文件 (.torrent 文件), 默认:false#bt-save-metadata=true 配置完成后在启动 aria2 时指定配置文件的位置即可，例如我把 aria.conf 与 aria2c.exe 放在同一个文件夹下，则启动时直接指定 1aria2c.exe --conf-path=aria2.conf 当然，这样做只是启动了 aria2，并没有开始创建下载任务，不像单个命令行那样简单，直接设置参数就起任务了。接下来还需要浏览器插件的配合，才能保证下载任务的创建与监控，虽然配置步骤麻烦一点，但是使用起来更为方便。为了避免启动时还要输入命令行，在 Windows 平台下可以写一个 bat 脚本，每次双击脚本即可，以下脚本内容供参考：12@echo off &amp; title Aria2aria2c.exe --conf-path=aria2.conf使用 1、使用命令行启动单个任务无需多做介绍，直接敲下命令行，等待文件下载就行了。如果需要连续下载多个文件，则唯一的做法就是多敲下几个命令，多等待而已。因此，这种方式不适合任务数量多的情况，那这种情况下显然是需要批量下载的，并且可以对下载任务进行管理，那就要看下面的一项了：后台起 aria2 服务。 生成下载 url 的过程需要借助 baiduexporter、YAAW for Chrome 插件，直接从 Chrome 浏览器的插件商店搜索安装即可，如果无法翻墙，也可以从离线镜像库下载离线文件进行安装，离线库可以参考本站点的 关于页面 给出的工具链接。 接下来描述使用方式，登录百度网盘账号，把需要下载的文件保存在自己的网盘中，选择需要下载的文件，然后可以看到本来的下载按钮旁边又多了导出下载按钮，包含几个选项：ARIA2 RPC、文本导出、设置。选择文本导出就会弹出当前下载文件的下载 url，复制粘贴到命令后即可直接下载该资源。导出的内容格式如下，当然实际使用的时候里面的参数也是可以更改的，但是下载 url 一定不不能变的。1https://pcs.baidu.com/rest/2.0/pcs/file?method=download&amp;app_id=250528&amp;path=%2F% E9%80%86% E5%90%91% E8% B5%84% E6%96%99%2FIDA%20Pro% E6%9D%83% E5% A8%81% E6%8C%87% E5%8D%97.pdf2、根据前面的描述，后台起了 aria2 服务，但是还没真正用起来，想要用起来，必须配合两个插件：baiduexporter、YAAW for Chrome。这 2 个插件中前者的作用是获取百度网盘的文件 url，这个 url 当然不是分享文件产生的 url，而是下载文件产生的 url；后者插件的作用是配合前者自动创建下载任务，实际下载利用的是已经启动的 aria2 后台，并时时监控任务状态，提供任务管理界面。插件的安装不再赘述，接下来直接描述使用流程，要确保以上两个安装的插件都已经启用。根据上一步骤已经知道导出下载这个按钮，里面包含着一个 ARIA2 RPC 选项，这个选项就是直接使用 后台 aria2 服务创建下载任务，然后 YAAW for Chrome 插件监控着所有下载任务。还有一个前提，就是启动 aria2 服务时要开启 RPC 模式。12# 启用 RPC, 默认:falseenable-rpc=true这样做了之后，aria2 后台服务会开启一个端口，一般默认 6800（如果 aria2 更改了端口，YAAW for Chrome 也要做相应的配置），这个端口用来给 YAAW for Chrome 汇报下载任务的情况，并提供管理下载任务的接口，这样的话，直接通过 YAAW for Chrome 就可以通过可视化的方式创建、暂停、查看任务。后台启动 aria2，开启 RPC 模式。打开 YAAW for Chrome 插件查看端口配置信息。通过 baiduexporter 插件，直接选择 PRC 下载，再去 YAAW 界面刷新查看下载任务。可以看到，aria2 参数还没优化（线程数、分块大小设置），下载速度已经有将近 400 Kb/s 了。使用油猴插件绕过浏览器下载大文件的限制 现象 还是刚才那个文件，文件大小只有 149 M，不想通过百度网盘客户端下载，只想通过网页版下载，那就直接点击下载按钮，发现被限制了，必须让你安装百度网盘客户端。本来还在想通过网页版直接下载，速度也不会很慢，但是被限制了，这个时候我们的万能插件要出场了：Tampermonkey，又称油猴、暴力猴。解决方式 使用万能的插件，屏蔽百度网盘网页版原来的网页内容，从而导致百度网盘的限制失效，这个插件就是 Tampermonkey：官网 、Chrome 浏览器插件商店。 这个插件的作用其实就是帮你管理各种自定义脚本，并运用在网页解析渲染中，从而实现对网页内容的改变，例如：去除网页的广告、去除百度搜索内容的广告条目、更改新浪微博展示界面。其中，也包括让百度网盘的下载文件大小限制失效，从而可以自由下载。1、好，现在需要在插件的基础上安装一个脚本：百度网盘直接下载助手。要安装这个脚本，则首先需要找到它，选择获取新脚本，会引导我们进入脚本仓库。2、各种脚本仓库，我们选择 GreasyFork。3、在搜索框中搜索：百度网盘直接下载助手，选择其中一个。4、安装选择的脚本。5、可以看到脚本内容，点击安装。6、安装完成后，选择管理面板可以查看已经安装的脚本以及是否启用，也可以删除或者二次编辑。7、回到百度网盘，选择文件，可以看到多了一个下载助手选项，选择 API 下载，下载，即可使用浏览器直接下载，不会因为文件太大有网页的限制。8、当然，如果自己会写脚本，或者从别处直接复制的源脚本代码，在插件中选择添加新脚本，自己编辑即可。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>Aria2</tag>
        <tag>百度网盘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WPS 关闭广告推送与自动升级]]></title>
    <url>%2F2018110301.html</url>
    <content type="text"><![CDATA[在工作和生活中，很多人使用金山的 WPS 套件，类似于微软的 Office 套件，而且是免费的。但是很多人会遇到 广告推送 或者 WPS 热点推送 ，每隔几天就会出现，有时候可以点击七天内不再出现，便可以清静几天，而且碍于是免费版本，不想购买会员，于是也就忍了。其实，WPS 自身是有设置可以关闭 广告推送 的，当然也可以关闭 WPS 热点推送 。WPS 的弹框 以下现象描述与截图均出自版本：WPS 2019，v11.1.0.8013 - Release 正式版。操作系统为：Windows 2007 专业版。在使用 WPS 的过程中，经常遇到广告推送与 WPS 热点推送，觉得很受打扰，但是碍于使用的是免费软件，又只能忍受。我一直在想以前是有设置可以关闭的，后来升级了就找不到是在哪里设置的了，后来又查阅了资料，发现果然是有地方可以设置的，只不过隐藏的太深了，不好寻找而已。接下来就一步一步说明具体设置步骤。设置关闭 如果你在互联网上搜索 WPS 广告推送相关话题，可以看到大量的帖子（或者说是方法教程）已经整理出了各种方案，可以帮你解决这个问题，例如：直接更改 WPS 安装目录中的某些文件、利用杀毒软件屏蔽广告推送、直接设置 WPS 等等。显然，前 2 种方案是在走弯路，而最后一种方案才是最简单直接的。1、打开 WPS 主页，在右上角找到 设置 按钮；2、点击 设置 ，选择 配置和修复工具 ；3、在弹出的对话框中选择 高级 ；4、选择对话框的 其它选项 标签页，取消截图中的 3 项勾选，即同时关闭 升级完成后推荐精选软件 、 订阅 WPS 热点 、 接受广告推送 ；此外，进入步骤 3 也可以直接通过系统的安装程序列表（开始 –&gt; 所有程序 –&gt;WPS Office–&gt;WPS Office 工具 –&gt; 配置工具），步骤如下图：按照以上步骤设置， WPS 就不再会弹出广告推送和 WPS 热点推送了，亲测有效。注意事项 1、说实话，我是没想到这个设置会隐藏的这么深，但至少暴露出来了；2、要注意版本区别，可能每个版本的设置步骤有所不同，而且也不排除以后更新的版本会取消这些设置选项，或者隐藏的更深。当然，如果 WPS 找到了其它盈利方式，也可能会取消这些广告推送；3、WPS 每次更新后，上述设置会还原，也就是又回到默认开启的状态，此时需要重新设置一次。当然，为了以后不会莫名其妙又弹出广告推送，可以直接关闭自动升级（和前面关闭广告的步骤一致，但是选择的是 升级设置 标签页），以后想升级的时候再手动升级。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>WPS</tag>
        <tag>关闭广告推送</tag>
        <tag>关闭自动升级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微博 URL 短网址生成算法 - Java 版本]]></title>
    <url>%2F2018101501.html</url>
    <content type="text"><![CDATA[微博短链接是微博官方提供的网址压缩功能产生的一种只包含少量字符的短网址，例如：http://finance.sina.com.cn ，压缩后为：http://t.cn/RnM1Uti 。这样的话，发微博时链接占用更少的字符长度。如果发微博时，内容中带了链接，例如视频地址、淘宝店地址，会被自动压缩为短链接。微博短链接可以直接在浏览器中访问，会被微博的网址解析服务器转换为原来的正常链接再访问。本文描述微博 URL 短网址生成算法，编程语言是使用 Java。 待整理。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>微博URL短网址</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android 逆向系列 2-- 破解第一个 Android 程序]]></title>
    <url>%2F2018101101.html</url>
    <content type="text"><![CDATA[本文简单介绍一下对一个简单的 Android 程序的逆向破解，算是对 Android 逆向的入门了解。 待整理。]]></content>
      <categories>
        <category>Android 逆向系列</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>逆向工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android 逆向系列 1-- 编写第一个 Android 程序]]></title>
    <url>%2F2018101001.html</url>
    <content type="text"><![CDATA[本文简单介绍一下 Android 开发的入门程序，编写一个简单的 Android 程序。 待整理。]]></content>
      <categories>
        <category>Android 逆向系列</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>逆向工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android 逆向系列 0-- 初识 Android 以及逆向工程]]></title>
    <url>%2F2018100901.html</url>
    <content type="text"><![CDATA[本文简单介绍一下 Android 开发以及关于 Android 的逆向工程，算是入门了解。 待整理。]]></content>
      <categories>
        <category>Android 逆向系列</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>逆向工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Kryo 异常]]></title>
    <url>%2F2018100801.html</url>
    <content type="text"><![CDATA[本文讲述使用 es-hadoop （版本 v5.6.8）组件，运行 Spark 任务遇到的异常：123Caused by: java.io.EOFExceptionat org.apache.spark.serializer.KryoDeserializationStream.readObject (KryoSerializer.scala:232)at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject (TorrentBroadcast.scala:217)以及通过 Maven 依赖查找分析的解决方法。遇到问题 由于最近的 elasticsearch 集群升级版本，到了 v5.6.8 版本，所用的功能为了兼容处理高版本的 elasticsearch 集群，需要升级 es-hadoop 相关依赖包到版本 v5.6.8，结果就遇到了问题：代码逻辑就是通过 es-spark 直接读取 elasticsearch 里面的数据，生成 RDD，然后简单处理，直接写入 HDFS 里面。本机在测试平台测试一切正常，没有任何问题，但是在线上运行就会抛出异常。解决方法 先分析一下这个问题产生的原因，在代码层面没有任何变动，只是更改了依赖的版本，所以问题在于更改版本之后是不是导致了传递依赖包的缺失，或者版本冲突，所以总体而言，肯定是 Maven 依赖包的问题，这个思路没问题。提前说明下面截图中出现的 Maven 中的常量：12&lt;elasticsearch-hadoop.version&gt;5.6.8&lt;/elasticsearch-hadoop.version&gt;&lt;spark-core_2.10.version&gt;1.6.2&lt;/spark-core_2.10.version&gt;1、local 模式 通过在本机连接测试平台，运行起来没有问题，但是部署到正式环境，运行不起来，直接抛出上图所示的异常信息。首先去依赖树里面查看与 kryo 相关的依赖信息（使用 mvn dependency:tree 命令）：发现两个依赖包（es-hadoop v5.6.8，spark-core_2.10 v1.6.2）里面都有与之相关的传递依赖，而且版本（奇怪的是 groupId 也稍有不同，这导致了我后续判断失误）不一致，这必然导致依赖包的版本冲突，通过 exclusions 方式去除其中一个依赖（其实不是随意去除一个，要经过分析去除错误的那个，保留正确的那个），local 模式可以完美运行。此图是 pom.xml 文件里面的移除信息，是我根据依赖树整理的，可以更加清楚地看到传递依赖的影响。2、yarn-client 模式 通过步骤 1 解决了 local 模式运行的问题，但是当使用 yarn-client 模式向 yarn 集群提交 Spark 任务时，如果移除的是 spark-core_2.10 里面的 kryo 依赖，异常信息仍然存在，无法正常运行。此时，我想到了前面所说的 2 个 kryo 依赖包的 groupId 有一点不一样，所以这 2 个依赖包虽然是同一种依赖包，但是可能由于版本不同的原因，导致名称有些不同。我认为使用的 es-hadoop 依赖的版本比较高，可能没有兼容低版本的 spark-core_2.10，所以需要保留 spark-core_2.10 里面的 kryo 依赖，而是把 es-hadoop 里面的 kryo 依赖移除。果然，再次完美运行。总结说明 这次通过 Maven 依赖找到了问题，但是版本仅仅限定在我使用的版本，其它的版本之间会有什么冲突我无法得知，但是这种处理问题的思路是正确的，避免走冤枉路，浪费不必要的时间。另外，提醒一下大家，更新 pom.xm 文件（包括新增依赖和更新依赖版本）一定要谨慎而行，并且对所要引入的依赖有一个全面的了解，知道要去除什么、保留什么，否则会浪费一些不必要的时间去查找依赖引发的一系列问题。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Kryo</tag>
        <tag>序列化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[依赖包缺失导致 Spark 任务无法启动]]></title>
    <url>%2F2018100701.html</url>
    <content type="text"><![CDATA[本文讲述使用 Spark 的过程中遇到的错误：1class "javax.servlet.FilterRegistration"'s signer information does not match signer information of other classes in the same package最终通过查找分析 Maven 依赖解决问题。遇到问题 由于最近的 elasticsearch 集群升级版本，到了 v5.6.8 版本，所用的功能为了兼容处理高版本的 elasticsearch 集群，需要升级相关依赖包，结果就遇到了问题。使用 es-hadoop 包（v5.6.8）处理 elasticsearch （v5.6.8）里面的数据，具体点就是通过 es-spark 直接读取 elasticsearch 里面的数据，生成 RDD，然后简单处理，直接写入 HDFS 里面。编译、打包的过程正常，运行代码的时候，抛出异常：1class "javax.servlet.FilterRegistration"'s signer information does not match signer information of other classes in the same package一看到这种错误，就知道肯定是 Maven 依赖出现了问题，要么是版本冲突，要么是包缺失，但是从这个错误信息里面来看，无法区分具体是哪一种，因为没有报 ClassNotFound 之类的错误。解决方法 现象已经看到了，问题也找到了，那么第一步就是直接搜索 Maven 项目的依赖，看看有没有 FilterRegistration 这个类，我的 IDEA 直接使用 Ctrl + Shift + T 快捷键，搜索 FilterRegistration，发现有这个类，但是包名对不上，注意包名是：javax.servlet。现在就可以断定，是包缺失，通过搜索引擎查找文档，需要引入 javax.servlet-api 相关的包， pom.xml 文件的具体依赖信息是：12345&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;4.0.1&lt;/version&gt;&lt;/dependency&gt;当然，版本信息根据实际的场景需要进行选择，我这里选择 4.0.1 版本。需要注意的是，有另外一个包，它的 artifactId 是 servlet-api，可能你会因为没看清而配置了这个依赖包，导致还是包缺失，所以一定要看清楚。我这里遇到的问题比较简单，只是包缺失而已，如果遇到的是包版本冲突，需要移除不需要的版本，只保留一个依赖包即可，此时可以借助 Maven 的 dependency 构建来进行分析查找：1mvn dependency:tree这个命令会输出项目的所有依赖树，非常清晰，如果内容太多，可以使用：1mvn dependency:tree &gt; ./tree.txt重定向到文本文件中，再进行搜索查找。总结 1、还遇到一种情况，在正式环境运行正常【没有单独配置这个依赖，使用的是别的依赖包里面的同名类，org.eclipse.jetty.orbit:javax.servlet】，但是在本机跑，创建 SparkContext 的时候就会报错，无法创建成功。其实还是因为包缺失，确保要使用 javax.servlet-api 这个依赖，其它的都不好使。2、在本机连接测试环境的 yarn，创建 SparkContext 的时候无法指定用户名，默认总是当前系统的用户名，导致创建 SparkContext 失败，伪装用户无效，只有打 jar 包执行前使用命令切换用户名：export HADOOP_USER_NAME=xx，而在代码中设置 System.setProperty (“user.name”, “xx”)、System.setProperty (“HADOOP_USER_NAME”, “xx”) 是无效的（这个问题会有一篇文章专门分析，需要查看源代码）；3、针对 2 的情况，简单通过 local 模式解决，暂时不使用 yarn-client 模式；4、针对 2 的情况，还有一种简单的方法，那就是直接设置 IDEA 的环境变量参数（不是设置操作系统的环境变量，我试了无效），如下图（和设置运行参数类似）； 设置 IEDA 的环境变量：5、此外，还有一种情况，当需要操作 HDFS 的时候，发现无论怎么设置环境变量都不可以（配置文件配置、代码设置），总是读取的系统默认用户，就和 2 中讲的一致，其实如果只是单纯地操作 HDFS，还可以在创建文件流的时候指定用户名（不过这种方法要先从 conf 中获取 uri）；12String uri = conf.get ("fs.defaultFS");FileSystem fs = FileSystem.get (new URI (uri), CONF, "zeus");]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Maven</tag>
        <tag>依赖问题</tag>
        <tag>FilterRegistration</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[地锅鸡做法总结（皖北、苏北地区）]]></title>
    <url>%2F2018100601.html</url>
    <content type="text"><![CDATA[地锅鸡是流行于皖北、苏北地区的一道传统名菜，并经过改良产生了地锅鱼、地锅豆腐、地锅牛肉、地锅三鲜等一系列菜品，但是配饼始终不变，一般是面饼或者玉米饼。本文用于记录地锅鸡做法总结。地锅鸡简介 地锅鸡是一道起源于江苏省的北部、山东省的南部的传统名菜，在安徽北部地区也非常流行，主要食材就是鸡肉、面饼、辣椒，做成菜品后既有主食又包含配菜，口味醇香，饼借菜味，菜借饼香，吃起来回味无穷。另外，经过不断的改进创新，还产生了地锅鱼、地锅豆腐、地锅牛肉、地锅三鲜等一系列菜品，它们的核心都在于使用地锅制作，并配以面饼、玉米饼。材料准备 2 人份的材料： 面粉 150 克 鸡肉 300 克 鸡蛋 1 个（和面使用，也可以不用）花椒 10-15 粒 姜片 4 片 大葱 4 段 干辣椒 5 个 桂皮 1 小块 八角 2 个 大蒜 5 瓣，不用切 青椒、红椒各半个，滚刀切好 酵母菌 各种调味料 主要步骤 1、使用面粉 150 克和面，使用温水，加入 0.5 克酵母菌，也可以加一个鸡蛋一起，和完的面很软但是不粘手，使用保鲜膜包住，注意先撒一点面粉再包（或者直接放碗里用保鲜膜盖住密封，也需要先在碗里撒一层面粉），这样是避免最后粘住。大概需要发 20-30 分钟，等待的过程可以去做其它准备工作了。2、取出配料，花椒、姜片、大葱、干辣椒、桂皮、八角、大蒜。锅烧热，放油，多放一点油，先放花椒、桂皮、八角，几秒后再放入葱段、姜片、大蒜、干辣椒，大概 10 秒煸出香味，捞出大蒜备用，其它配料不用捞出。3、鸡肉洗干净，放入锅内中火炒 5-10 分钟，放入老抽、生抽、白糖、料酒、豆瓣酱，混合后加入开水，稍微没过鸡肉一点，转为大火，烧开。烧开后中小火焖 15-20 分钟，此时鸡肉已经熟了，要保证还有一些水汤在锅里，因为等一下还需要贴饼、调味、继续焖、收汁等步骤。4、在步骤 3 的过程中，面已经发好，均匀分成条状，具体做法是先拉伸，变成长条，然后揪断就行了，大概 20 个左右（如果锅小了一次贴不完，就分 2 次，贴 1 次先吃着，吃完再加一点汤继续贴下一锅），放入清水中，主要不要再动了，就让它们浸泡在水中。5、步骤 3 结束后，改为小火，加盐调味，并准备贴饼（如果感觉鸡肉的颜色不够，可以再加一点酱油上色）。步骤 4 的面团在水中浸泡了大概 10 分钟，一个一个取出，是湿漉漉的，用手扯成长条饼状，一半贴在锅沿，用力压一下确保贴紧，一半放入汤中。这样，上半部分会焦脆，下半部分吸收了汤汁很美味。饼贴满后，小火继续焖 10-15 分钟，此时注意如果锅受热不均匀，需要每隔几分钟旋转一下锅。6、在步骤 5 中，几分钟后，饼快熟了，就可以加入步骤 2 中捞出来的大蒜，和滚刀切的青椒、红椒，加点香油，稍微搅拌一下，此时汤汁已经基本没了。饼完全熟透了，开锅，大火稍微翻炒几下，就可以吃了，直接在锅里吃。 注意事项 1、锅、灶的选择，在农村地区、乡镇地区、城市周边的农家乐，才会有地锅这种设备，所以在家里自己做是很难找到地锅的，只能退而求其次使用普通的炒锅，也是可以的，注意尺寸要大一点的（做 2 人份的地锅鸡就选 3-4 人份的炒锅）；另外最好保证灶的火力能大一点，有用。2、面发的时间，和面时可以放鸡蛋也可以不放，最好使用温水，发面的时间不要太久，一般 30 分钟就行了，甚至可以不发，直接使用死面。3、贴饼的时候速度一定要快，不然刚刚贴了半圈就已经熟了，丧失了饼的香味；另外饼要贴紧一点，粘在锅上，如果锅受热不均匀，注意每隔几分钟旋转一下锅，保证饼的上半部分能焦脆，这也是需要灶的火力大一点的原因。 上图 在安徽合肥吃到的 在 2018 年中秋期间，回老家经过合肥，于是在以前的同学的带领下吃了一次地锅鸡，非常满足。这个店的地点在安徽大学（馨苑校区）的西门附近，那条路叫九龙路，一条街都是吃的，又名九龙美食街。自己做的 自己在 2018 年国庆期间做了一次，由于没有地锅可以使用，只好选用了普通的炒菜锅，做起来味道还是不错的，只不过饼没有达到香脆的水平，稍有遗憾。此外，家用煤气灶的火力不行，需要更大火的时候不够，导致温度不够高，间接导致了鸡肉的香味和饼的香味没有充分融合，吃的时候感受不到纯正的地锅鸡的香味。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>地锅鸡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 博客集成 travis-ci 自动化测试发布]]></title>
    <url>%2F2018100201.html</url>
    <content type="text"><![CDATA[我的博客一开始是搭建在阿里云上面的，购买了一台 VPS，然后在上面部署了 Git 仓库服务、Nginx 服务，同时也购买了一个域名。每次在本地写完博客，generate 后，直接使用 Hexo 自带的 deploy 命令把内容发布，同时在服务端的 Git 里面再设置一个 WebHook，触发自动拉取更新到 Nginx 项目目录的操作。这样，只要安心在本地写博客就行，写完就 generate、deploy 两下，看似很完美。然而，使用了几个月就发现了各种麻烦事，例如：VPS 流量不够【恶意攻击或者爬虫】、VPS 服务不稳定【怀疑是被恶意攻击】、每次都要在本地生成【generate 命令】、两台电脑的环境配置不一致导致的兼容性问题、博客的整个管理还是略显繁琐。于是，我思考了好几天，并查看了别的成熟案例，最终决定使用 GitHub、travis-ci 搭配，完全的自动化测试部署，本地只负责写 Markdown 文档。这种方案才是真的简约、安全、方便。前提 开源项目 首先说一下大前提，由于使用的所有工具、服务都是免费的，会暴露项目的信息、配置文件、日志信息，也就是说在 GitHub 中的项目类型要设置为 public 类型，使用 travis-ci 也要使用免费版本的【域名是 org 后缀的，不是 com 后缀的】。把本地的项目提交到 GitHub 上面去，所以需要在 GitHub 上面新建一个空白项目，用来对接本地的项目。这里需要注意，为了使用 GitHub Pages 提供的独立子域名【https://username.github.io 】，需要把项目名称设置为 username.github.io 格式的。当然，任意项目名称命名的项目都可以作为 GitHub Pages，可以在 GitHub 的域名后面指定项目名称访问，但是不能使用独立子域名访问。关于这个项目的分支管理，就比较独特了，因为源代码可以说只有配置文件、Markdown 文本文件，而经过 Hexo 编译生成文件基本全部是 HTML 文件。为了方便管理，这两种文件不要放在一起。所以可以用 matser 分支存放 Hexo 编译生成 HTML 文件，给 GitHub Pages 使用；而重新建立一个 source 分支【可以设置为孤儿分支，orphan】用来存放配置文件、Markdown 文本文件，用来更改、提交博客内容。这两个分支是完全没有关联的，不需要合并，他们存放的是完全不同类型的文件。总结一下，对于 GitHub 中的项目，命名最好遵循 username.github.io 这样的格式，方便 GitHub Pages 分配独立的子域名，设置两个分支：master、source。其中，master 分支用来给 travis-ci 提交编译生成的 HTML 文件，可以通过 GitHub Pages 访问；source 分支用来本地提交写博客内容的 Markdown 文件，同时也给 travis-ci 监控，用来获取最新的博客并生成 HTML 文件提交到 master 分支。为了直观起见，下面画一个流程图，清晰地表现出 GitHub 项目的设置：子模块管理 此外，还需要特别注意主题对应的项目，因为在 Hexo 项目中，主题是独立的项目，存放在 themes 文件夹内，例如默认的 landscape 主题。由于是独立的项目，而且是嵌套在 Hexo 中的，对于 Git 来说它属于子模块【submodule】，所以在对接 GitHub 时会遇到一些子模块的问题。具体如何解决我在这里不赘述，可以自行搜索解决方案，但是我要提出必要的思路：一、由于主题子模块默认是在 GitHub 上 clone 下来的，而为了优化显示，我们一定会更改配置文件以及源代码，所以此时一定要解除主题与原来官方 GitHub 仓库的关系，转而把它连接到自己的 GitHub 仓库中。这样自己维护主题配置，避免更新时被官方的空白配置覆盖掉，当然，这样做就失去了与原来官方 GitHub 仓库的联系，导致不能及时获取更新【优化升级、bug 修复等】。二、针对一中的情况，最好的做法是先在自己的仓库中 Fork 一份官方的主题源代码，然后在 Hexo 中的主题文件夹中使用自己 Fork 的主题源代码，这样既可以自己维护主题的配置，也能及时从官方拉取更新。此处会用到 git submodule init、git submodule update 等命令。三、还有一种最简单但是不合理的做法，直接取消主题的 Git 项目属性，即把 Git 相关的配置删除，这样主题项目就不是 Hexo 的子模块了，只是一个普通的文件夹，可以随意更改，并且仅仅作为 Hexo 的一个子文件夹而已，永远不会更新。持续集成 travis-ci 是一种持续集成的工具，持续集成【Continuous Integration】简称为 CI，当然类似的工具有好几种，例如：Jenkins、GitLab CI、Go CD 等，这里就不再赘述。这种工具可以提供一个持续集成功能的平台，在平台上面为你的项目配置好需要执行的操作，例如测试、编译、打包、部署等，这些配置都会有特定的方言规则，不同工具之间大同小异。此外，travis-ci 有两种版本，一种是收费的版本，网址为：https://travis-ci.com ，还有一种是免费的版本，网址为：https://travis-ci.org ，我使用的是免费的版本。 那么，使用这种持续集成的工具有什么好处呢？下面就会一一列举，当然，只是看解释说明可能无法感受，所以可以亲手测试一下各种场景，或者等完全部署完成之后再回头看它的好处。优点一修改可以立即生效 例如你几天前发表了一篇博客，过了几天你发现里面竟然有错别字或者概念性的歧义，为了保证博客的质量，这个最好及时修复。如果自己的电脑在身边还可以操作，直接 修改文件、generate、deploy 即可，但是如果只有一台普通的电脑，里面没有 Git、Hexo、Node.js 等环境，这个就很麻烦，恐怕装开发环境就要很久。但是使用了 travis-ci 之后，这些和环境有关的操作都是它自动化完成的，你只要负责修改文件、提交更新到 GitHub 即可。你可以在本地只安装 Git 环境，clone 代码之后修改完再提交，当然更简单的是直接使用浏览器登录 GitHub 网站，在线直接修改文件提交。提交之后，会自动触发 travis-ci 的工作流程，编译、生成、部署等步骤全部由 travis-ci 自动完成，真的是解放了双手。优点二自动部署到多个远程仓库 这个优点在 Hexo 自带的 deploy 功能中也有，可以把最新的代码同步到 GitHub 以外的远程仓库。有的人觉得在中国大陆 GitHub 的访问速度比较慢，就想着多部署几个站点，例如 Gitcafe、码云等，很简单，在 travis-ci 的脚本中多配置几行命令脚本即可，完全是自定义。优点三部署快捷方便 当后期博客项目过大的时候，会需要一些自定义的优化点，例如搜索、字数统计、文件压缩等，这些特性都需要插件的支持，所以会安装很多插件，同时生成的静态文件也会越来越多。如果按照传统的手动管理方式，每次都需要提交大量的文件更新到远程仓库，耗时长而且没有必要，但是使用 travis-ci 之后，只需要提交 Markdown 文件的变更，其它的大量文件都交给 travis-ci 去生成，这样一来速度就很快了，时间消耗都转移到 travis-ci 上面去了。优点四显示构建图标 对接 travis-ci 后，可以在 README 文件里显示持续集成工具构建结果的图标，是失败还是成功就很形象了。其它优点 1、通过设置失败邮件提醒，在构建失败时会发送邮件通知。2、如果设置了优点四中的构建图标，可以在 GitHub 的项目中，直接点击图标，然后会自动跳转到 travis-ci 的构建页面，可以查看构建日志、历史记录、项目配置等信息。3、此外还可以配置自动化测试、构建成功才继续部署等选择项。 配置详解 登录持续集成帐号 打开官方网站：https://travis-ci.org ，登录帐号，注意不需要注册，一定要使用 GitHub 帐号授权的方式登录，只有这样 travis-ci 才能获取到 GitHub 上面的项目信息，进而进行管理。开启项目管理权限 在 Settings、Repositories 中可以看到自己在 GitHub 上面的项目列表，但是 travis-ci 默认是不会管理任何一个项目的，需要手动开启。由于只需要监控我的博客项目，所以只是开启这一个，然后在开启按钮的右边点击设置按钮，进行下一步的设置。开启管理权限 进入到具体项目的设置页面，在这里，需要勾选 General 下面的 Build pushed branches、Build pushed pull requests，这两个选项的意思是当 GitHub 的项目有 push 操作时，则 travis-ci 会执行配置的流程。设置 General再看下面的 Auto Cancellation，需要勾选 Auto cacel branch builds、Auto cancel pull request builds，这两个选项的意思是自动取消构建过程，主要用于当有连续多个 push、push pull request 发生时，可能先后间隔很短时间内触发了多次构建操作，这样显然浪费资源，没必要构建中间的 push，所以构建排队队列中的任务会自动取消，只保留最新的 push 触发的构建作业。当然，前提是必须等待正在运行的构建作业完成。设置 Auto Cancellation按需设置环境变量 在设置中的 Environment Variables 里面，可以指定一些环境变量【其实就是全局变量】，这些变量可以在后面的构建脚本中直接使用。那么为什么要这么设置呢，有必要吗？其实，一是为了安全，如果直接在构建脚本中使用字符串的形式暴露出来，势必会泄漏个人信息，例如在 GitHub 申请的 TOKEN、邮箱、用户名，这些信息肯定不能泄漏；二是为了方便，设置了全局变量，在构建脚本中直接引用，简化脚本内容，而且以后如果有更改，直接来到 travis-ci 更改环境变量即可，不需要更改构建脚本。因此，这些信息的设置是很有必要的。当然，也不用在这里设置太多信息，有一些不重要的信息直接在构建脚本中使用字符串设置变量即可【构建方言规则里面有 env、global 可以设置只在脚本中有效的全局变量】。我在这里设置了四个全局变量：REPO_TOKEN、GITHUB_URL、USER_EMAIL、USER_NAME，其中 REPO_TOKEN 是在 GitHub 中生成的访问验证信息，下面会讲怎么生成。设置 Environment Variables切记，不要勾选 Display value in build log，否则这些信息还是会在 travis-ci 的构建日志中显示出来，也就是暴露了。其它设置选项 还有一个名称为 Cron Jobs 的设置选项，这个设置就是开启定时任务，可以选择分支、周期、任务操作，如果你需要周期性地对 GitHub 项目进行操作就可以在这里配置。举个例子，如果你不要求你的博客的实时性，即不需要实时更新，可以不用设置上面的 General，直接在这里添加一个周期性任务，每天自动构建一次，这样你更新的博客内容要等一天才能更新到博客网站上面。显然，如果为了保证实时性，这个选项是没有意义的，一般不需要开启。至此，在 travis-ci 中的设置内容已经完成，travis-ci 已经在监控 GitHub 中的项目了，但是还有两件重要的事没做：生成验证信息、写自动构建脚本。生成项目的访问验证信息 在设置环境变量的步骤中，用到了一个变量：REPO_TOKEN，这个是访问 GitHub 项目的验证信息，作用等同于用户名、密码，下面我们需要生成它。在 GitHub 的个人设置中【不是单个项目的设置】，依次找到 Settings、Developer settings、Personal access tokens，这里面的内容就是开发者设置信息，可以生成一些隐私信息给开发者使用，方便程序直接调用接口，我也需要给 travis-ci 生成一个。设置 Personal access tokens选择 Generate new token 按钮，为了安全，GitHub 还会要求验证一次密码，接着就进入到配置页面。在配置页面填写名称、权限即可，我这里只选择了 repo 权限，其它的目前没有必要。注意，生成的 TOKEN 信息是一串字符串，GitHub 只会显示一次，所以要及时复制，刷新页面或者后续再回来查看是看不到的，只能重新申请。把这里生成的 TOKEN 作为设置环境变量步骤中的 REPO_TOKEN 的值即可，这样，在 travis-ci 构建脚本中就能访问 GitHub 从而进行 push 代码更新了。生成 token 填写信息 写自动构建脚本 在 GitHub 项目的 source 分支的根目录下，添加一个 .travis.yml 文件，这是 travis-ci 官方要求的，里面填写构建流程。构建脚本内容的格式需要符合 travis-ci 的方言规范，里面也会用到上面设置的环境变量【只有具有脚本属性的内容才可以使用环境变量，其它不可以，例如邮件通知里面就不能使用环境变量】，获取环境变量的值使用 ${环境变量名称} 格式，与使用 Linux 平台的环境变量格式一致，完整内容如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192# 使用语言为 Node.jslanguage: node_js# Node.js 版本，Node 11.0.0 版本 yarn install 会报错不支持 # node_js: stablenode_js: 10.10.0# 设置只监听哪个分支 branches: only: - source# 缓存，可以节省集成的时间，这里用了 yarn, 如果不用可以删除 cache: apt: true yarn: true directories: - node_modules - CNAME# env# 全局变量，为了安全，不要在这里设置，我这里设置只是示例，其实没有用到 env: global: - GITHUB_XXX_URL: github.com/iplaypi/iplaypi.github.io.git# tarvis 生命周期执行顺序详见官网文档 before_install:# 更改时区 - export TZ=&apos;Asia/Shanghai&apos;- git config --global user.name $&#123;USER_NAME&#125;- git config --global user.email $&#123;USER_EMAIL&#125;# 由于使用了 yarn, 所以需要下载，不用 yarn 这两行可以删除 - curl -o- -L https://yarnpkg.com/install.sh | bash- export PATH=$HOME/.yarn/bin:$PATH# hexo 基础工具 - npm install -g hexo-cli# 初始化所需模块，在 package.json 中配置的有，这样 node_modules 就不用提交到 GitHub 了 # 下次如果换电脑，安装完 Node.js, 全局 (-g 参数) 安装完 hexo-cli, 直接在项目根目录初始化即可 (在 package.json 配置的都会自动下载)- npm install# 本地搜索需要工具 - npm install hexo-generator-searchdb --save# 字数统计，时长统计需要工具，Node 版本 7.6.0 之前，请安装 2.x 版本，npm install hexo-wordcount@2 --save- npm install hexo-wordcount --save# 站点地图，seo 优化使用 - npm install hexo-generator-sitemap --save- npm install hexo-generator-baidu-sitemap --save# 中英文之间自动增加空格插件 # - npm install hexo-filter-auto-spacing --save (这个人用的少，不用了)- npm install hexo-pangu-spacing --save# rss 订阅插件 - npm install hexo-generator-feed --save# 三维卡通人物 - npm install --save hexo-helper-live2d# 三维卡通人物 - 小猫咪模型下载 (保险起见，把模型文件放到自己的文件夹里，不用每次下载)# - npm install --save live2d-widget-model-hijiki# 压缩文件 - npm install hexo-neat --saveinstall:# 不用 yarn 的话这里改成 npm i 即可 - yarn# script# 以后把主题发布到 GitHub 中，先从官方 Fork 再更新自定义的，这里还需要更新子模块内容，示例:git submodule init,git submodule update# 新建.gitmodules 文件，内容 # [submodule &quot;themes/next&quot;]# path = themes/next# url = git://github.com/xin053/MyHexo_NexT_Theme# 此外，还有那个评论系统的升级，Valine, 思考怎么做 script:- hexo clean- hexo generate# 成功之后才会提交，如果失败就会跳过并发送失败邮件通知 after_success:# 获取 master 分支内容 - git clone https://$&#123;GITHUB_URL&#125; .deploy_git- cd .deploy_git- git checkout master- cp -rf ../public/* ./# 必要配置文件 - cp -rf ../README.md ../.gitignore ../CNAME ../index.php ./# 更改 .gitignore 内容，与 source 分支的不一样，直接置空，目前没有需要过滤的 - echo &quot;&quot; &gt; ./.gitignore- git add .# 提交记录包含时间，跟上面更改时区配合 - git commit -m &quot;Travis CI Auto Builder at `date +&quot;% Y-% m-% d % H:% M&quot;`&quot;# 推送到主分支 - git push --force --quiet &quot;https://$&#123;REPO_TOKEN&#125;@$&#123;GITHUB_URL&#125;&quot; master:master# 邮件通知机制，我在这里设置了成功 / 失败都会通知，这里不能使用环境变量 # configure notifications (email, IRC, campfire etc)# please update this section to your needs!# https://docs.travis-ci.com/user/notifications/notifications: email: - playpi@qq.com on_success: always on_failure: always我在构建脚本里面已经尽可能添加了注释说明，解释地很清晰了，那我在这里再说明一下构建脚本具体的思路：指定环境、工具版本、监控的分支【只监控 source 分支】，开启缓存，设置全局变量 在 before_install 流程中进行环境的初始化 在 install 流程中安装基础环境【在这里 yarn 会使用缓存的内容，节约时间】在 script 流程中生成 HTML 文件 在 after_success 流程中将最新代码推送到 master 分支【构建成功才会进行】在 notifications 流程中执行通知邮箱与通知方式 这里要特别注意，我没有在构建脚本中使用 hexo deploy，所以在安装插件阶段也没有安装 hexo-deployer-git【对于我来说它不够灵活，只能发布 public 文件夹下面的内容】，因为我这里情况比较复杂，需要复制一些自定义的文件到 master 分支，而且我这里不再提交更新到 VPS 或者其它托管网站【这个我使用别的方式另外做了】。如果你的情况只是简单地自动部署，则在 after_success 流程中不需要那么多脚本，直接 hexo deploy 就行【当然别忘了在 Hexo 的配置文件 _config.yml 中加上 deploy 相关配置】。此外，我这里直接把主题的 Git 属性取消了，主题的文件夹被当作普通的文件夹提交到了 source 分支，这只是暂时的做法【而且显然不妥】，后面会把主题独立发布为一个项目，作为 Hexo 的一个 Git 子模块使用，敬请期待。我现在这么做虽然不妥，但是没有办法，因为我自己更改了太多的主题配置文件，而且版本过于陈旧，把这个主题项目单独整理出来需要消耗一些时间，以后会作为独立的博客发表一篇的，到时候构建脚本也会有小改动，敬请期待。原理简述 travis-ci 为什么可以做到实时更新，只要有 push pull request 或者 push 就能被 travis-ci 监控到，然后去执行构建脚本。其实，背后使用的还是 GitHub 的 Webhooks 技术，由于给了 travis-ci 认证权限，travis-ci 就申请了 Webhooks 的发送请求，每当监控的项目有 push 或者 push request 请求时，GitHub 会发送项目的信息给 travis-ci，这样 travis-ci 就可以做出相应的动作，例如执行构建脚本。 这种做法我在另外一篇博客中也有实战经验，为了体验一下 VPS 上面的博客自动更新，也折腾了好几天，使用 PHP 自己搭建的服务脚本，有 GitHub 通知时自动拉取更新到本地指定的目录，有兴趣的可以参考一下：使用 Github 的 WebHooks 实现代码自动更新 。 可以去项目的设置中【不是用户设置】，找到 Settings、Webhooks，可以看到有一条名称为 https://notify.travis-ci.org 的 Webhooks 记录，它就是 GitHub 用来通知 travis-ci 的接口。效果预览 简化命令 完成了整个浩大的自动化部署工程后，以前需要使用的 Hexo 三部曲：hexo cleanhexo generatehexo deploy完全不需要了，取而代之的是 Git 三部曲：git add .git commit -am ‘update message’git push当然，在本地测试时还是要使用 Hexo 三部曲 hexo cleanhexo generatehexo sever 查看项目构建信息 在本地更新 source 分支的文本内容，提交到 GitHub 上面，然后去看 travis-ci 的构建结果。构建基础信息，包含构建消耗时间、项目 Commit 的 id 值、分支名、构建流程消耗时间等等。接着往下看，有构建日志、构建脚本内容。查看一下构建历史列表，可以看到所有的构建信息，每一个都会有编号，从 1 开始增加。成功的构建是绿色的，失败的是红色的。给项目添加构建图标 在 travis-ci 的项目名称右边，可以看到有一个图标，点击，在弹出的对话框中选择分支与格式，我选择 source 分支，Markdown 格式，然后在下面的文本框中就会生成一个链接，直接复制粘贴到 GitHub 项目中的 README 文件中即可。构建图标链接生成 GitHub 项目的 README 文件填写GitHub 项目的构建图标查看 点击这个图标，是可以直接跳转到 travis-ci 的构建页面，可以查看构建日志、历史记录、项目配置等信息。至此，整个自动化部署方案完全实现，并验证通过，效果超级好，以后可以直接使用 Git 命令提交更新了。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>GitHub</tag>
        <tag>travis-ci</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10 取消 Skype for Business 自动登录]]></title>
    <url>%2F2018090701.html</url>
    <content type="text"><![CDATA[最近在使用 Win10 系统，遇到一个问题，每次开机，Skype for Business 都会自动弹出来，提示登录，每次我都会关掉它。遇到多次之后，我想这个应用我不需要，直接卸载掉算了，但是却找不到这个应用的信息，最后只能通过关闭 开机自动启动 的方式来解决问题，本文记录解决问题的过程。问题出现 最近在使用 Win10 的时候，每次开机后，Skype for Business（这个应用不同于 Skype，虽然功能一样）总会弹出来，提示我登录，我每次都会毫不犹豫地关掉它。Skype for Business 登录界面 正常的 Skype 应用登录界面 但是出现多次之后，很麻烦，当我想卸载这个应用的时候，发现从应用列表里面找不到，也就无从卸载。后来就想能不能关闭开机启动，找了一些文档发现可以，那就这么办了（而且还发现 Skype for Business 根本卸载不了）。问题解决 1、Skype for Business 是属于 Office 套件中的一个软件，所以在安装整个 Office 的同时也会自动安装上 Skype for Business。由于是一次安装整个 Office 套件，所以无法单独删除其中的一个软件（Skype for Business）。如果不需要开机自动启动 Skype for Business（也就不会提示我登录了），可以在 Skype for Business 的 设置 菜单中的 个人 选项里将 当我登录到 Windows 时自动启动应用 这个设置取消。设置（在登录界面的右上角，有一个齿轮按钮）取消当我登录到 Windows 时自动启动应用 2、不知道哪一天 Windows 升级到了新的系统后，Skype for Business 不见了（怎么找也找不到），随之而来的是 Skype，尽管它也属于 Office 中的一个应用（还有很多其它一系列应用），但是这个应用可以单独安装卸载，不再与 Office 绑为一个整体。 打开我的 Office查看应用列表，也可以直接安装显示的应用 问题总结 1、我是一开始关闭了 Skype for Business 的登录界面，然后再想打开它，就找不到了，不知道在哪（理论上应该隐藏在某个应用列表里面，目前我还没找到，可能是 Windows 系统升级导致的），但是现在却自动有了 Skype 这个应用（可能是 Windows 系统升级替换了以前的 business 版本），其实这 2 个应用应该差不多。2、现在 Windows 系统升级到最新版本（最新升级时间是 2019-02-17）后，Skype for Business 已经不存在了，替换它的是 Skype，而这个应用是可以单独卸载的。3、参考： 官方回复 、 设置方式、Skype for Business 应用介绍 。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>Skype for Business</tag>
        <tag>自动登录</tag>
        <tag>Win10</tag>
        <tag>Skype</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[踩坑特殊字符之硬空格]]></title>
    <url>%2F2018090601.html</url>
    <content type="text"><![CDATA[最近处理数据的过程中，发现一个奇怪的问题，处理数据逻辑如下：有一个短字符串，我需要从一个长字符串寻找这个短字符串是否出现。这个逻辑很简单，使用任何一种编程语言，基本上都会有 包含 这种方法，直接就可以判断了，但是我遇到的情况明明就是包含了，子串就是存在，但是判断结果却不包含。另外我又直接把 2 个字符串单独取出来，肉眼去看，也是包含的。愁眉苦展之际，突然灵光闪现：会不会字符串中包含一些奇怪的特殊字符，并且肉眼难以发现。于是立马去验证一下，果然是这样，解决了我的问题，本文记录解决问题的过程。问题出现 在使用 Java 编程语言处理数据的时候，有一个字符串包含判断的逻辑，明明是包含关系，判断的结果却是不包含，代码示例如下：1234@Testpublic void containsTest() &#123; System.out.println ("每当 #每月 28 日京东企业会员日 #来临，就会有优惠".contains ("# 每月 28 日京东企业会员日 #".trim ()));&#125;运行结果：尽管加上了 trim () 方法，但是返回结果仍然是 false，足以说明子串中头尾有某个符号很特殊，并没有被清除掉，所以我觉得很蹊跷。虽然在代码和截图中，那个大大的空格（其实不是空格字符）看起来很显眼，但是在实际运用中是在文本中存放的，而且不止这一个字符串，有很多，所以在检查时使用搜索替换功能（把空格替换为空白），也是没把这个特殊符号清除掉。同时，也没有料想到文本中会出现这样特殊的符号，所以只是简单地使用 trim () 方法来清除头尾空白符（还以为生效了，其实遇到这种特殊的符号就没作用了）。把内容复制到 Notepad++ 文本编辑器中，并且设置文本编辑器的视图显示所有的字符（会用带有颜色的特殊图标来表示文本中的特殊字符，例如肉眼看不到的字符：空格、换行等），然后可以看到空格（橙色点点）、Tab 符号（橙色箭头）、回车换行（黑块）都会显示出来，但是唯独这个特殊字符没有显示出来，仍然是空白一片。使用文本编辑器打开 问题解决 已经知道这是个特殊字符了，下一步只要搞清楚这是个什么字符就行了，问题就会迎刃而解。先把特殊字符复制出来，找一个转码器，把字符转为十六进制编码，看看它的编码是什么，在线编码转换工具参考：http://ctf.ssleye.com/jinzhi.html 。在 文本 这个文本框中输入特殊字符（–&gt; &lt;–），然后在 十六进制 文本框中可以看到编码是 a0。可以看到十六进制的结果是 a0好，接下来去 Unicode 字符列表（维基百科里面的：https://zh.wikipedia.org/wiki/Unicode% E5% AD%97% E7% AC% A6% E5%88%97% E8% A1% A8 ）查看这到底是个什么特殊字符。直接搜索 00A0，就可以找到，发现这是 不换行空格 ，在 拉丁字符 - 1 辅助 里面。不换行空格 更进一步，我去维基百科查看这一特殊符号的介绍，发现这个符号还是挺有用的，附上链接：https://zh.wikipedia.org/wiki/% E4% B8%8D% E6%8D% A2% E8% A1%8C% E7% A9% BA% E6% A0% BC 。不换行空格介绍，一般用在网页排版中 至此，问题原因找到了，我竟然被一个特殊符号坑了（以前也被输入法的全角、半角问题坑过），那解决办法就很简单了，针对这种符号做替换清除就行了。我仍在思考，这个符号是怎么被输进文本文件给我的，因为正常人通过输入法不可能打出这个符号。后来询问相关人，得知他们是从网页上直接复制的内容粘贴到文本文件中，这就可以解释了，因为这个符号就是针对网页的自动压缩空白符问题，量身定做的，怪不得。问题总结 1、这个特殊字符（–&gt; &lt;–，编码 a0）有些编辑器不一定支持，会产生丢失，例如我把这条字符串内容放在了 Tower 上面的任务回复中，想记录一下，结果再复制下来就变成了空格字符，说明丢失了（并且被转为了空格字符）。但是没关系，我们记住它的 Unicode 编码就行了，找一个工具（例如：http://ctf.ssleye.com/jinzhi.html ）就可以转换了，Unicode 编码是：U+00A0，把 a0 复制到 十六进制 的文本框中，则 文本 这个文本框中出现的就是它的字符，当然，直接肉眼看不出来，要选中变为蓝色才会发现有一个字符。编码恢复到字符2、以后处理字符串问题的时候，一定要区分场景，真的是自己知识面之外的情况都可能出现，而且是正常的，自己千万不要怀疑人生，而是要抽丝剥茧，一步一步找问题。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>硬空格</tag>
        <tag>不换行空格</tag>
        <tag>hard-space</tag>
        <tag>fixed-space</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[归来]]></title>
    <url>%2F2018090501.html</url>
    <content type="text"><![CDATA[好，整理完成后，重新出发。二级标题，自动创建锚点和目录 三级标题 - 开始 1$ hexo new "My New Post"More info: Writing 三级标题 - 中间 1$ hexo serverMore info: Server 三级标题 - 结束 1$ hexo generateMore info: Generating 三级标题 - 备注 1$ hexo deployMore info: Deployment 踩坑记录 注意使用 hexo 对 Markdown 文件进行解析时，有一些转义字符是会失败的（使用反斜杠 \ 进行转义的，例如美元符号 &#36;，成对出现有特殊含义，所以需要转义，在 Markdown 中可以使用 \$ 进行转义，但是 hexo 解析完成 html 文件是失败的），所以最好使用编码解决，例如美元符号使用 &amp;#36; 替代。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>建站</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven 使用中遇到的证书问题以及 JDK8 问题]]></title>
    <url>%2F2018082501.html</url>
    <content type="text"><![CDATA[在 Java 开发过程中，使用 Maven 往私服 deploy 构件【Java 打成的 jar 包】的时候，原本正常的流程突然出问题了，报错信息：1[WARNING] Could not transfer metadata org.leapframework:leap:0.4.0b-SNAPSHOT/maven-metadata.xml from/to bingo-maven-repository-hosted ($bingo.maven$): sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target看到里面的 security 和 certification 关键词就猜测是安全与证书的问题。恰好最近私服的域名访问从 http 升级为了 https，也就是增加了 SSL 证书，我想可能和这个有关。本文就记录解决这个问题的流程，以及后续由此又引发了其它的问题，例如 JDK8 导致的注解问题、IDEA 的乱码问题。问题出现 在分析问题现象之前，首先需要了解一个基本事实，Maven 是依赖于 JDK 环境运行的，所以使用 Maven 之前必须安装 JDK，并且配置好 JAVA_HOME 。在使用 Maven 过程中遇到的一些问题可能会和 JDK 环境有关，例如 SSL 证书问题、JDK 版本问题。以下内容现象基于 Windoes7 X64 操作系统，JDK 版本为 1.7u89，Maven 版本为 3.5。好，言归正传，继续关注遇到的问题。在某一天，发现公司的 Maven 私服仓库更新了管理系统，并且增加了 SSL 证书【证书类型是 Let’s Encrypt ，有效期三个月，这为后续的各种问题埋下了伏笔】，这样访问的链接全部变为 https 开头的了。但是我又注意到一个现象，我发现仓库里的很多 SNAPSHOT 类型的 jar 包消失了，这应该是管理系统设置了自动清除机制，把没用的快照版本的 jar 包全部清除，节约空间。但是，其中有一些 jar 包对于我来说是有用的，更麻烦的是这些 jar 包根本没有 RELEASE 正式版，都是前人留下的坑，为了图方便临时打了一个快照版本 jar 包给别人使用，竟然把资源文件也打进去，导致一个 jar 包有将近 100M 大小。这种操作显然是违背 Maven 的理念的，面对这种情况，再想申请发布一个 RELEASE 版本的 jar 包也麻烦，而且这种业务类型的代码就不应该打成 jar 包给别人使用。思考了半天，我决定采用一个折中的办法：取得代码阅读权限，移除没用的资源文件，仅仅发布代码，仍旧发布 SNAPSHOT 版本，以后有时间再把业务代码复制出来，不再使用 jar 的形式。思路确定了，就开始行动。一开始我发现开发环境本地仓库有这些 jar 包，还想手动上传到 Maven 私服仓库的，把坐标定义准确就行了。但是后续发现 Maven 私服仓库的管理系统不支持手动上传 jar 包，只支持通过账号、密码认证的方式，从源代码发布 jar 包到仓库，而且个人账号只能发布 SNAPSHOT 版本的，管理员账号才能发布 RELEASE 版本的。路走到了这里，那我只能先获取项目代码的权限，然后新开分支，删掉无用的配置文件，仅仅发布源代码到私服仓库，先发布 SNAPSHOT 版本使用。准备工作做好后，接着开始 deploy，就遇到了一连串的问题，在确认账号密码没有问题的前提下，deploy 失败，错误信息如下：1[WARNING] Could not transfer metadata org.leapframework:leap:0.4.0b-SNAPSHOT/maven-metadata.xml from/to bingo-maven-repository-hosted ($bingo.maven$): sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target看到里面的 security 和 certification 关键词就猜测是安全与证书的问题，公司私服的其它环境并没有变化，只能从 SSL 证书入手解决问题，然而一开始没有头绪。 但是又问了同事，发现他们可以正常 deploy，又测试了一下线上的发布系统，也可以正常发布 jar 包到公司私服仓库。那只有 2 个怀疑方向了，一个是本地 Maven 的版本问题，一个是本地 JDK 的版本问题。后来通过对比发现，的确是 JDK 版本的问题，某些低版本的 JDK 不会自动导入 Let’s Encrypt 的证书，才会导致 Maven 进行 deploy 时认证失败【Maven 底层是依赖于 JDK 的】，自然而然 delpoy 也就失败了。参考：stackoverflow 讨论一例 。The Let’s Encrypt certificate is just a regular public key certificate. Java supports it (according to Let’s Encrypt Certificate Compatibility , for Java 7 &gt;= 7u111 and Java 8 &gt;= 8u101). 在找原因的过程中还一度怀疑是公司私服仓库的 SSL 证书问题，后面发现是本地环境的问题，但是背后的根本原因还是 SSL 证书的问题。恰好遇到了 Let’s Encrypt 类型的证书，又恰好 JDK 版本过低，引起一系列连锁反应。问题解决 既然找到了问题，那就容易解决了，直接升级 JDK 即可，JDK7 需要升级到 &gt;=7u111，JDK8 需要升级到 &gt;=8u101。手动导入证书 其实，如果不升级 JDK，还有一种繁琐的解决办法，那就是手动导入证书。解决思路就是从需要访问的 https 站点下载证书，然后导入本地的 Java 环境证书库，缺点就是每次证书更新都需要重新导入，显得麻烦。其实这种做法更有助于我们理解这个问题的核心所在，高版本的 JDK 会自动帮我们导入 Let’s Encrypt 证书，但是低版本的不会，我们不仅能知其然，也知其所以然。下载证书 证书可以在访问网站时，在 url 文本框的左侧，有一把小绿锁，选中点击，接着查看证书，下载即可。点击小绿锁 下载证书 此外也可以通过浏览器调试工具的 Security 标签查看下载证书，下图是使用 Chrome 浏览器的效果，其它浏览器可能会略有不同。把证书文件导入证书库 先要清楚本地 Java 的证书库的位置，一般在 {JAVA_HOME}/jre/lib/security/ 目录下面，里面有一个 cacerts 文件，它就是所有证书的集合组成的文件。另外还要清楚 keytool 工具，它是 JDK 提供的可以操作证书的工具，可以直接使用。Java 证书库的位置 以下命令供参考，特别需要注意 命令执行权限 、 文件写权限 ：12345678-- 把证书文件 maven.datastory.cer 导入 Java 证书库 cacerts 中，别名为 maven.datastory，密码是 changeitkeytool -import -alias maven.datastory -keystore cacerts -file maven.datastory.cer -trustcacerts -storepass changeit-- 在证书库中查看指定别名的证书信息，需要输入密码 keytool -list -keystore cacerts -alias maven.datastory-- 删除证书库中指定别名的证书 keytool -delete -keystore cacerts -alias maven.datastory后续维护问题 这种手动导入证书的方式，只能确保一时可以使用，因为证书是会过期的，特别是 Let’s Encrypt 证书，有效期只有 3 个月。所以后期维护起来会很麻烦，如果某一天发现 deploy 又报一样的错，那估计是证书过期了，也有可能是站点的证书被更换了。因此，还是升级高版本的 JDK 比较好，把证书的维护更新工作都交给 JDK 来执行，自己安心写代码就行了。我后期就经历了这一过程，用了没多久发现 deploy 还是失败，只好把证书下载下来重复了导入的过程。而且一开始没有往证书过期上面怀疑，浪费了一些找问题的时间。自定义证书库 其实还有一种更为繁琐的做法，那就是自定义证书库，思路就是把 Java 的证书库复制一份，并且把自己的证书添加进去，然后为 Maven 指定这个自己的证书库。指定证书库的参数【例如路径、密码、证书库类型】需要在 Maven 命令执行之前配置，就像设置一些环境变量一样。更为详细的做法就不演示了，毕竟一般都没有必要这样做，可以参考：自定义 Maven 证书库 。 题外话 如果 IDEA 因为 Maven 的依赖问题，有红色的线条提醒，可能是没有及时更新 UI 界面导致的，其实依赖都完整了，此时重新启动 IDEA 即可，我一直怀疑这是 IDEA 的 bug，有时候明明缺少依赖，IDEA 也不提示错误。而判断是否真的缺失 Maven 依赖，应该使用 Maven 命令编译、打包【mvn compile package】，看看日志是否正常，不能以 IDEA 的显示为依据【有时候 IDEA 会抽风】。如果真的缺少 Maven 依赖，使用 Maven 命令编译、打包是会失败的，并且有提示。而如果明明不缺少依赖，但是代码中报错一大堆，此时强制更新【reimport 重新导入】 Maven 依赖即可，必要时也需要重启 IDEA。另外有一个很好用的小技巧，如果本地仓库存在一个可以使用的 jar 包，可以直接复制给别人使用，按照相同的目录放在指定的路径下面即可，这样 Maven 就会认为本地已经存在 jar 包了，不再去私服仓库下载。这种做法虽然很低级，但是却实用，可以快速解决私服仓库没有 jar 包，初始化环境时无法下载依赖的情况。其它问题记录 在同一时期，同事的环境已经升级到 JDK8 以上，并且 &gt;=8u101，Maven 版本是 3.2.1，可以正常 deploy 构件。但是突然有一天就出错，从日志来看是认证问题，切换为别人的账号密码就正常，使用他自己的账号密码却报错，我猜测是账号的问题，找运维解决。报错信息如下，留意关键部分【Not authorized , ReasonPhrase:Unauthorized.】：123456789101112131415161718192021[INFO] --- maven-install-plugin:2.4:install (default-install) @ project-name ---[INFO] Installing D:\datastory\workspace\study\project-name\target\project-name-1.1.11-SNAPSHOT.jar to D:\pro\env\maven\repository\com\datastory\radar\project-name\1.1.11-SNAPSHOT\project-name-1.1.11-SNAPSHOT.jar[INFO] Installing D:\datastory\workspace\study\project-name\pom.xml to D:\pro\env\maven\repository\com\datastory\radar\project-name\1.1.11-SNAPSHOT\project-name-1.1.11-SNAPSHOT.pom[INFO] [INFO] --- maven-deploy-plugin:2.7:deploy (default-deploy) @ project-name ---Downloading: http://maven.domain/nexus/content/repositories/snapshots/com/datastory/radar/project-name/1.1.11-SNAPSHOT/maven-metadata.xml[WARNING] Could not transfer metadata com.datastory.radar:project-name:1.1.11-SNAPSHOT/maven-metadata.xml from/to snapshots (http://maven.domain/nexus/content/repositories/snapshots): Not authorized , ReasonPhrase:Unauthorized.[INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 5.860 s[INFO] Finished at: 2018-08-10T21:48:24+08:00[INFO] Final Memory: 24M/326M[INFO] ------------------------------------------------------------------------[ERROR] Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy (default-deploy) on project project-name: Failed to retrieve remote metadata com.datastory.radar:project-name:1.1.11-SNAPSHOT/maven-metadata.xml: Could not transfer metadata com.datastory.radar:project-name:1.1.11-SNAPSHOT/maven-metadata.xml from/to snapshots (http://maven.domain/nexus/content/repositories/snapshots): Not authorized , ReasonPhrase:Unauthorized. -&gt; [Help 1][ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionExceptionJDK8 注解问题 切换到 JDK8 并且升级之后，在 deploy 构件到私服仓库的时候，出现了另外一个问题，直接 deploy 失败，报错信息如下：12Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:2.7:jar (attach-javadocs) on project [projectname]: MavenReportException: Error while generating Javadoc:Exit code: 1 - [path-to-file]:[linenumber]: warning: no description for @paramdeploy 失败日志截图 查看里面的关键信息，可以找出 maven-javadoc-plugin 这个插件，说明是这个插件在生成 Javadoc 的时候出问题了。而我回想了一下，最近的插件版本、代码结构并没有变化，唯一变化的就是开发环境，JDK 由 1.7 版本切换为了 1.8 版本，那就往这方面找问题了。查了一下资料，由于 JDK8 的 Javadoc 生成机制比之前的版本要严谨许多，在 Javadoc 中添加了 doclint，而这个工具的主要目的是获得符合 W3C HTML 4.01 标准规范的 HTML 文档。所以使用 maven-javadoc-plugin 插件 deploy 的时候，JDK8 环境触发了 Javadoc 验证，验证自然不能通过，Maven 插件直接报错，deploy 不成功。为了验证这个过程，我又把本地环境的 JDK 切回到了 1.7 版本，可以正常 deploy，成功发布 SNAPSHOT 版本的构件到私服仓库。而由于线上发布系统的 JDK 版本强制设置为了 1.8，无法更改，所以无法在线上做验证，只能发现在线上发布的 RELEASE 版本构件一定是失败的。既然找到了原因所在，接下来就容易操作了，可以选择关闭 Javadoc 验证，或者直接不使用 maven-javadoc-plugin 这个插件。而我选择继续使用这个插件，但是可以选择是跳过生成 Javadoc 还是关闭 Javadoc 验证，根据自己的需要了，具体步骤与效果我会在下面演示。先来看一下这个插件的 pom.xml 文件配置：12345678910111213141516171819202122232425&lt;!-- javadoc 打包插件 --&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt; &lt;version&gt;2.9&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-javadocs&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;!-- 直接跳过 Javadoc 生成 --&gt; &lt;skip&gt;true&lt;/skip&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;!-- 此参数针对 jdk8 环境使用，如果本机是 jdk7 环境会报错，所以增加上述 skip 配置，保证线上和本地都可以部署 (install/deploy), 当然最好使用 profile 激活灵活的配置 --&gt; &lt;!-- add this to disable checking, 禁用 Javadoc 检查 --&gt; &lt;additionalparam&gt;-Xdoclint:none&lt;/additionalparam&gt; &lt;!-- 使用 profile 激活灵活的配置 --&gt; &lt;additionalparam&gt;$&#123;javadoc.opts&#125;&lt;/additionalparam&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt;以上配置是非常完整的，把所有的重要配置项都列出来了，并给出了注释，实际使用中选择自己需要的即可，如果看不懂没有关系，接着往下看，详细解释了参数的使用以及最终的优化配置方案。在这里需要注意一个问题，如果你的 pom.xml 文件中根本没有配置 maven-javadoc-plugin 插件，但是这些错误仍旧存在，那是为什么呢？其实是因为 Maven 已经默认给每个生命周期都绑定了对应的插件，如果没有在 pom.xml 中配置自定义的插件，则使用 Maven 默认的【这里的默认有 2 层意思，一个是插件类型默认，一个是版本号默认】。例如当前项目如果没有配置 javadoc 插件，则会默认使用仓库里版本最高的稳定版 maven-javadoc-plugin 插件，插件的配置也都是默认的，无法更改。而使用 Maven 默认的插件，很可能会引发莫名的问题，根本原因就在于对于某个插件、某个版本的插件、插件的默认配置，我们都是未知的，出了问题也比较难定位，所以在一些重要的插件上面还是手动显式配置出来比较好，这样问题都能在自己的掌握之中。跳过 Javadoc 的生成 如果直接配置了跳过 Javadoc 的生成【使用 skip 参数】，configuration 下面的内容都不需要配置了，配置了也不会用到。由于插件会直接跳过 Javadoc 的生成，所以也就不存在验证的过程了。然而，这种做法对于构件的使用方是不友好的，因为缺失了 Javadoc，当查看源码遇到问题时就无法寻求有效的帮助了。123&lt;configuration&gt; &lt;skip&gt;true&lt;/skip&gt;&lt;/configuration&gt;开启 Javadoc 生成但是关闭 Javadoc 验证 因此，还是要开启 Javadoc 的生成，但是关闭 JDK8 对于 Javadoc 的严格验证，此时需要在 configuration 里面增加参数：123&lt;configuration&gt; &lt;additionalparam&gt;-Xdoclint:none&lt;/additionalparam&gt;&lt;/configuration&gt;一般为了方便他人查看项目的参数，最好把这种重要的参数值设置为全局变量，在 pom.xml 文件的 properties 节点下面声明即可，例如：123456789-- 设置全局变量 &lt;properties&gt; &lt;additionalparam.val&gt;-Xdoclint:none&lt;/additionalparam.val&gt;&lt;/properties&gt;-- 使用全局变量 &lt;configuration&gt; &lt;additionalparam&gt;$&#123;additionalparam.val&#125;&lt;/additionalparam&gt;&lt;/configuration&gt;潜在的问题 难道这样配置就完了吗，显然有潜在的问题，作为经历过的人，我告诉你，附加参数 -Xdoclint:none 是只有 JDK8 及以上版本才会支持的，如果有人在构建项目时使用了 JDK7 的环境，最终的结果还是失败，失败的原因是参数不合法，无法支持。报错信息举例：12Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:2.7:jar (attach-javadocs) on project [projectname]: MavenReportException: Error while generating Javadoc:Exit code: 1 - javadoc: 错误 - 无效的标记: -Xdoclint:none所以接下来还要想一个更好的办法，不仅能关闭 Javadoc 的验证，还要根据当前的实际 JDK 环境来自动切换参数的取值，这样就可以兼容所有的环境了。显然，没有什么比 profile 更适合这个情况了，配置一个 profile 激活信息，根据 JDK 的版本激活全局变量，参数值传入给 additionalparam 使用，比起上面的固定的全局变量，这种可变的全局变量更灵活。详细配置如下：1234567891011121314151617-- 全局变量 javadoc.opts 在 JDK8 及以上版本才激活 &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;doclint-java8-disable&lt;/id&gt; &lt;activation&gt; &lt;jdk&gt;[1.8,)&lt;/jdk&gt; &lt;/activation&gt; &lt;properties&gt; &lt;javadoc.opts&gt;-Xdoclint:none&lt;/javadoc.opts&gt; &lt;/properties&gt; &lt;/profile&gt;&lt;/profiles&gt;-- 使用激活的全局变量，如果没有激活则为空 &lt;configuration&gt; &lt;additionalparam&gt;$&#123;javadoc.opts&#125;&lt;/additionalparam&gt;&lt;/configuration&gt;插件版本的踩坑 在解决问题的过程中还遇到了一个典型的问题，由插件版本引起。一开始在项目的 pom.xml 文件中没有配置插件 maven-javadoc-plugin 的版本号，即 version 参数，导致项目使用的是公司私服仓库最新的版本：v3.0.0，而在这个版本中使用 -Xdoclint:none 关闭验证是无效的，不知道是插件本身的问题还是参数 -Xdoclint:none 对 3.0.0 版本的插件无效。后来指定版本为 2.9，就没有这个问题了。由于一开始没有指定插件 maven-javadoc-plugin 的版本号，出错了也不知道为啥，在 deploy 的输出日志中看到使用的 v3.0.0 版本的插件，猜测可能和插件版本有关系，于是更换了版本，就没有问题了。因此这种重要的插件还是要手动指定自己认为稳定的版本，这样有问题也能在自己的掌握之中。参考 在 JDK8 中禁用 Javadoc 验证 stackoverflow 中的问答一例IDEA 乱码问题 在解决上面的 JDK8 注解的问题过程中，遇到了一个乱码问题，系统环境是 Windows7 X64。当在 JDK7 的环境中配置了以下内容时：123&lt;configuration&gt; &lt;additionalparam&gt;-Xdoclint:none&lt;/additionalparam&gt;&lt;/configuration&gt;本意是想测试这个参数在 JDK7 环境中的效果【前面已经验证过在 JDK8 中是完美运行的】，发现报错了，但是错误信息是乱码的，导致看不出来错误信息是什么，也就没法解决问题。在 JDK7 中 deploy 报错乱码 其实，这只是 IDEA 的编码设置问题，更改一下编码就行了。在 setting –&gt; maven –&gt; rumnner –&gt; VMoptions ，添加参数：-Dfile.encoding=GB2312 ，就可以正常输出了。当然，Windows 系统配置编码为 GBK 也行。为什么要这么配置呢，因为 Maven 是依赖于当前系统的编码的，可以使用 mvn -version 命令查看编码的信息，查看 Default locale 那一项，可以看到是 GBK。配置完成后，报错信息正常显示 问题总结1、通过手动导入证书的方式，一开始解决了问题，后来过了一段时间突然又不能使用，这时候我很是疑惑的。问了问同事却都能正常使用，我还以为是我的 Maven 的版本问题，换了 Maven 版本也不行，最后折腾了很久发现是私服域名的 SSL 证书失效了，再重新导入一份就行了。因为私服域名的 Let’s Encrypt 证书有效期只有三个月，所以每次证书续期或者更换的时候，都要手动重新导入，旧证书会自动失效。这样多麻烦，所以还是直接升级 JDK 比较好，一劳永逸。2、在与同事的开发环境对比的过程中，仔细对比了 Maven 的版本和 JDK 的版本，发现都是 Maven 3.5 与 JDK1.7，但是别人能用我的就不能用，一度怀疑人生。最终才发现根本原因是没有对比小版本号，同样是 JDK1.7，没有 &gt;=7u111 也不行。3、关于 Maven 的插件版本问题，切记要手动指定自己认为可靠的版本，不要让 Maven 使用仓库最新的稳定版本，哪怕的确是使用最新的版本，也要指明，确保出了问题自己可控，否则就像无头的苍蝇乱打乱撞。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Maven</tag>
        <tag>JDK</tag>
        <tag>证书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[武功山两天徒步登山总结]]></title>
    <url>%2F2018071601.html</url>
    <content type="text"><![CDATA[公司部门组织团建，在 2018-07-13 这天开始，一行二十多人傍晚从公司出发，踏上了去往江西武功山的旅程。本文就详细讲述 2018-07-13 晚上从广州出发，2018-07-16 凌晨到达广州的整个旅程。 图文并茂，等待整理。]]></content>
      <categories>
        <category>游玩</category>
      </categories>
      <tags>
        <tag>武功山</tag>
        <tag>徒步</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[辣椒炒肉做法总结]]></title>
    <url>%2F2018063001.html</url>
    <content type="text"><![CDATA[辣椒炒肉，农家小炒肉，青椒肉片，很多类似的菜品，有的是川菜，有的是湘菜，但是它们有一个共同点，都是食材简单、可口下饭，本文就讲述辣椒炒肉的步骤以及需要注意的地方，本文讲述的做法是湘菜的做法。食材准备 辣椒炒肉的食材比较简单：辣椒 4 个（最好是螺丝椒，次之也可以用普通的青椒）二刀肉 300 克（或者是普通的后腿肉也行，实在没有用前腿肉也勉强可以，确保肥瘦比例是 2：3 就行了，一定要有肥肉）老抽、生抽、料酒（用来腌制瘦肉使用，最后也要使用生抽提鲜）食用盐 大蒜 5 瓣 炒制步骤 1、螺丝椒 4 个，脆嫩皮薄，买的时候一定要挑选颜色比较鲜亮的（颜色发暗的不能要，不新鲜了），同时捏起来比较脆硬（软的不能要），说明新鲜。此外尽量挑大个的，比较容易去籽、容易滚刀切。买回来后去除头部，去除籽粒（一定要去除干净，否则炒制的时候辣椒籽容易变黑变苦，影响颜色与口感，当然稍微有个别的辣椒籽可以忽略），滚刀切成小块，沥干水分（尽量让它保持干燥）。 认准螺丝椒 螺丝椒去籽洗干净 滚刀切螺丝椒 2、将二刀肉去皮，肥瘦分离（这个步骤自己处理比较麻烦，最好让卖肉的大叔大姐帮忙处理了），肥肉切片，稍微切大一点，方便后续炼油，沥干水分，不做任何处理，直接放入碗中备用。瘦肉也是切片，放入碗中，接着放入生抽、老抽、食用盐、料酒，用手搅拌 1 分钟，再腌制 10 分钟。 肥肉沥干水分装碗备用 腌制瘦肉 3、大蒜切片，每一瓣切 3-5 片，尽量厚一点，备用。 忽略掉旁边的葱花和蒜粒，那是炒其它菜使用的 4、干锅煸炒辣椒（这个过程大概 3 分钟），先将炒锅烧干烧热，不放油、不放水，直接将步骤 1 中切好的辣椒片扔进去（这也是辣椒切好后必须沥干水分的原因），就是干煸，这是决定这道菜口感的第一步。控制中小火（大火容易糊），让辣椒块水分逐渐挥发，表皮起皱，即俗称起虎皮，这时候会有少量烟冒出，并伴随着独特的呛香辣味（像我这样家里没有抽油烟机的，只能在旁边准备好湿毛巾，不时去捂一下口鼻）。此时可以稍微放一点食用盐，这样辣椒才能入味，看到辣椒表皮起皱了，就可以了，切记不能熟透（熟透严重影响口感，因为最后还要回锅调味）。此时大概七八分熟，俗称断生，盛出放入碗中备用。5、炒制肥肉（这个过程大概 6 分钟），大火将炒锅烧热，放入少量花生油（少量就可以，只是为了防止肥肉下锅时粘锅，不是为了炒菜），下肥肉，先煸炒 1 分钟，待煸炒出少量猪油，火力转中小火，开始炼油。这里耗费时间长一点，大概 5 分钟，肥肉慢慢消失，本来的肥肉块越来越小，越来越干，炼出了很多猪油，很香。 肥肉炼油，很香 6、炒制瘦肉（这个过程大概 1 分钟），等步骤 5 中的肥肉炼成了肉干（这里自己把握，也可以炼油时间短一点，保留多一点肥肉，吃起来更香），火力转大火，将步骤 2 中腌制的瘦肉倒进来翻炒，大概 1 分钟，炒到瘦肉变色，基本熟了。7、调味翻炒，辣椒回锅（这个过程大概 2 分钟，根据放调味料和辣椒的速度而定），等瘦肉基本熟了，放入大蒜片（不是一开始和瘦肉一起放，否则大蒜片都炒烂了），此时按照湖南的做法，还要放点豆豉进来（注意这是几秒内就要完成的动作，如果大蒜片和豆豉没放在边上，记得先关小火，等放完了再开大火）。接着把步骤 4 中的干煸后的辣椒倒进来，先翻炒几下，然后不停地用锅铲戳，戳 1 分钟左右，倒点生抽提鲜，放点食用盐，再翻炒 10 秒，关火。 辣椒炒肉成品，不过辣椒有点炒过头，老抽放的少了 注意事项 1、所谓 二刀肉 ，是指屠户旋掉猪尾巴那圈肉以后，靠近后腿的那块肉，因为它是第二刀，顾名思义，就称为二刀肉。那地方的肉有肥有瘦，肥瘦搭配，一刀肉肥的多，二刀肉是 肥四瘦六 ，比较适合做这道菜，同时也适合做 回锅肉 这道菜。2、腌制瘦肉，煸炒辣椒块，都已经放过少量的盐了，所以最后千万不要又放了很多盐，根据个人口味添加。3、购买辣椒时，一定要选择新鲜的，否则口感不好。4、步骤 4 中的干煸辣椒千万不要把辣椒炒熟了，否则最后的成品辣椒不够脆嫩，而且辣味会全部丢失。5、这道菜看起来注定非常油腻，因为肥肉炼出了大量的猪油，但是吃起来不会腻，而且菜底的这个油非常香，甚至可以用来拌饭吃。但是要注意，如果炼油的时候发现炼出来的油过多，还是要倒出来一些的，否则真的很油腻。6、为了让成品的颜色好看一点，腌制瘦肉时需要老抽，如果在炒制瘦肉的最后发现颜色不够，可以再补加一点老抽。当然，如果有豆瓣酱，基本不用放老抽了。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>辣椒炒肉</tag>
        <tag>川菜</tag>
        <tag>湘菜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[红烧肉做法总结]]></title>
    <url>%2F2018060301.html</url>
    <content type="text"><![CDATA[红烧肉，是一道做法非常简单的传统菜品，而且有多种版本，也有多种口味，同时基于红烧肉再补充其它配菜，又创新出了很多菜品。本文就讲述红烧肉的做法总结，口味偏甜，江南一带的做法。食材准备 以下的食材份量大约 2 人份（实在吃多了也会腻的，要和其它菜配在一起吃）：1、五花肉 500 克；2、冰糖 30 克；3、大葱 3 段、姜片 3 片、香叶 2 片、八角 2 个、桂皮 2 小块、大蒜 2 瓣（可以不用）；4、老抽（糖色不够的时候加老抽补充颜色）、料酒、食用盐（看口味，有时候不需要加盐了）；制作步骤 1、配料准备，装盘备用，实际上只需要少量即可，比我想象中的少很多； 几种大料准备 2、五花肉切块，本来应该切大块的，我的炒锅小，同时五花肉的量也少，不方便做，就切小块了（如果买的五花肉质量不好，肥肉肥油比较多的话，不适合直接下锅，最好焯水一下，把肥油过滤掉一些，如果肉的质量好，直接沥干水分就可以）； 原始的五花肉 3、炒糖色，下五花肉，炒糖色其实就是放少量油，把冰糖融化，然后在高温下冰糖渐渐变色，类似棕色或者深红色，此时放进去的五花肉小块就有颜色了（如果炒糖色成功了，后面就不用放老抽了，但是要注意不能炒太久，否则糖会变苦），此步骤同时也可以把五花肉小块定型，防止煮的过久烂掉； 给五花肉小块上色 4、加水（水量很重要，漫过所有的肉块再多一点，否则最后无法完成收汁操作），放大料，开大火煮开，然后转为小火，煮 50 分钟（中途可以晃一下锅，最好不要打开锅盖，还要注意一下水够不够）； 加水，放大料 煮了 30 分钟 5、50 分钟后，开锅，拣出大料，扔掉，开中火，收汁（一般 10 分钟就够，如果水量少了可能 3 分钟水就干了，看情况根据需要及时放老抽、食用盐等调料），我放的糖不多，于是加了一点食用盐，收汁完成盛出（可以看到我这份颜色还是不够啊），成品看起来虽然有点油腻，但是吃起来绝对肥而不腻，入口即化。 拣出大料 收汁完成，成品 附加一份以前做的另外一份红烧肉，当时忘记炒糖色，只好使用老抽上色，并且加了很多盐，味道也是非常棒（由于变成了咸口味，感觉味道和卤肉差不多）注意事项 1、炒糖色这一步骤是为了替代生抽给红烧肉上色，那种棕色或者深红色是糖遇热产生的颜色，很好看（一开始炒完颜色很好看，然后加水煮的时候可能看不出来了，没关系，等最后收汁的时候颜色会回来的），此外，炒糖色这一步骤也可以不进行，直接在煮的时候放糖（增甜增鲜），然后最后加老抽上色即可；2、大火煮开之后一定要转小火，慢慢煮（小火才能把肉煮的又透又软，达到入口即化的地步），就和普通的煲汤的火力一样，煮够 50 分钟；3、其实只有在五花肉的质量非常好的情况下，才能免除焯水，一般的五花肉肥油太多，不提前处理一下做出来的红烧肉非常油腻。 而且，很多人看到五花肉那么肥，就不想吃，或者觉得不好吃。其实不是的，正是由于肥肉的存在才能煮出那种香味，如果大部分都是瘦肉，也做不出来红烧肉，那种三肥七瘦的五花肉最好。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>红烧肉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[玉米胡萝卜排骨汤做法总结]]></title>
    <url>%2F2018053001.html</url>
    <content type="text"><![CDATA[排骨汤，是一道做法非常简单的汤，需要的只是新鲜的食材与足够的耐心而已。除了排骨，还可以增加玉米、胡萝卜这两种配菜，以增加排骨汤的甘甜与鲜美。本文就讲述玉米胡萝卜排骨汤的做法总结。食材准备 用最少的食材做排骨汤，才能做成真正的排骨汤，不需要各种配料，也不需要各种调味料，最终炖出来的排骨汤才能有排骨的鲜美。以下的食材分量大约 3 人份：排骨 300 克（排骨越好汤越好，我用过各种价格的排骨，12 元 / 斤 - 40 元 / 斤）甜玉米 1 根（稍微大一点嫩一点最好，买玉米的时候可以品尝一粒生的）姜片 2-3 片（不能太多，或者也可以不放）胡萝卜一根 食用盐适量 制作步骤 不计算食材的准备，从开火到关火总耗时预计 70 分钟；1、清洗排骨，稍微用水冲洗一下，然后在冷水中浸泡 10 分钟 - 20 分钟，目的是去除杂质与血水，但是油脂仍然保留，一般购买排骨的时候会让卖菜的帮忙剁好，否则自己用菜刀处理很麻烦；2、清洗玉米，切片，可以切薄一点，大概每片的厚度是 3 粒玉米的距离，玉米很难切，所以菜刀一定要使用锋利点的，否则会损坏玉米粒的，如果购买的玉米很新鲜，玉米棒里面也是很甜的，有助于增加排骨汤的甘甜；3、清洗胡萝卜，去皮，切片，注意最好去皮，否则最终部分胡萝卜皮会混在汤里，影响汤的品质；4、姜片，准备 2-3 片即可；5、汤锅准备好，加水，加姜片，冷水就下排骨，开大火煮，水开后立马转小火，把水表面的浮沫撇去，小火继续煮 30 分钟；6、步骤 5 的 30 分钟后，加玉米片，继续小火煮 20 分钟；7、步骤 6 的 20 分钟后，加胡萝卜片，继续小火煮 10 分钟；8、加盐调味，关火；一锅排骨汤 一碗排骨汤 以前做的另外一锅排骨汤，当时买的排骨 38 元 / 斤 注意事项1、排骨不能洗的太彻底，比如洗排骨的时候用力搓，不仅洗掉了血水，还把油脂洗没了，这样会导致汤里面丧失了香味；2、如果选择了排骨焯水（或者是选择了先在水里煮一下），切记不要焯太久（或者煮太久），10-30 秒即可，目的只是去除血水，否则也会流失油脂；3、为了省事，我一般不会对排骨怎么清洗，稍微用水冲一下，确保没有杂质与血水即可，然后直接煮，但是切记煮开后立即用勺子撇掉浮沫（油脂与碎渣），否则最终的汤会浑浊，而且没有香味；4、玉米一定要选择甜玉米，嫩的最好；5、时间一定要控制好，总计 70 分钟，也要注意火力的控制，除了一开始是大火，后面的 60 分钟全部是小火；6、如果只想喝汤，排骨可以用来做红烧排骨、糖醋排骨、酱排骨等等。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>排骨汤</tag>
        <tag>玉米排骨汤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch 根据查询条件删除数据的 API]]></title>
    <url>%2F2018022401.html</url>
    <content type="text"><![CDATA[在使用 Elasticsearch 的时候，有时候免不了存入了一些脏数据，或者多余的数据，此时如果想把这部分数据删除，第一时间想到的就是删除接口，类似于关系型数据库中的 delete 操作。尽管 删除 这个操作在 IT 的世界里是大忌，甚至 从删库到跑路 这句话早已经成为了段子，但是只要控制好流程，经过多人审核，并做好备份，必要的时候删除这个操作还是要出场的。好，言归正传，本文记录 Elasticsearch 中的删除接口的使用，以及不同版本之间的差异。网络接口 这里的网络接口其实就是指 HTTP 的 RESTful 接口，优点就是接口稳定，各版本不会有差别，兼容多种编程语言的调用，只要能发送 HTTP 请求即可，缺点就是返回的数据结果是原生的 Elasticsearch 数据，需要调用方自己解析处理。好，关于接口的情况不再多做解释，直接进入正题，怎么删除数据。不得不再多解释一点，关于 Elasticsearch 的删除数据接口在不同版本之间有变化。在 1.x 的版本中，可以直接使用 DELETE 请求加上 _query 查询语句来删除数据 在 2.x 的版本中，此功能被从 core 中移除，单独作为插件使用，因为官方认为可能会引发一些错误，如果需要使用，安装插件即可：bin/plugin install delete-by-query，使用方式与上述一致 在 5.x 的版本中，此功能又回归到 core，无需安装插件，但是使用方式变化了，是使用 POST 请求加上 _delete_by_query 查询语句来删除数据 下面直接举例说明：1、在 1.x、2.x 的版本中，发送的是 DELETE 请求，并且使用 _query 关键字。123456DELETE 主机地址 / 删除数据的索引名 / 删除数据的索引 type/_query&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125;2、在 5.x 的版本中，发送的是 POST 请求，并且使用 _delete_by_query 关键字。123456POST 主机地址 / 删除数据的索引名 / 删除数据的索引 type/_delete_by_query&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125;我这里为了代码简洁，查询条件设置的为 match_all，会命中所有数据，因此会删除指定索引 type 下的所有数据。如果只是删除部分数据，只要指定自己的查询条件即可，例如删除用户索引下 uid 为特定值的数据【以 5.2 版本语法演示】。123456789101112POST 主机地址 / 用户的索引名 / 用户的索引 type/_delete_by_query&#123; &quot;query&quot;: &#123; &quot;terms&quot;: &#123; &quot;uid&quot;: [ &quot;uid1&quot;, &quot;uid2&quot;, &quot;uid2&quot; ] &#125; &#125;&#125;返回的数据格式如下，包含删除记录条数、消耗时间等信息：1234567891011121314151617&#123; &quot;took&quot; : 147, &quot;timed_out&quot;: false, &quot;deleted&quot;: 119, &quot;batches&quot;: 1, &quot;version_conflicts&quot;: 0, &quot;noops&quot;: 0, &quot;retries&quot;: &#123; &quot;bulk&quot;: 0, &quot;search&quot;: 0 &#125;, &quot;throttled_millis&quot;: 0, &quot;requests_per_second&quot;: -1.0, &quot;throttled_until_millis&quot;: 0, &quot;total&quot;: 119, &quot;failures&quot; : []&#125;客户端接口 这里的客户端接口就是官方或者社区开发的适配各个编程语言的接口，它和 RESTful 接口相匹配，但是必须写代码实现查询或者取数的操作，例如：Java、Python、Go 等。这种接口的优点是不用担心使用出错【例如在 HTTP 中，参数构造错误导致请求失败】，只要按照接口写代码，请求的构造过程按照官方提供的接口来就行，而且，请求的数据结果是被封装为实体的，不需要额外解析数据结构，直接使用即可。但是，有时候优点也是缺点，由于接口过于死板，导致多版本之间不兼容，例如 1.x 和 2.x 之间的 TransportClient 生成方式就不兼容，导致一份代码只能请求一个版本的 Elasticsearch 集群，此时对于多版本 Elasticsearch 集群之间的请求则无能为力。好，直接进入正题，给出删除数据的示例，直接使用 5.1.1 版本的示例，其它的请参考文末的官方文档【此外还可以根据 client 指定 id 进行单条删除，不属于根据查询条件删除的范畴，不再赘述】，这个操作如果耗时很长，还会有异步的问题，也可以参考官方文档。此处需要特别注意 5.1、5.6 之间的使用方式也略有不同，具体以官方文档为准，我就被坑了，例如 BulkIndexByScrollResponse、BulkByScrollResponse。12345678BulkIndexByScrollResponse response = DeleteByQueryAction.INSTANCE.newRequestBuilder (client) .filter (QueryBuilders.matchQuery (&quot;uid&quot;, &quot;uid1&quot;)) .filter (QueryBuilders.typeQuery (&quot; 需要删除的索引类型 & quot;)) .source (&quot; 需要删除的索引名称 & quot;) .get ();// 返回删除的行数 long deleted = response.getDeleted ();注意如果是 2.x 版本是不支持删除的，但是可以更新，如果更新还需要增加相关依赖：12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.module&lt;/groupId&gt; &lt;artifactId&gt;reindex&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt;&lt;/dependency&gt;上面的删除代码示例中用到了 client 变量，其实就是 TransportClient 的实例，下面再给出不同版本 Elasticsearch 的 TransportClient 初始化方式【需要注意，访问不同版本的 Elasticsearch 集群，需要依赖不同版本的 org.elasticsearch 官方包，而且特别要注意 5.x 版本的还需要额外的 transport 依赖包】。1、1.7.5 版本，需要集群名字、主机名、tcp 端口等信息。12345678910111213141516171819Settings settings = ImmutableSettings.settingsBuilder () .put (&quot;cluster.name&quot;, &quot;your_cluster_name&quot;) .put (&quot;client.transport.ping_timeout&quot;, &quot;60s&quot;) .put (&quot;client.transport.sniff&quot;, true) .put (&quot;discovery.zen.fd.ping_retries&quot;, 5) .build ();String [] hostArr = new String []&#123;&quot;hostname1:port&quot;, &quot;hostname2:port&quot;, &quot;hostname3:port&quot;&#125;;TransportAddress [] transportAddresses = new InetSocketTransportAddress [hostArr.length];TransportClient client = new TransportClient (settings);for (int i = 0; i &lt; hostArr.length; i++) &#123; String [] parts = hostArr [i].split (&quot;:&quot;); try &#123; InetAddress inetAddress = InetAddress.getByName (parts [0]); transportAddresses [i] = new InetSocketTransportAddress (inetAddress, Integer.parseint (parts [1])); &#125; catch (UnknownHostException e) &#123; &#125;&#125;client = client.addTransportAddresses (transportAddresses);2、2.3.2 版本，需要集群名字、主机名、tcp 端口等信息，生成方式与 1.7.5 版本的略有不同。1234567891011121314151617181920Settings settings = Settings.settingsBuilder () .put (&quot;cluster.name&quot;, &quot;your_cluster_name&quot;) .put (&quot;client.transport.ping_timeout&quot;, &quot;60s&quot;) .put (&quot;client.transport.sniff&quot;, true) .build ();String [] hostArr = new String []&#123;&quot;hostname1:port&quot;, &quot;hostname2:port&quot;, &quot;hostname3:port&quot;&#125;;TransportAddress [] transportAddresses = new InetSocketTransportAddress [hostArr.length];for (int i = 0; i &lt; hostArr.length; i++) &#123; String [] parts = hostArr [i].split (&quot;:&quot;); try &#123; InetAddress inetAddress = InetAddress.getByName (parts [0]); transportAddresses [i] = new InetSocketTransportAddress (inetAddress, Integer.parseint (parts [1])); &#125; catch (UnknownHostException e) &#123; &#125;&#125;TransportClient client = TransportClient.builder () .settings (settings) .build () .addTransportAddresses (transportAddresses);3、5.1.1 版本，需要集群名字、主机名、tcp 端口等信息，生成方式与 2.3.2 版本的略有不同，特别要注意 5.1.1 版本的还需要额外的 transport 依赖包，否则找不到 PreBuiltTransportClient 类。额外的依赖包信息：12345678910&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;5.1.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;5.1.1&lt;/version&gt;&lt;/dependency&gt;代码信息：123456789101112131415161718Settings settings = Settings.builder () .put (&quot;cluster.name&quot;, &quot;your_cluster_name&quot;) .put (&quot;client.transport.ping_timeout&quot;, &quot;60s&quot;) .put (&quot;client.transport.sniff&quot;, true) .build ();String [] hostArr = new String []&#123;&quot;hostname1:port&quot;, &quot;hostname2:port&quot;, &quot;hostname3:port&quot;&#125;;TransportAddress [] transportAddresses = new InetSocketTransportAddress [hostArr.length];for (int i = 0; i &lt; hostArr.length; i++) &#123; String [] parts = hostArr [i].split (&quot;:&quot;); try &#123; InetAddress inetAddress = InetAddress.getByName (parts [0]); transportAddresses [i] = new InetSocketTransportAddress (inetAddress, Integer.parseint (parts [1])); &#125; catch (UnknownHostException e) &#123; &#125;&#125;TransportClient client = new PreBuiltTransportClient (settings) .addTransportAddresses (transportAddresses);参考 参考内容来自于官方文档，注意不同版本有不同的文档，某些内容稍有不同：5.2 版本的 Java 接口5.2 版本的 HTTP 接口]]></content>
      <categories>
        <category>大数据技术知识</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
        <tag>API</tag>
        <tag>delete</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[注册 Facebook Twitter Tumblr 遇到的问题]]></title>
    <url>%2F2018020101.html</url>
    <content type="text"><![CDATA[在这个地球上，有很多好用的工具或者网站被封了，在中国大陆地区无法访问，如果需要使用它们，首要的问题就是翻墙。然而，事情没有那么简单，有时候可以翻出去了，但是在使用的过程中还会遇到奇怪的问题，每一步都不好走。本文记录注册使用 Facebook、Twitter、Tumblr 等社交账号时可能遇到的问题、解决的办法，给自己留一个备份，同时也能给大家带去一些方便。提前说明，需要翻墙的读者，可以使用浏览器插件或者一些免费的工具，但是大多数都不够稳定，如果想要稳定的方式，最好还是自己搭建梯子。我这里有一个硬核教程【需要一点点技术，或者找一个懂技术的人，10 分钟可以搞定】，自己搭建 shadowsocks，手把手教学，图文并茂，顺便把客户端的使用方式也记录下来，参考我的另外一篇博文：使用 Vultr 搭建 Shadowsocks（VPS 搭建 SS） 。当然，这种方式肯定不是免费的，租用云服务器每月需要一定的钱，但是也不多，几美元足够，如果几个人合伙使用的话，平摊费用算下来也不多。Facebook待整理。Twitter以下内容中涉及到的操作环境、截图示例都是基于 Web 版的 Twitter，没有使用手机客户端。注册 注册 Twitter 帐号，首先需要一个邮箱帐号，或者手机号，进入注册首页，进行信息填写 注册页 ，填写完成后，接下来也就是常规流程，发送短信验证码、语音验证码、邮箱激活链接等，基本没什么问题。 绑定手机号问题 由于我选择的是 Google 邮箱注册，注册完成之后可以正常登录，但是进入不了主页面，就被绑定手机号页面拦截了， 一直提示需要添加一个手机号 ，要不然就停留在当前页面，什么也做不了，除非退出。那这个就是坑人了，登录之后卡在那里，什么都看不了，只能退出，那我还注册帐号干什么。接着我就按照提示，绑定手机号吧，但是，诡异的问题再次出现，我使用自己的手机号进行绑定时，提示错误：由于技术问题，无法完成当前的请求，请重试【Due to a technical issue, we couldn&#39;t complete this request. Please try again.】。我是不相信 Twitter 能有什么技术问题，我只能怀疑是中国的手机号无法进行绑定，当然只是猜测。于是上网搜索了一下资料，发现果然是这个原因，铺天盖地的结果展现在搜索引擎的搜索列表中，很多人都遇到了这个问题。再看看，一些过来人都建议一开始直接使用手机号注册，不要使用邮箱注册，就不会有这个问题了。唉，一开始不知道，接下来没有办法了，只能尝试寻找可行的办法，毕竟邮箱已经注册过了，不想浪费。绑定手机号解决方案尝试 官方说法是当前帐号疑似是机器人【不是一个真实的人类】，所以被冻结了，必须添加一个可用的手机号，用来接收验证码，才能证明当前帐号是人为注册的，才能进行接下来的操作。那这样看起来很好办，想办法找一个可以使用的手机号接收验证码，或者能不能通过和客服沟通人工解封呢？利用浏览器调试工具 利用 Chrome 浏览器的开发者工具更改下拉列表的值，把日本的编号 81 改为 86，应用在页面上，从而伪造手机号码的所属国家编码，看看能不能收到验证码。 实际操作发现不行，所以大家不要使用这种方式，没有用，请使用第二种邮件申诉的方式，亲测可以使用，并且已经帮助好几个人成功注册激活 。没有用的原因在于 Twitter 验证的时候还是会重新刷新下拉列表，把国家编码更新。具体操作为：在 Chrome 浏览器的对应页面，按下键盘的 F12 按键，就可以打开调试工具【或者点击鼠标的右键，选择检查】，在 Elements 选项中可以看到源代码，更改表单里面的下拉列表的值，即可，需要一点点技术知识。邮件申诉 首先声明， 推荐大家使用这种方式来解封、激活帐号 ，我自己使用有效，而且已经帮助过好几个人解决了问题，基本在 24 小时内可以解决问题【节假日不知道客服上不上班】。去帮助中心，找客服，发送申诉邮件，内容解释说明你是一个真实的人，现在注册帐号被冻结了，请求解封。Twitter 帮助中心网址：Twitter 帮助中心 ，在帮助中心选择 Contact us，进一步选择 View all support topics 。 进入选择页面后，进一步选择 Suspended or locked account，表示对冻结或者锁定的帐号进行申诉处理。最终进入的页面就是这样的：申诉信息填写 ，可以开始填写申诉信息了。 这里面最主要的内容就是问题描述，请描述清楚你的问题，另外设备的选择按照自己的实际情况填写，全名和手机号也按照实际情况填写。此外，注意填写信息前需要登录帐号【虽然卡在验证页面，也要保持登录状态】，否则页面是锁定状态，无法填写任何信息。而且，登录后，大部分信息都是自动填充完成的，无需手动一个一个填写，只需要填写重要的几项内容即可。例如我填写的问题描述，仅供参考：1Account suspended.Could not unsuspend it through phone number.Pls help to unsuspend the account.Thanks.提交后会收到一封由 Twitter 官方技术支持【Twitter Support &#115;&#117;&#112;&#x70;&#x6f;&#114;&#x74;&#x40;&#x74;&#119;&#105;&#116;&#x74;&#101;&#x72;&#x2e;&#99;&#111;&#109;】发送的邮件【邮件基本是秒回，肯定是系统自动回复】，告诉你应该怎么做，邮件内容如下截图。但是看内容也看不出来什么，只是表明说你的帐号疑似是机器人帐号，需要绑定手机号码，后面列出来一系列的步骤。其实我也是想做这一步的，但是奈何中国的手机号码不支持，仔细看最后一句话： 如果还有问题，可以直接回复此邮件并说明问题详细 ，好，机会来了。接下来我又回复了一封邮件，说明自己遇到的问题，内容大概如下，解释说明自己是一个真实的人，但是由于手机号码是中国的，无法接收到验证码，请求人工解决：1234Hello, I try in this way,But i am in China,i can not receive messages. I am a human indeed,and my phone number is +86 1********06. best wishes.接下来就是等待官方的回复了，由于是客服人工操作的，或者系统审核，速度比较慢，需要等好几个小时，希望晚上睡一觉后明天会有好消息。在等待了一夜后，又过了半天时间【总共大概 17 个小时】，收到了 Twitter 官方的回复，说我的帐号已经解冻，并解释了原因。这次回复等待了这么长时间，不像上次申诉回复那么快，说明很大可能是人工审核的，然后解冻了你的帐号，再回复这封通知邮件给我。不管怎样，帐号可以使用了。接下来为了保证不被封号，最好重新设置一下昵称，并且填写一些必要的信息： 用户名【id】、头像、生日、国家、描述 等，也可以关注一些其他推主。更改用户名在 Settings and Privacy 里面，由于用户名是唯一的【和 GitHub 的策略一样】，所以常用的都被别人注册过了，自己要注意寻找，如果遇到更改失败的情况，会显示用户名被占用，再换一个试试。更改 昵称、头像、背景墙、描述 等，在 Profile 里面。流程总结：1、此篇博文只针对使用 Google 邮箱注册的情况，注册后帐号被冻结，什么也做不了，绑定手机号又说不支持，只能通过申诉来解决【如果一开始注册时使用的就是手机号，应该没有问题】；2、申诉的目的是为了解冻帐号，但是官方是自动回复，让绑定手机号，此时又回到了原地；3、在步骤 2 的基础上可以直接回复邮件【邮件中有提示】，说明遇到的问题，等待将近一天就行了；【如果没有步骤 2，直接给官方技术支持发邮件，应该是不行的】4、步骤 3 官方回复的邮件中，问题已经解决，并提示说不要回复此邮件；【回复了应该也没人理】Tumblr待整理。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>Facebook</tag>
        <tag>Twitter</tag>
        <tag>Tumblr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown 语法手册]]></title>
    <url>%2F2017123101.html</url>
    <content type="text"><![CDATA[首先声明，这份文档可能不完整，因为它是基于我日常的使用来整理的，如果有一些语法我还没有使用到，就不会记录在这份文档上面。但是，我会不定期更新这份文档，把自己使用到的越来越多的语法整理进来，这样就可以不断完善这份文档，从而越来越接近标题中的 语法手册 。最近更新于：2019-06-07 。语法手册 标题 代码块 加粗 斜体 链接 列表 无序列表 有序列表]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
        <tag>handbook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017102901.html</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.Quick StartCreate a new post1$ hexo new "My New Post"More info: WritingRun server1$ hexo serverMore info: ServerGenerate static files1$ hexo generateMore info: GeneratingDeploy to remote sites1$ hexo deployMore info: Deployment]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>建站</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Hexo、GitHub 搭建个人博客教程]]></title>
    <url>%2F2017093001.html</url>
    <content type="text"><![CDATA[对于喜欢写作的技术人员来说，可以使用 CSDN、 简书 、WordPress、 博客园 等产品，不仅可以记录自己在日常工作中遇到的难点、解决的 bug 等，还可以分享经验，让别人也学习进步。但是，有的人可能会觉得这些产品不好用，或者功能的扩展太掣肘，或者觉得这种方式不够酷，他们想追求更加自由的方式来写博客。这时候，我觉得 Hexo 就可以出场了，它其实只是一个博客框架，如果你是新手，只需要几个命令十几分钟，就可以搭建自己的博客，而如果你技术高超并且愿意花时间的话，可以折腾出很多花样，博客从里到外你都可以自定义实现。本文记录我的博客搭建过程以及优化过程。在此先声明，这篇博客内容是从 2017-09-30 开始写的，为了与时俱进，会保持不断更新，可能会删掉无用的旧内容并添加新内容，目前最新修改时间为 2019-06-09，所以请读者以最新的内容为准。历史的记忆 我从 2015 年就开始整理包含各种知识点的笔记，大部分都是和技术有关的，其它的都是一些生活感悟、阅读笔记、游记等等。当然，2015 年我还没有毕业，只是在外面的公司实习，由于当时每天都会接触到很多重要的技术知识，在询问同事、查阅资料的过程中又会延伸涉及到更多的知识点，我迫切需要记笔记，方便我快速查阅学习。一开始我本想听从身边同事的建议，使用 CSDN 写博客，但是我觉得 CSDN 使用起来不方便，对于字数多一点的笔记，排版、添加图片都很麻烦，因此没有使用。后来自行查阅资料，有人建议使用 简书 、WordPress、 有道云笔记 等产品，我觉得好像都不错，并使用了一段时间的有道云笔记。但是突然有一天我发现了 Hexo 这个博客框架产品以及 Markdown 这种语法，我被深深吸引了，当即决定改用它。当然，这时候不得不提一下 Jekyll 这个博客框架，它其实是一个静态文件生成器，和 Hexo 类似，用户可以基于 Markdown 文件写博客，然后 Jekyll 帮助用户生成静态文件。更重要的是，GitHub 默认是支持 Jekyll 的，用户可以直接管理 Markdown 文件，其它的都交给 GitHub、Jekyll 了，也就是说用户可以直接在线编辑 Markdown 文件，生成过程、发布过程都无需关心。那为什么我没有选择 Jekyll 呢？上面的介绍看起来很美好是不是，但是不能忽略了它的缺点：一是 Liquid 语法不友好，需要学习成本；二是如果在本地搭建 Jekyll 环境的话，步骤比较复杂【基于 Ruby，听说有 Docker 环境可以快速搭建 Jekyll 环境】，出了问题很是折腾人；三是博客内容过多的时候【几百篇】，生成的速度会极慢，也就是性能低下；四是 GitHub 对 Jekyll 的插件有很多限制，导致很多插件不能使用，这就违背了自定义的概念，不够灵活。因此，此时此刻我暂且按下 Jekyll 不表，需要了解的可以参考 GitHub Pages 的帮助文档：help.github.com ，让继续我回到 Hexo 这个话题上面来。回来继续说 Hexo，这里面还有一段趣事，不久之前，我经常浏览 郭俊的博客 ，里面的大数据技术知识整理归纳得很好，我读了几天之后，突然有一天我的注意力竟然被博客的框架吸引住了：界面简洁、交互优雅、配色简单，我就在那里点来点去，越来越喜欢。然后我决定我也要使用这个框架，但是我对这个框架一无所知，而且郭俊的博客底部也没有留下类似 Powered by xx 的标识，一开始一筹莫展。接着我就想到去郭俊的微博、微信公众号里面留言，说我很喜欢这个框架，能不能透露一下关于这个框架的简介。过了几天他回了一个单词：Hexo，然后我就疯狂地搜索关于这个框架的信息，并着手开始使用它。 一开始我并没有考虑那么多，例如博客主题的各种优化配置、远程仓库协作、搭建自己的云主机、绑定域名、SEO 优化等等一系列的升级改造，我只是想使用这个框架，然后搭配 Markdown 语法进行写作。我先在本地创建了一个博客站点，然后逐步把有道云笔记里面的内容迁移过来，现在我发现也积累了不少内容，才决定投入 GitHub 与 GitHub Pages 的怀抱，想把个人博客慢慢做起来。我知道这将是一个漫长而消耗精力的过程，希望我能坚持住，这就快到国庆节了，我会抽出 2-3 天时间好好整理一下博客站点的优化清单，并在以后的日子里逐步实现。初次搭建 环境初始化 参考 Hexo 的安装文档进行环境初始化，新建一个站点并在本地启动，用浏览器打开查看默认的站点。新建站点 详细过程待整理。优化教程 优化教程主要整理针对博客站点做的优化点，例如 UI 优化、SEO 优化、部署管理优化等等，这一部分内容会非常多，并且会在以后不断更新，至于有没有尽头我也不好说。添加版权声明 版权声明，一般就是指在博客的末尾添加类似 本博客内容除特别声明外，均采用 xx 许可协议，转载请注明出处！ 这样的声明，一是用来提醒读者能意识到博客站点的版权，二是警告读者不要随意抄袭搬运博客内容。对于 Hexo 来说，由于默认已经集成了这个功能，用户开启版权声明就比较简单了，直接在主题配置文件中【例如我使用的主题为 NexT，则在主目录中找到 themes/next 子目录，再找到 _config.yml 配置文件】，找到 post_copyright 配置项，设置为开启。1234post_copyright: enable: true license: CC BY-NC-SA 3.0 license_url: https://creativecommons.org/licenses/by-nc-sa/3.0/其中，enable: true 表示开启版权声明，license 表示版权协议的名称，license_url 表示版权协议的官方网站。至于使用哪一种版权协议，大家根据实际情况选择，如果需要了解更多的版权协议知识，请自行查询。开启版权声明后，在每一篇博客的最后都会看到如下图所示的版权声明信息。此外，为了在站点概览中增加版权协议的图标和链接，继续在上述的配置文件中找到 creative_commons 配置项，根据上述的版权协议情况填写。1creative_commons: by-nc-sa配置完成后，在侧边栏的站点概览中，可以看到版权协议的图标，如果点击图标会自动跳转到版权协议的官方网站。搜索引擎优化 关于在百度、Google 的站长工具中管理站点的方法，参考我的博客内容：博客待整理 。 使用远程仓库 我使用 GitHub 作为远程仓库，方便多环境写博客，并使用 GitHub Pages 作为免费云主机，参考我的博客内容：博客待整理 。 绑定独立的域名 由于我使用了 GitHub Pages，所以域名的绑定、SSL 证书的申请都比较简单，参考我的博客内容：GitHub 个人站点绑定独立的域名 。 解决百度蜘蛛爬虫的问题 由于众所周知的原因，GitHub Pages 把百度蜘蛛爬虫的请求全部拦截，这就导致百度无法爬取博客的内容【当然主动提交的还是可以的】，我使用多主机、百度请求跳转的方式解决这个问题，参考我的博客内容：GitHub Pages 禁止百度蜘蛛爬取的问题 。 添加宠物挂件 在博客站点的左下角添加宠物挂件，我选择了一只猫，并且它会根据光标的位置来回看，希望能给看我博客的读者带去一点好心情。参考我的博客内容：博客待整理 。 图床选择 一开始我经过多天的调研，最终选择的是微博图床，理由有：免费、自动压缩、SSL 协议、CDN 加速，使用了很长时间，效果很好。但是后来发生了黑产攻击微博图床的事件【时间点大概在 2019-04-24】，给微博图床带去了安全风险，于是微博图床开启了防盗链，所有正常的访问均被拒绝，进而导致博客文章里面的图片全部打不开。后来经过反复思考，又在网上看到很多别人的经验，最终我决定直接使用 GitHub 作为图床工具，安全可靠，参考我的博客内容：解决微博图床防盗链的问题 、 使用 Java 代码迁移微博图床到 GitHub 图床 。 站内搜索 我使用的站内搜索是 Hexo 提供的解决方案，只要在配置文件 _config.yml 中配置是否开启本地搜索功能即可，很简洁，不必关心其它，如图：它的背后其实使用了一个叫 hexo-generator-search 的插件，详情见：https://github.com/wzpan/hexo-generator-search ，原理也很容易理解，就是把网站的关键词全部收集起来，存在一个文件中：search.xml，搜索时直接根据关键词从文件中查找。Hexo 内置了这个插件，所以我只需要在配置文件中配置几行信息，就可以开启站内搜索功能，如果是比较老旧的 Hexo 的版本，可能还没有内置这个站内搜索插件，可以升级 Hexo 或者自行安装这个插件：npm install --save hexo-generator-search。当然，如果内容过多，查找速度会变慢，例如我的博客有十几万字，我已经觉得很慢了。另外，在 Hexo 的插件仓库也可以找到这个插件的介绍，Hexo 插件仓库地址：https://hexo.io/plugins 。添加开源协议 我使用 MIT 开源协议，增加 LICENSE 文件，使用模板即可，必须放在 master 分支才会生效。在项目中选择 Create new file，创建一个新文件，命名为：LICENSE，然后在右侧可以看到会有开源协议列表给你选择，选择需要的开源协议，其它内容会自动生成。新建文件时选择开源协议模板 根据模板生成开源协议内容 创建成功后，就可以在项目的属性中看到开源协议信息，例如：MIT。开源协议生效后在项目中的效果如下图。添加徽章 除了构建结果徽章使用的是 travis，其它徽章使用的是 GitHub 的官方徽章库：shields.io，增加了 Issue 的打开关闭数量、使用的语言【自定义的徽章】、开源协议。README 文件中的文本内容如下：12345[![Build Status](https://travis-ci.org/iplaypi/iplaypi.github.io.svg?branch=source)](https://travis-ci.org/iplaypi/iplaypi.github.io)![GitHub issues](https://img.shields.io/github/issues/iplaypi/iplaypi.github.io?color=blue&amp;style=flat)![GitHub closed issues](https://img.shields.io/github/issues-closed/iplaypi/iplaypi.github.io?color=red&amp;style=flat)![](https://img.shields.io/badge/language-markdown-orange.svg)![GitHub](https://img.shields.io/github/license/iplaypi/iplaypi.github.io?color=green)徽标可视化效果展示如下图。参考资料1、Hexo 的官方网站：https://hexo.io 。2、Hexo 的安装文档中文版：https://hexo.io/zh-cn/docs 。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>GitHub</tag>
        <tag>NexT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录一个 Kafka 错误：OffsetOutOfRangeException]]></title>
    <url>%2F2017060101.html</url>
    <content type="text"><![CDATA[在使用 Kafka 的过程中，某一天项目中莫名其妙出现了一个异常信息：kafka.common.OffsetOutOfRangeException项目的业务场景是使用 SparkStreaming 消费 Kafka 数据，进一步进行 ETL 处理 ，没有复杂的逻辑。平时一切正常运行，某一天我想在测试环境测试一下更新的逻辑代码，就出现了这个问题，导致整个进程任务失败。本文记录分析问题、解决问题的过程，运行环境基于 Kafka v0.8.2.1，Spark v1.6.2、spark-streaming v2.10，其它版本的内容会与这个版本存在部分不一致的地方，我会特殊说明。问题出现 某一天我修改了项目的代码，在本地连接测试环境，开始测试，出现以下异常信息：123456789101112131415161718192021222324Caused by: kafka.common.OffsetOutOfRangeException at sun.reflect.NativeConstructorAccessorImpl.newInstance0 (Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance (NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance (DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance (Constructor.java:423) at java.lang.Class.newInstance (Class.java:442) at kafka.common.ErrorMapping$.exceptionFor (ErrorMapping.scala:86) at org.apache.spark.streaming.kafka.KafkaRDD$KafkaRDDIterator.handleFetchErr (KafkaRDD.scala:184) at org.apache.spark.streaming.kafka.KafkaRDD$KafkaRDDIterator.fetchBatch (KafkaRDD.scala:193) at org.apache.spark.streaming.kafka.KafkaRDD$KafkaRDDIterator.getNext (KafkaRDD.scala:208) at org.apache.spark.util.NextIterator.hasNext (NextIterator.scala:73) at scala.collection.convert.Wrappers$IteratorWrapper.hasNext (Wrappers.scala:29) at com.datastory.banyan.v3.consumer.BaseRhinoDirectConsumerV3$1.call (BaseRhinoDirectConsumerV3.java:81) at com.datastory.banyan.v3.consumer.BaseRhinoDirectConsumerV3$1.call (BaseRhinoDirectConsumerV3.java:72) at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply (JavaRDDLike.scala:225) at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply (JavaRDDLike.scala:225) at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply (RDD.scala:920) at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply (RDD.scala:920) at org.apache.spark.SparkContext$$anonfun$runJob$5.apply (SparkContext.scala:1858) at org.apache.spark.SparkContext$$anonfun$runJob$5.apply (SparkContext.scala:1858) at org.apache.spark.scheduler.ResultTask.runTask (ResultTask.scala:66) at org.apache.spark.scheduler.Task.run (Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run (Executor.scala:227) ... 3 more项目中的异常信息本来有很多行，但是关键的就是这部分内容，关键异常信息截图如下。重点就看 Caused by: kafka.common.OffsetOutOfRangeException 这一句即可，可以明显看出问题所在： 下标越界 ，下一步开始分析问题、解决问题。问题分析解决 分析 原因既然是 下标越界 ，就要先搞清楚 Kafka 在什么场景下会出现这个异常。通过查看源代码得知，这是 Kafka topic 的 offset 下标越界异常，对应我这个场景，就是 Spark 任务在消费 Kafka topic 的数据时，指定的下标不在 有效数据 范围之内。有点看不明白？的确，此处有必要插入一些基本知识点。 消费者 客户端在消费处理 Kafka topic 的数据时，会有一个 偏移量 【取值是数字】记录已经消费数据的位置，也可以说是下标，称之为 offset，并同步更新到 Zookeeper 中。[^1] 如果 消费者 客户端在消费中途出问题而停止，等下一次消费时会从上一次中断的 偏移量 位置开始继续消费数据，[^2] 这样就可以避免重复消费数据，节约资源。以上注解 1、注解 2：[^1]: 注意，Kafka v0.9.0.x 以及之后的版本不再是这个策略，不再使用 Zookeeper 存储，改成存储到 kafka 的 broker 节点上面，更方便管理。[^2]: 注意，这里的消费策略是通过参数 auto.offset.reset 设置的，从上一次中断的位置继续消费数据只是消费策略选择之一，取值 smallest。另外，Kafka v0.7.x 以及之前的版本这个参数曾经的名称为：autooffset.reset。这个参数的取值在 v.0.9.0.x 以及之后的版本也更名为：earliest、latest、none。接着回到正题，如果发生 下标越界 现象，说明 Zookeeper 中保存 消费者 的 offset 的值小于 topic 中存在的最早的 message 的 offset 值，即 zookeeper_offset &lt; 最早_offset。这就导致 消费者 程序运行时需要消费的数据在 Kafka topic 中并不存在，进而引发异常的发生。表面上是因为消费的 Zookeeper 缓存信息不正确，实际上是因为 Kafka 的数据过期被清除了，下面我将使用 Kafka 自带的命令来一一验证。验证 先查看 消费者 的消费进度信息，指定 Zookeeper 主机、消费 group 组名、topic 名称，使用命令：bin/kafka-consumer-offset-checker.sh --zookeeper zkhost:2181 --group consumer_group_name --topic topic_name。查看消费进度，可以看到 offset 是 0，表示从头开始消费，logSize 是 10044740，表示 Kafka topic 已经生产了这么多数据。从上图可以看出 topic 的 消费者 只有 1 个消费组分区，为了保险起见，再验证一下这个 topic 的分区数是怎样的，使用命令：bin/kafka-topics.sh --describe --zookeeper zkhost:2181 --topic topic_name。查看分区数，可以看到只有 1 个分区，编号为 0 。综上截图，可以得出总结， 消费者 程序是从下标 0 开始消费数据，也就是从头开始，而 topic 的数据已经生产了 1000 多万。那么，读者就会怀疑，这种情况下怎么可能会 下标越界 呢，0 就是开始的位置，还怎么越界。除非 topic 当前的数据量为 0，而不是 1000 多万。我思考了一下，上述的结论是基于 Zookeeper 的缓存信息得到的，如果 Kafka topic 里面真的有数据存在，的确不可能下标越界。但是，此处还会有另外一种情况，如果 Kafka 里面的数据已经过期了【Kafka 有相关的参数可以设置过期策略】，那就会找不到数据，则报错 下标越界 。再结合我的业务场景，由于我的 消费者 程序给消费组重新定义了名字【使用 group.id 参数】，所以会从头消费【offset 为 0】，但是测试环境的 Kafka topic 里面的数据极有可能是很久之前的，从创建 topic 开始到现在累积了 1000 多万数据，大量数据由于过期策略已经被清除了，现在肯定找不到。接下来去查看 Kafka broker 服务的相关配置：过期策略、数据存储位置，在 kafka broker 的安装目录查看 conf/server.properties 配置文件，验证我的猜测。首先查看名称为 log.retention.* 的相关参数，看看设置的值是什么：可以看到，相关参数设置的值是 log.retention.hours=48，也就是说数据的有效期是 48 小时，过期会自动清理，而 log.retention.bytes=-1 表示不限制数据空间大小，即不会因为数据占用空间太大而删除。那么，Kafka topic 里面的数据是不是真的不在了呢，让我一探究竟，继续从配置文件中查看数据存储目录，参数名称为：log.dirs 。可以看到，数据与索引存储在 /kafka-logs 目录，进入目录，找到指定的 topic、partition 对应的目录，我这里是 /kafka-logs/topic_name-partition_number。可以从上图看到目录里面有 2 个文件，分别是数据文件：00000000000010044740.log、索引文件：00000000000010044740.index。通过查看文件空间大小，发现数据文件的大小的是 0B，什么意思呢，表示没有数据，看来数据已经全部过期而且都被清理掉了。至此，验证了我全部的分析猜测，下面可以简单复现一下这个异常现象。重现异常 1、修改配置文件，把过期时间设置为 3 分钟：log.retention.minutes=3，然后重启 Kafka 服务。2、使用 Kafka producer 生产一批数据，100 条，并等待 3 分钟，数据由于过期会被清理。3、启动 SparkStreaming 消费程序处理数据，出现异常。4、使用 Kafka 命令查看 消费者 消费进度信息，offset 是 0，logSize 是 100 。5、去 Zookeeper 里面查看 zk_offset 的值，是 0 。复现异常，现象完全一致，至此问题原因找到。总结一下：我这里的 Kafka topic 已经生产了 1000 多万的数据，但是旧数据由于过期被清理，而且全部被清理掉了。然而 Zookeeper 中的 Kafka topic 信息仍旧保留， 消费者 程序从头消费的时候，实际上已经获取不到 Kafka topic 的真实数据，所以一定会有异常。解决 那怎么办呢，如果 Kafka topic 继续生产数据，我的 消费者 程序怎么才能消费到新数据呢？其实还是有办法的，最简单的就是不要使用新的消费组名【group.id 参数指定】，如果能继续使用以前的消费组名，并且以前已经把数据消费处理完了，那么它的 offset 也就是最大的值。此时如果继续消费数据，是从最大的 偏移量 位置开始消费的，即只会消费最新生产的数据，不会有 下标越界 的异常出现。但是，如果非要使用新的消费组名称，并且也想从最新生产的数据开始消费【从头再重复消费 1000 多万数据太浪费资源】，有没有办法呢。当然也有，可以手动在 Zookeeper 查询一下消费者的 偏移量 ，主要查看当前消费组对某个 Kafka topic 的消费 偏移量 ，然后根据实际情况重置即可。先登录 Zookeeper 服务，在指定目录查看消费者的 偏移量 ，需要指定消费组名称、topic 名称，使用命令：get /consumers/consumer_group_name/offsets/topic_name/0 。可以看到当前取值是 0，接着重置消费者的 偏移量 ，使用命令：set /consumers/consumer_group_name/offsets/topic_name/0 10044740 。我把它重置为最大值，接下来再测试消费程序就会从最新生产的数据开始消费。好了，接下来就成功运行了。总结备注 不同版本之间的参数差异 本文是基于低版本的 Kafka 进行分析问题的：v0.8.2.1，关于里面的参数信息可以参考官网：Kafka-v0.8.2-configuration 。其中，auto.offset.reset 这个参数【v0.7.x 之前参数名称为 autooffset.reset】的解释说明如图。原文如下：What to do when there is no initial offset in ZooKeeper or if an offset is out of range:smallest : automatically reset the offset to the smallest offsetlargest : automatically reset the offset to the largest offsetanything else: throw exception to the consumer参数含义的总结归纳：smallest：当各分区下有已提交的 offset 时，从提交的 offset 开始消费；无提交的 offset 时，从头开始消费 largest：当各分区下有已提交的 offset 时，从提交的 offset 开始消费；无提交的 offset 时，从该分区下新产生的数据开始消费anything else：topic 各分区都存在已提交的 offset 时，从 offset 后开始消费；只要有一个分区不存在已提交的 offset，则抛出异常信息 关于 v0.7.x 版本的参数信息参考官网：Kafka-v0.7.x-configuration 。其中，autooffset.reset 这个参数名称和以后的都不一样，解释说明如图。原文如下：smallest: automatically reset the offset to the smallest offset available on the broker.largest : automatically reset the offset to the largest offset available on the broker.anything else: throw an exception to the consumer.至于高版本的配置信息，也可以参考官网：Kafka-v0.9.0.x-configuration 。其中，auto.offset.reset 这个参数的解释说明如图，自从 v0.9.0.x 版本之后，它的取值已经变化。原文如下：What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted):earliest: automatically reset the offset to the earliest offset.latest: automatically reset the offset to the latest offset.none: throw exception to the consumer if no previous offset is found for the consumer’s group.anything else: throw exception to the consumer.消费者信息存储位置 消费者 信息存储位置的问题，新版本【v0.9.x 以及之后】不存储在 Zookeeper 了，转而存到 Kafka 的 broker 节点。如果有 消费者 启动，那么这个 消费者 的组名和它要消费的那个 topic 的 offset 信息就会被记录在 broker 节点上。关于偏移量的另一个常见异常 关于偏移量 offset 的问题，还有一个常见异常：numRecords must not be negative，它主要是由删除 Kafka topic 后又新建同名的 topic 引起的。根本原因在于删除 topic 后没有把 Zookeeper 中的 消费者 的信息也一同删除，导致遗留的 消费者 的信息在新建同名后 topic 被作为当前 topic 的 消费者 的信息，如果此时启动一个消费程序，在计算 numRecords 的时候会出现负数的情况【0 减去 old_offset】，接着就会抛出这个异常。]]></content>
      <categories>
        <category>大数据技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Zookeeper</tag>
        <tag>Kafka</tag>
        <tag>OffsetOutOfRangeException</tag>
        <tag>SparkStreaming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VLOOKUP 函数跨工作表跨文件使用方式]]></title>
    <url>%2F2017051401.html</url>
    <content type="text"><![CDATA[今天在处理 Excel 文件的时候，需要使用 VLOOKUP 函数，感觉很方便。内心有一种掌握了一个小技巧就可以节省很多时间的骄傲感，同时，除了入门级别的使用，还进一步发现了可以跨工作表、跨文件使用这个函数，顿时觉得更加方便了。我觉得这个函数在日常工作中应该很常用，而且很好用，所以本文就记录这个函数的使用方式，以及简单介绍 Excel 中的函数概念。基础概念介绍 什么是函数 在 Excel 中，函数就是官方定义的一种通用公式，例如求和、求平均数、计数、查找，可以帮助用户方便快捷地处理数据。掌握了这些函数，用起来 Excel 才算是合格水平，有时候遇到难以处理的数据，可能使用几个公式就解决了。Excel 的基础功能就是存数据，然后基于这些数据再做进一步处理，此时为了方便高效，就产生了很多函数。以下内容中提及的 函数 或者 公式 都是同一个概念。函数的优点 各个行业的人使用 Excel 肯定有自己的感悟，以及觉得 Excel 某些函数特别好用，也就是可以说，一千个使用 Excel 函数的人眼里，有一千个函数的优点。我在这里只是泛泛地列举几个例子。1、常用的四则运算函数完全可以替代计算器。2、财务方面的函数可以帮助财务工作人员快速处理数据。3、逻辑方面的函数可以处理一些简单的逻辑，即使用简单的函数脚本解决。4、查找引用方面的函数，可以高效地进行搜索替换。在 Excel 中可以看到常用的函数分类：函数使用 先简单介绍一下求和函数【SUM】，以保证有个基本的认知。例如有一个 Excel 文件，有三列数字，分别是语文、数学、英语的成绩，现在需要计算每一行的第一列到第三列的和，求和结果放在第四列，也就是总成绩。只要在第四列中，输入函数以及参数：1=SUM (A2:C2)截图如下：其中，=SUM () 是函数名称，表示累加求和，括号里面的 A2:C2 是单元格的位置，: 表示从 A2 到 C2，总结起来就是把第二行的 A 列到 C 列的值累加求和，得出结果。按回车键，就会触发计算，得出结果，结果存放在写有函数的那个单元格。如果需要接着计算第三行、第四行、第五行，是不需要重复输入函数以及参数的，直接选中已经有结果的单元格，鼠标的光标放在单元格右下角，光标会变成一个黑色的十字，鼠标左键长按往下拖拽即可。选中单元格，注意观察单元格右下角的大点 往下拖拽，自动计算 注意，使用复制下拉功能时，行号参数是会自动变化的，也就是说 每一行 的求和结果都是 当前行 的第一列到第三列的数值之和。可以任意选择一行的结果单元格，查看单元格的内容：那这个是怎么做到自动化的呢？其实这是 Excel 自带的功能，术语称为 自动填充 ，不知道你有没有看到，在往下拖拽完成后，可以点开右下角的三角下拉列表，看到里面有三种模式选择，默认的就是 复制单元格 【你也可以试玩一下其它的两种模式】。 复制单元格 对于普通的单元格来说，直接复制内容，对于函数单元格来说，还会自动变更里面的参数。结果已经完全出来，可以正常使用。另外再说一个隐藏的注意点，这种通过函数产生的结果，是不能复制粘贴到别的地方使用的，因为复制粘贴过去的内容是函数公式，不是那个计算结果值。因此如果直接复制粘贴到别的地方，它还会用这个函数计算，得到的结果就与单元格数据当前所在的地方有关，结果肯定和以前不一样，或者根本没有结果。例如我把总成绩和姓名这两列复制粘贴到别的 Excel 文件里面，可以看到得到的结果都是 0，这是因为通过函数计算得出的结果就是 0，表格的第一列到第三列根本没有值。那怎么解决这个问题呢，其实方法是有的：复制时还是正常的复制函数，粘贴时不能默认了，要选择 粘贴为数值 ，或者 选择性粘贴 。这样，粘贴结果单元格里面就是真实的数值了，函数公式已经不见了。此时的单元格就是普通的单元格，里面是文本内容，可以随意复制粘贴使用。粘贴为数值 选择性粘贴 接下来介绍本文的重点：VLOOKUP 函数。先提前说明， 工作表 就是指 Excel 文件中的 Sheet 概念，新建的 Excel 文件一般默认有 3 个 Sheet。以下内容基于两份数据：学生表、成绩表。基本使用方式 VLOOKUP 是一个文本类型的函数，用来匹配搜索的。如果在单一的工作表中使用，例如根据姓名搜索总成绩，可以使用：1=VLOOKUP (H2,A:E,5,FALSE) 截图如下：其中，VLOOKUP 是函数名称，H2 表示需要查找的数据列，A:E 表示搜索的数据范围【此时不需要指定单元格的行号】，5 表示搜索命中的的结果列，5 一定是在 A:E 之间，FALSE 表示关闭模糊搜索，即精确搜索。这里总结起来就是根据 H2 的值，在 A:E 之间搜索【会搜索所有的行】，如果命中了结果，把 A:E 之间的命中那一行的第 5 列单元格【也就是 E 列】的值返回，搜索时使用精确匹配。结合上面的截图，更通俗地说，就是根据 H2 的姓名，在成绩表 A:E 的所有行中搜索同姓名的人，把命中行 E 列的成绩返回，匹配姓名时必须完全相等才算命中。也可以选中结果下拉，自动填充其他人的总成绩。跨工作表使用 上面的内容是在同一个工作表中搜索，如果是跨工作表使用怎么办呢？，例如一个 Excel 文件有两个工作表：学生表、成绩表，现在要根据学生表的姓名从成绩表中搜索总成绩。其实做法也是很简单，函数都是同一个函数，只不过在指定数据范围这个参数的时候，需要加上工作表的名称：1=VLOOKUP (A2, 成绩表！A:E,5,FALSE)成绩表信息 学生表信息 这里除了额外指定了参数 成绩表！A:E 来指定 成绩表 这个 Sheet，其它的参数仍旧与前面一致。跨文件使用 接着又有问题了，如果两个表是分开两个文件呢，聪明人已经可以想到，肯定是继续更改参数的值：1=VLOOKUP (A2,[成绩表.xlsx] 成绩表！A:E,5,FALSE)其中，[成绩表.xlsx] 成绩表！A:E 就是用来指定文件、工作表、数据列的。但是这里需要同时打开两个文件，这样编辑器才能找到文件的内容。这里也可以看出来，为什么在 Excel 中不能同时打开两个同名的文件了，因为 Excel 要以文件名作为一份数据的唯一标识，哪怕两份同名的文件存放在不同的目录，也会被 Excel 当做同一份文件。此外，还有一个小特性，只要函数生成完毕，就可以把成绩表关闭了，不会影响已经搜索出来的结果。而且，如果继续在学生表中增加姓名，还可以继续完成搜索，也就是说 Excel 是把成绩表做了缓存，可以一直使用。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>VLOOKUP</tag>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 框架 Next 主题添加自定义 Page]]></title>
    <url>%2F2017050701.html</url>
    <content type="text"><![CDATA[在整理博客的过程中，发现需要新增一些页面，对于 Hexo 框架来说是 page 的概念，例如 首页 、 关于 、 分类 、 搜索 等页面。这种页面不同于每一篇博客文章那种发表的内容，对于 Hexo 框架来说是 post，而是可以交互的页面，例如可以在 搜索 页面中搜索博客的内容，可以在 分类 页面中查看博客文章的分类统计。当然，类似于 关于 这种页面也是静态的，没有交互的概念。上面提到的这些页面都是 Next 主题自带的，只要在 _config.yml 配置文件中开启相关配置即可，不需要关心它是怎么实现的，例如开启了 分类 页面，它会自动把博客的分类统计好，展示出来。但是我的想法其实是新增一个页面，并且自定义图标、名称、内容，其实也可以实现，本文记录这个过程。自带的页面 Hexo 自带的页面有好几种，例如：关于、首页、分类、搜索、站点地图、404 页面等，可以在主题的配置文件中查看 menu 选项 。例如我使用的是 Next 主题，在 themes/next/_config.yml 中查看 menu 选项，我这里已经配置好 home、about、tags、categories、archives，此外还有没有开启的 schedule、sitemap、commonweal 等，先忽略我新增的 books 页面。 这里面的配置有固定的格式，一共有四列：第一列是展示的名字以及页面标识、第二列是 url 地址、第三列是固定的双竖线、第四列是图标名称。我这里使用 about: /about/ || user 举例，about 就是页面的名字【虽然配置的是英文，但是有汉化字典转为中文，汉化字典文件为：themes/next/languages/zh-Hans.yml】，/about/ 是页面的 url 地址，表示从主页跳转的地址，前面加上域名可以直接访问，|| 双竖线是固定标识符，user 是图标名称，来自于一个图标库：https://fontawesome.com 。只要开启这个配置，就可以看到关于的页面。这些页面都不需要特殊的处理，直接配置完成就可以直接使用，可以在项目的 source 目录里面查看子文件夹，每个子文件夹都会对应一个页面，文件夹里面有一个 index.md 文件，就是页面的原始数据。但是对于搜索、分类、归档等可以交互的页面，Hexo 在渲染时还会重新计算，这里面的 index.md 文件没有内容，只是表示开启了这个页面。而对于静态页面，直接在相应的 index.md 文件里面写上内容就行了，Hexo 值了渲染不会再重新计算内容。例如关于页面，就可以使用 Markdown 语法在 about/index.md 文件里面写上关于作者的简介，我下面要新增的页面也是类似这种格式。各种页面对应的子文件夹 新增页面 了解完了自带的页面，接下来准备新增自定义页面。我需要新增的是一个静态页面，名称为 书籍 ，里面会列出我的读书清单，并给出书籍的部分信息。生成页面并编辑 经过查询 Hexo 的语法，生成新页面的命令为：hexo new page name，page 是关键字，name 表示页面的名字，我直接使用 hexo new page books 即可。执行完命令后，可以在 source 目录看到生成了一个 books 目录，里面有一个 index.md 文件，直接编辑这个页面即可。简单编辑内容如图：这里需要注意文件头的内容，有固定的格式：123456---title: 书籍 date: 2019-04-25 00:16:58type: bookscomments: false---其中，title 就是渲染后 html 网页的居中标题以及网页的 title 标签值，会在浏览器的 tab 页上面显示【这里也可以使用英文名称 books，但是需要在汉化文件的 title 选项下面增加中英文配置，和后面的 menu 汉化类似】。type 就是页面的类别，与自定义页面名称保持一致。此外 comments 切记关闭，因为博客如果开启了评论功能，会默认在所有的页面都开启评论框，而这种自定义页面是不需要评论框的，因此选择关闭，即设置为 false。开启页面配置 在主题的配置文件 themes/next/_config.yml 中，配置自定义页面，在 menu 选项下面，配置内容如下：1books: /books/ || book截图如下：其中，books 是新建的页面名称，/books/ 是链接，book 是图标【因为没有 books 图标可以使用，只能使用 book 图标了，原因在最后会描述，主要是收费问题】。汉化页面名称 配置 themes/next/languages/zh-Hans.yml 文件，也是在 menu 选项下面，配置内容如下：1books: 书籍 截图如下：打开页面预览 在博客点击书籍页面或者直接输入 域名 /books/ 链接，打开页面。注意事项 注意，图标是来自于图标库：https://fontawesome.com ，只要提供图标的名字即可，Hexo 会自动匹配对应的图标展示。需要特别注意的是，这里面的图标有大部分是收费的【搜索时会显示灰色状态，能免费使用的才会显示黑色状态】，所以不能使用，即使配置了名称 Hexo 也不会展示出来。例如我想使用一个名字为 books 的图标，是收费的，发现 Hexo 不会展示，我换成了另外一个名字为 book 的免费图标，Hexo 就可以正常展示了。搜索图标结果]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
        <tag>page</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 输出重定向的问题]]></title>
    <url>%2F2017050401.html</url>
    <content type="text"><![CDATA[最近遇到一个好玩的事，在使用 Linux 命令执行任务【Java 程序或者 Python 程序】时，需要把输出内容日志重定向到文件中，并且保持任务后台执行，这样就可以继续执行其它的命令，不占用 SSH 工具的 session。而且，如果等不了任务的运行，直接退出 SSH 登录即可，任务会在后台继续执行，下次重新登录时可以继续查看任务的状态、分析日志的内容。这里面会涉及到输出重定向、设备文件、输出类型的概念，本文记录这个问题以及涉及的相关知识点。基础概念 首先，通过一个很常见的具体的例子来说明基础概念，让读者有一个深刻印象。在 Linux 机器上面运行程序的时候，很多人都很熟练地使用类似于 nohup run_your_program &gt; /dev/null 2&gt;&amp;1 &amp;，一些人一看就明白了，还有一些人可能会使用，但是不太了解什么意思。当然，大多数人肯定都查过，了解过相关知识，只不过有时候不在意它，久而久之就忘记了。下面我就以这个例子为样本分析各个符号的含义。起始的 nohup 和结尾的 &amp; 是一对命令，常常放在一起使用，表示让运行的进程忽略 SIGHUP 信号，并进入后台运行，这样就可以保证这个进程一直运行下去，不受用户退出、系统 SIGHUP 信号的影响 run_your_program，表示运行的程序命令，可以是 Java、Shell、Python、Go 等&gt;，表示 重定向 ，可以把输入或者输出重定向到一个地方，后面一般跟着的是文件 &gt; 重定向符号前面缺省了默认值 1，完整应该是 1&gt;，表示 标准输出文件 ，即 stdout/dev/null，表示 空设备文件 ，它是一个特殊的文件，表示什么都没有，它跟在重定向符号后面则表示把运行程序产生的 标准输出文件 重定向到空设备文件，即不会输出 标准输出文件 的内容 2&gt;&amp;1，根据前面的解释，这些符号可以放在一起理解，2 表示 标准错误文件 ，即 stderr，&gt; 仍旧是重定向，&amp;1 则表示重定向的文件和 1 一样，即把 标准错误文件 也重定向到 /dev/null 中，即不会输出 标准错误文件 的内容 想必读者已经看明白了，上面的描述已经很清楚，但是有些地方我觉得还是需要再总结完善一下。对于 nohup 与 &amp; 的使用，还需要了解一下控制信号、后台进程相关的知识点，这样才能知其然并且知其所以然。关于控制信号的知识点，可以参考我另外一篇博文：Linux 之 kill 命令入门实践 ，关于后台进程的知识点，也可以参考我另外一篇博文：Linux 让进程在后台运行的几种方法 。1、2 这 2 个数字，是文件描述符，都是有特殊含义的，1 是缺省值，其实还有一个 0。一般情况下，Linux/Unix 系统在启动进程时，会打开三个文件，由这三个文件描述符来表示，它们的具体含义如下： 标准输入文件，stdin，文件描述符为 0，进程默认从 stdin 读取数据，一般是从键盘输入读取 标准输出文件，stdout，文件描述符为 1，进程默认向 stdout 输出数据，一般是输出到终端界面 标准错误文件，stderr，文件描述符为 2，进程默认向 stder 输出错误数据【例如异常日志】，一般是输出到终端界面 这里再多提一点，有时候读者还会见到 &amp;&gt; /dev/null 这种重定向的使用，这里的 &amp; 其实表示的是 1、2 的合集，即把 标准输出文件 、 标准错误文件 都重定向到 /dev/null 中，效果等价于使用 1&gt; /dev/null 2&gt;&amp;1，包括 IO 效率也是一致的【涉及到文件的管道，下文会有验证过程】。/dev/null 是一个极为特殊的设备文件，输出到这里的数据是不可见的，也就是数据会被丢弃，如果尝试从这个文件中读取数据，什么也读不到。如果读者希望在运行程序时，程序产生的日志、计算结果都不要在屏幕上输出，那么就可以选择将 标准输出文件 、 标准错误文件 都重定向到 /dev/null，此时无论程序产生了什么异常，你都看不到了。总的来说，这个空设备文件虽然看上去很奇怪，似乎没有什么价值，但是如果将所有的输出重定向这里，就可以达到禁止输出的效果。好，至此概念讲解完毕，接下来会使用更加详细的命令来演示重定向的神奇功能，并且还会对比一些看起来差不多的命令。举例演示 仍旧拿标准的命令格式 run_your_program &gt; /dev/null 2&gt;&amp;1 来举例，为了适配 Linux 机器以及简化程序【读者可以直接使用我的 Shell 脚本在任何一台 Linux 机器上面运行】，我直接使用 Shell 脚本来演示。我写了一个简单的脚本：test_redirect.sh，只有 6 行内容【有 3 行内容是打印日志的，方便查看输出】，如下：123456echo &apos;========list all file in current path&apos;ls -aecho &apos;========list all file in ./not-exist-dir&apos;ls ./not-exist-direcho &apos;========print date&apos;date这个脚本只做了三件事，其中，第 2 行列出当前目录的所有文件、文件夹，第 4 行列出当前目录下的 not-exist-dir 子目录中的所有文件、文件夹【当然，这是一个不存在的目录，目的是为了让脚本有标准错误信息】，第 6 行输出当前的日期。直接运行打印在终端屏幕 使用 sh test_redirect.sh 直接运行脚本，由于没有重定向的操作，默认就是把标准输出、标准错误全部打印在屏幕上面，如图可以看到输出内容【其中第 4 行的报错信息 No such file or directory 属于标准错误信息】。我来仔细分析一下屏幕上输出的内容，3 行以 ======== 开头的内容就不多说了。第 2 行的内容是 ls -a 产生的，列出了当前目录的所有文件、文件夹 第 4 行的内容是 ls ./not-exist-dir 产生的，注意它是一个系统报错，也就是标准错误，用来提示用户命令执行失败【访问文件夹失败】，原因是文件夹不存在 第 6 行的内容是 date 产生的，输出当前系统的时间 重定向到空设备文件 接下来做一个操作，把标准输出重定向到 /dev/null，标准错误仍旧打印在屏幕上面，运行命令改为 sh test_redirect.sh &gt; /dev/null 即可。运行后可以看到，屏幕上面只有标准错误信息，没有标准输出信息，这是因为标准输出信息被重定向到空设备文件【参数 &gt; /dev/null】，在屏幕上是看不到了。接着把标准输出、标准错误都重定向到 /dev/null，运行命令改为 sh test_redirect.sh &gt; /dev/null 2&gt;&amp;1 即可。运行后可以看到，屏幕上面没有打印任何信息，而且由于被重定向到空设备文件，信息无法找回。重定向到文本文件 为了保存程序的输出信息【持久化】，方便以后排查问题，在实际场景中不会把标准输出、标准错误直接输出到屏幕，更不会重定向到空设备文件【这种操作会导致无法查看输出的信息，相当于永远丢失】，一般我会指定一个日志文件，用来存放程序所有的输出信息，只要文件还在，随时可以查看。运行命令 sh test_redirect.sh &gt; ./test_redirect.log，可以把标准输出重定向到 test_redirect.log 文件，标准错误仍旧打印在屏幕上面。使用命令 sh test_redirect.sh &gt; ./test_redirect.log 2&gt;&amp;1，可以把标准输出、标准错误都重定向到 test_redirect.log 文件，没有任何信息打印在屏幕上面。各种各样的重定向 以上都是常规的重定向输出，比较常用，读者看一遍也就懂了，下面再列举一些奇怪的、令人疑惑的重定向输出，需要细细分析才能理解其中的含义。奇葩操作方式 如果使用显示指定的重定向命令来运行程序 sh test_redirect.sh 1&gt; ./test_redirect.log 2&gt; ./test_redirect.log【简称为： 奇葩操作方式 】，也就是在命令中显示地指定标准输出、标准错误全部重定向到 test_redirect.log 文件中，那么它和 sh test_redirect.sh &gt; ./test_redirect.log 2&gt;&amp;1【简称为： 正常操作方式 】的效果一样吗？从上图的运行结果来看，两者效果 大概一样 ，都是把标准输出、标准错误全部保存在 test_redirect.log 日志文件中，读者肯定也是这样想象的。如果仔细观察一下，发现完全不一样，虽然日志文件中的内容有部分和 正常操作方式 产生的一致，但是明显少了几行，而且顺序还错乱了，最明显的就是 No such file or directory 这个标准错误信息为什么在第 1 行就出现了。由于运行机器环境的原因，读者如果在自己的机器上面测试一下，可能会发现结果和我的不一样，甚至自己前后运行几次的结果也不一样，遇到这种现象是正常的，不要怀疑人生。此外，如果有读者碰巧遇到了和 正常操作方式 一样的结果，也不要用来反驳我的结果，更不能草率地得出结论：这两种操作方式的效果一致。因为这只是读者你运气好，碰到了这个结果，其实不是你想象的那样。总的来说， 奇葩操作方式 产生的结果一切皆有可能，内容缺失、位置错乱、内容正常这些结果都有可能产生，那它背后的原因是什么呢，让我来探究一番。首先，根据目前的现象，我只能猜测是输出时有 2 个文件流对 test_redirect.log 日志文件有竞争【类似于多线程竞争一个锁、多个进程竞争一个 CPU 资源】，导致输出的内容相互覆盖、部分内容丢失，位置也就错乱了。那怎么验证这个猜测呢，以及使用 正常操作方式 时为什么没有这个问题呢？为了验证这个猜测，必须知道程序在运行时对文件做了什么操作，我可以使用 strace 这个命令来追踪程序对文件的操作。由于我没有找到使用 strace 直接追踪重定向命令的方法，例如如果使用命令 strace sh test_redirect.sh &gt; ./test_redirect.log 2&gt;&amp;1，其实这里面的重定向是对于 strace 生效的，即把 strace 的输出信息重定向到文件中了，从而追踪不到重定向时对文件的操作，这不是我想要的结果【添加 -o 参数也不行】。于是，我只好采取了一种迂回的思路：先把完整的重定向命令整理到 Shell 脚本中，然后使用 strace 追踪运行脚本的过程。我整理出 2 个 Shell 脚本，如下：1、 正常操作方式 对应的 Shell 脚本，strace_normal.sh，脚本内容如下 1sh test_redirect.sh &gt; ./test_redirect.log 2&gt;&amp;12、 奇葩操作方式 对应的 Shell 脚本，strace_strange.sh，脚本内容如下 1sh test_redirect.sh 1&gt; ./test_redirect.log 2&gt;./test_redirect.log 接着开始使用 strace 命令进行验证，由于输出内容过多，先使用 -o 参数保存在文件中，再查看，-f 参数是为了追踪子进程的：1、使用 strace -o strace_normal.log -f sh strace_normal.sh，追踪 正常操作方式 的系统调用，输出到 strace_normal.log 文件中，主要为了追踪对文件的操作，运行完成后使用 cat strace_normal.log 查看系统调用的内容。由于输出信息内容太多，都是一些看不懂的系统调用，所以只需要看和重定向时操作的日志文件、文件描述符有关的内容即可，使用 cat strace_normal.log |grep &#39;test_redirect.log\|dup&#39; 过滤掉无关内容。1234532828 dup2 (3, 255) = 25532830 open (&quot;./test_redirect.log&quot;, O_WRONLY|O_CREAT|O_TRUNC, 0666) = 332830 dup2 (3, 1) = 132830 dup2 (1, 2) = 232830 dup2 (3, 255) = 255我关心的重点内容也就是这几行了，可以明显看到打开了 test_redirect.log 日志文件【open 调用】，然后有 2 次文件描述符的复制【dup2 调用】，分别把 3 复制给 1、1 复制给 2，这样操作后 stdout、stderr 都会写入到 test_redirect.log 日志文件，但是文件只被打开了 1 次。关于 dup 的概念：dup、dup2：复制一个文件描述符，有 2 种调用方式 int dup (int oldfd)int dup2 (int oldfd, int newfd)2、使用 strace -o strace_strange.log -f sh strace_strange.sh，追踪 奇葩操作方式 的系统调用，详细过程省略，只查看重要的系统调用，使用 cat strace_strange.log |grep &#39;test_redirect.log\|dup&#39; 过滤输出信息。12345659916 dup2 (3, 255) = 25559917 open (&quot;./test_redirect.log&quot;, O_WRONLY|O_CREAT|O_TRUNC, 0666) = 359917 dup2 (3, 1) = 159917 open (&quot;./test_redirect.log&quot;, O_WRONLY|O_CREAT|O_TRUNC, 0666) = 359917 dup2 (3, 2) = 259917 dup2 (3, 255) = 255明显可以看到一个不同，调用了 2 次 open，也就是打开了 2 次日志文件。通过上面的对比分析验证，结论不言而喻，我来简单总结一下：1&gt; ./test_redirect.log 2&gt; ./test_redirect.log：把标准输出、标准错误都直接重定向到 test_redirect.log 日志文件，但是 test_redirect.log 日志文件会被打开 2 次，即产生了 2 个文件输出流，导致标准输出、标准错误互相覆盖，也就出现了上面比较奇怪的现象。&gt; ./test_redirect.log 2&gt;&amp;1：把标准输出、标准错误都直接重定向到 test_redirect.log 日志文件，但是标准错误输出时没有打开日志文件的操作，而是直接继承了标准输出的文件流，这样的话 test_redirect.log 日志文件只被打开了 1 次，也就不会出现上面比较奇怪的现象。另外，如果从 IO 效率方面来看，显然 正常操作方式 的效率更高。一个特殊的符号 这里要说的特殊符号是 &amp; 符号，如果把它作为文件描述符使用，它表示标准输出、标准错误的集合，示例命令为 &amp;&gt; ./test_redirect.log，它表示把标准输出、标准错误全部重定向到 test_redirect.log 日志文件中，即和 正常操作方式 的效果一致。我也可以使用上面的 strace 命令验证它的 IO 效率也是和 正常操作方式 一致的，我需要像前面那样，新建一个 Shell 脚本，strace_special.sh，内容如下：1sh test_redirect.sh &amp;&gt; ./test_redirect.log使用 strace -o strace_special.log -f sh strace_special.sh，追踪这种重定向方式的系统调用，详细过程省略，只查看重要的系统调用，使用 cat strace_special.log |grep &#39;test_redirect.log\|dup&#39; 过滤输出信息。1234570626 dup2 (3, 255) = 25570627 open (&quot;./test_redirect.log&quot;, O_WRONLY|O_CREAT|O_TRUNC, 0666) = 370627 dup2 (3, 1) = 170627 dup2 (1, 2) = 270627 dup2 (3, 255) = 255结果不言而喻，系统调用时对文件的操作和 正常操作方式 是一致的。重定向顺序产生的影响 有的读者可能会遇到一种更加奇葩的操作方式，示例命令为 sh test_redirect.sh 2&gt;&amp;1 1&gt; ./test_redirect.log，即把文件描述符 1、2 的重定向操作调换一下位置，先指定 2 和 1 一样，再指定 1 重定向到文件。通过上面的学习总结，读者应该已经可以猜出这种重定向操作的结果：标准错误重定向到终端屏幕，标准输出重定向到 test_redirect.log 日志文件，而不是两者都重定向到日志文件。这是为什么呢？其实是因为在指定 2 和 1 一样的时候，1 仍然是重定向到终端屏幕的，那 2 也就是跟着重定向到终端屏幕，接着指定 1 的时候，1 才改变重定向的操作，但是 2 仍然保持不变，这个思路和编程语言中的中间变量类似。读者也可以使用前面的 strace 命令追踪这种重定向方式对文件的操作、对文件描述的复制过程，然后就明白为什么是这样的结果了，我的验证结果如下：1234514004 dup2 (3, 255) = 25514005 dup2 (1, 2) = 214005 open (&quot;./test_redirect.log&quot;, O_WRONLY|O_CREAT|O_TRUNC, 0666) = 314005 dup2 (3, 1) = 114005 dup2 (3, 255) = 255常用重定向总结区分 下面整理一些常用的重定向【文件描述符使用术语表达】，读者必须要区分开来，不要乱用：run_your_program：没有使用重定向，stdout、stderr 全部输出到终端屏幕 run_your_program &gt; /dev/null：stdout 输出到空设备文件、stderr 输出到终端屏幕run_your_program &gt; /dev/null 2&gt;&amp;1：stdout、stderr 全部输出到空设备文件run_your_program &gt; log_file：stdout 输出到日志文件、stderr 输出到终端屏幕run_your_program &gt; log_file 2&gt;&amp;1：stdout、stderr 全部输出到日志文件run_your_program &gt; log_file 2&gt; log_file：stdout、stderr 全部输出到日志文件，但是输出内容会混乱，这种方式不建议使用run_your_program 2&gt;&amp;1 1&gt; log_file.log：stdout 输出到日志文件、stderr 输出到终端屏幕，有点迷惑人，不建议使用，要确认的确懂这个重定向的真实含义才能使用run_your_program 2&gt;&amp;1：stdout、stderr 全部输出到终端屏幕，相当于没有重定向，没必要使用【如果系统默认没有把 stdout 输出到终端屏幕，需要使用这种方式】run_your_program &amp;&gt; log_file：stdout、stderr 全部输出到日志文件，&amp; 表示 stdout、stderr 2 个的集合 注意事项 1、&amp; 和后台作业 在这里如果只使用 nohup 不使用 &amp; 把程序后台运行【仍旧是前台运行】，表面上看把输出日志重定向文件中了，屏幕不再滚动打印出来大量的文本内容。其实，程序此时仍旧在占用着键盘的输入流，你的终端命令行在等待着输入，你无法使用键盘进行其它的命令操作，而且你不能使用 ctrl + c 的方式中断，否则程序会退出。此时如果想把进程调到后台运行，可以使用 ctrl + z 暂停进程【同时调到后台，变为暂停状态的作业】，然后使用 bg 让作业在后台继续运行，这样就可以手动把运行在前台的进程调整到后台，而没有影响到进程的运行。如果后台的作业比较多，先使用 jobs 查看作业的编号，使用 bg 作业编号 指定某个作业在后台运行。2、Python 脚本的输出缓存机制 在执行 Python 脚本时，把打印的日志都重定向到一个日志文件中，发现日志内容并没有及时更新【在 Python 脚本中 print 的日志内容】，实时查看日志文件的内容【使用 tail -f log_file.log 命令】，发现不会像 Java 程序那样实时刷出来新的内容，而是会卡住一段时间，然后突然一大段日志出来。造成这种现象的原因是 Python 有输出流缓存机制，不会把输出内容实时写入输出流，而是等待缓冲区积累一定的内容再操作，这样一来，重定向到文件中的内容总是一批一批的。当然，可以选择关闭这个选项，在执行 Python 脚本时，使用 -u 参数就可以强制把输出内容实时写入到输出流，也就可以实时重定向到日志文件了，命令示例 nohup python -u your_python_file &gt; log_file.log 2&gt;&amp;1 &amp;。3、读者可以继续探索一下 tee 命令的使用，它可以帮助我们把 stdout、stderr 即输出到终端屏幕、又输出到文件，关键是不用写多行命令。同时，它还有一个优点，即使用 tee 命令是不影响原来的 IO 效率的。]]></content>
      <categories>
        <category>Linux 命令系列</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>redirect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于 Spark 或者 mapreduce 的累加器]]></title>
    <url>%2F2017043001.html</url>
    <content type="text"><![CDATA[在 Spark 和 Hadoop 的 MapReduce 中都有累加器的概念，顾名思义，累加器就是用来做累加【或者累减】使用的，有时候为了统计某些值，在程序中埋入指标，这样在程序运行中、运行后都可以清晰观察到统计指标，还能辅助检查程序的问题。在 Spark、MapReduce 中，它们的使用方式尽管有一点点不同的地方，甚至在 Spark 的不同版本中使用方式也会不一致，但也算是大同小异。本文简单记录在 Spark、MapReduce 中累加器的使用，并补充说明一些重要的坑，Spark 环境基于 v1.6.2，Hadoop 环境基于 v2.7.1 。累加器基础概念 在 Spark 任务中，如果想要在 Task 运行的过程中统计某些指标，例如处理了多少数据量、过滤了多少数据量，使用普通的变量是不行的，会有并发的问题。此时，累加器就可以出场了，使用方式简单，统计结果准确。只要在代码中指定了累加器，并在 Task 中使用它，通过 Action 算子触发后，在 Spark 任务运行中或者运行完成后，都可以观察到累计器的值。例如在 Spark 任务运行的过程中，通过 SparkUI 可以观察累加器的取值变化，在 Stages 标签页中选择带有累加器的某一个 Stage，查看详情，就可以看到在 Accumulators 指标列表中，列出了所有累加器的名字和取值。而在 Hadoop 的 MapReduce 中，累加器的用法也是一样，不同的是，在 MapReduce 的调度系统 Yarn 中，无法观察到累加器的取值变化，只能等待 MapReduce 运行完成，才能在输出日志中查看累加器的最终取值。而且这是自动统计打印出来的，不需要手动输出。下面详细介绍累加器的使用方式。累加器的使用 在 Spark 中的使用 首先说明一下，注意不同版本的影响，使用方式会不一致，我下面列出的例子都是基于 Spark v1.6.2，例如在 Spark v2.x 的版本中，初始化累加器的方式就改变了一些，在此不再赘述。Spark 提供的 Accumulator，主要用于多个节点【Excutor】对同一个变量进行共享性的操作。Accumulator 只提供了累加的功能【调用 add () 方法】，给我们提供了多个 Task 对一个变量并行操作的功能。但是 Task 只能对 Accumulator 进行累加操作，不能读取它的值，只有 Driver 端的程序可以读取 Accumulator 的值【调用 value () 方法】。代码接口 为了使用累加器，首先要有上下文对象，对于 Java 的接口来说就是 JavaSparkContext，然后利用上下文对象创建累加器。下面列出几个简单的示例：123456JavaSparkContext jsc = initJsc ();final Accumulator&lt;Integer&gt; totalAcc = jsc.accumulator (0, &quot;total&quot;);final Accumulator&lt;Integer&gt; saveTotalAcc = jsc.accumulator (0, &quot;saveTotal&quot;);final Accumulator&lt;Integer&gt; defaultAcc = jsc.accumulator (0);final Accumulator&lt;Double&gt; doubleAcc = jsc.doubleAccumulator (0);final Accumulator&lt;Integer&gt; intAcc = jsc.intAccumulator (0);代码片段截图如下：这里把累加器的修饰符定义为 final 是有必要的，因为在 Spark 的 Function 中使用时，内部匿名类会要求变量必须为 final 类型的。应该尽量为累加器命名【唯一标识】，这样在查看时才能区分是哪个累加器。此外，除了普通的数值型累加器，还有集合型累加器，或者用户可以自定义累加器，只要实现特定的接口即可：Accumulator，然后通过 JavaSparkContext 对象进行注册，在此不再赘述。创建完成后，就可以使用了，在自定义实现的 Function 中，可以使用累加器进行累加操作，代码片段如下：12totalAcc.add (1);saveTotalAcc.add (1);代码片段截图如下：注意一点，在 Function 中对累加器只能增加，不能取值，如果在 Spark 的 RDD 中试图取出累加器的值，Spark 任务会抛出异常而失败。因为累加器，顾名思义，就是用来累加的，只能在 Spark 任务运行中【Task 端】进行累加，而且用户不用担心并发的问题，但是想要使用代码获取累加器的取值，只能等待 Spark 任务运行完成后，才能在 Driver 端进行取值操作。使用代码取值代码片段如下：12System.out.println (&quot;====total:&quot; + totalAcc.value ());System.out.println (&quot;====saveTotal:&quot; + saveTotalAcc.value ());代码片段截图如下：前端界面查看 而如果有一个需求就是要在某时刻查看累加器的值，或者说需要实时查看累加器的值，能不能实现呢，当然可以，这就需要 SparkUI 出场了。在提交 Spark 任务时，创建 JavaSparkContext 对象成功后，注意观察输出日志，会发现有一个重要的链接信息出现：SparkUI 的地址。当然，如果已经知道了自己所使用的 Yarn 或者 Standalone 集群的信息，就不需要关心这个日志了，直接打开浏览器就可以查看了。在上面的截图中，可以看到重要的一行信息：116:50:10 [main] INFO ui.SparkUI:58: Started SparkUI at http://192.168.10.99:4041这个就是 SparkUI 的地址，直接在浏览器中打开，就可以看到这个 Spark 任务的运行状态，其它的信息我在这里不关心，直接选择 Stages 标签页，可以看到下面有一个 Stages 列表，里面是运行中或者运行完成的 Stage：选择一个带有累加器的 Stage，查看详细信息，可以看到 Executor、Tasks、Accumulators 等信息，在这里重点关注 Accumulators 的信息：可以看到，我这里有 2 个累加器：total、saveTotal，它们的取值分别为 70552、70552。根据我的代码逻辑，这表示我的 Spark 任务已经处理了 70552 条数据，并且没有过滤掉 1 条数据，全部写出到文件。奇技淫巧 如果需要使用累加器进行减法操作，可行吗，当然，把累加器的累加数值改为负数即可。12totalAcc.add (-1);saveTotalAcc.add (-1);在 MapReduce 中的使用 在 MapReduce 中使用累加器的方法就很简单了，不需要初始化，直接通过枚举类型 Enum 定义累加器的唯一标识，然后在 Map 或者 Reduce 中，利用 Context 上下文对象对累加器进行操作，例如增加指定数值。代码示例如下：12345678910context.getCounter (MREnum.MAP_READ).increment (1);if (result == null || result.isEmpty ()) &#123; context.getCounter (MREnum.MAP_FILTER).increment (1); return;&#125;String pk = new String (result.getRow ());if (StringUtil.isNullOrEmpty (pk)) &#123; context.getCounter (MREnum.MAP_FILTER).increment (1); return;&#125;代码片段截图如下：使用累加器的核心代码就是：1context.getCounter (MREnum.MAP_FILTER).increment (1);其中，Context 就是 MapReduce 中的上下文对象，MREnum.MAP_FILTER 是自定义的枚举类型，每个累加器对应一个。MapReduce 任务完成后，不需要手动输出累加器的取值，Hadoop 框架会自动统计输出各种指标，当然也包括累加器的取值。可以从上图中看到有 5 个累加器：DONE、READ、REDUCE、SHUFFLE、WRITE，它们的取值都是千万级别的数字。此外，在 Yarn 等调度系统中无法查看 MapReduce 任务的累加器取值变化，这是一个遗憾。注意踩坑1、前面已经说过，使用累加器时，只能在 Spark 任务运行中【Task 端】进行累加，然后等待 Spark 任务运行完成后，才能在 Driver 端进行取值操作。如果强行在 Task 中对累加器进行取值，Spark 任务会抛出异常而失败。2、在 Spark 中，由于累加器是在 Task 中进行的，所以针对 RDD 的 Transform 操作【例如 map、filter】是不会触发累加器的执行的，必须是 Action 操作【例如 count】才会触发。所以如果读者发现自己的程序中输出的累加器取值不正确，看看是不是这个原因。3、正是因为 2 的原因，用户可能会进行多次 Action 操作后，发现累加器的数值不对，远远大于正确的数值，然后懵了。这种现象是正常的，属于人为误操作，因此用户一定要正确使用累加器，控制好 Action 操作，或者及时使用 cache () 方法，这样可以断开与前面 DataSet 的血缘关系，保证累加器只被执行一次。4、通过 2 也可以发现一个问题，如果 Spark 任务的某个 Task 反复执行了多次【Spark 的容错性，例如某个 Task 失败重试了多次之后才成功】，那累加器进行累加时会不会重复计算。当然会重复计算，这也是一个坑，为了避免这个坑，尽量把对累加器的操作放在 Action 算子中，这样就可以保证累加器被操作一次。5、在创建累加器时，如果没有指定累加器的名字，那么只能在程序中通过代码操作累加器，而在 sparkUI 中无法看到累加器的取值。]]></content>
      <categories>
        <category>大数据技术知识</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
        <tag>Java</tag>
        <tag>Spark</tag>
        <tag>Accumulator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch 中的 429 错误 es_rejected_execution_exception]]></title>
    <url>%2F2017042601.html</url>
    <content type="text"><![CDATA[今天在处理数据，处理逻辑是从 HBase 中扫描读取数据，经过转换后写入 Elasticsearch 中，程序的整体方案使用的是 mapreduce 结构。map 负责扫描 HBase 数据，并转换为 Map 结构，reduce 负责把 Map 结构的数据转为 JSON 格式，并验证合法性、补充缺失的字段、过滤非法数据等，最后使用 elasticsearch 官方发布的 BulkProcessor 把数据批量写入 elasticsearch。在处理数据的过程中，遇到了一个诡异的问题，说它诡异是因为一开始不知道 BulkProcessor 存在的坑。关于这个问题，表面现象就是漏数，写入 elasticsearch 中的数据总是少于 HBase 中的数据，而且差距巨大。当然，如果是有经验的工程师，可以猜测好几个原因：扫描读取 HBase 的数据时设置过滤器过滤掉了不该过滤的数据、ETL 的处理逻辑中有误过滤数据的 bug、写入 elasticsearch 时数据不合法导致写入失败、由于 BulkProcessor 潜在的问题导致写入漏数。本文就记录解决这个问题的过程。问题出现 问题其实就是漏数，HBase 里面的数据写入到 Elasticsearch 后发现数据量对不上，而且重跑了几次作业，每次重跑都会有多一点点的数据写入 Elasticsearch，这就很诡异了，不像普通的漏数。这个漏数现象复现不了，虽然每次重跑作业都会漏数，但是数据量对不上，说明背后有一只无形的手在操控着这一切，而且操控过程随心所欲，让人疑惑不解。漏数现象出现后，作为一个有经验的工程师，我先初步怀疑了几个关键点，然后逐步分析，抽丝剥茧，找到了问题所在。Elasticsearch 版本为 v5.6.8。问题解决 怀疑点排查 1、扫描读取 HBase 的数据时设置过滤器过滤掉了不该过滤的数据，经过查看，扫描过滤器只是设置了某个时间字段的范围，并且提交作业时设置的参数属于正常范围，不会影响数据量，排除此种可能。2、ETL 的处理逻辑中有误过滤数据的 bug，仔细查看了 ETL 的处理逻辑，里面有多处过滤数据的处理逻辑，例如发表时间、id 等必要的字段必须存在，但是不会过滤掉正常的数据，而且给对应的过滤指标设置了累加器。一旦有数据被正常过滤掉，累加器会记录数据量的，在作业的日志中可以查看，排除此种可能。3、写入 elasticsearch 时数据不合法导致写入失败，在作业运行中，如果出现这种情况，一定会抛出异常【使用 BulkProcessor 不会抛出异常，但是有回调方法可以使用，从而检测异常情况】，所以在业务代码中，考虑了异常情况的发生，把对应的数据格式输出到日志中，方便查看。我仔细搜索检查了日志文件，没有发现数据不合法的异常日志内容，排除此种可能。4、由于 BulkProcessor 潜在的问题导致写入漏数，这个怀疑点就比较有意思了，使用 BulkProcessor 来批量把数据写入 elasticsearch 时，会有两个隐藏的坑：一是写入失败不会抛出异常，注意，批量的内容全部失败或者部分失败都不会抛出异常，只能在它提供的回调方法【afterBulk ()】中捕捉异常信息，二是资源紧张会导致 elasticsearch 拒绝请求，导致写入数据失败，注意，此时也不会抛出异常，只能通过回调方法捕捉错误信息。所以有可能是这个原因。 重点排查 好了，已经逐条分析了可能的原因，并初步定位了最有可能的原因，接下来就是利用 BulkProcessor 提供的回调方法，把异常信息捕捉，并在日志中输出所有必要的信息，以方便发现问题后排查具体原因。代码更新完成后重跑作业，为了速度快一点，先筛选少量的数据进行重跑，然后观察日志。查看日志，发现有大量的错误信息，就是从 BulkProcessor 的回调方法 afterBulk () 里面捕捉打印的【以下日志片段本来是一行，为了友好地显示，我把它格式化多行了】：12345678910111213...... 省略 2019-04-23 17:37:22,738 ERROR [I/O dispatcher 68] org.playpi.blog.client.es.ESBulkProcessor: bulk [43 : 1556012242738] - &#123; &quot;cause&quot;: &#123; &quot;reason&quot;: &quot;Elasticsearch exception [type=es_rejected_execution_exception, reason=rejected execution of org.elasticsearch.transport.TransportService$7@45c00a5f on EsThreadPoolExecutor [bulk, queue capacity = 1500, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@53ffdb18 [Running, pool size = 32, active threads = 32, queued tasks = 4527, completed tasks = 26531491]]]&quot;, &quot;type&quot;: &quot;exception&quot; &#125;, &quot;id&quot;: &quot;f176fd6b68d22ad357a61714313d2748&quot;, &quot;index&quot;: &quot;org-playpi-datatype-post-year-2018-v1&quot;, &quot;status&quot;: 429, &quot;type&quot;: &quot;post&quot;&#125;...... 省略 更多错误内容如截图所示 找到里面的关键信息：es_rejected_execution_exception、“status”: 429，到这里，可以确定这个错误不是由于数据格式不合法导致写入 Elasticsearch 失败，否则错误信息应该携带 source invalid 字样。可惜，进一步，我看不懂这个异常错误，只能借助搜索引擎了。经过搜索，发现这个问题的原因在于 Elasticsearch 集群的资源不足，处理请求的线程池队列全部被占用，无法接收新的请求，于是拒绝，这也就导致了数据漏掉。在这里先提前说明一下，以下内容的配置信息是基于 数据索引所在的集群、节点 ，例如索引 A 在某个集群，分配了 3 个节点，那就只看这个集群的这 3 个节点，可能还有其它几百个节点存放的是其它的数据索引，不用关心。这样才能准确找到问题所在，否则如果看到配置信息对不上，就会感到疑惑。另外在使用 API 接口时，可以在 url 结尾增加 ?pretty 协助格式化结果数据，查看更容易，?v 参数可以协助返回结果增加表头，显示更为友好。其实，Elasticsearch 分别对不同的操作【例如：index、bulk、get 等】提供不同的线程池，并设置线程池的线程个数与排队任务上限。可以在数据索引所在节点的 settings 中查看，如果有 head 插件【或者 kopf 插件】，在 概览 -&gt; 选择节点 -&gt; 集群节点信息 中查看详细配置。其中在 settings -&gt; thread_pool 里面有各个操作的线程池配置。这里面，有两种类型的线程池，一种是 fixing，一种是 scaling，其中 fixing 是固定大小的线程池，默认是 core 个数的 5 倍，也可以指定大小，scaling 是动态变化的线程池，可以设置最大值、最小值。如果不使用 head 插件，直接通过 Elasticsearch 集群的 http 接口【前提是开放 http 端口或者设置了转发端口，否则无法访问】也可以获取这个数据，例如通过 /_nodes / 节点唯一标识 /settings/ 查看某个节点的配置信息。这个节点唯一标识【uuid】可以通过 head 插件获取，我这里使用 q6GpFsnCSOOfLoLl72MVAg 演示。使用 head 插件获取节点的唯一标识。使用 API 接口查看节点的配置信息 可以看到数据所在节点的线程池配置，对于 bulk 类型的操作，线程池的大小为 32【由于 min 和 max 都设置为了 32，并且线程池类型为 fixing，所以是 32】，队列上限为 1500。好，至此，再结合上面错误日志中的信息：bulk, queue capacity = 1500、Running, pool size = 32, active threads = 32, queued tasks = 4527，可以发现，当前节点【某个 node，不能说整个集群】处理数据时线程的队列已经超过了上限 1500，而且我惊讶地发现已经到达了 4527，这种情况下 Elasticsearch 显然是要拒绝请求的。此外，使用集群的 API 接口也可以看到节点的线程池使用情况，包括拒绝请求量，/_cat/thread_pool?v，查看详情如下图所示。不妨再次探索一下 mapreduce 的日志，搜索关于 bulk 的错误，可以看到大量的错误都是这种，超过队列上限而被拒绝请求。解决方案 原因找到了，解决方案也可以定下来了。1、给 Elasticsearch 的索引增加更多的节点，这样就可以把线程池扩大了，但是需要消耗资源，一般无法实现。2、优化批量的请求，尽量不要发送多个小批量的请求，而是发送少量的大批量请求。这个方法还是适合的，把 bulk 请求的数据量增大一点，收集多一点数据再发送请求。3、改善索引性能，让文档编制索引速度更快，这样处理请求就更快，批量队列就不太容易阻塞了。这个方法说起来容易，做起来有点难，需要优化整个索引设计，例如取消某些字段的索引、删除冗余的字段等。4、在不增加节点的情况下，把节点的线程池设置大一点、队列上限设置大一点，就可以处理更多的请求了。这个方法需要改变 Elasticsearch 集群的配置，然后重启集群，但是一般情况下会有风险，因为节点的硬件配置【内存、CPU】没有变化，单纯增加线程池，会给节点带来压力，可能会宕机，谨慎采用。配置信息参考如下：1234-- 修改 elasticsearch.yml 配置文件 threadpool.bulk.type: fixedthreadpool.bulk.size: 64threadpool.bulk.queue_size: 15005、如果确实在硬件、集群方面都无法改变，那就直接在使用方式上优化吧，例如把并发设置的小一点，请求一批后休眠一段时间，保障 Elasticsearch 可以把请求处理完，接着再进行下一批数据的请求。这种做法立竿见影，不会再影响到 Elasticsearch 的线程池，但是缺点就是牺牲了时间，运行作业的时间会大大增加。迫于资源紧张，我只能选择第 5 种方式了，减小并发数，数据慢慢写入 Elasticsearch，只要不再漏数，时间可以接受。问题总结 除了上面的排查总结，再描述一下一开始针对业务逻辑的具体的思路。拿到错误日志后，简单搜索统计了一下，一个 reduce 任务的错误信息有 16 万次，也就是有 16 万条数据没有成功写入 Elasticsearch。而整个 mapreduce 作业的 reduce 个数为 43，可以预估一下有 688 万次错误信息，也就是有 688 万条数据没有成功写入 Elasticsearch，这可是个大数目。再查看作业日志的统计值，累加器统计结果，在 driver 端的日志中，发现一共处理了 1413 万数据，这样一计算，漏掉了接近 49% 的数据，太严重了。再对比一下我文章开头的描述，每次重跑作业，总是有一部分数据可以重新写入 Elasticsearch，但是成功的数据量仅仅限于几十条、几条。最终还差 500 多条数据的时候，已经重跑了 5 次以上了，所以我才会更加怀疑是程序写入 Elasticsearch 方式的问题。]]></content>
      <categories>
        <category>踩坑系列</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
        <tag>HBase</tag>
        <tag>Elasticsearch</tag>
        <tag>BulkProcessor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决 IDEA 无法创建子包的问题]]></title>
    <url>%2F2017042201.html</url>
    <content type="text"><![CDATA[最近在使用 IDEA 的时候，发现一个奇怪的问题，如果新建了一个多层的包，再想新建一个和除第一层包之外的包等级别的子包就不行。说的这么绕口，什么意思呢？举例来说，比如我新建了一个包，完整路径为：a.b.c.d，如果再想新建一个和 d 等级别的子包 e：a.b.c.e，就不行，IDEA 会默认在 d 下面新建一个子包，那整个包就变成了：a.b.c.d.e，这显然是不合常理的，也不是我需要的。本文记录这个问题的解决方案。问题出现 当在 IDEA 中新建一个 Java 项目的包时，完整路径为：org.playpi.blog，再想新建一个和 blog 等级别的包：www，结果发现 www 是建在了 blog 下面，那就变成了 org.playpi.blog.www，这不是我想要的结果。新建一个包 新建一个子包 注意，上面的现象是在包只有一个的情况下，还没有创建等级别的其它包，如果创建了等级别的其它包就不会有这种现象了。例如如果已经有了 org.playpi.blog、org.playpi.www，再想创建一个 org.playpi.doc，是可以做到的。问题解决 有一种粗暴并且略微繁琐的方法，那就是在新建多层的包时，不要单独创建，而是一层一层创建，并且和类文件【或者其它类型的文件也行，只要不是单纯的包即可】一起创建，这样就可以稳妥地创建多个等级别的包了。但是这种做法显然很傻，而且有时候根本不需要每个包都有等级别的包存在。其实，IDEA 有自己的设置方式，可以看到在项目树形结构的右上方，有一个设置按钮【齿轮形状】，点开，可以看到 Hide Empty Middle Packages，意思就是 隐藏空白的中间包 ，这个选项默认是开启的。注意，这里的 Empty Packages 并不是严格意义上的空包【对应对操作系统的空文件夹】，而是指包里面只有一个子包，并没有其它的类文件或者任意文件。所以，新建的多层的包都会被隐藏，再新建子包时，默认是从最深处的包下面创建，这样也就发生了问题出现的那一幕。解决起来就很容易，把这个选项去掉，不要隐藏，全部的包都显示，这样就可以轻松地新建多个子包了。这样做有一个缺点，对于那些有多个空白包的情况， 都显示出来很难看，不友好，所以最好还是在需要时临时关闭这个选项，等不需要了再打开，毕竟隐藏空白包的效果看起来还是很清爽的。注意，当去掉隐藏选项时，选项的名字会变为 Compact Empty Middle Packages，收起空白包，其实意思和隐藏空白包一样。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>IDEA</tag>
        <tag>package</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 入门系列 0-- 初识 Hadoop]]></title>
    <url>%2F2017040101.html</url>
    <content type="text"><![CDATA[今天是愚人节，可以说是个好日子，也可以说是个坏日子。那我就选择从今天开始整理 Hadoop 入门系列 的博客内容，给自己开个玩笑，同时也给自己定一个目标，看看自己能不能坚持写下去。本文是这一系列博客内容的第零篇： 初识 Hadoop，会讲一些关于 Hadoop 的基础概念以及基本知识点，不需要技术基础，也不需要手动操作，能看懂就行。首先声明，以下内容基于 Hadoop v1.x 讲解，不存在 HA（High Availability，高可用） 的概念，了解 v1.x 的基础概念后，才能更好的继续学习 v2.x、v3.x 的内容。由于 Hadoop 的 v2.x、v3.x 与 v1.x 的版本差异比较大，更为复杂，一些概念一开始难以理解，所以采用这种由易入难的方式能循序渐进，有利于初学者、零基础者。入门概念 首先需要了解，Hadoop 是什么？简单来说，Hadoop 是适合大数据的 分布式存储平台 与 分布式计算平台 ，包含两个核心组件，即 HDFS 与 MapReduce。其中，HDFS 的全称是 Hadoop Distributed File System，表示一种 分布式文件系统 ，MapReduce 表示一种 并行计算框架 。它的创始人是 Doug Cutting，现在是 Cloudera 的首席架构师。再说一下关于 Hadoop 这个名字的趣事，Hadoop 的发音是 hædu:p，它的来源是这样的：Doug Cutting 的儿子在牙牙学语时，抱着一个黄色的小象玩偶，嘴里发出类似于 hædu:p 的发音，Doug Cutting 灵光闪现，就把当时正在开发的项目命名为 Hadoop。所以，这个名字不是一个缩写，也不是一个单词，它是一个虚构的名字。该项目的创建者，Doug Cutting 如此解释 Hadoop 的命名：首先它是我的孩子在玩耍时发出的声音，而我的命名标准就是简短、容易发音、拼写，小孩子都能发出来，就说明选对了。众所周知，给软件命名不是件太容易的事，要尽量找没有被使用过、没有带有特殊意义的词、不会被用于别处，否则把它写进了程序就可能会影响编程。Hadoop 的版本发布有两种：Apache 开源版本，官方版本 Cloudera 公司维护的打补丁版本，稳定、有商业支持，下载使用比较多HDFS 基础知识HDFS 是基于 Google 的论文 The Google File System 而开发实现的。 文件结构 HDFS 中的文件有自己独特的存储方式，一个文件会被划分为大小固定的多个文件块，称之为 block，分布存储在集群中的多个数据节点上，每个文件块的大小默认为 64MB【在 v1.x 中是这个默认值，在 v2.x 默认是 128MB】。 文件块的大小参数如下，单位是字节【134217728B 即 128MB】：1234&lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;134217728&lt;/value&gt; &lt;/property&gt;关于这个文件块参数的解释说明：The default block size for new files, in bytes. You can use the following suffix (case insensitive): k (kilo), m (mega), g (giga), t (tera), p (peta), e (exa) to specify the size (such as 128k, 512m, 1g, etc.), Or provide complete size in bytes (such as 134217728 for 128 MB).参考官网：hdfs-default.xml 。副本概念 同时，为了保证数据文件安全，避免丢失，同一个文件块在不同的数据节点中有多个副本。HDFS 设置副本数的参数是在 hdfs-site.xml 配置文件中，默认为 3，在一般场景下都是可以保证数据安全的。如果觉得浪费资源可以设置为 2，但是磁盘的价格是不贵的，多一份可以保障数据更加安全，当然，太多了也没有必要，会造成磁盘的浪费。1234&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;注意，HDFS 中的副本参数的概念不是中文语境下的复制次数，而是总的数量，例如把 dfs.replication 设置为 3，虽然翻译为副本数为 3，其实表示的总共有 3 份数据【主本 1 份 + 副本 2 份】。注意，如果按照中文语境下的含义来解释，副本数应该是 2，加上主本一共有 3 份数据，这看起来有歧义，也会令人疑惑。它不像在 Elastisearch 中设置副本的参数 number_of_replicas，表示的就是副本数，如果想保存总共 3 份数据，需要把 number_of_replicas 设置为 2 。下图中此参数的值设置为 1，即副本为 1，表示总共保存 2 份数据。文件系统 到了这里，我们会思考，文件结构好像还有点复杂，那么 HDFS 是怎么管理这些文件的呢？这是接下来的重点。HDFS 这个分布式文件系统遵循 主从结构 ，有一个主节点，称之为 NameNode【NN，管理节点】，有多个从节点，称之为 DataNode【DN，数据节点】。它们各自的作用如下。NameNode 节点的作用：接收客户端的请求 维护分布式文件系统的目录结构 管理文件与 block 之间的关系，管理 block 与 DataNode 节点之间的关系 DataNode 节点的作用： 真正存储文件 文件被分成文件块存储到磁盘上，文件块默认大小为 64MB，可以配置 为了保证数据安全，文件会被有多个副本，即备份，默认值为 3问题思考 1、如果从客户端上传一个文件，大小为 128MB，会有多少个文件块？ 答：根据文件块的大小，此时应该有 2 个文件块，但是由于副本的存在，总计是 6 个文件块。2、如果从客户端上传一个文件，大小为 65MB，会有多少个文件块？答：此时有 2 个块，一个块大小是 64MB，一个块大小是 1MB，后者占用的真实存储空间也是 1MB，并不是 64MB。副本也是同样的大小。3、如果从客户端再上传一个大小为 24MB 的文件，那么这个文件块会与 2 中的那个 1MB 的文件块进行合并吗？答：不会，文件块不会合并，它只是一个逻辑概念，实际占用的存储空间是以文件大小为准的。所以这个 24MB 的文件上传后是一个独立的文件块，占用存储空间大小为 24MB。那么有读者可能会疑惑，既然是这样，那这个文件块有什么必要，它的作用是什么？请看 Hadoop Community 的解释说明：The block size is a meta attribute. If you append tothe file later, it still needs to know when to split further - so it keeps that value as a mere metadata it can use to advise itself on write boundaries.可见，文件块的概念是一种 元数据 信息，这种概念在很多大数据框架中都能见到，它可以在追加文件时合理地给文件分块【随着对 HDFS 的深入理解，后面的博客内容还会解释文件块的合理性】。此外，使用 hdfs fsck 命令【也可以使用已经被废弃的 hadoop fsck】可以查看 HDFS 文件的详细信息。这里需要看两个重点内容：Total size:24135B、Total blocks (validated):22 (avg. block size 1097B)，其中，前者表示文件的总大小，总计 24135B，后者表示文件块的个数，有 22 个。但是留意一下最后还有一个备注信息 ：avg. block size 1097B，看起来像是文件块的大小【元数据】，其实不是，它是单个文件的平均大小【文件真实大小】，乘以 22 就等于前面的文件总大小。关于这一点，Hadoop Community 也给出过解释说明：The fsck is showing you an “average blocksize”, not the block size metadata attribute of the file like stat shows. In this specific case, the average is just the length of your file, which is lesser than one whole block.此外，从上图中还可以看到文件的 备份数 、 机架数 等信息。副本存放策略 副本的存放策略在不同版本之间存在差异，主要是很久之前的低版本与现在的版本有差异，主要版本分界点在 v0.17，下面就以默认的三份副本数为例描述副本的存放策略【如果有更多的副本数，参见 其它副本 的描述】。v0.17 之前【不包含 v0.17】：副本一：同 Client 一个机架的不同 DataNode 节点 副本二：同 Client 一个机架的另一个 DataNode 节点 副本三：不同 Client 机架的另一个 DataNode 节点 其它副本：随机挑选 v0.17 之后【包含 v0.17】： 副本一：同 Client 的 DataNode 节点 副本二：不同 Client 机架的一个 DataNode 节点 副本三：同副本二的机架中的另一个 DataNode 节点 其它副本：随机选择 这种副本存放策略有利于数据的安全，就算有的 DataNode 节点出问题，也不会引起数据丢失，哪怕事故很严重【例如某个交换机损坏】，导致整个机架的 DataNode 节点都出问题，也不会引起数据丢失，这也是副本设置为三份的好处。机架知识 上文的 副本存放策略 中出现了一个名词： 机架 【Rack】，下面使用文字与图片简单描述一下 机架 的概念，免不了还会涉及到 主机 【服务器】、 交换机 、 机柜 的概念。主机：物理概念，就是一台服务器，永远在运行。机柜：物理概念，有序存放主机的一个柜子，为了合理利用空间，一个机柜中可以存放多台主机。交换机：物理概念，连接多台主机的设备，用来给不同主机之间通信使用，一般有连接数量限制，要看交换机上面有多少个网口。机架：逻辑概念，使用一台交换机连接的所有主机以及机柜构成了一个机架，一个机架可以包含多个机柜、多台主机。下面给出一个简单的示意图：MapReduce 基础知识 MapReduce 是基于 Google 的论文 Simplified Data Proceessing on Large Clusters 而开发实现的。 基本概念 MapReduce 是一种编程模型，具体实现后是一种并行计算框架，用于大规模数据集的并行计算，过程可以拆分为： Map、Reduce，它是分布式计算的利器，采用分而治之的思维，节约内存【速度比较慢】，并行计算，适合海量数据的处理。 其它的计算框架都采用了类似的思想，理解了 MapReduce 就更容易在以后的时间学习其它的框架，例如 Spark、Hive、HBase。场景举例 例如当前有一个文本文件，每行有一个数字，求出所有数字中最大的那个，需要分别考虑文件大小为 1MB、1GB、1TB 等情况。如果文件比较小，可以随便读取文件的内容加载到内存中，然后遍历元素，进行简单判断，保留最大的那个数字即可。但是，当文本文件非常大的时候，是不可能加载到内存中的，此时需要采用基础算法中的一种常用思想： 分而治之 ，即把文本文件拆分为很小的多份文本文件，分别计算最大的数字，最后再汇总，汇总后数据量已经小了很多，最终再计算即可得出想要的结果。可以从下图看出这种简单的思路过程，比较符合人类自然的思考过程：当然，这种方式必然牺牲了时间，拆分文件越多越耗时，但是却节约了空间，不用很大的内存也可以处理海量的数据，这就是 MapReduce 的核心思想。主从架构 和 HDFS 一样，MapReduce 也是 主从架构 ，两种节点分别为 JobTracker、TaskTracker，前者称之为主节点【管理节点】，只有一个，后者称之为从节点【计算节点】，可以有多个。下面总结一下主节点、从节点各自的作用，读者可以与 HDFS 中的 NameNode、DataNode 的作用对比一下。JobTracker 的作用：接收客户端提交的计算任务 把计算任务分配给 TaskTracker 执行 监控 TaskTracker 的执行情况 TaskTracker 的作用： 执行 JobTracker 分配过来的计算任务 向 JobTracker 汇报任务的执行情况 读者可以思考一下，这种架构是不是很像生活中的领导与员工的关系，或者更为具体一些，像是产品经理与工程师之间的关系，一个人负责接收需求、分配任务、监督执行情况，另外一群人负责具体实现，并定时汇报进度。]]></content>
      <categories>
        <category>Hadoop 从零基础到入门系列</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
        <tag>HDFS</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis Desktop Manager 的安装及使用]]></title>
    <url>%2F2017030301.html</url>
    <content type="text"><![CDATA[在技术类岗位的工作中，一般都会使用到 Redis 这个非关系型数据库，在很多场景下它会被作为一个缓存数据库使用。而如果在日常工作中接触到了 Redis，哪怕使用得不是很深入，也需要了解一些 Redis 的基础知识以及常用的命令，以备不时之需。然而，为了方便使用 Redis，还有另外一条路可以选，那就是借助可视化管理工具，让新手或者非技术人员也可以轻松使用 Redis 数据库【例如产品经理、测试人员都可以灵活查询数据库】。而在众多的可视化管理工具中，Redis Desktop Manager 又是比较好用而且轻量的一款工具。本文除了简单介绍一下 Redis 的基础知识，其它篇幅主要讲解这款工具的安装使用，环境基于 Windows 10 X64，Redis Desktop Manager 的版本为 v0.8.8。数据库的知识入门 在这里，我先简单介绍一下数据库分类的入门知识，方便大家理解。关系型数据库 如果是学计算机技术相关专业的人，或者学统计学、数学应用专业的人，一定会接触到数据库的知识，而且一定用过数据库的产品，否则日常的学习无法进行下去。例如在做统计分析的时候，在数据量比较小的情况下，可能使用 Excel 或者 csv 即可完成，但是如果数据量稍微大一点【例如几十万上百万】，此时再使用文件的方式处理数据就比较吃力了，而且效率低下。那么，在这种情况下可以顺势使用数据库【而且一般都是关系型数据库】，数据处理起来迅速而且方便。上面提到的 Excel、csv、数据库，在日常工作学习中，一般需要用到它们时处理的都是结构化的数据，通俗点说就是数据的格式是规范的行列形式，数据一共有 m 行 n 列，很规范，每一行是一条数据，每一列是一个属性，这也符合人类的基本认知。而为了方便存储、分析这类数据，产生的数据库都是关系型数据库，比较常见的产品有：Oracle【甲骨文公司出品】、MySql【免费开源，小巧好用】、SQL Server【微软公司出品】。使用这些产品，你就可以把数据直接导入数据库，然后做查询分析，例如全校的学生信息、成绩信息、选课信息等。非关系型数据库 但是，在某些场景下，关系型数据库的缺点暴露了出来，或者说关系型数据库根本无法适应这些场景。最常见的场景就是在当前的互联网环境下，每天会有海量的数据产生，而且数据格式五花八门，为了分析、存储这些数据，只靠关系型数据库根本行不通。一是需要高并发的读写需求【分布式读写】，二是需要高效率的读写需求【高速读写】，三是需要高扩展性【例如在数据量增加的情况下灵活扩展资源、数据结构灵活变更】，四是需要高可用性【负载均衡、备份迁移】，在这些方面，传统的关系型数据库产品无法应对。此时，非关系型数据库的概念也就出来了，相应的产品也就问世了，例如 MongodDB【可以在海量的数据中快速地查询数据】、HBase【海量非结构化数据的分布式存储，可扩展】、Neo4j【高性能的 NoSql 图形数据库】、Redis【应用广泛，具有极高的并发读写性能】。如果要提一个非关系型数据库与关系型数据库最直观的不同点，则可以说是数据结构，非关系型数据库的数据结构不固定，可以根据实际场景灵活变更。例如在 HBase 中，列【在 HBase 中称为 colomn qualifier，另外还有一个列簇的概念 colomn family】的个数可以任意指定【这种特性称为列式存储】，列名称也可以任意指定，不会受到表结构的限制。还有一点需要说明，非关系型数据库出现的时间比较短，而且为了应对多样的实际场景，产品众多，不像关系型数据库那样只有几款产品就可以一统江湖，这也是非关系型数据库的天然特性。但是，非关系型数据库的产品大多数都是开源的，任何人都可以使用。Redis 介绍 Redis 是一款具有高性能并发读写的 key-value 数据库，而且是开源的。如果大家没有用过它的话，我也无法给大家描述清楚它的概念，只好列举一下它具有的特点【没有使用过的人可以了解一下】，如下： 基于内存 支持数据持久化，可以将内存中的数据保存在磁盘中，重启的时候再次加载 数据类型丰富，除了 String，还有 Set、List、Hash 等结构 高性能读写 原子性，所有的操作都是原子性的 丰富的特性，例如 publish/subscribe、key 过期等 常用命令举例 我在这里只是列举一种数据结构【List】的常用命令，其它的可以举一反三，或者根据实际需要查看备注中给出的帮助文档。llen key，获取 List 的长度 lpush key value，从左侧添加一个元素，key 不存在则新建lindex key index，在 key 存在的情况下才从左侧添加一个元素，否则不添加lpop key，弹出左侧的第一个元素lrange key start stop，根据下标获取所有元素，从 0 开始，-1 表示最大下标lset key index value，在指定下标添加一个元素linsert key BEFORE|AFTER pivot value，从左侧开始，在指定值的前 / 后插入一个元素lrem key count value，从左侧开始，移除指定数量的值等于指定值的元素 可以看到命令都以字母 l 【单词 left 的首字母，L 对应的小写字母】开头，表示所有的操作都是从 List 的左侧【小索引的位置】开始，如果把 l 改成 r 则表示操作从右侧【大索引的位置】开始。例如 lpush 表示给 List 的 0 号位索引添加一个元素，而 rpush 表示给 List 的最大索引位置添加一个元素。可视化管理工具的安装使用 就像在使用关系型数据库的时候，有众多的可视化管理工具可以使用，例如：MySQL Workbench、Navicat、phpMyAdmin 等等。类比一下，在管理 Redis 的时候，就可以使用 Redis 的管理工具 Redis Desktop Manager，以下内容介绍这款工具的安装使用，只是入门级别的使用，零基础完全可以上手。安装 从官方网站购买下载，下载地址 ，安装包不大，大概 27MB。下载完成后直接双击应用程序，根据引导完成安装即可，注意根据实际需要选择安装目录。 下载程序 双击安装 选择安装目录 安装完成 启动主界面 创建连接 打开主界面，可以看到左下角有一个绿色的加号，并标识：Connect to Redis Server，也就是创建连接，直接点击加号，弹出一个对话框，里面填写连接 Redis 的基本信息：连接名称、主机、端口、认证信息。创建连接 请大家根据实际情况填写参数，我填写的如下图，连接名称可以是任意的字符串，但是为了有意义不要随便起名字，以免混淆。填写连接信息 本来填写完参数就可以直接创建连接了，但是实际的场景可能没有这么简单，可能工作环境不允许直接从本机连接远程的 Redis 服务器，而是需要经过 SSL 秘钥 、SSH 隧道 【SSH Tunnel】等认证方式。无论是哪种认证方式，都需要额外配置，例如我的环境需要 SSH 隧道 认证，其实就是经过一个中间的代理服务器去连接真实线上环境的 Redis 服务器，都是为了安全。因此，我需要额外配置，在 SSH Tunnel 选项卡中填写补充信息，需要填写 SSH 主机的地址、端口、用户名、密码等，记得勾选 Use SSH Tunnel 选项。填写 SSH 隧道信息 所有的信息填写完成后，不要着急点击 OK 创建连接，可以先点击左下角的 Test Connection 来测试一下能不能连接成功，用测试连接的结果来验证参数填写是否有误。如果连接失败，会显示失败的具体原因，例如认证不通过、找不到主机、端口访问拒绝等错误，遇到错误再根据实际情况解决即可。下图是我的测试连接，直接成功通过。测试连接成功 最后一步，点击 OK 按钮，创建连接成功，可以看到主界面左侧的面板中有一个连接，它的名字就是刚才指定的连接名字。主界面左侧面板 使用 在主界面的左侧面板中，选择任意一个连接【如果有多个连接的话】，鼠标左键选中时它会自动连接，可以在主界面的中部下方看到有一个 System log 的选项卡，里面会实时输出打印连接日志。打开连接后，可以看到 Redis 数据库的默认 16 个桶，编号从 0 到 15，一般只会用到其中的一个桶，可以看到我这里有两个桶【0 号和 3 号】被使用。打开连接 如果需要进一步操作数据，直接在展开的树形结构中，选择需要操作的 key，然后可以使用右侧的操作按钮直接操作，例如 删除 、 添加 、 设置 TTL 值 等，也可以使用鼠标右键选择对应的功能。查看 Redis 数据 到这里可以看出，一切操作都是肉眼可见的，很方便而且很容易理解，非技术人员也可以熟练操作。但是，如果你作为一个技术人员，觉得这样用鼠标点来点去很麻烦，想直接使用命令操作怎么办？有办法，这款工具也支持使用命令行操作。在选中连接后，使用鼠标右键打开选择列表，选择 Console 打开连接，即表示连接后进入命令行。打开命令行 接着在主界面的下方就会打开一个名字和连接名字一样的选项卡，里面的背景是灰黑色的，可以看到里面有一个 Connected 关键词，这就是进入命令行的样子，接着就可以自由自在地敲下你熟悉的命令了。使用命令 备注1、Redis Desktop Manager 官方网站：https://redisdesktop.com ，它是一款收费的软件，价格不贵，不过项目是开源的，贡献代码可以免费使用一定的时间【目前是一年】。2、Redis 命令大全，参见官方网站：https://redis.io/commands ，列举了十几个系列的命令，总计两百多个命令，请根据实际情况查询使用。]]></content>
      <categories>
        <category>大数据技术知识</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>Manager</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA 热部署配置方法总结]]></title>
    <url>%2F2016120801.html</url>
    <content type="text"><![CDATA[前不久临时在搞一个 Java Web 项目，需要做一点点修改，由于对原有的代码不熟悉，所以附带需要大量的测试，搞清楚程序执行的流程。在一开始的操作过程中，我就是不断修改代码，然后关闭 Tomcat 服务器，再重启，操作了几次我就不想这么干了，太浪费时间了。大量的时间都用在了关闭重启服务上面，在最新更改的代码没有加载完成前，只能干等着，后面我就发现了有 热部署 这个技巧，可以节省大量的时间。本文就记录在 IDEA 中热部署的配置方式，操作系统环境基于 Windows7 X64，Web 容器基于 Tomcat 6.x。大背景 在 Web 开发中，如果需要调试最新更新的代码，最先想到的思路就是重新启动 Web 容器，以加载最新的资源文件。但是显然，这种做法是很浪费时间的，而且当 Web 项目整体比较大的时候，重启一次需要很长的时间，更加凸显了这种做法的低效。回头思考一下，有时候只是更改了某个方法的几行代码，却要重启整个服务，这动作太大了，肯定有更加方便快捷的方式来做这件事，例如 热部署 。还有，有时候更新的不一定是 Java 类文件【或者其它编程语言的后台资源文件】，而是 HTML 静态文件、JavaScript 静态文件、项目配置文件【例如 Spring 的配置文件、日志配置文件】等，这时候是不是一定要重启 Web 服务呢，能不能也做到热部署，其实是可以的。热部署配置 热部署基础配置 如果还没有为 Web 项目配置好 Tomcat 服务器，可以在 Run -&gt; Edit Configurations 中先把 Web 服务器的基本信息【例如本地服务器安装目录、默认浏览器、服务端口、JRE 环境】配置好。具体配置可以参考下图：名字、本地安装的服务器、启动的浏览器、本地 JRE 环境、HTTP 服务端口。配置完成 Tomcat 服务器的基本信息，再接着配置部署优化信息，如图打开 Tomcat 的 Edit Configurations。如果刚刚执行完前面的 Tomcat 基础信息配置，可以不用关闭窗口，直接可以进入下一步骤进行配置，如果已经使用过 Tomcat 服务器，可以按照如图快捷方式打开配置窗口。打开 Deployment 选项卡，默认在启动项【startup】里面是没有任何东西的【面板中部也提示 Nothing to deploy】，需要手动添加，也就是为 Tomcat 服务器添加一个应用，这样在 Tomcat 启动时就会加载这个应用，也可以说是把这个应用部署到 Tomcat 服务器上面。点击右侧的绿色小加号，选择 Artifact。接着选择部署应用的类型，建议选择 exploded 的类型，不需要单纯的 war 包类型，这个相当于更改 Tomcat 的 CATALINA_HOME，效率比较高一点，选择后点击 ok 确认即可。配置完成 Deployment 信息后，如果有默认的 Make 操作记得删除【在 Deployment 选项卡的底部，有一个 Before launch 清单】，可以提高效率，只保留项目的 exploded 类型即可，选中后使用红色的减号来删除。接着返回 Server 选项卡，可以看到有两项重要的配置：On &#39;Update&#39; action、On frame deactivation，如果在前面的 Deployment 选项卡中没有配置 exploded 类型的 war 包的话，这里是不会有 On frame deactivation 这个配置项的。接着把这两个重要的配置选项都设置为 Update classes and resources，否则类修改热部署不会生效，或者第三方模版框架例如 Freemarker 的热部署也不会生效。配置完成了以上信息，在 Debug 模式下，IDEA 失去焦点时【场景：开启 Debug 模式，更改完代码去浏览器操作测试】，则会自动加载更新从而达到热部署的效果。不难发现，这个过程还是有点缓慢，原因是什么呢，因为前面的配置效果都是基于 JVM 提供的热加载来实现的，仅支持方法块内代码的修改，并且只有 Debug 模式下，同时是在 IDEA 失去焦点时【On frame deactivation 配置产生的效果】才会触发热加载，相对来说整个流程的速度仍旧略显缓慢。那么，怎么优化这个流程呢，继续往下面看。热部署进阶配置 前面提到的热部署速度仍旧缓慢，必须在 Debug 模式下并且焦点离开 IDEA 时才会触发热加载，是因为还没有开启 IDEA 的自动编译功能。众所周知，在 Eclipse 中是默认开启自动编译的特性的，也就说你只要更改了项目的文件，点击保存，Eclipse 就会立即执行编译操作，把文件编译一遍，虽然优点消耗资源，但是能在开发人员无感的情况下保证所有的执行文件都是最新的，避免诡异的问题出现。但是在 IDEA 中，自动编译这个特性默认是关闭的，也就是说如果你更改了文件，但是没有及时编译，等到运行时使用的仍旧是上次的可执行文件，这就会导致一些诡异的现象发生。例如刚刚改了代码然后运行，发现运行的代码逻辑和更改的不一致，这就是因为虽然原文件被改了【肉眼可以看到改变】，但是可执行文件仍旧是旧的【运行程序观察到的现象是没变】。举个具体的例子就是在 IDEA 中写 Maven 项目，尽管在每次 Run 的时候 IDEA 都会重新编译 Java 类文件，保证运行的 class 文件都是最新的，但是有时候不知道怎么回事运行的 class 文件并不是最新的，我一直认为这是 IDEA 的 bug，另一方面，如果变更的是 xml 配置文件，IDEA 也会漏掉，因此，此时运行前最好先执行一下 mvn clean 来清空一下项目的编译结果。下面开始进入正题，记录开启 IDEA 自动编译的方法。首先设置自动构建【包含编译】，在 Settings -&gt; Build，Execution，Deployment -&gt; Compiler 中，勾选 Build project automatically，这个配置项表示自动构建项目，但是仅在项目没有运行或者 Debug 的状态下才会有效。那这样肯定不行，因为热部署就是表示项目一直在运行中，热加载更改代码后自动编译的文件，这个设置在运行状态下不会自动编译，别急，还有下一个步骤的设置。设置允许运行中的程序自动编译，先使用 Ctrl + Shift + a 调出搜索 Action 的对话框【这个快捷方式是基于 Windows 平台的 Eclipse 快捷方式，也可以在主界面根据 Help -&gt; Find Action 进入】，在里面搜索 Registry，选择结果中的 Registry…，接着就会进入详细设置页面。在详细设置页面继续搜索 compiler.automake.allow.when.app.running，直接输入即可，不需要搜索框，或者输入关键词 app.running，会在左上角显示搜索内容：Search for: app.running，下面也会实时展示搜索结果。看到我想要的结果了，直接勾选 compiler.automake.allow.when.app.running，这就表示允许在程序运行时自动编译，但是还要留意一点，看到最上面的红色字体的提示：Changing these values may cause unwanted behavior of IntelliJ IDEA.Please do not change these unless you have been asked，其实就是在警告你不要随意更改这里面的配置，可能会对 IDEA 造成影响，除非你完全了解你更改的内容。关于这个选项的含义，可以直接看最下面的 Description 里面的描述：Allow auto-make to start even if developed application is currently running. Note that automatically started make may eventually delete some classes that are required by the application.。配置效果总结 如此一来，每当我的 Web 项目里面的 Java 文件、JavaScript 文件等资源更新时【例如更改了代码，重新配置了参数等】，Tomcat 服务会重新载入这些原始文件对应的编译文件，从而达到热部署的效果，这样我就可以不用在每次更改了一点东西，想测试一下效果，还需要关闭重启，浪费时间。Tomcat 热部署时重新载入资源给出的提示信息 还要注意一点，在 Debug 时，为了验证刚刚更改的代码有没有被热加载，可以添加一个断点，看看断点的状态有没有生效，即在红色的断点标记处有一个对勾标识【√】，如果没有对勾也没有叉【×】，而是一个单独的红圆圈，说明更改的代码没有生效。备注1、如果需要方便地调试 JavaScript 代码【需要在 Server 配置中勾选 with JavaScript debugger】，还可以借助 IDEA 的官方浏览器插件：JetBrains IDE Support ，可以很方便地对静态资源进行调试。2、另外关于热部署还有一款超级好用的工具：JRebel ，可以使用 Tomcat 参数配置的方式或者 IDEA 插件的方式，这款工具不是免费的，可以免费试用一段时间，请大家支持正版。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>IDEA</tag>
        <tag>Tomcat</tag>
        <tag>deploy</tag>
        <tag>JRebel</tag>
      </tags>
  </entry>
</search>
