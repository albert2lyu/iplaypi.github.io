<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[使用 Java 代码迁移微博图床到 GitHub 图床]]></title>
    <url>%2F2019050201.html</url>
    <content type="text"><![CDATA[由于微博图床开启了防盗链，导致我的博客里面的图片全部不可见，因此要切换图床。当然，一开始我使用的是极其简单的方法，直接设置博客页面的 referer 属性即可【设置为 noreferrer】，这样微博图床就检测不到引用来源，也就不会拒绝访问了。但是后续又遇到了其它问题，这些内容我在前几天的博客里面都记录了：解决微博图床防盗链的问题 。后来我实在找不到更为恰当的解决方案，于是决定直接迁移图床。本来一开始准备使用 PicGo 这个工具，但是发现有问题，在我比较着急的情况下，决定自己写一写代码，完成迁移操作。本文就记录这些代码的逻辑。 代码结构 写代码也比较简单，主要有四个步骤：读取 markdown 文件内容并利用正则抽取微博图床的图片链接、下载所有图片并上传至 GitHub、替换内容中抽取出的所有图片链接为 GitHub 的图片链接、内容写回新文件。使用 Java 处理不需要多少代码，主要要依赖几个 jar 包：处理文件的 io 包、处理网络请求的 httpclient 包、处理 git 的 jgit 包。在代码的细节中，可以看到我是每个文件单独处理的，比较耗时间的就是下载图片、上传到 GitHub 这两个过程，而且由于我是文件分开处理，所以总的时间更长了。如果想节约点时间，可以一次性把所有的图片全部下载完成，最后一次提交到 GitHub 即可，这样就节约了多次频繁地与 GitHub 建立连接、断开连接所消耗的时间，如果是几次提交无所谓，但是几十次提交就多消耗很多时间了。例如按照我这个量，78 个文件，500-600 张图片，运行程序消耗了十几分钟，但是我估计如果一次性处理完成，时间应该在 5 分钟以内。代码放在 GitHub 上面，仅供参考。并且新建一个项目，专门用来存放临时代码小工具的。迁移结果 在 GitHub 的仓库中查看。图。。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>weibo</tag>
        <tag>GitHub</tag>
        <tag>image</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决微博图床防盗链的问题]]></title>
    <url>%2F2019042701.html</url>
    <content type="text"><![CDATA[对于不少自己搭建博客的人来说，图床的选择可真是一个大难点，以前还有各种免费好用的图床工具，例如七牛云、又拍云、SM.MS、Imgur、GitHub、微博图床等，当然还有腾讯云、阿里云的云存储服务，但是免费的意味着不稳定，说不定哪天图片就没有了，有一些国外的访问速度又不行，国内的云存储服务商收费又比较高，还有的必须绑定认证的域名才能使用。本来搭建一个小小的博客，只为了记录知识，传播技术，遇到耗财或者耗精力的这种问题，都比较头疼。后来纠结了好几天，最终决定使用免费的 微博图床 ，一是因为新浪微博这家厂商体量大，微博图床短期内应该不会出问题，二是看到好多网友说他们已经稳定使用微博图床 3-5 年了，没有出过问题。我大概使用的时间还没有一年，以前都是本地化的，没有整理成完整的文章，后来开始慢慢整理并部署上线。没想到最近【2019 年 4 月 24 日左右发现】微博图床出问题了，访问图片链接全部是返回 403 状态码，表示拒绝访问，其实是微博图床开启了防盗链，本文就记录这个现象以及可行的解决方案。微博图床防盗链开启 初始现象 在 2019 年 4 月 24 日的时候，我发现一个严重的问题，我的博客里面的图片显示不出来了，并不是被封了，如果被封也会显示图片的，只不过是马赛克图片。发现这个问题的缘由是新写了一篇博客，本地生成测试的时候，发现图片全部不显示了，一开始还以为是网络问题。博客里面的图片全部无法正常显示 接着我随机抽了一些图片链接在浏览器中直接打开看，发现是可以看到图片的，然后在博客中还是看不到图片，如果在博客中选择图片链接，使用右键 在新标签页中打开 ，也是不能看到。这就说明微博图床开始检测请求的合法性了，对于不正常的请求统统拒绝。当然，如果直接使用图片的链接在浏览器中单独打开，是可以看到图片的，紧接着在博客中就可以看到对应的图片了，但是这并不是说明图片可以使用了，其实是浏览器的缓存作用，如果及时清除浏览器的缓存，发现又不能使用了。复制图片地址在浏览器中打开，图片可以正常显示 分析现象 接着使用浏览器的调试工具查看详细的请求信息，按 F12 按键，调出调试工具，刷新网页，使用 jpg 过滤无效内容，可以看到所有的图片访问请求结果都是 403，也就是拒绝访问。随便点开一个链接的请求信息，查看 Status Code 为 403，也就是拒绝访问，注意查看请求头的 Referer 参数，值是一个链接，表示当前请求所属的页面，即 引用来源 ，而新浪微博恰好会检测这个参数，拒绝所有的外链请求，即不是从新浪的站点发送的图片请求。原来，近期微博图床对图片 CDN 添加了引用来源【Referer】检测，非微博站内引用将会返回 403 错误码，即拒绝访问。那能不能伪造或者清除这个参数呢，其实是可以的，只不过伪造、清除都需要增加一些 Javascript 动态脚本来处理，需要一些技术支持。如果选择清除 Referer 参数，可以先验证一下，把图片的链接直接复制到浏览器中访问，就不会有这个参数，发现可以正常访问，没有 403 错误。注意，一开始我发现使用浏览器能直接访问，紧着着博客里面的图片也能访问了，我还以为是需要单独访问一次图片，然后就可以任意访问了，后来发现其实是浏览器缓存的作用，空欢喜一场。也看到有说法是，微博图床仅仅针对开启 SSL 的链接【即 HTTPS】实行站外禁止访问，而普通的 HTTP 链接仍旧安然无恙，这种说法是错误的【但是确实有这种现象出现】。我测试了一下，的确是有这样的现象，前提是来源页面开启了 SSL，而图床链接使用基本的 HTTP，这样的话由于 Referer 的特性，请求图片链接时不会传输 Referer 这个参数的值【即来源页面的信息不会传递给请求页面】，微博图床自然也就无法检测了。所以最简单的方案就是把所有微博图床的链接全部由 HTTPS 替换为 HTTP，但是由于我的博客全面开启了 SSL，为了加绿锁，因此不引用普通的 HTTP 链接，这种简单的方案我就无法采用了，只能遗憾舍弃。解决方案 考虑切换图床，免费的已经基本没有了，收费的比较贵，或者找到方案先临时使用，不然会给查看博客的人带来很大困扰，毕竟没有图片的博客怎么能看，这也影响博客的质量与声誉。尝试清除来源引用 在静态网页的 头部 代码中【即 head 标记】添加如下配置项：1&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot; /&gt;它的作用就是阻止浏览器发送 Referer 信息，对整个页面的所有链接生效【当然也有针对单个链接设置的方法：&lt;a rel=”noreferrer” href=”your-website-url” /&gt;，这里不采用】，这样一来微博图床就不知道请求的引用来源了，可以达到和直接在浏览器中访问一样的效果。 但是要注意，不是每种浏览器都支持这种语法的，此设置对有的浏览器来说无效。那么在 Hexo 框架中怎么增加呢，显然不会有相关配置项，只能更改源代码，而且使用了 Next 主题，应该要更改主题的源代码，以保证 Hexo 在渲染静态页面为每个页面都增加这个配置。查阅文档，了解了渲染模板所在位置，打开 themes/next/layout/_partials/head.swig 文件，在里面添加 meta 标记就行。修改完成后查看页面的源代码，已经有这个属性了，并且所有的图片都可以正常访问了，完美。但是我觉得这肯定不是长久之计，以后说不定还会有幺蛾子出现，所以要随时准备着。尝试其他方案 先观察一段时间，这段时间要考虑其他方案的可行性和成本。建议 微博图床开启防盗链，个人博客对于新浪图床的依赖时代基本要告别了，虽然有其他免费图床可以使用，但稳定性和可持续性上显然无法与大企业维护的图床相比。为了博客内容稳定考虑，还是考虑后续逐渐把图片迁移到其他云存储空间，费用方面能承受就行。其他知识点 微博图床简单介绍 对于大多数个人博客维护者而言，免费的图床既节省成本，也能够提升页面访问的速度，而新浪微博图床则成了首选。新浪微博由于本身体量大，其图床免费无限容量，只需要有一个微博账号就可使用。同时具备全网 CDN 加速，支持 HTTPS，无论是国内还是国外网络访问，速度都很不错。而且新浪如此企业，不会像其他个人或者团队经营的免费图床一样随时可能会关掉。基于这些优势，不少人会优先选择新浪微博图床作为网站提供图片服务。毕竟直接挂 CDN 或者自建图床的话，也是一个持久的付费维护，如果一旦被攻击，更是造成费用暴增。RefererReferer 首部包含了当前请求页面的来源页面的地址，即表示当前页面是通过此来源页面里的链接进入的。服务端一般使用 Referer 首部识别访问来源，可能会以此进行统计分析、日志记录以及缓存优化等。同时也让服务器能够发现过时的和错误的链接并及时维护。需要注意的是 referer 实际上是 referrer 的误拼写，它可能是 HTTP 协议中第一个被拼写错误的标准头，为保持向下兼容就将错就错了。可以参见 RFC 文档的 Referer 的介绍：https://tools.ietf.org/html/rfc2616#section-14.36 ，原文有这样的描述：the “referrer”, although the header field is misspelled.此外还可以参考维基百科的相关介绍：https://zh.wikipedia.org/wiki/HTTP% E5%8F%83% E7%85% A7% E4% BD%8D% E5%9D%80 。在以下几种情况下，Referer 不会被发送 来源页面采用的协议为表示本地文件的 file 或者 data URI当前请求页面采用的是非安全协议，而来源页面采用的是安全协议【HTTPS】为整个页面设置 &lt;meta name=”referrer” content=”no-referrer” /&gt;为单个链接设置 &lt;a rel=”noreferrer” href=”your-website-url” /&gt;注意第二种情况，如果你的博客开启了 SSL，可以使用 HTTP 的图片链接，就可以正常访问了。但是要牺牲你的博客的安全性，因为浏览器会检测到你的博客内容里面有普通的 HTTP 链接，就会导致不可信【尽管存在有效的证书，也没有用】，小绿锁会消失，并给出警告。例如我的博客，为了测试，使用了一个 HTTP 图片链接，其它的图片都是 HTTPS 链接，可以发现 HTTP 的图片可以正常访问，其它的图片仍旧被拒绝访问了。此时，发现博客的小绿锁已经没有了，并且给出了警告提示。后记 使用上述的解决方案后，我又发现了一个严重的问题，由于清除了引用来源 referer，博客文章的地址就不会发送出去，导致我的 不蒜子 统计失效，也就是每篇文章的阅读数、整个站点的访问量【pv】、整个站点的访客数【uv】都会停止统计。这会导致整个博客的动态流量不可见，对于写博客的我来说内心会有一点点失落，所以我要想办法解决这个问题。已经知道问题的根源了，解决起来也是很容易的，直接开启引用来源 referer 即可，但是由于和微博图床的图片防盗链冲突，不能同时开启。也就是说除了微博图床的防盗链要关闭 referer，其它的链接仍旧正常开启，看看能不能想办法只把微博图床的链接关闭 referer。标记 a 可以增加 ref=”noreferrer” 属性，但是在 Hexo 中我无法找到合适的方式来完成这个操作。本来准备在 _macro/post.swig 中对渲染后的标记属性进行替换，示例 swig 语句：1&#123;&#123; post.content|replace (&apos;group&apos;, &apos;noreferrer&apos;, &apos;g&apos;) &#125;&#125;把 ref=”group” 替换为 ref=”noreferrer”，但是测试后发现行不通，传递过来的 content 只包含 p 标记，并没有 a 标记，也就是说明 a 标记是在其它地方渲染的。而如果直接在渲染标记 a 的地方进行选择性替换，发现微博图床的图片链接，就把 ref 属性替换掉，需要去更改 Hexo 的源代码，其中有一个 markdown (str) 方法，显然这种临时方案不合理，也很麻烦。为了稳定地解决这个问题，我还是决定更换图床，然后使用第三方工具进行图片迁移。更换图床 和以前一样，挑选了一圈，也是很纠结，最终还是下定决心直接使用 GitHub 了，稳定又方便。其实就是新建一个仓库，专门用来存放图片，只不过需要考虑一下图片过多、图片过大会不会被 GitHub 限制。去 GitHub 搜索帮助文档，帮助文档信息 ，可以得知仓库最大为 100GB，但是官方建议保持在 1GB 以下，单个文件低于 100MB，因此用来存放文件绰绰有余。另外需要注意，仓库文件超过 1GB 时会收到 GitHub 的提醒邮件，超过 75GB 时，每次在提交时都会收到警告。 原文描述如下：We recommend repositories be kept under 1GB each. Repositories have a hard limit of 100GB. If you reach 75GB you’ll receive a warning from Git in your terminal when you push. This limit is easy to stay within if large files are kept out of the repository. If your repository exceeds 1GB, you might receive a polite email from GitHub Support requesting that you reduce the size of the repository to bring it back down.In addition, we place a strict limit of files exceeding 100 MB in size.既然有这种限制，最好还是把图片压缩一下，推荐使用图片压缩工具：Imagine ，这个工具可以实时看到压缩效果，而且压缩率还不错，能到 50%。但是，如果想要保持图片的色彩度、还原度，压缩效果肯定是不行的，甚至有时候压缩后的图片比压缩前的还大。压缩图片示例 迁移图片 迁移图片本来是个很麻烦的事情，要把图片迁移、博客文章里面的链接替换掉，但是还好有现成的工具可以使用，在这里推荐：PicGo ，这个工具本来不是做图片迁移的，仅仅是图片上传生成链接而已，但是有人开发了插件，专门用来迁移 markdown 文件里面的图片，会自动迁移图片并且更新 markdown 里面的图片链接。这个插件是：picgo-plugin-pic-migrater ，而且，还可以支持批量迁移，指定一个文件夹，直接迁移文件夹里面的所有 markdown 文件。迁移过程 详细的迁移步骤就不再记录，几个重要的步骤：在 GitHub 建立仓库、使用 PicGo 工具迁移图片，重新整理 markdown 文件。操作前切记备份好自己的 markdown 文件，以免迁移出现问题导致文件丢失。在使用 PicGo 的过程中，发现总是迁移失败，重试了多次之后确定是因为在 markdown 语法中增加了注释，相当于给图片链接增加了 alt 属性【生成时图片会有一个 img 标记】，导致 PicGo 的插件识别不了，迁移失败。我已经在 GitHub 的项目中提了 issue：https://github.com/PicGo/picgo-plugin-pic-migrater/issues/1 ，作者也回复了，后续会修复。而我比较着急，等不了，又不可能把这些注释全部清除，也不好，所以我决定自己迁移，通过 Java 写代码解决。写代码也比较简单，主要有四个步骤：读取 markdown 文件内容并利用正则抽取微博图床的图片链接、下载所有图片并上传至 GitHub、替换内容中抽取出的所有图片链接为 GitHub 的图片链接、内容写回新文件。使用 Java 处理不需要多少代码，主要要依赖几个 jar 包：处理文件的 io 包、处理网络请求的 httpclient 包、处理 git 的 jgit 包。详细内容可以参考我的另外一篇博客：使用 Java 代码迁移微博图床到 GitHub 图床 。 小细节 针对 PicGo 的使用还有一些小细节可以注意一下：自定义域名、子文件夹路径、图片压缩【不压缩针对 GitHub 速度会很慢，能压缩到 200KB 最好】、文件重命名。未来考虑 迁移完成之后，以后新的图片就直接使用 PicGo 上传到 GitHub 图床了，同时需要注意区分子文件夹。在 GitHub 仓库中，暂时每年新建一个文件夹，以年份数字为名称。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>weibo</tag>
        <tag>https</tag>
        <tag>referer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mapreduce 错误之 bin bash-line 0-fg-no job control]]></title>
    <url>%2F2019042401.html</url>
    <content type="text"><![CDATA[今天在开发 mapreduce 程序的过程中，为了快速开发，程序的整体框架是从别的业务复制过来的，自己增加一些数据处理逻辑以及环境的参数配置。接着就遇到问题，在本地本机测试的时候，Job 作业无法启动，总是抛出异常，然后进程退出。本机系统为 Windows 7 X64。异常错误信息简略如下：12Exit code: 1Exception message: /bin/bash: line 0: fg: no job control本文记录这个现象以及解决方案。问题出现 在本地本机启动 Job 时无法正常运行作业，直接抛出异常后退出进程，完整错误信息如下：12345678910111213141516171819202122Diagnostics: Exception from container-launch.Container id: container_e18_1550055564059_0152_02_000001Exit code: 1Exception message: /bin/bash: line 0: fg: no job controlStack trace: ExitCodeException exitCode=1: /bin/bash: line 0: fg: no job control at org.apache.hadoop.util.Shell.runCommand (Shell.java:576) at org.apache.hadoop.util.Shell.run (Shell.java:487) at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute (Shell.java:753) at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer (DefaultContainerExecutor.java:212) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call (ContainerLaunch.java:303) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call (ContainerLaunch.java:82) at java.util.concurrent.FutureTask.run (FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624) at java.lang.Thread.run (Thread.java:748)Container exited with a non-zero exit code 1Failing this attempt. Failing the application.2019-04-22_22:46:04 [main] INFO mapreduce.Job:1385: Counters: 0其中的重点在于：Exception message: /bin/bash: line 0: fg: no job control，由于我不了解这种错误，只能靠搜索引擎解决了。问题解决 问题解决很容易，在 Job 的配置中增加一项：mapreduce.app-submission.cross-platform，取值为 true，截取代码片段如下：1234Configuration conf = job.getConfiguration ();conf.set (&quot;mapreduce.job.running.map.limit&quot;, &quot;50&quot;);// 本机环境测试加上配置，否则会抛出异常退出：ExitCodeException: /bin/bash: line 0: fg: no job controlconf.set (&quot;mapreduce.app-submission.cross-platform&quot;, &quot;true&quot;);这个配置的含义就是跨平台，保障 Job 作业可以在 Windows 平台顺利运行。备注 参考：stackoverflow 讨论一例 。]]></content>
      <categories>
        <category>踩坑系列</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于 httpcore 的 Maven 依赖冲突问题解决]]></title>
    <url>%2F2019042201.html</url>
    <content type="text"><![CDATA[今天，又遇到一个 Maven 冲突的问题，这种问题我遇到的多了，每次都是因为项目依赖管理混乱或者为新功能增加依赖之后影响了旧功能，这次就是因为后者，新增加的依赖的传递依赖覆盖了原有的依赖，导致了问题的产生。大家如果搜索我的博客，搜索关键词 maven 或者 mvn，应该可以看到好几篇类似的文章，每次的情况都略有不同，每次解决问题的过程也是很崩溃。不过，每次崩溃之后都是一阵喜悦，毕竟感觉自己的经验又扩充了一些，以后遇到此类问题可以迅速解决。问题出现 写了一个 mapReduce 程序从 HBase 读取数据，写入到 Elasticsearch 中，整体的框架是从别的项目复制过来的，自己重写了处理逻辑以及环境相关的参数，但是跑起来的时候，map 过程很顺利，几百个 task 全部成功完成，但是 reduce 过程直接挂了，几十个 task 全部失败，重试了还是失败。我只能去查看日志，去 Hadoop 监控界面，看到对应任务的报错日志如下：123456789101112131415161718192021222324252627282930313233343536373839402019-04-22 16:01:30,469 ERROR [main] com.datastory.banyan.spark.ScanFlushESMRV2$FlushESReducer: org/apache/http/message/TokenParserjava.lang.NoClassDefFoundError: org/apache/http/message/TokenParser at org.apache.http.client.utils.URLEncodedUtils.parse (URLEncodedUtils.java:280) at org.apache.http.client.utils.URLEncodedUtils.parse (URLEncodedUtils.java:237) at org.apache.http.client.utils.URIBuilder.parseQuery (URIBuilder.java:111) at org.apache.http.client.utils.URIBuilder.digestURI (URIBuilder.java:181) at org.apache.http.client.utils.URIBuilder.&lt;init&gt;(URIBuilder.java:91) at org.apache.http.client.utils.URIUtils.rewriteURI (URIUtils.java:185) at org.apache.http.impl.nio.client.MainClientExec.rewriteRequestURI (MainClientExec.java:494) at org.apache.http.impl.nio.client.MainClientExec.prepareRequest (MainClientExec.java:529) at org.apache.http.impl.nio.client.MainClientExec.prepare (MainClientExec.java:156) at org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.start (DefaultClientExchangeHandlerImpl.java:125) at org.apache.http.impl.nio.client.InternalHttpAsyncClient.execute (InternalHttpAsyncClient.java:129) at org.elasticsearch.client.RestClient.performRequestAsync (RestClient.java:343) at org.elasticsearch.client.RestClient.performRequestAsync (RestClient.java:325) at org.elasticsearch.client.RestClient.performRequestAsync (RestClient.java:268) at org.elasticsearch.client.RestHighLevelClient.performRequestAsync (RestHighLevelClient.java:445) at org.elasticsearch.client.RestHighLevelClient.performRequestAsyncAndParseEntity (RestHighLevelClient.java:423) at org.elasticsearch.client.RestHighLevelClient.bulkAsync (RestHighLevelClient.java:206) at com.datastory.banyan.client.es.ESBulkProcessor.lambda$new$0 (ESBulkProcessor.java:154) at org.elasticsearch.action.bulk.Retry$RetryHandler.execute (Retry.java:230) at org.elasticsearch.action.bulk.Retry.withAsyncBackoff (Retry.java:87) at org.elasticsearch.action.bulk.BulkRequestHandler$AsyncBulkRequestHandler.execute (BulkRequestHandler.java:138) at org.elasticsearch.action.bulk.BulkProcessor.execute (BulkProcessor.java:350) at org.elasticsearch.action.bulk.BulkProcessor.executeIfNeeded (BulkProcessor.java:341) at org.elasticsearch.action.bulk.BulkProcessor.internalAdd (BulkProcessor.java:276) at org.elasticsearch.action.bulk.BulkProcessor.add (BulkProcessor.java:259) at org.elasticsearch.action.bulk.BulkProcessor.add (BulkProcessor.java:255) at org.elasticsearch.action.bulk.BulkProcessor.add (BulkProcessor.java:241) at com.datastory.banyan.client.es.ESBulkProcessor.addIndexRequest (ESBulkProcessor.java:237) at com.datastory.banyan.spark.ScanFlushESMRV2$FlushESReducer.reduce (ScanFlushESMRV2.java:212) at com.datastory.banyan.spark.ScanFlushESMRV2$FlushESReducer.reduce (ScanFlushESMRV2.java:158) at org.apache.hadoop.mapreduce.Reducer.run (Reducer.java:171) at org.apache.hadoop.mapred.ReduceTask.runNewReducer (ReduceTask.java:627) at org.apache.hadoop.mapred.ReduceTask.run (ReduceTask.java:389) at org.apache.hadoop.mapred.YarnChild$2.run (YarnChild.java:168) at java.security.AccessController.doPrivileged (Native Method) at javax.security.auth.Subject.doAs (Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs (UserGroupInformation.java:1709) at org.apache.hadoop.mapred.YarnChild.main (YarnChild.java:162)截图如下：看到关键部分：java.lang.NoClassDefFoundError: org/apache/http/message/TokenParser，表面看是类未定义，但是真实情况是什么还要继续探索，例如依赖缺失、依赖冲突导致的类不匹配等。问题解决 初步分析 先搜索类 TokenParser 吧，看看能不能搜索到，在 IDEA 中搜索，我的环境是使用 ctrl + shift + t 快捷键，搜索之后发现存在这个类，记住对应的 jar 包坐标以及版本：1org.apache.httpcomponents:httpcore:jar:4.3.2这里需要注意一点，如果你的项目是由多个子项目聚合而成的，此时使用 IDEA 的搜索功能并不准确，会搜索出来其它子项目的同名依赖，从而误导你的视线，所以还是使用依赖分析插件比较好，例如：depedency，下面也会讲到。既然类已经存在，说明有极大可能是依赖冲突导致的 NoClassDefFoundError。继续从错误日志中寻找蛛丝马迹，看到 at org.apache.http.client.utils.URLEncodedUtils.parse (URLEncodedUtils.java:280) 这里，接着搜索类 URLEncodedUtils 并查看第 280 行的 parse 方法。1org.apache.httpcomponents:httpclient:jar:4.5.2上面是依赖坐标以及版本，看到这里有经验的工程师已经可以发现问题所在了：两个同类型的依赖 jar 包版本差别太大，这里暂且不分析。接着查看源码：好，到这里已经把基本情况分析清楚了，程序异常里面的 NoClassDefFoundError 并不是类缺失，所以没有报错 ClassNotFound。根本原因是类版本不对，导致 URLEncodedUtils 找不到自己需要的特定版本的类，尽管有一个同名的低版本的类存在，但是对于 Java 虚拟机来说这是完全不同的两个类，这也是容易误导人的地方。再延伸一下话题，如果真的是类不存在，使用 IDEA 查看源码时会显示红色字体提示的，如图：详细分析 接下来就使用依赖分析插件 dependency 来分析这两个 jar 包的来源以及版本差异，在项目的根目录执行 mvn dependency:tree -Dverbose &gt; tree.txt ，把依赖树信息重定向到 tree.txt 文件中，里面的 -Dverbose 参数可以使我们更为清晰地看到版本冲突的 jar 包以及实际使用的 jar 包。找到 httpclient 和 httpcore 的来源，依赖树片段截取如下：12345678910[INFO] +- com.company.commons3:ds-commons3-es-rest:jar:1.2:compile[INFO] | +- org.apache.httpcomponents:httpclient:jar:4.5.2:compile...... 省略 [INFO] | +- org.apache.httpcomponents:httpasyncclient:jar:4.0.2:compile[INFO] | | +- org.apache.httpcomponents:httpcore:jar:4.3.2:compile[INFO] | | +- (org.apache.httpcomponents:httpcore-nio:jar:4.3.2:compile - omitted for duplicate)[INFO] | | +- (org.apache.httpcomponents:httpclient:jar:4.3.5:compile - omitted for conflict with 4.5.2)[INFO] | | \- (commons-logging:commons-logging:jar:1.1.3:compile - omitted for duplicate)可以看到 httpclient 来自于 ds-commons3-es-rest，版本为 4.5.2，而 httpcore 来自于 httpasyncclient，版本为 4.3.2。特别注意：httpasyncclient 里面还有一个 4.3.5 版本的 httpclient 由于版本冲突被忽略了，这也是导致问题的元凶。依赖树片段截图如下：到这里已经可以知道问题所在了，httpclient、httpcore 这两个依赖的版本差距太大，前者 4.5.2，后者 4.3.2，导致前者的类 URLEncodedUtils 在调用后者的类 TokenParser 时，找不到满足条件的版本，于是抛出异常：NoClassDefFoundError。解决方案 那这个问题也是很容易解决的，指定版本接近的两个依赖即可，但是还是要根据实际情况而来。本来最简单的方案就是移除所有相关依赖，然后在 pom.xml 中显式地指定这两个依赖的版本。但是这么做太简单粗暴了，因为这两个依赖不是一级依赖，而是传递依赖，不必手动管理。所以要适当地移除某一些传递依赖，保留另一些传递依赖，让它们不要交叉出现。我的做法就是移除 ds-commons3-es-rest 里面的传递依赖，保持 httpasyncclient 里面的传递依赖，这样它们的版本号接近，而且是同一个依赖里面传递的，基本不可能出错。pom.xml 配置如图：httpclient 的小版本号是可以比 httpcore 高一点的，继续查看依赖树，可以看到 httpclient 的版本为 4.3.5，httpcore 的版本为 4.3.2。引申插件 除了 dependency 插件外，还有另外一个插件也非常好用：enforcer，插件的坐标如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;!-- 帮助分析依赖冲突的插件，可以在编译时期找到依赖问题 --&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-enforcer-plugin&lt;/artifactId&gt; &lt;version&gt;1.4.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;enforce-ban-duplicate-classes&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;enforce&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;!-- 设置规则，否则没法检查 --&gt; &lt;rules&gt; &lt;!-- 检查重复类 --&gt; &lt;banDuplicateClasses&gt; &lt;!-- 忽略一些类 --&gt; &lt;ignoreClasses&gt; &lt;ignoreClass&gt;javax.*&lt;/ignoreClass&gt; &lt;ignoreClass&gt;org.junit.*&lt;/ignoreClass&gt; &lt;ingoreClass&gt;org.aspectj.*&lt;/ingoreClass&gt; &lt;ingoreClass&gt;org.jboss.netty.*&lt;/ingoreClass&gt; &lt;ingoreClass&gt;org.apache.juli.*&lt;/ingoreClass&gt; &lt;ingoreClass&gt;org.apache.commons.logging.*&lt;/ingoreClass&gt; &lt;ingoreClass&gt;org.apache.log4j.*&lt;/ingoreClass&gt; &lt;ingoreClass&gt;org.objectweb.asm.*&lt;/ingoreClass&gt; &lt;ingoreClass&gt;org.parboiled.*&lt;/ingoreClass&gt; &lt;ingoreClass&gt;org.apache.xmlbeans.xml.stream.*&lt;/ingoreClass&gt; &lt;ingoreClass&gt;org.json.JSONString&lt;/ingoreClass&gt; &lt;/ignoreClasses&gt; &lt;!-- 除了上面忽略的类，检查所有的类 --&gt; &lt;findAllDuplicates&gt;true&lt;/findAllDuplicates&gt; &lt;/banDuplicateClasses&gt; &lt;!-- JDK 在 1.8 以上 --&gt; &lt;requireJavaVersion&gt; &lt;version&gt;1.8.0&lt;/version&gt; &lt;/requireJavaVersion&gt; &lt;!-- Maven 在 3.0.5 以上 --&gt; &lt;requireMavenVersion&gt; &lt;version&gt;3.0.5&lt;/version&gt; &lt;/requireMavenVersion&gt; &lt;/rules&gt; &lt;fail&gt;true&lt;/fail&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;!-- 官方的默认规则 --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;extra-enforcer-rules&lt;/artifactId&gt; &lt;version&gt;1.0-beta-6&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/plugin&gt;这个插件需要配置在 pom.xml 中，并且绑定 Maven 的生命周期，默认是绑定在 compile 上面，然后需要给 enforcer 配置一些规则，例如检查重复的类。接着在编译期间，enforcer 插件就会检验项目的依赖中所有的类【可以设置忽略容器中的类，例如作用域为 provided 的依赖包】，如果有重复的类，就会报错，编译不会通过。注意，这个插件除了可以检查依赖、类的冲突【通过设置规则 rule 来实现】，还可以设置一些其它的开发规范，例如规定 JDK 版本、开发系统环境必须为 Windows、使用的 Maven 版本等等。此外，官方也提供了一些规则列表可以参考：http://maven.apache.org/enforcer/enforcer-rules/index.html ，而且还有 API 允许我们自定义规则，非常灵活。问题总结 抽象总结 总结一下现象，其实就是项目本来依赖了 B 包，B 包里面有传递依赖包 1、包 2，由于包 1、包 2 都来自于 B 包，所以版本差别不大，很适配。包 1 的类调用包 2 的类很顺利，不会有问题。后来由于其它功能需要，项目又加入了 A 包，此时没有注意到 A 包里面也有包 1，而且比 B 包里面的包 1 版本高，这本来不是问题，只是潜在风险。但是，编译打包时 A 包里面的包 1 把 B 包里面的包 1 覆盖了，包 2 仍旧是来自于 B 包，这就出问题了，风险变成灾难了。当程序运行时包 1 需要调用包 2，由于版本差别过大，找不到符合条件的类了，抛出异常：NoClassDefFoundError。这里面的验证机制浅显地描述就是每个类都会有自己的序列化编号，如果有严格要求同版本依赖的类，调用方法时会严格验证。关于编译的疑问 到这里，读者会有疑问，为什么编译不报错，能顺利通过呢？其实从上面就能看到答案了，这种依赖包之间相互引用的类，类是存在的，只是版本不一致而已，编译时并不能检测出来。如果是你自己写的类源码，引用了别的依赖包的类，同时对版本要求严格的话，编译是一定会报错的。但是，如果你提前知道了是哪个类，一般不可能知道，只有报错了才会知道，而且会有不止一个类，这也是令人头疼的地方。如果进一步分析异常信息，发现它归属于 ERROR，并不是运行时异常，更不用谈编译时异常了，这种错误和 OutOfMemoryError 类似，是虚拟机运行时出现问题，比较严重。感悟 找到这种问题的原因是没有什么难度的，一眼就可以看出来是依赖冲突。但是解决过程可谓是难度极大，而且可以让人崩溃，对于初学者来说可以放弃了，折腾三天可能都不会有结果的。特别在依赖庞大的情况下，几百个依赖包，几百 M 大小，这时候找起来特别麻烦，有时候改动了一点会影响到其它的依赖，引起连锁反应，可能问题还没解决，又引发了其它问题。所以，在项目开发的初始阶段，一定要管理好项目的依赖，并且在依赖变更时要一起讨论，否则后患无穷。此外，在解决依赖冲突的过程中，有 2 个插件工具很好用：dependency、enforcer。]]></content>
      <categories>
        <category>踩坑系列</category>
      </categories>
      <tags>
        <tag>httpcore</tag>
        <tag>maven</tag>
        <tag>dependency</tag>
        <tag>enforcer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 之 kill 命令入门实践]]></title>
    <url>%2F2019042101.html</url>
    <content type="text"><![CDATA[kill 命令只是用来向进程发送信号的，而不是直接杀死进程的。基础知识 操作实践 备注]]></content>
      <categories>
        <category>Linux 命令系列</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>kill</tag>
        <tag>jobs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JNI 字段描述符基础知识]]></title>
    <url>%2F2019041301.html</url>
    <content type="text"><![CDATA[平时在做 Java 开发的时候，难免遇到异常信息中包含一种特殊的表达字符串，例如：1method: createWorker signature: (Ljava/util/concurrent/Executor;) Lorg/jboss/netty/channel/socket/nio/AbstractNioWorker;或者 1java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.JavaType.isReferenceType () Z 可以看到，异常信息中有一种特殊的字符串出现了：L 后面跟着类名 、 方法后面跟了一个 Z。其实，这就是 JNI 字段描述符【Java Native Interface FieldDescriptors】，它是一种对 Java 数据类型、数组、方法的编码。此外，在 Android 逆向分析中，通过反汇编得到的 smali 文件，里面的代码也会遵循这种方式，即 Dalvik 字节码。本文就记录一些数据类型、数组、方法的编码方式以及解释说明，方便以后查阅。基本概念 这种编码方式把 Java 中的基本数据类型、数组、对象都使用一种规范来表示：八种基本数据类型都使用一个大写字母表示 void 使用 V 表示 数组使用左方括号表示 方法使用一组圆括号表示，参数在括号里，返回类型在括号右侧 对象使用 L 开头，分号结束，中间是类的完整路径，包名使用正斜杠分隔 基本编码 基本编码如下表格，并配有解释说明：Java 类型 JNI 字段描述符booleanZbyteBcharCshortSintIlongJfloatFdoubleDvoidVObject 以 L 开头，以；结尾，中间是使用 / 隔开的完整包名、类型。例如：Ljava/lang/String;。如果是内部类，添加 $ 符号分隔，例如：Landroid/os/FileUtils$FileStatus;。数组 [ 方法 使用 () 表示，参数在圆括号里，返回类型在圆括号右侧，例如：(II) Z，表示 boolean func (int i,int j)。举例说明 数据类型 1、[I：表示 int 一维数组，即 int []。2、Ljava/lang/String;：表示 String 类型，即 java.lang.String。3、[Ljava/lang/Object;：表示 Object 一维数组，即 java.lang.Object []。4、Z：表示 boolean 类型。5、V：表示 void 类型。 方法1、() V：表示参数列表为空，返回类型为 void 的方法，即 void func ()。2、(II) V：表示参数列表为 int、int，返回类型为 void 的方法，即 void func (int i,int j)。3、(Ljava/lang/String;Ljava/lang/String;) I：表示参数列表为 String、String，返回类型为 int 的方法，即 int func (String i,String j)。4、([B) V：表示参数列表为 byte []，返回类型为 void 的方法，即 void func (byte [] bytes)。5、(ILjava/lang/Class;) J：表示参数列表为 int、Class，返回类型为 long 的方法，即 long func (int i,Class c)。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>JNI</tag>
        <tag>字段描述符</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[预估 Mysql 数据表的数据大小和索引大小]]></title>
    <url>%2F2019041001.html</url>
    <content type="text"><![CDATA[最近接到一个小的新需求，需求很容易实现，就是定时把一些分析得出的指标从 Elasticsearch 中离线存储到 Mysql 数据库中，方便以后查询。离线存储的原因是因为资源不足，Elasticsearch 会自动删除 15 天以前的原始数据，而且 Elasticsearch 每天都会新产生数十万到数百万的数据，依据这些原始数据只会产生几十条分析结果，显然离线存储到 Mysql 中更为合理。在处理这个需求时，接着就遇到了一个小问题，当前业务组没有数据库资源，需要申请，而且由于资源不足，不能随便申请，要给出合理的预估值。这样，就涉及到数据库占用空间大小的预估了，本文记录一种简单的方法。数据大小和索引大小预估 我当前使用的是 Mysql 数据库，其它数据库产品查询方式可能会有所不同，请根据实际情况操作。在数据库中，使用系统数据库的表 TABLES 进行查询：1234SELECT data_length,index_lengthFROM information_schema.TABLES tWHERE table_schema=&apos;your_db_name&apos;AND table_name = &apos;your_table_name&apos;;其中，系统数据库是 information_schema，存储表信息的表是 TABLES，data_length、index_length 这 2 个字段表示数据大小、索引大小，单位是字节 B。当然，如果使用可视化的数据库连接管理工具，也可以通过管理工具直接鼠标点击查看，其实背后的逻辑仍旧是查询 TABLES 表，例如我通过 Navicat 工具查看。可见，无论使用哪种方式，都可以把需要的信息查询出来，然后就可以预估数据大小了。我截图的信息显示，数据大小 8.5MB，索引大小 0MB，还要结合数据条数，我查了一下有 10000 条数据，因此可以粗略估计每条数据的大小为 0.85KB。这里需要注意一下，预估数据大小之前要保证数据的字段取值接近真实情况，最好能有数据示例可以参考，而且数据量要尽量大一些，例如几万条，不能只有几十条、几百条。如果确实没有数据示例参考，需要自己模拟生成，尽量把字段的取值多生成一些实际中可能出现的值。例如字符串类型如果是 vachar，要把每种长度的取值都生成一些，或者根据实际场景，某些长度的字符串出现的可能性大一点，那就多生成一些。如果觉得这样计算比较麻烦的话，其实还有一种更简单的方法，直接查询 avg_row_length 字段，这个字段表示数据表的平均行大小，和上面自己计算的结果类似。总之，就是为了接近真实，才能更为准确地预估出数据占用的空间大小，实际去申请资源时才能有理有据。此外，这个 TABLES 表里面的内容很丰富的，有需要的可以查询一下，查看数据表的字段信息 SQL 语句：1SHOW COLUMNS FROM TABLES;]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>数据库</tag>
        <tag>database</tag>
        <tag>space</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 JDK 命令行工具分析内存泄漏或内存溢出问题]]></title>
    <url>%2F2019040301.html</url>
    <content type="text"><![CDATA[最近遇到一个棘手的问题，有业务方在调用存储系统封装的 SDK 取数的过程中，遇到了 OOM 问题，但是数据量很小，只有 12000 条。同时进程启动时申请的内存高达 12g，使用 Xmx、Xms 参数控制，实际指定参数取值为：-Xms12g -Xmx12g。但是如果只看报错日志信息，抛出异常的代码位置指向了 SDK 的内部代码。根据这个现象，我猜测可能是业务方的处理逻辑问题、SDK 内部处理逻辑问题、申请的内存过小问题，这些问题归根结底，要么是内存不够【内存溢出】，要么是内存不当使用【内存泄漏】。所以，我要在 Java 虚拟机参数方面或者业务方代码逻辑方面入手，一步一步测试，找出问题的元凶。本文就记录这一过程，以及适当引申一些关于 JVM 的知识。解释说明一下，上述中的 SDK 表示存储系统独立封装的取数、查询接口，它屏蔽了 Elasticsearch 自带的接口，并封装成公共组件，提供给各个业务方使用。各个业务方在使用前，需要申请开通 token 验证码，存储系统会根据业务方的使用量分配合适的资源，业务方在调用时需要传入 token 验证。这样做的好处，一是可以监控所有的业务方的取数、查询情况，收集所有的请求日志，统计一些常用的指标，然后反过来指导存储系统的改进，例如根据业务方的调用情况进行资源分配的伸缩、针对常用的数据类型进行索引优化。二是可以保障整个数据库集群的正常运行，由于屏蔽了 Elasticsearch 自带的接口，业务方不能随意操作超大额的数据量，SDK 会做限制，因此不会产生某些不合理的查询、取数请求，从而不对数据库造成巨大的压力。三是限制了一些不需要的查询、取数方式，在保障业务方基本需求的情况下又可以保障数据库集群的稳定，例如多层聚合、日期聚合等操作，这些操作不合理，而且会对数据库集群造成压力【无论数据量大小都可能会出事】。问题出现 简单描述现象，查看日志，猜测可能的原因。问题解决 分析现象 一开始没有指定 JVM 参数，因为使用的是 JDK1.7 版本的参数，不会生效，这就导致分配的默认堆取值偏小。JVM 参数设置：12345678JAVA=$&#123;JAVA_HOME&#125;/bin/java# 设置 jvm 的参数 #HEAP_OPTS=&quot;-Xms12g -Xmx12g&quot;HEAP_OPTS=&quot;-Xms6g -Xmx6g -Xmn2g&quot;# JDK8 以后取消了 PermSize#PERM_OPTS=&quot;-XX:PermSize=1024M -XX:MaxPermSize=2048m -XX:+UseConcMarkSweepGC -XX:+UseParNewGC&quot;#JDK8 的 MetaspaceSizePERM_OPTS=&quot;-XX:MetaspaceSize=1024m -XX:MaxMetaspaceSize=2048m -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+HeapDumpOnOutOfMemoryError&quot;待整理，重试，重现现象，确保问题准确复现。掌握内存分析工具 待整理。各种工具介绍，举例，截图。对症下药解决问题 待整理。减小内存，使用普通的 list。重试，使用命令行工具查看现象，截图。问题总结 不同版本的参数不一致 主要是针对 JDK 来说的，不同的 JDK 版本的参数会有一些不同，例如以下 2 个虚拟机参数，在 JDK1.8 的环境中是 -XX:MetaspaceSize=1024m -XX:MaxMetaspaceSize=2048m，已经不是 -XX:PermSize=1024M -XX:MaxPermSize=2048m 了【JDK 1.7 以及以前的版本】，在进程启动的时候查看日志会有警告信息的，提示参数设置无效，会被忽略。CopyOnWriteArrayList占用内存，待整理。模拟内存溢出 待整理。进程已杀死问题 加大内存，机器内存不够分配，进程异常退出，待整理。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>JDK</tag>
        <tag>内存泄漏</tag>
        <tag>内存溢出</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[青椒炒蛋做法总结]]></title>
    <url>%2F2019033101.html</url>
    <content type="text"><![CDATA[青椒炒蛋，是一道非常普通的家常菜，基本家家户户都会做。有的家庭喜欢吃辣，放的是稍微辣一点的辣椒，有的家庭不喜欢吃辣，就放菜椒或者甜椒，总之，对于辣椒的选择非常多。对于辣椒的处理方式，有的人喜欢切小块，有的人喜欢斜切小段，还有的人直接剁碎，做法也多种多样。本文记录青椒炒蛋的做法总结，使用的是菜椒【不辣微甜】，由于故意多放了生抽，做出来的口味是咸香的。食材准备 以下食材的份量为一大盘，足够 2 人吃，食材非常简单：鸡蛋 3 个 青椒 2 棵 大蒜 6 粒 食用盐、生抽酱油 全部的食材 制作过程 这是一道快菜，制作过程根据家庭灶的火力大小，2-5 分钟即完成。处理食材备用 青椒洗净去籽切小快，鸡蛋液加少量食用盐搅拌均匀，大蒜切片。青椒切块，大蒜切片 鸡蛋液 炒鸡蛋备用 锅里加油，要多加一点，鸡蛋液很吸油，烧热后下鸡蛋液，定型后炒散，炒散后盛出备用。由于鸡蛋液里面已经加了食用盐，就不用再加盐调味了。鸡蛋液下锅 定型炒散 鸡蛋盛出备用 炒青椒大蒜 锅里留底油，如果炒完鸡蛋后不够再适当加点油，烧热，大蒜片和青椒同时下锅翻炒【注意是同时下锅】，翻炒至青椒 5 成熟，关小火准备加生抽调味。青椒大蒜同时下锅 翻炒至青椒 5 成熟 加生抽调味后下鸡蛋 青椒翻炒至 5 成熟时，关小火，加生抽调味。注意一定要加多一点生抽，那种大的汤勺可以加将近一汤勺，然后开大火翻炒，把青椒炒至 8 成熟，生抽遇到大火热量时会散发独特的香味。加生抽炒至青椒 8 成熟 此时倒入前面炒过的鸡蛋，混合翻炒，如果觉得不够味再加一点食用盐，我加的生抽已经够味，不再加食用盐了。倒入鸡蛋 翻炒均匀 出锅装盘 翻炒均匀后可以出锅装盘了。装盘侧视图 装盘俯视图 配上剩下的几块红烧肉，美滋滋。注意事项 1、炒青椒时和大蒜片一起下锅，不需要先下大蒜片爆香，这很关键。2、油量，炒鸡蛋的时候一定要多放一点油，才能保持鸡蛋的嫩滑，因为鸡蛋液吸油很厉害。如果油加的少，会导致鸡蛋炒的有点干有点糊，影响口感。3、炒青椒的时候为什么要多加一点生抽呢，毕竟会影响这道菜的颜色，一般炒菜都只会在最后调味时加一点点。因为使用青椒来炒鸡蛋，这种青椒是没有什么味道的，那样只会有鸡蛋的香味，显得太单调，而生抽遇到热量会散发出独特的香味，同时生抽里面又有盐分，从而达到了咸香的效果。就如北方有些地方做番茄炒蛋的时候，是做成咸味的，也会加入大量的生抽，味道也非常好，特别是拌面吃，既是菜又能调味。 致谢 感谢微博用户 @开心的柠檬日记 ，在微博上放了很多做菜的方子，微博主页为：开心的柠檬日记的微博 。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>青椒炒蛋</tag>
        <tag>辣椒炒蛋</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[参加 Elastic 社区第三次线下活动广州站]]></title>
    <url>%2F2019033001.html</url>
    <content type="text"><![CDATA[在 2019 年 3 月 30 日，我去参加了 Elastic 社区第三次线下活动广州站的分享会，活动简介：Elastic 社区第三次线下活动广州站 。看到各位行业顶尖分享者的分享，不能说受益匪浅，至少给我打开了一些思路，拓展了我的知识面，同时我也学到了一些知识，既包括技术方面的，也包括处事方面的。这篇博文就简单记录一下这个过程。 出发 先看一下地图指引 到达公交站，上冲南站，天气不错 走路路过特斯拉服务站，听说最近交付的特斯拉电动车有很多问题 到达 到达的比较早，因为要帮忙安排桌子凳子，一切准备就绪后，一起吃了个午饭。13:30 开始签到，签到现场 我充当了一会儿签到员，坐着的那个是我 各种各样的 Elasticsearch 贴纸 这是一种比较特殊的 Elasticsearch 贴纸 静听分享 先简单看一下这个分享会的大概流程与分享内容 分享一 Elasticsearch 在数说全量库的应用实践 现场场景一 现场场景二 现场场景三 分享二 Elasticsearch 在慧算账技术运营中的应用 现场场景 分享三 Elasticsearch 在大数据可视化分析中的应用 现场场景 分享四 打造云原生的 Elasticsearch 服务 现场场景 分享五 Elasticsearch 集群在雷达大数据平台的演进 现场场景 分享者合影留念 认真的观众 分享者合影留念 我的思考以及学到的东西 0、虽然有一些分享听不懂，例如腾讯云的 Elasticsearch 云服务，做了什么优化、达到了什么效果，或者是数说雷达的架构演进，这些目前对于我来说都太不切实际，因为还没接触到这么高深的知识，平时也使用不到，所以听起来云里雾里。但是，能从中提取 1-2 个重要知识点也是有用的，例如腾讯云的索引碎片化，导致读写速度严重下降，这与我在工作当中遇到的问题一模一样。再例如数说雷达演进过程中遇到的坑，某个字段没有做 doc_values，导致不支持 aggregation 查询，这与我很久之前遇到的问题一模一样，此时又加深了我的认知。1、多版本 Elasticsearch 的兼容解决办法，需要设置拦截器，把请求的不兼容参数部分替换掉，可以使用 SpringBoot 整合，需要注意已知版本的种类。2、针对 long 类型字段的聚合【即 aggregation】请求根据自己的业务场景，如果判断为实际上没有必要【例如只是对年份、月份、日做聚合，并不考虑时区、毫秒时间戳的问题】，可以换一种思路，转化为字符串存储，针对字符串做聚合操作效率就高多了。3、在现场提问时，有的人是带着自己业务实际遇到的问题来提问探讨的，提问时描述问题已经消耗了将近 10 分钟。接下来如果真的探讨起来，估计没有半个小时一个小时搞不定，这显然是在浪费大家的时间。所以分享者也及时打断了提问，并留下联系方式，分享会后线下接着再讨论。这种做法很得体，虽然不能在现场解答【为了节约大家的时间】，但是会后讨论也是一样，有时候根据实际情况就是需要这样的取舍。4、在 Elasticsearch 中，字段类型是可以节约存储空间与请求耗时的，例如 integer、long、short 的合理使用，但是切记存储的目的最终都是为了使用。 备注 如果需要查看分享者的 PPT 文档，可以在 Elastic 社区下载：https://elasticsearch.cn/slides 。]]></content>
      <categories>
        <category>游玩</category>
      </categories>
      <tags>
        <tag>Elastic</tag>
        <tag>线下活动</tag>
        <tag>广州</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FFmpeg 使用总结]]></title>
    <url>%2F2019032701.html</url>
    <content type="text"><![CDATA[FFmpeg 是一款开源的软件，可以进行多种格式的视频、音频编码转换、片段剪辑。它包含了 libavcodec – 这是一个用于多个项目中音频和视频的解码器库，以及 libavformat – 一个音频与视频格式转换库。FFmpeg 这个单词中的 FF 指的是 Fast Forward。FFmpeg 官网：https://ffmpeg.org ，下载时会跳转到这里：https://ffmpeg.zeranoe.com/builds ，请选择合适的版本下载使用。本文记录 FFmpeg 的使用方法，基于 Windows X64 平台。下载安装 下载 在 https://ffmpeg.zeranoe.com/builds 下载页面，选择适合自己操作系统的版本，我这里选择 Windows X64 的 static zip 包，解压后直接使用，无需安装。解压配置环境变量 下载到指定的目录【最好放在方便管理的目录，不显得混乱】，直接解压，得到一个文件夹，里面有 bin、doc、presets 这 3 个子文件夹，其中 bin 里面就包含了主程序：ffmpeg、ffplay、ffprobe，这里不涉及安装的概念，程序可以直接使用。解压主目录 子文件夹 bin为了方便使用这 3 个主程序，需要把 bin 所在目录配置到环境变量 PATH 中【我这里是 D:\Program Files\ffmpeg\bin】，这里就不再赘述，如果不配置，每次使用命令时都要给出完整的目录，我觉得很麻烦。使用示例 ffmpeg 的命令行参数的位置会影响执行的结果，例如时间参数，这与我所知道的其它工具不一样，所以参数位置不能乱放。此外，还需要注意涉及到转码的操作会比较耗时，几十分钟的视频不是几分钟能处理完的，和视频的清晰度也有关系，这个要有一定的心理准备。1、把 mkv 格式的视频文件转为 mp4 格式的文件，视频使用 libx264 编码。12-- 如果没有配置环境变量 PATH, 命令需要指定 D:\Program Files\ffmpeg\bin\ffmpegffmpeg -i imput.mkv -c:v libx264 output.mp42、查看视频文件的流信息，包括视频、音频、字幕。12-- 其中类似 Stream #0:0 格式的内容就是流信息，指定参数时可以直接使用数字编号表示流 ffmpeg -i input.mkv3、mkv 文件剪辑，截取片段，指定音轨。12-- -ss 表示开始时间，-to 表示结束时间，ffmpeg -ss 01:22:08 -to 01:32:16 -accurate_seek -i in.mkv -map 0:v -map 0:a:1 -codec copy -avoid_negative_ts 1 out.mkv 其中，-accurate_seek 表示画面帧数校准，-avoid_negative_ts 1 表示修复结尾可能的空白帧，-map 0:v 表示截取所有视频，-map 0:a:1 表示截取第 2 道音轨。此外，如果把时间参数放在 -i 前面，结果总会多截取 1-2 秒【如上面示例】。但是如果放在后面，截取的视频片段时间准确了，然而开头的音频正常，视频有 20-30 秒的漆黑一片，不知道为啥。4、rmvb 文件转为 mp4 文件，涉及到编码转换。12-- 视频使用 h264 编码，音频使用 aac 编码 ffmpeg -i input.rmvb -c:v h264 -c:a aac out.mp4这里需要注意，涉及到编码转换的比较消耗 CPU，上面这个命令把我的 CPU 消耗到 100%，动态视频详见微博：FFmpeg 视频转码 CPU 飙升到 100% 。其中，留意流输出信息：123Stream mapping: Stream #0:1 -&gt; #0:0 (rv40 (native) -&gt; h264 (libx264)) Stream #0:0 -&gt; #0:1 (cook (native) -&gt; aac (native))此外，FFmpeg 不支持 rmvb 格式的文件，只能转码为 mp4 的格式再使用，这里的不支持不是指不能处理，而是不能直接输出 rmvb 格式的文件，处理输入是可以的。其它1、如果只是为了转换 mkv 文件的格式为 mp4，也可以使用一款软件：MkvToMp4 。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>FFmpeg</tag>
        <tag>视频剪辑</tag>
        <tag>音频剪辑</tag>
        <tag>视频转码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分别 是为了再次相聚]]></title>
    <url>%2F2019032702.html</url>
    <content type="text"><![CDATA[这是我还在大学读书的时候，有一年过年回家，组织了高中同班同学的聚会，其实主要目的就是一起吃个饭，玩半天，交流一下。我还隐隐约约记得当时提前约好了十几个人，然后到了当天早上再确认的时候，只有八个人能来了，刚好凑一桌。那次一别，以后再也没见过，只在微信上聊过，大家天各一方，有的求学做科研，有的成家立业。而如今，当我碰巧再拾起这段文字的时候，只觉得沙流指尖，微风拂面。当然，我再也写不出这么稚嫩的、无病呻吟的文字了，因为现在整天在写代码，我的思维也变化了很多。最近三年，在工作环境中经历了多次的相聚离别，现在年后又是离职大潮，刚好整理出这篇旧笔记，提醒自己的成长之路。开篇 我应该算是一个善感的人，但不多愁。此时努力寻找记忆中的碎片，感受那些我能看到的瞬间，尝试从记忆中寻找值得书写的部分，以作想念。每次只要有久违的聚会，之前我都会想象见面时的情景，以及每个人的样子，认为这样能追寻到由于长时间不见面而淡去的亲切感。同时内心也会积聚起来丰富的情感，构思出许多可以表达的文字，藏在心底，待到意兴浓的时候说出来。相聚 我记得那天早晨起的特别早，是假期中最早的一次，可能是内心的激动，亦或是天气转暖，也可能是因为我定了三个闹钟。出发的时候由于出现了三个不在我计划中的意外，导致我的出发时间比我预计的晚了一个小时，这已经使我的内心感到些许急躁。后来在路上没想到会堵车，又耽搁了 20 分钟，望着拥挤的车流，我再也按捺不住急躁的内心，便直接下车走完了最后一段路，到达的时候已经是中午 12 点了。在路上有点风，微风拂面，我能感觉到微微颤抖，为自己没有围围巾而感到后悔，这可是冬天，温度只有几摄氏度。可是到了集合地点太阳都出来了，阳光明媚，冬风和煦，的确是个好天气，相当适合聚会，内心的寒冷一扫而空。走到集合的地点，那是一个破旧的学校操场，我曾在这里度过一年光阴，扶了扶眼镜，向四周望了望，看到在操场旁的屋檐下，他们正在打桌球。毕竟两年多没见过面了，他们的样子在我脑海中渐渐模糊，此时努力搜寻，看着一张张熟悉又陌生的脸庞，心中情感翻涌，暗暗对着他们的名字，辛福感扑面而来。但准备好的话一句也没有说出口，这就是我的性格，见了面在一起比什么语言都美好，直接加入他们，我想这也是最好的表达。快速从脑海中搜索记忆，在我仔细看来，大家仍然是高中的模样，没有怎么变化，还是那么青春，还是那么快乐，没有老练，没有隔阂，脸上洋溢着年少时的单纯感，都像这个年龄应该有的样子。我装出很「酷」的样子，至少我是这么认为，可毕竟长时间没有打过桌球了，手生，出杆结果总是不尽人意，我总感觉自己处于尴尬的境地，可是仍然装出不在乎的样子，说几句玩笑话，淡淡一笑，希望没人看到，说到底还是生疏了。本应该可以去吃饭了，但大家兴致未尽，在「混乱」的状态下又玩了几十分钟，随着黑色八号球的入袋，结束了应该有的结局，已经过了午饭的时间。说实话，此时我已经饥肠辘辘，但大家看起来都很亢奋，兴奋喜悦的表情洋溢于脸，在说笑中集体寻找吃饭的地点。八个年轻人一起走在马路上，蹦蹦跳跳，三两成堆，不时对旁边的人吐露心声，开着不着边际的玩笑，说着没心没肺的笑话。此时我的脑海中浮现出一幅和谐美好的画面，也必将印在我的心底。走走停停，说说笑笑，乘坐免费的公交，不多时便到达了新建落成的七彩世界，映入眼帘的是花花绿绿的色彩，心情豁然开朗，已经做好了吃喝的准备。进入大楼入口，乘坐电梯，直奔饮食区域。首先寻找最佳的位置，足够容纳八个人入座，围在一起，谈笑风生。我实在是饿坏了，便独自一人去点了一碗面，首先尝尝咸淡。没想到我等了一会儿，他们大部分人都来了，都要吃面，那就点吧。七七八八大家议论了一番，各自点了一碗面。等待上面的过程中，几个男生又去点了一些副食，女生去点了饮品，相互搭配，应该足够果腹了。东西上齐后，本想随心所欲，大快朵颐，可是不知怎么的，先前的食欲减小了，不过以我的大饭量还是能消耗很多食物的。吃饭的过程就是吃饭的过程。吃饱喝足之后，大家聊了一会儿，有人提议打牌，反正闲着没事儿干，打就开打。本来准备去购买纸牌的，没想到王飞的书包里居然带着这玩意儿，看来他那书包里面装着很多现金也是真的了。打牌真的是体力活也是脑力活，不仅需要工于计算，还需要相互配合，可是牌不好什么都白搭。在这个过程，我可以说是遭遇了打牌生涯的滑铁卢，无论怎么样打，都是输牌。当然，值得说明的是，输牌并不是因为牌技不好，而是因为牌不好，就算什么都算出来了也打不赢，这不能怪我。身为理科生，我清楚地知道这只是概率问题，风水会轮流转，但为了缓解气氛，我也可以承认这可能是人品的问题。况且竟然牌从头到尾一直不好，想想这也是一个小概率事件，我不得不承认可能确实和我的人品有一点关系。输输输……输了牌，就会有惩罚，惩罚过后，换了一波人继续打。可是万万没想到，我这一门虽然换了人打，牌还是不好，虽然比我打的时候好了那么一点点，仍然是稳输不赢，牌差的简直令人发指。打牌成绩的最顶峰也就是赢了一个而已，很快又变成负数了，真是令人伤悲。分别 不知不觉中，在说说笑笑、打打闹闹的氛围下过去了几个小时，天也已经快黑了，是时候该分别了，各自回家，要不然有些人恐怕赶不上末班车了（十八线小县城过了六点就没车了）。而后大家收拾东西，开始动身，临走时总要合照一张吧。合照一没有我 合照二没有我 每一张照片，王飞的脖子和头看起来很奇怪，我没法挑出效果更好的了。第一张解丰的全身没有入镜，当然，这怪我，是我拍的。请看下面零散的几张照片，当时的红米手机拍照效果也就这样了，每张照片的大小只有 800KB 左右。大家闲聊 二位在比划啥 二位正经的样子 正经人 这里面有我 这是干啥 下了楼，大家一起往车站行走，途中许梦宇有事首先分别，王继雪顺车回家第二个分别。剩下的六人继续走在繁华的街头，沿着热闹的商铺，趁着剩下不多的时间聊天说笑。本来是一段很短的路程，我们却走了很长时间，可能大家心里都想着多聊一会儿吧，才故意放慢了脚步。当然，最终这也导致了我错过了回家的末班车。到了车站，大家又聊了一会儿，当作告别的寒暄。我不得不乘坐另外一辆车，和李欢一起，只是到达离家还有十多公里的永兴街上，然后让家里的亲人来接。结尾 「天下没有不散的宴席」，这句老话大家都知道，可大概只有经历过的人才能理解其中蕴含着的意义吧。时间总是很快，最终的时刻总要分别。 分别，是为了再次相聚。下一次再相聚会是什么时候呢？下一次再相聚大家会变吗？静静等待着再次相聚。哦，对了，我离开家乡去学校的那天，火车开动的时候，家乡飘起了小雪。后来火车远离家乡的时候，听说雪已经很大了，可惜我没有见到。是为记。2015 年 02 月 26 日 鹏飞]]></content>
      <categories>
        <category>游玩</category>
      </categories>
      <tags>
        <tag>聚会</tag>
        <tag>利辛一中</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爆炒花甲做法总结]]></title>
    <url>%2F2019032501.html</url>
    <content type="text"><![CDATA[爆炒花甲是一道做法简单、快速出菜、可口下饭的家常菜，一般晚上去大排档吃夜宵，基本都会有这道菜。本文记录爆炒花甲的家常做法，以及需要注意的地方。食材准备 以下份量为一盘：花甲 1.5 斤 - 2 斤，花甲的价格一般在 6-10 块钱之间【广州地区】二荆条辣椒 2 个，细长条那种，不是很辣 小米椒 5 个，我用干辣椒和辣椒酱代替了 豆瓣酱、蚝油、食用盐、淀粉 大蒜、香葱，香葱我用青椒代替了 辣椒食材 辣椒切碎备用 准备工作中最重要的就是让花甲吐沙，一般买回来的花甲里面都会有大量的沙子，所以要让花甲把沙子吐出来，这样吃起来才不会硌牙，否则会严重影响口感，没法吃。花甲吐沙方法主要有两种：方法 1【时间长，简单】：放入清水、食用盐、香油，浸泡一个小时，基本可以把沙吐干净。方法 2【时间短，麻烦】：放入锅中，加入清水、料酒、姜片、香油，小火煮热【不要很热，保持不烫手的温度，否则会严重影响肉质】，持续 15 分钟，也可以把沙吐干净。我一般选择第一种，花甲买回来之后放在盆里，加入盐、香油等着就行了，可以先去处理其它食材了，不用管，很方便。制作步骤 花甲焯水 花甲吐沙完成之后，捞出进行焯水，焯水之后花甲基本熟了，然后捞出花甲迅速过冷水，过冷水的目的是保持花甲肉的鲜嫩。注意焯水不要太久，否则花甲肉就老了，影响口感。我把花甲焯水的时候还加了姜片和料酒去腥。爆香锅底 油烧热，一定要比平时炒菜多放一点油，毕竟是爆炒，加入大蒜、辣椒、姜片，十几秒爆香，我这里省略了姜片，因为焯水的时候用的水里面加了姜片和料酒。大蒜也不加了。豆瓣酱或者蚝油调味 关小火，加入豆瓣酱，调和味道，我没有豆瓣酱，就用蚝油代替了。爆炒花甲 味道调和好后，加入过冷水的花甲，开大火，爆炒。爆炒 3 分钟后关小火调味【时间实际依据灶的火力大小，家用灶一般火不够大，要多炒一会】，加入食用盐【如果豆瓣酱或者生抽够味就不用加盐了】、生抽、辣椒酱，大火翻炒十几秒。我又补了一点辣椒酱 最后一步也很关键，使用淀粉液勾芡，目的是让味道均匀包裹在花甲的表面，否则味道都会遗留在锅里，导致花甲味道偏淡。勾芡可以实现真正入味的效果。出锅装盘 如果有香葱的话，出锅前再放一点点香葱，更好看，味道也更好。我这里没放香葱，使用青椒代替了。其它配菜 顺手做了一道红烧肉，加了香菇和 2 个鸡蛋，有点偏卤肉的口味了。红烧肉收汁阶段 红烧肉成品 注意事项 花甲吐沙 花甲吐沙一定要做好，不可匆匆了事，否则花甲吃起来全部都是沙子就不好了，严重影响口感，另外还要注意在爆炒的时候会有一些花甲壳碎掉，混在花甲肉里面，吃的时候也要注意一下，虽然不是沙子，但是也会硌牙。花甲焯水 花甲焯水，可以适当放一点姜片、料酒，去腥味。当然，如果吐沙的方法采用的是温水慢煮的方式，可以不用再放任何调料了，直接焯水就行了。焯水时要切记水不能太沸腾，或者不要一直放在沸水里面，要适当用漏勺翻一下，否则会导致花甲肉全部脱离花甲壳了，这个和花甲新不新鲜无关，水太沸腾会导致大量的花甲肉脱离花甲壳。勾芡 勾芡，才能保证味道均匀分布在花甲表面，吃起来才有入味的效果，否则调味料大部分都粘在锅的表面了。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>爆炒花甲</tag>
        <tag>麻辣花甲</tag>
        <tag>炒花甲</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Valine 给 Hexo 博客添加评论系统]]></title>
    <url>%2F2019032001.html</url>
    <content type="text"><![CDATA[我的博客已经搭建得差不多了，一些配置也固定下来了，最近重点一直在补充博客内容，把以前的笔记都整理出来。然后有一天我就想，好像总感觉少点什么，发现评论这个功能是没有的。以前是为了追求简洁的风格，而且评论这个功能不稳定，主要是评论系统不好选择，很多都关闭了。思前想后，考虑了好几天，最终还是决定先加上评论功能，实验一阵子，看看有没有必要，后续再决定是取消还是继续，反正也就是改一下配置就行了，没有多大工作量。接下来查了一下当前还活着的评论系统的种类，最后选择了 Valine 这个评论系统。它不需要登录，无后台管理，非常简洁，比较符合我追求的理念。参考相关内容：https://github.com/xCss/Valine 、https://valine.js.org 、https://leancloud.cn 。注册帐号创建应用 Valine 诞生于 2017 年 8 月 7 日，是一款基于 Leancloud 的快速、简洁且高效的无后端评论系统。 所以，第一步就需要注册 Leancloud 账号，然后才能申请应用的 appid 和 appkey。注册过程我就不赘述了，和注册普通的账号一样，官网地址：https://leancloud.cn 。接下来重点来了，需要申请免费的应用【有钱的话也可以购买收费的版本】，这里面有一些需要注意的地方，否则最后评论的时候没有效果，会导致 Leancloud 后台接收不到评论数据。1、登录 Leancloud 系统，进入系统的控制台，然后创建应用。从主页进入控制台 创建应用，我这里已经创建好一个应用了。2、填写、选择应用的参数，这里需要填写应用的名字，选择开发版本【免费版本，限制请求并发数】。3、创建完成后，进入设置详情页面。点击齿轮，进入设置详情页面。在设置详情页面里面，选择 设置 -&gt; 应用 Key，就可以看到应用的 appid 和 appkey，这 2 个字符串要记下来，等一下在 Hexo 里面配置的时候有用。4、在 存储 -&gt; 数据 里面查看默认的 Class 信息，有一些默认的 Class，例如 _User、_File、_Role 等，这些都用不到，而 Hexo 的评论功能需要一个名称为 Comment 的 Class，现在发现没有这个 Class，要不要手动配置一个呢。其实不用担心，经过我的测试 Hexo 会自动生成这个 Class，所以不需要自己手动配置了。5、在 设置 -&gt; 安全中心 ，把 文件上传、短信服务、推送服务、实时通信 这几个服务全部关闭，因为用不到。然后需要特别注意的就是 Web 安全域名 这一个选项，里面一定要填写自己站点的域名，并且带上端口号，例如 http 请求的默认端口就是 80，htps 请求的默认端口就是 443。这里如果没有配置好，评论的时候也会失败的。配置 Hexo 参数 上一步骤已经把 Leancloud 里面的应用申请好了，并且设置了重要的选项，获取到 appid 和 appkey，接下来配置 Hexo 就简单多了。打开 Hexo 主题的配置文件 _config.yml，搜索一下 Valine，找到默认配置【这是因为 Hexo 已经自动集成了 Valine 评论系统，不需要安装什么，如果没有请升级 Hexo 版本】。默认是关闭的，把配置更改如下图，更为详细内容参考：https://valine.js.org/configuration.html 。主要配置的内容如下【重点是 appid、appkey、placeholder，至于验证、邮件提醒就按照自己的需要来配置吧】：123456789101112131415161718valine: # 开启 Valine 评论 enable: true # 设置应用 id 和 key appid: CCCJixxxxxxXXXxxxXXXX000-gzGzo000 appkey: AA1RXXXXXhPXXXX00F0XXXJSq # mail notifier , https://github.com/xCss/Valine/wiki # 关闭提醒与验证 notify: false verify: false # 文本框占位文字 placeholder: 没有问题吗？ # 需要填写的信息字段 meta: [&apos;nick&apos;,&apos;mail&apos;] # 默认头像 avatar: wavatar # 每页显示的评论数 pageSize: 10这里面我发现一个问题，就是有一些配置项不生效，例如：meta、avatar，我也不知道是 Hexo 的问题还是 Valine 的问题，我也不懂，就先不管了，因为不影响评论这个功能。另外还有一个就是评论的时候总会强制检验邮箱和 url 的规范性，如果没填或者填的不规范就弹框提示，我不知道怎么取消，只好在在 GitHub 提了一个 Issue，详见：https://github.com/xCss/Valine/issues/168 ，但是作者一直没回。等了几天，作者回复了，说是我的 Valine 版本太低，让我升级。我看了本地的 Valine 的版本，已经是 v1.3.5 了，然后我就怀疑可能是 Hexo 的版本问题，但是我自己做了很多自定义的配置，改了很多 css、js 文件，不能随便升级，等以后有时间做一个大版本的升级，再好好整理。那怎么才能让博客文章的底部显示评论对话框呢，其实很简单，什么都不用做，Hexo 默认是给每个页面都开启评论的【前提是在 Hexo 的配置文件中开启了一种评论系统】。它背后的配置就是 Markdown 文件的 comments 属性，默认设置是 true，所以不用配置了，如果非要配置也可以，如下图。此外，还需要注意，如果博客还有除正文内容之外的页面存在，例如关于、分类、标签，要把他们的 Markdown 文件的 comments 属性设置为 false，否则这些页面在展示的时候也会有评论的功能出现，总不能让别人随便评论吧。测试效果 打开任意一篇博客文章，可以看到底部已经有评论的文本框了。试着填写内容，评论一下，可以看到评论列表的内容。好了，此时可以再回到 Leancloud 系统，看一下评论数据吧。直接在 存储 -&gt; 数据 -&gt;Comment 里面，可以看到已经有评论数据了。由于 Valine 是无后端的评论系统，所以数据直接被存储到了 Leancloud 系统的数据库表里面，看看就行了，不方便管理。如果评论数据很多，为了更方便管理评论数据，能收到更友好的邮件通知提醒，可以使用 Valine-Admin 来实现，我暂时先不用。经过几天的测试，可以看到应用的请求量统计信息。附加 Valine-Admin 进行评论数据管理 这个插件我现在先不使用，因为还不知道评论数据会怎么样呢，等以后如果确实有需要再考虑增加，参考项目：https://github.com/zhaojun1998/Valine-Admin 。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Valine</tag>
        <tag>评论系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 客户端设置 Windows 下的字符编码]]></title>
    <url>%2F2019031901.html</url>
    <content type="text"><![CDATA[在 Linux 以及大多数托管网站上，默认的字符编码均是 UTF-8，而 Windows 系统默认编码不是 UTF-8，一般是 GBK。如果在 Windows 平台使用 Git 客户端，不设置 Git 字符编码为 UTF-8，Git 客户端在处理中文内容时会出现乱码现象，很是烦人。但是，如果能正确设置字符编码，则可以有效解决处理中文和中文显示的问题。大多数技术从业者应该都遇到过各种各样的编码问题，后来渐渐习惯了使用英文，尽量避免中文，但是也有一些场景是必须使用中文的。本文就记录解决 Git 中文处理和中文显示的问题的过程，系统环境基于 Windows7 X64，Git 基于 v2.18.0。乱码现象 Git 是一款非常好用的分布式版本控制系统，为了更好地使用它，一般都需要 Git 客户端的配合，下载使用参考：https://git-scm.com/downloads 。 在 Windows 平台使用 Git 客户端的过程中，有一个问题你一定逃不掉，那就是乱码问题。这是因为 Windows 系统的编码是 GBK，而 Git 客户端的编码是 UTF-8，当两种不同的编码相遇，必然有一方会乱码。如果设置 Git 客户端的编码为 GBK，那么在使用 Git 客户端处理系统文件的时候可以正常显示，但是处理 Git 版本控制内容的时候，就会乱码，无法支持中文。如果反过来呢，把 Git 客户端的编码设置为 UTF-8，那么处理版本控制内容就可以有效支持中文，但是处理系统文件的时候又会乱码。Git 客户端设置 UTF-8 编码，处理系统文件显示乱码 解决方式 这样看起来似乎没有解决方法，其实不是的，还是有很好的解决方法的。我这里为了完全支持版本管理系统，版本管理优先，肯定要统一设置为 UTF-8 编码，然后通过 Git 客户端的编码自动转换来支持系统的 GBK 编码。这里先提前说明，在使用 Git 客户端的时候，Git 的安装目录【一般默认是 C:\Program Files\Git】，也就是 Git 的根目录。在使用 ls 等命令处理文件时，如果携带了 / 字符，其实就表示从 Git 的安装目录开始。例如在里面寻找 etc 目录，如果是使用 Git Bash 打开的，可以直接使用根目录的方式，cd /etc/。再例如 vi /etc/git-completion.bash 不是表示从系统的根目录开始寻找文件【Windows 系统也没有根目录的概念】，而是表示从 Git 的安装目录开始寻找文件。设置 Git 客户端 打开 Git 客户端的主页面，右键打开菜单栏【或者点击窗口的左上角也可以打开】，选择 Options 选项。接着选择 Text 参数配置，把编码方式由 GBK 改为 UTF-8【locale 也要设置为 zh_CN】。设置完成后，一定会导致一个现象，那就是使用 ls 查看系统文件时，带有中文的目录和带有中文的文件，一定是乱码的，根本看不清楚显示的是什么。不过不用担心，后面会通过设置让它恢复正常的。接下来要解决的是显示的问题，目的是保证 Windows 的 GBK 编码可以在 Git 客户端正常显示。由于 Git 客户端被设置为了 UTF-8 编码，使用 ls 命令查看目录文件详情的时候，一定是乱码的，什么也看不出来【数字和英文不受影响】。那就需要设置 ls 命令的参数，让它按照 Git 客户端的编码来显示，不支持的字符也要显示，这样再使用 ls 命令的时候，就会自动把 GBK 编码转为 UTF-8 编码，那么带有中文的目录、带有中文的文件都能正常显示了。最简单的做法，就是需要指定 ls 命令的附加参数【–show-control-chars】，为了方便，直接更改配置文件 /etc/git-completion.bash 【没有的话新建一个既可】，在行尾增加配置项 alias ls=”ls –show-control-chars –color” 。其实就是通过新建别名这个技巧把 ls 命令的含义扩展了，让它可以根据 Git 客户端的编码转换系统的编码【在这里就是把 GBK 转为 UTF-8】。12vi /etc/git-completion.bashalias ls=&quot;ls --show-control-chars --color&quot;更改完成后，可以看到能正常显示系统中的带有中文名称的文件了。但是还要注意一点，如果使用 Git 客户端的 Bash 处理其它命令，一定会乱码的，因为不像 ls 那样做了编码转换。以下 2 例【分别时使用 elasticsearch、java 命令】：那这个现象有没有办法解决呢，网上大多数解决办法都是说把 Git 客户端的编码设置为和 Windows 系统一样，一般设置为 GBK，这显然是又倒退回去了【为了满足 Git 一定要设置为 UTF-8】。其实唯一的解决办法就是从命令的参数下手，把原生的命令利用别名机制给加上编码有关的参数，和修改 ls 命令的做法一致。以下供参考：1234-- 在文件最后追加，不要修改文件原有的内容 vi /etc/bash.bashrcalias javac=&quot;javac -J-Dfile.encoding=UTF-8&quot;alias java=&quot;java -Dfile.encoding=UTF-8&quot;设置 Git接下来就是设置 Git 进行版本控制时使用的编码方式，例如提交信息时支持输入中文日志、输出 log 可以正常显示中文。设置 Git 有两种方式，一种是通过更改配置文件，另一种是通过 Git 自带的命令来配置参数。为了显得没有手动去破坏 Git 的原有配置文件，我就使用 Git 自带的命令来配置编码。当然，通过更改配置文件的方式也会一同描述出来。1、通过命令行把 Git 的各种编码都设置为 UTF-812345git config --global core.quotepath false # 显示 status 编码 git config --global gui.encoding utf-8 # 图形界面编码 git config --global i18n.commit.encoding utf-8 # 处理提交信息编码 git config --global i18n.logoutputencoding utf-8 # 输出 log 编码 export LESSCHARSET=utf-8 # 因为 git log 默认使用 less 分页，所以需要 bash 对 less 命令处理时使用 utf-8 编码 2、如果通过配置文件的方式来更改，则需要编辑配置文件 /etc/gitconfig 【没有则新建一个】，在里面设置以下内容。1234567[core] quotepath = false [gui] encoding = utf-8 [i18n] commitencoding = utf-8 logoutputencoding = utf-8另外还需要在配置文件 /etc/profile 中新增 1export LESSCHARSET=utf-83、特殊说明gui.encoding = utf-8 是为了解决 git gui 和 gitk 中的中文乱码问题，如果发现代码中的注释显示乱码，可以在所属项目的根目录中 .git/config 文件中添加：12[gui] encoding = utf-8i18n.commitencoding = utf-8 是为了设置 commit log 提交时使用 UTF-8 编码。i18n.logoutputencoding = utf-8 是为了保证在 git log 时使用 UTF-8 编码。export LESSCHARSET=utf-8 是为了保证 git log 翻页时使用 UTF-8 编码，这样就可以正常显示中文了【配合前面的 i18n.logoutputencoding 设置】。 验证 add 执行的时候 Git 输出的日志都是中文显示的，特别是带有中文名称的文件。 验证提交时填写日志信息，可以直接填写中文日志，另外 Git 的输出日志也是以中文来显示的，可以看到哪些文件变更了。验证使用 git log 查看历史日志时正常显示中文内容 注意事项1、此外，Cygwin 在 Windows 平台上也有同样的问题，设置方式也是类似的。当然，如果只是查看目录文件，使用基本的命令，请尽量脱离带有中文的目录和带有中文的文件，避免踩坑，这样还可以把编码直接设置为 GBK 了，但是遇到特殊的情况还是脱离不了 UTF-8 编码。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Windows</tag>
        <tag>中文乱码</tag>
        <tag>gbk</tag>
        <tag>utf-8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[麻婆豆腐做法总结]]></title>
    <url>%2F2019031601.html</url>
    <content type="text"><![CDATA[麻婆豆腐是一道川菜，口味特色就是麻、辣、鲜，而且非常下饭。我常听说正宗的麻婆豆腐要用郫县豆瓣酱【回锅肉也是这样】，才能做出来正宗的味道，但是我身边没有那么多材料，只能做一道简单版的麻婆豆腐。本文就记录麻婆豆腐的做法总结。食材准备 食材就很简单了，以下的量为一盘：嫩豆腐一块，一般 2 块钱左右 瘦肉 50 克，剁成肉沫 豆瓣酱，我没有豆瓣酱就用一种混合调味酱【辣椒酱、豆瓣酱、胡椒粉】替代了 辣椒粉，或者辣椒酱 青花椒，最好用青花椒，才够麻的味道【买的散装青花椒里面会有一些其它植物的种子，要细心挑拣出去】一块嫩豆腐 青花椒 制作步骤 初步处理食材 豆腐切小块，稍微焯水，备用。如果豆腐质量比较好的话，可以一整块冲一下水就行，不用再焯水了。我买的这个豆腐有点碱的味道【类似魔芋一样】，所以稍微焯一下水为好。焯水时不能用大火，否则豆腐会碎掉的，也可以在焯水时稍微放一点点盐进去。豆腐切小块 豆腐简单焯水 瘦肉剁成肉沫，稍微腌制一下，备用。我为了保持肉沫的鲜嫩，还裹了一层淀粉液。瘦肉剁成肉沫 肉沫腌制 肉沫裹淀粉液 炒肉沫 锅里放油烧热，稍微多放一点油，然后倒入肉沫翻炒，基本 30 秒就可以了。肉沫下锅 肉沫翻炒 炒豆腐 一般的做法应该是接着放豆瓣酱，炒出红油，然后加青花椒、辣椒粉，加水煮了一段时间后，再下豆腐。但是我就不搞这么复杂的过程了，直接炒一下豆腐，把豆腐和肉沫混合在一起。豆腐下锅炒 豆腐肉沫混合 加混合调味酱、辣椒酱、调味料、青花椒 加水开煮 因为我的豆腐已经焯水了，所以很容易就熟了，接着再加热水煮开，转为小火再煮 5 分钟就行了。切记别加太多热水，否则变汤了，我这个加的有点多，要多煮一会儿水才能蒸发。加热水，刚刚淹没豆腐 煮开后转为小火 小火慢煮 收汁出锅装盘 煮了 5 分钟就可以准备收汁了，接着还要进行勾芡，我使用淀粉液进行勾芡。勾芡完成稍微再煮 30 秒就可以关火，出锅装盘。收汁 调淀粉液，少量淀粉加水 调淀粉液成品 勾芡完成，可以看到有点浓稠 出锅装盘 注意事项1、为了保证肉沫的鲜嫩，千万不要炒太久，下锅后稍微炒一下就行了，因为后续还要加水煮很久呢。我这里没有采用炒豆瓣酱出红油的做法，所以就用淀粉液裹了一下，肉沫炒熟后直接下豆腐。2、切豆腐时豆腐一般都会粘在刀上，所以有一个技巧就是从手心往手指的方向反着切，切完一刀就可以用手压住，这样豆腐就不会粘在刀上了。参考如下图【我是左手持刀，右手压豆腐】：3、收汁时最好使用淀粉液勾芡一下，这样才能保证调味料都裹在豆腐上，达到入味的效果。否则味道可能都遗落在汤汁里面了，导致豆腐没有什么味道。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>麻婆豆腐</tag>
        <tag>豆腐</tag>
        <tag>麻辣豆腐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[青椒肉丝做法总结]]></title>
    <url>%2F2019031101.html</url>
    <content type="text"><![CDATA[青椒肉丝，本来是一道做法非常简单的菜肴，但是想做好却不容易，为什么呢？一是因为肉丝和青椒丝很多人切不出来，可能切出来的是条状的，那就做不出来青椒肉丝；二是因为腌制肉的时候没有进行裹淀粉的步骤【有条件裹鸡蛋清当然更好】，导致肉刚下锅就老，炒不出鲜嫩滑爽的效果；三是因为炒的时候油量和油温没有控制好，导致肉炒老了，不好吃。本文则记录青椒肉丝的炒制过程以及需要注意的地方，请观察一下我是怎么做出来一道家常的青椒肉丝的。食材准备 食材就很简单了，以下是一盘的份量：纯瘦肉 200 克 大青椒一棵 大红椒一棵【为了好看，搭配一棵红椒】淀粉少量、花生油少量 制作步骤 食材处理 主要就是切肉丝，切青椒丝，腌制肉丝，注意肉丝要切细一点。切肉丝的时候先把肉切片【一只手掌按着肉，刀躺着从手掌下划过】，最后用肉片去切肉丝，可以保证肉丝的质量。腌制肉丝的时候除了调味料，还需要加一点点水和淀粉【淀粉不要多放，否则菜炒出来会偏甜】，用手抓均匀，让淀粉液充分裹在肉丝表面【有条件就用鸡蛋清替代更好，但是肉丝少的时候就没必要了，用不完一个鸡蛋】。最后还要加一点点花生油，防止肉丝下锅的时候粘锅。肉丝腌制 10 分钟。青红椒切丝 腌制肉丝加调料 腌制肉丝加水和淀粉 抓均匀后放一点花生油 肉丝炒制，盛出备用 锅里加油，多加一点，先烧热，然后关火让油冷却一下。冷却到 3 成热再倒入肉丝，然后开大火开始翻炒，基本 30 秒就可以把肉丝炒熟了【取决于肉丝切得好不好】。然后盛出备用。油加热后冷却 倒入腌制好的肉丝 不停地翻炒 盛出备用 青椒炒制，混合翻炒 由于一开始加了偏多的油，此时不需要再放油，或者根据实际情况放一点点也行。油烧热后放入青红椒丝，大火快速翻炒，基本 1 分钟以内就可以把青红椒丝炒至断生。关小火，放入备用的肉丝，翻炒几下，开始调味，放入食用盐、鸡精、耗油，接着大火快速翻炒几下，放几滴香醋，准备出锅。放入青红椒丝翻炒 翻炒至断生【为了拍图青红椒丝炒太熟了】放入肉丝调味翻炒 出锅装盘 盛出装盘 还配了一道麻婆豆腐 注意事项1、买肉一定要买纯瘦肉，最好不要带一丝肉筋，并且形状要规整，薄厚均匀，这样才容易切片进而切肉丝。买肉的时候肯定不需要店方帮忙切肉了，因为他们不可能有时间给你切肉丝出来。此外一定要保证刀比较锋利，锋利的刀更容易处理，如果刀用了很久都没磨过，恰好找这个机会磨磨刀。2、腌制肉丝的时候可以适当加一点水【用来溶解淀粉】，然后加一点淀粉，抓均匀，让淀粉充分裹上肉丝。当然，有条件的直接使用鸡蛋清最好，不需要水和淀粉了，味道还更香。抓均匀后再加一点花生油，防止下锅的时候粘锅。3、肉丝下锅之前要确保油温不高，如果油温过高要开小火让油冷却一下，否则裹着淀粉的肉丝一下锅表面就会糊掉。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>青椒肉丝</tag>
        <tag>青红椒</tag>
        <tag>炒肉丝</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[腊肠炒饭做法总结]]></title>
    <url>%2F2019030901.html</url>
    <content type="text"><![CDATA[蛋炒饭，是一种常见的菜肴，日常生活中听到的最多的就是扬州炒饭、传统炒饭。其实炒饭的做法非常多，口味也非常多，还被改成了很多版本，例如：虾仁炒饭、滑蛋炒蛋、老干妈炒饭。但无论怎么改变，它们的共同点都是做法简单，准备米饭和配菜就行了，炒出来吃起来香喷喷的，口感极好，也不用单独配其它的菜了，很方便。本文就记录腊肠炒饭的家常做法。食材准备 以下准备的是两人份的食材：腊肠，我这里选择的皇上皇腊肠，口味偏甜了，不适合炒饭，去买的时候广州酒家的咸香腊肠卖光了，先凑合着用 鸡蛋 2 个 米饭 2 小碗 青菜 1 小棵，普通的青菜即可，或者放绿豌豆也行，主要为了点缀一下 胡萝卜 1 小段 腊肠，我买的这种偏甜了，不适合炒饭，下次还是买广州酒家的咸香腊肠比较好。米饭 全部食材 制作步骤 1、处理配菜 配菜全部都切好备用：青菜切碎，腊肠切斜片，胡萝卜切丁，鸡蛋打入碗里搅拌，米饭捣碎。这里需要注意的有三点：一是胡萝卜要去皮，更能凸显颜色，而且没有胡萝卜皮的影响，炒饭吃起来口感也更好。二是米饭要捣碎，让米粒分开，不要一整块下锅，所以最好选择比较干硬的米饭来做炒饭，炒起来更方便，口感也更好。三是搅拌鸡蛋之前可以稍微放一点点食用盐，这样做为了鸡蛋更入味。2、炒鸡蛋备用 开火，锅里放油，一定要多放点油，因为鸡蛋非常吃油。放多点没关系，因为后续炒饭可以少放点。油烧热后，把鸡蛋液倒放进去，摇晃炒锅，让鸡蛋在里面呈圆形，防止鸡蛋液堆在一起。如果鸡蛋液太多了，可以在底部的鸡蛋液成型后，用锅铲拨到一边，让上边的鸡蛋液流下来，继续成型。鸡蛋基本成型后，就可以用锅铲搅拌捣碎。实际上用炒勺做就方便一点，如果是用锅铲就不太方便。这里需要注意，鸡蛋不要炒制太熟，要让它保持嫩嫩的，因为等一下还要和米饭一起重新下锅。捣碎后盛出备用，如果有时间的话，可以把鸡蛋的蛋清和蛋黄分别炒制，做出来的颜色会更好看。3、炒配菜和饭 鸡蛋盛出后，锅里表面其实还有大量的油，接着再稍微放一点点油就行了。继续加热，放入腊肠片、胡萝卜、碎青菜，大火炒 30 秒。加腊肠片 加胡萝卜青菜【我忘记放碎青菜了，出锅前才补上，看后面的图】接着就开始加入米饭和刚才的鸡蛋，我这个米饭看起来是一块一块的，其实一碰就散了，开大火不停地翻炒。翻炒 3 分钟左右【如果一开始米饭没有捣碎，或者刚从冰箱拿出来的冷米饭，炒起来会比较慢】，基本就熟了，关小火，准备调味。加入米饭和鸡蛋 炒熟了，准备调味 4、调味出锅 放入盐、鸡精、生抽、老抽，然后继续开大火，翻炒几十秒，出锅装盘，美滋滋。我还放了一点榨菜和辣椒酱。放入调味料，因为放了老抽，可以看到颜色有一点点变化 前面忘记放青菜了，补回来 出锅装盘，这图片有点糊了 注意事项1、米饭的选择，米饭不是随便都适合做炒饭的，要选择那种稍微干硬一点的，米粒都分开的，炒出来会更香。如果是剩米饭，已经是一整块了，千万不要一整块的下锅，很难分开，要提前捣碎，处理好再下锅炒。2、一开始炒鸡蛋的时候，不要炒制太熟，要保持嫩嫩的，因为后面还要和米饭一起下锅。3、腊肠的选择，不要选择偏甜的口味，否则最终的炒饭吃起来会有点腻，所以还是选择咸香的口味比较好。广州酒家的那种咸香的腊肠，用来炒饭真的很合适。4、关于分量的建议，一般做炒饭至少两人的份量。因为如果只炒一份，各种菜只能放一点，放多了就不是炒饭了。那问题来了，剩下的菜不好办【半个胡萝卜、半棵青菜】，又不能存放太久，只能下次接着炒，甚至连续吃炒饭。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>腊肠炒饭</tag>
        <tag>蛋炒饭</tag>
        <tag>鸡蛋炒饭</tag>
        <tag>炒饭</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Github 的 WebHooks 实现代码自动更新]]></title>
    <url>%2F2019030601.html</url>
    <content type="text"><![CDATA[我的静态博客为了百度爬虫单独部署了一个镜像，放在了我的 VPS 上面【在 vultr 购买的主机】，并单独设置了二级域名 blog.playpi.org。但是，每次 GitHub 有新的提交时【基本每周都会有至少三次提交】，为了及时更新，我都会登录到 VPS 上面，到指定的项目下做一下拉取更新的操作，即执行 git pull。这样操作了三五次，我就有点不耐烦了，自己身为做技术的人，怎么能忍受这个呢，做法既低效又不优雅。于是，我就在想有没有更好的方法来实现自动拉取更新。一开始想到，直接在 VPS 起一个周期性脚本不就行了，比如每隔 1 分钟自动执行 git pull，但是立马又被我否定了，虽然做法很简单，但是太不优雅了，而且极大浪费 CPU。后来想到，GitHub 自带了 WebHooks 功能，概念类似于回调钩子，可以给 GitHub 的项目设置各种各样的行为，满足一定的场景才会触发【例如当有新的 push 时，就会向设置的 url 发送请求，并且在请求体中携带 push 的相关信息】。我的自动化构建就是这样的原理，每当 source 分支有提交时，都会通知 tavis-ci【这就是一个行为】，然后在 travis-ci 中设置好脚本，自动运行脚本，就完成了自动生成、部署的操作。根据这个思路，就可以给 GitHub 的项目设置一个 WebHooks，每当 master 分支有提交时【代表着静态博客有更新了】，会根据设置的链接自动发送消息到 VPS 上面，然后 VPS 再执行拉取更新，这样的话就优雅多了。但是问题又来了，满足这种场景还需要在 VPS 设置一个后台服务，用来接收 GitHub 的消息通知并执行拉取更新的操作。我想了一下，既然 VPS 上面已经起了 Nginx 服务，那就要充分利用起来，给 Nginx 设置好反向代理，把指定的请求转给另外一个服务就行了。那这个服务怎么选呢，当然是选择 PHP 后台了，毕竟 PHP 号称世界上最好的语言， PHP 后台搭建起来也容易。本文就记录从基础环境安装配置到成功实现自动拉取更新的整个过程，本文涉及的系统环境是 CentOS 7 x64，软件版本会在操作中具体指明。配置服务器的 PHP 支持 VPS 上面的 Nginx 已经安装好了，就不再赘述过程，不清楚的可以参考我的另外一篇文章：GitHub Pages 禁止百度蜘蛛爬取的问题 。配置 PHP 的后台服务支持主要有三个步骤：一是配置安装 PHP，包括附加模块 PHP-FPM，二是配置启动 PHP-FPM 模块，三是配置重启 Nginx。由于我的机器资源问题【配置太低】，在这个过程踩了很多坑，我也会一一记录下来。 毕竟我是新手，有很多地方不是太懂，所以先参考了官网和一些别人的博客，有时候看多了也会迷惑，有些内容大家描述的不一样，所以要结合自己的实际环境来操作，有些步骤是可以省略的。这些链接我放在这里给大家参考：参考 PHP 官网 、CentOS 7.2 环境搭建实录 (第二章：php 安装) 、PHP-FPM 与 Nginx 的通信机制总结 、 使用 Github 的 WebHooks 实现生产环境代码自动更新 。 先安装软件仓库，我的已经安装好了，重复安装也没影响。1yum -y install epel-release踩着坑安装 PHP1、下载指定版本的 PHP 源码，我这里选择了最新的版本 7.3.3，然后解压。1234-- 下载 wget http://php.net/get/php-7.3.3.tar.gz/from/this/mirror -O ./php-7.3.3.tar.gz-- 解压 tar zxvf php-7.3.3.tar.gz2、configure【配置】，指定 PHP 安装目录【默认是 /usr/local/php，使用 –prefix 参数】和 PHP 配置目录【默认和 PHP 安装目录一致，使用 –with-config-file-path 参数】，我这里特意指定各自的目录，更方便管理。12-- 配置，并且开启 PHP-FPM 模块 [使用 --enable-fpm 参数]./configure --prefix=/site/php/--with-config-file-path=/site/php/conf/--enable-fpm遇到报错：configure: error: no acceptable C compiler found in $PATH。竟然缺少 c 编译器，那就安装吧。12-- 安装 gcc 编译器 yum install gcc安装 gcc 编译器成功 安装 gcc 编译器完成后，接着执行配置，又报错：configure: error: libxml2 not found. Please check your libxml2 installation.。这肯定是缺少对应的依赖环境库，接着安装就行。123-- 安装 2 个，环境库 yum install libxml2yum install libxml2-devel -y安装依赖环境库成功 接着就重复上述的配置操作，顺利通过配置。3、编译、安装。12-- 编译，安装一起进行 make &amp;&amp; make install遇到报错：12345cc: internal compiler error: Killed (program cc1)Please submit a full bug report,with preprocessed source if appropriate.See &lt;http://bugzilla.redhat.com/bugzilla&gt; for instructions.make: *** [ext/fileinfo/libmagic/apprentice.lo] Error 1这是由于服务器内存小于 1G 所导致编译占用资源不足【好吧，我的服务器一共就 512M 的内存，当然不足】。解决办法：在编译参数后面加上一行内容 –disable-fileinfo，减少内存的开销。接着执行编译又报错：12345cc: internal compiler error: Killed (program cc1)Please submit a full bug report,with preprocessed source if appropriate.See &lt;http://bugzilla.redhat.com/bugzilla&gt; for instructions.make: *** [Zend/zend_execute.lo] Error 1这是因为虚拟内存不够用，我的主机只有 512M。没办法了，降低版本试试，先降为 v7.0.0【或者开启 swap 试试，后面发现不用了，切换低版本后就成功了】，接着重新下载、配置、编译、安装，从头再来一遍。12-- 下载的时候更改版本号就行 wget http://php.net/get/php-7.0.0.tar.gz/from/this/mirror -O ./php-7.0.0.tar.gz更换了版本后，一切操作都很顺利，就不再考虑开启 swap 了，最终执行编译、安装完成。真正开始配置 配置、编译、安装完成后，开始编辑各个模块的配置文件，更改默认参数，包括配置 PHP 与 PHP-FPM 模块。确认配置无误，再启动对应的服务或者重新加载对应的配置【也可以使用命令验证参数配置是否正确，下文会有描述】。PHP 配置文件 在执行编译安装的目录，复制配置文件 php.ini-development 粘贴到 PHP 的配置目录【如果一开始 configure 时没有显示指定 PHP 的配置目录，默认应该和 PHP 的安装目录一致，也就是要复制粘贴在 /usr/local/php 中，而我指定了 PHP 的配置目录 /site/php/conf】。1cp php.ini-development/site/php/conf/php.ini更改 PHP 的配置文件，修改部分参数，更改 cgi.fix_pathinfo 的值为 0，以避免遭受恶意脚本注入的攻击。12vi /site/php/conf/php.inicgi.fix_pathinfo=0PHP-FPM 配置文件 在 PHP 的安装目录中，找到 etc 目录【如果在一开始的 configure 时没有显示指定 PHP 的安装目录，默认安装在 /usr/local/php 中，则需要到此目录下寻找 etc 目录，而我指定了 PHP 的安装目录 /site/php/】，复制 PHP-FPM 模块的配置文件 php-fpm.conf.default，内容不需要更改。123-- PHP 的附加模块的配置默认安装在了 etc 目录下 cd /site/php/etccp php-fpm.conf.default php-fpm.conf在上面的 etc 目录中，继续复制 PHP-FPM 模块的默认配置文件。因为在上述的配置文件 php-fpm.conf 中，指定了 include=/site/php/etc/php-fpm.d/*.conf，也就是会从此目录 /site/php/etc/php-fpm.d/ 加载多份有效的配置文件，至少要有一份存在，否则后续启动 PHP-FPM 的时候会报错。12-- 先直接使用模板，不改配置参数，后续需要更改用户和组 cp php-fpm.d/www.conf.default php-fpm.d/www.conf配置完成后，开始启动 PHP-FPM 模块，在 PHP 的安装目录中执行。123456789-- PHP 的附加模块的脚本默认安装在了 sbin 目录下 -- 为了方便可以添加环境变量，把 sbin、bin 这 2 个目录都加进去 cd /site/php-- 配置文件合法性测试 ./sbin/php-fpm -t-- 启动，现在还不能使用 service php-fpm start 的方式，因为没有把此模块配置到系统里面 ./sbin/php-fpm-- 检验是否启动 ps aux|grep php-fpm配置文件合法性检测 可以看到正常启动了 那怎么关闭以及重启呢，PHP 5.3.3 以后的 PHP-FPM 模块不再支持 PHP-FPM 以前具有的 ./sbin/php-fpm (start|stop|reload) 等命令，所以不要再看这种古老的命令了，需要使用信号控制：INT，TERM，立刻终止 QUIT 平滑终止USR1 重新打开日志文件USR2 平滑重载所有 worker 进程并重新载入配置和二进制模块 注意，这里的信号标识和 Unix 系统中的一样，被 kill 命令所使用，其中 USR1、USR2 是用户自定义信号，PHP-FPM 模块需要自定义实现，仅供参考。其中，根据 Unix 基础知识，INT【2】表示中断信号，等价于 Ctrl + C，TERM【15】表示终止信号【清除后正常终止，不同于编号 9 KILL 的强制终止而不清除】，QUIT【3】表示退出信号，等价于 Ctrl + \，USR1【10】、USR2【12】这 2 个表示用户自定义信号。所以可以使用命令 kill -INT pid 来停止 PHP-FPM 模块，pid 的值可以使用 ps aux|grep php-fpm 获取。当然，也可以使用 kill -INT pid 配置文件路径 来停止 PHP-FPM 模块，pid 配置文件路径 可以在 php-fpm.conf 中查看，pid 参数 ，默认是关闭的。为了能使用 service php-fpm start|stop|restart|reload) 的方式来进行启动、停止、重启、重载配置，这种方式显得优雅，需要把此模块配置到系统里面。在 PHP 的编译安装目录，复制文件 sapi/fpm/init.d.php-fpm ，粘贴到系统指定的目录即可。1234567cd /site/php-7.0.0-- 复制文件 cp sapi/fpm/init.d.php-fpm/etc/init.d/php-fpm-- 添加执行权限 chmod +x /etc/init.d/php-fpm-- 添加服务 chkconfig --add php-fpmNginx 的配置文件 接下来就是更改 Nginx 的配置文件，让 Nginx 支持 PHP 请求，并且同时设置好反向代理，把请求转给 PHP-FPM 模块处理【前提是在不影响 html 请求的情况下】，在 server 中增加一个配置 location。12345678910111213-- 打开配置文件 vi /etc/nginx/nginx.conf-- 更改 server 模块的内容，增加 php 的配置 -- 80 端口就不用管了，直接在 443 端口下配置 location ~* \.php$ &#123; fastcgi_index index.php; fastcgi_pass 127.0.0.1:9000; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param SCRIPT_NAME $fastcgi_script_name; &#125;-- 重新加载 nginx 配置，不需要重启 nginx -s reload这样配置好，就会把所有的 PHP 请求转给 PHP-FPM 模块处理，同时并不会影响原来的 html 请求。额外优化配置项 此外，还有一些环境变量配置、开机启动配置，这里就不再赘述了，这些配置好了可以方便后续的命令简化，不配置也是可以的。12345678910-- 设置开机启动的 chkconfig 方法，以下是添加服务 cp sapi/fpm/init.d.php-fpm/etc/init.d/php-fpmchmod +x /etc/init.d/php-fpmchkconfig --add php-fpm-- 设置开机启动 chkconfig php-fpm on-- 添加环境变量，之后 php 的相关命令就可以直接使用了 vi /etc/profileexport PATH=$PATH:/site/php/bin:/site/php/sbinsource /etc/profilePHP 脚本 先在静态站点的根目录下，添加默认的 index.php 文件，用来测试，内容如下，内容的意思是输出 PHP 的所有信息。注意，PHP 文件的格式是以 &lt;?php 开头，以 ?&gt; 结尾。12vi index.php&lt;?php phpinfo (); ?&gt;打开浏览器访问，可以看到成功，这就代表着 PHP 与 Nginx 的配置都没有问题，已经能正常提供服务。接下来就来测试一下复杂的脚本，可以用来自动拉取 GitHub 的提交。再创建一个 auto_pull.php 文件，内容如下，会自动到执行目录拉取 GitHub 的更新，这样就能实现镜像的自动更新了，还加入了秘钥验证【先不用管功能性是否可用，而是先测试一下复杂的 PHP 脚本能不能正常执行，脚本内容后续还要优化更改】，内容大致如下。123456789101112131415161718192021222324252627282930313233343536vi auto_pull.php&lt;?php// 生产环境 web 目录 $target = &apos;/site/iplaypi.github.io&apos;;// 密钥，验证 GitHub 的请求 $secret = &quot;test666&quot;;// 获取 GitHub 发送的内容 $json = file_get_contents (&apos;php://input&apos;);$content = json_decode ($json, true);// GitHub 发送过来的签名 $signature = $_SERVER [&apos;HTTP_X_HUB_SIGNATURE&apos;];if (!$signature) &#123; return http_response_code (404);&#125;list ($algo, $hash) = explode (&apos;=&apos;, $signature, 2);// 计算签名 $payloadHash = hash_hmac ($algo, $json, $secret);// 获取分支名字 $branch = $content [&apos;ref&apos;];// 判断签名是否匹配，分支是否匹配 if ($hash === $payloadHash &amp;&amp; &apos;refs/heads/master&apos; === $branch) &#123; $cmd = &quot;cd $target &amp;&amp; git pull&quot;; $res = shell_exec ($cmd); $res_log = &apos;Success:&apos;.PHP_EOL; $res_log .= $content [&apos;head_commit&apos;][&apos;committer&apos;][&apos;name&apos;] . &apos; 在 & apos; . date (&apos;Y-m-d H:i:s&apos;) . &apos; 向 & apos; . $content [&apos;repository&apos;][&apos;name&apos;] . &apos; 项目的 & apos; . $content [&apos;ref&apos;] . &apos; 分支 push 了 & apos; . count ($content [&apos;commits&apos;]) . &apos; 个 commit：&apos; . PHP_EOL; $res_log .= $res.PHP_EOL; $res_log .= &apos;=======================================================================&apos;.PHP_EOL; echo $res_log;&#125; else &#123; $res_log = &apos;Error:&apos;.PHP_EOL; $res_log .= $content [&apos;head_commit&apos;][&apos;committer&apos;][&apos;name&apos;] . &apos; 在 & apos; . date (&apos;Y-m-d H:i:s&apos;) . &apos; 向 & apos; . $content [&apos;repository&apos;][&apos;name&apos;] . &apos; 项目的 & apos; . $content [&apos;ref&apos;] . &apos; 分支 push 了 & apos; . count ($content [&apos;commits&apos;]) . &apos; 个 commit：&apos; . PHP_EOL; $res_log .= &apos; 密钥不正确或者分支不是 master, 不能 pull&apos;.PHP_EOL; $res_log .= &apos;=======================================================================&apos;.PHP_EOL; echo $res_log;&#125;?&gt;接下来先手工测试一下 PHP 文件的访问是否正常，可以使用 curl 模拟请求，或者直接使用 GitHub 的 WebHooks 请求。我这里为了简单，先使用 curl 命令来测试，后续的步骤才使用 GitHub 来真正测试。1curl -H &apos;X-Hub-Signature:test&apos; https://blog.playpi.org/auto_pull.php可以看到，访问正常，先不管功能上能不能正常实现，至少保证 PHP 可以正常提供服务，后面会和 GitHub 对接。测试 WebHooks 效果 在 GitHub 中使用 WebHooks，为了表现出它的效果是什么样，我画了一个流程图，可以直观地看到它优雅的工作方式。在上一步骤中，自动拉取更新的脚本已经写好，并且使用 curl 测试过模拟访问可用，那接下来就测试功能是否可用，当然，踩坑是避免不了的，优化脚本内容也是必要的。特别要注意用户权限和脚本内容这两方面，用户权限方面我直接使用 nginx 用户，踩坑比较少，脚本内容方面要保证你的服务器支持 shell_exec () 这个 PHP 函数，可以在 index.php 文件中加一段代码 echo shell_exec (‘ls -la’);，测试一下。我的机器经过测试时支持的。在 GitHub 设置 WebHooks在 GitHub 对应项目的设置【Settings】中，找到 Webhooks 选项，可以看到已经有一些设置完成的 WebHook，这里面就包括 travis-ci 的自动构建配置。然后点击新建按钮，创建一个新的 WebHook【这个过程需要重新填写密码确认】，填写必要的参数，url 地址、秘钥、触发的事件，然后确认保存即可。注意，秘钥只是为了测试使用，实际应用时请更改，包括 WebHooks 的秘钥设置和 PHP 脚本里面的秘钥字符串。如果是第一次创建完成，还没有触发请求的历史记录，可以先手动在 master 分支做一次变更提交，然后就会触发一次 WebHooks 事件。我这里已经有触发历史了，拿一个出来看就行了。注意，为了方便测试，只要有一次请求就行了，因为如果后续更改了脚本，不用再手动向 master 分支做一次变更提交，可以直接点击重新发送【redeliver】。触发请求的信息，就是 http 请求头和请求体 VPS 的 PHP 后台服务返回的信息，可以看到正常处理了 WebHooks 请求，但是没有做拉取更新的操作，原因可能是秘钥不对或者分支不对。 测试功能是否可用 以下内容所需要的 PHP 脚本：index.php、auto_pull.php 。12345&lt;?phpecho shell_exec (&quot;id -a&quot;);echo shell_exec (&apos;ls -la&apos;);phpinfo ();?&gt;12345678910111213141516171819202122232425262728293031323334353637383940&lt;?php// 生产环境 web 目录 $target = &apos;/site/iplaypi.github.io&apos;;// 密钥，验证 GitHub 的请求 $secret = &quot;test666&quot;;// 获取 GitHub 发送的内容，解析 $json = file_get_contents (&apos;php://input&apos;);$content = json_decode ($json, true);// GitHub 发送过来的签名，一定要大写，虽然 http 请求里面是驼峰法命名的 $signature = $_SERVER [&apos;HTTP_X_HUB_SIGNATURE&apos;];if (!$signature) &#123; return http_response_code (404);&#125;// 使用等号分割，得到算法和签名 list ($algo, $hash) = explode (&apos;=&apos;, $signature, 2);// 在本机计算签名 $payloadHash = hash_hmac ($algo, $json, $secret);// 获取分支名字 $branch = $content [&apos;ref&apos;];// 日志内容 $logMessage = &apos;[&apos; . $content [&apos;head_commit&apos;][&apos;committer&apos;][&apos;name&apos;] . &apos;] 在 [&apos; . date (&apos;Y-m-d H:i:s&apos;) . &apos;] 向项目 [&apos; . $content [&apos;repository&apos;][&apos;name&apos;] . &apos;] 的分支 [&apos; . $content [&apos;ref&apos;] . &apos;] push 了 [&apos; . count ($content [&apos;commits&apos;]) . &apos;] 个 commit&apos; . PHP_EOL;$logMessage .= &apos;ret:[&apos; . $content [&apos;ref&apos;] . &apos;],payloadHash:[&apos; . $payloadHash . &apos;]&apos; . PHP_EOL;// 判断签名是否匹配，分支是否匹配 if ($hash === $payloadHash &amp;&amp; &apos;refs/heads/master&apos; === $branch) &#123; // 增加执行脚本日志重定向输出到文件 $cmd = &quot;cd $target &amp;&amp; git pull&quot;; $res = shell_exec ($cmd); $res_log = &apos;Success:&apos; . PHP_EOL; $res_log .= $logMessage; $res_log .= $res . PHP_EOL; $res_log .= &apos;=======================================================================&apos;.PHP_EOL; echo $res_log;&#125; else &#123; $res_log = &apos;Error:&apos; . PHP_EOL; $res_log .= $logMessage; $res_log .= &apos; 密钥不正确或者分支不是 master, 不能 pull&apos; . PHP_EOL; $res_log .= &apos;=======================================================================&apos;.PHP_EOL; echo $res_log;&#125;?&gt;上面已经测试了访问正常，但是为了保证 PHP 脚本的功能正常执行，接下来要优化 PHP 脚本内容了。我分析一下，根据脚本的内容，只有当秘钥正确并且当前变更的分支是 master 时才会执行拉取更新操作，看返回结果也是这样的。当前没有执行拉取更新的操作，但是我的这一个触发通知里面是表明了 master 分支【根据 ref 参数】，那就是秘钥的问题了，需要详细看一下秘钥计算的那段 PHP 代码。如果怕麻烦，直接把加密这个流程去掉【会导致恶意请求，浪费 CPU 资源】，GitHub 并没有要求一定要填写秘钥，但是我为了安全，仍旧填写。我看了一下代码，并没有发现问题，于是加日志把后台处理的一些结果返回，看看哪里出问题了。最终发现竟然是分支名字的问题，PHP 代码通过 $content 没有获取到任何内容，包括分支名字、项目名字、提交信息等，而秘钥签名的处理是正常的。思考了一下，然后我就发现，竟然是创建 WebHooks 的时候内容传输类型【Content type】设置错误，不能使用默认的，要设置为 application/json，否则后台的 PHP 代码处理不了内容解析，获取的全部是空内容。好，一切准备就绪，再来试一次，问题又来了，果然用户权限问题是逃不了的。这个问题我早有防备，本质就是没有设置好 PHP 的用户，导致 PHP 执行脚本的时候，没有权限获取与 Git 有关的信息【执行脚本的用户没有自己的家目录，也没有存储 ssh 认证信息】。接下来就简单了，去设置 PHP 的执行用户，可能还要涉及到 Nginx。先在原先的 index.php 脚本中增加内容 echo shell_exec (“id -a”);，用来输出当前用户信息，发现是 nobody，那就和我想的一样了。为了规范起来便于管理，还是改为和 Nginx 同一个用户比较好，还记得 PHP-FPM 模块的配置文件吗 /site/php/etc/php-fpm.d/www.conf ，去里面找到用户和组的配置项 user、group，把 nobody 改为 nginx。为什么选择 nginx 用户呢，因为我的 Nginx 服务使用的就是 nginx 用户，这样就不用再创建一个用户了，可以去配置文件 /etc/nginx/nginx.conf 里面查看。其实，用户的设置是随意的，如果把 PHP-FPM 的用户设置为 root 更方便，但是这样有很大风险，所以不要这么做。如果非要使用 nobody 也是可以的，我只是为了方便管理用户，和 Nginx 服务共同使用一个用户。一切配置完成后别忘记重启 PHP-FPM 模块。接着就是最重要的步骤了，把本地的 GitHub 项目所属用户设置为 nginx，并且保证 nginx 用户的家目录有 ssh 认证相关的秘钥信息，这样在以后的自动拉取更新时才能畅通无阻。我把原先的项目删掉，然后使用 sudo 命令给 nginx 用户生成 ssh 认证信息，并且重新克隆项目，克隆的同时指定所属用户为 nginx。【由于用户 nginx 没有登录 Shell 的权限，所以不能直接使用 nginx 用户登录后再操作的方式解决】12345678910-- 目录不存在先创建，赋给 nginx 用户权限 mkdir -p /home/nginx/.ssh/chown nginx:nginx -R /home/nginx/.ssh/-- H 参数表示设置家目录环境，u 参数表示用户名 cd /site/sudo -Hu nginx ssh-keygen -t rsa -C &quot;plapyi@qq.com&quot;sudo -Hu nginx git clone https://github.com/iplaypi/iplaypi.github.io.git-- 如果没有 iplaypi.github.io 目录的权限，也要赋予 nginx 用户 mkdir iplaypi.github.iochown nginx:nginx iplaypi.github.io好，一切准备就绪，我再来试一次。可以看到，完美执行，热泪盈眶。为了方便，本来我把这 2 个 php 文件直接放在项目里面了，放在 source 分支，再更新一下 travis-ci 的配置文件，把它们提交到 master 分支去。但是这样做的风险就是把秘钥暴露出去了，显然不可取，所以折中的办法就是把这 2 个文件当做模板，把秘钥隐去，放在 source 分支，以后用的时候直接复制就行了。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Github</tag>
        <tag>PHP</tag>
        <tag>WebHooks</tag>
        <tag>自动更新</tag>
        <tag>钩子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx 配置 SSL 证书实现 HTTPS 访问]]></title>
    <url>%2F2019030501.html</url>
    <content type="text"><![CDATA[由于 GitHub Pages 把百度爬虫屏蔽了，导致百度爬虫爬取不到我的个人主页，所以被百度收录的内容很少，能收录的基本都是我手动提交的。后来我的解决办法就是自己搭建了一台 Web 服务器，然后在 DNSPod 中把百度爬虫的访问流量引到我的 Web 服务器上面，服务器主机是我自己购买的 VPS，服务器应用我选择的是强大的 Nginx。本文就记录 Web 服务器搭建以及配置 SSL 证书这个过程。安装 NginxNginx 官方网站：https://www.nginx.com/resources/wiki/start/topics/tutorials/install 。我的 VPS 是 CentOS 7 X64 版本的，所以安装 Nginx 的过程比较麻烦一点，需要自己下载源码、编译、安装，如果需要用到附加模块【例如 http_ssl 证书模块】，还需要重新编译，整个过程比较耗时。如果不熟悉的话，遇到问题也要折腾半天才能解决。所以，我在不熟悉的 Nginx 的情况下选择了一种简单的方式，直接自动安装，并自带了一些常用的模块，例如 ssl 证书模块。但是缺点就是安装过程稍微长一点，在网络好的情况下可能需要 3-5 分钟。我还参考了别人的文档：https://gist.github.com/ifels/c8cfdfe249e27ffa9ba1 ，但是仅供参考，因为我发现也有一些不能使用的地方。创建源配置文件 在 /etc/yum.repos.d/ 目录下创建一个源配置文件 nginx.repo，如果不存在这个目录，先使用 mkdir 命令创建目录，然后在目录中添加一个文件 nginx.repo，使用命令：1vi nginx.repo进入编辑模式，填写如下内容：12345[nginx]name=nginx repobaseurl=http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck=0enabled=1编辑完成后保存即可。自动安装 Nginx接下来就是使用命令自动安装 Nginx 了【敲下命令，看着就行了，会有刷屏的日志输出】：1yum install nginx -y安装完成后，使用以下命令启动：1service nginx start可以使用命令 service nginx status 查看 Nginx 是否启动：然后你就能看到 Nginx 的主页了，默认是 80 端口，直接使用 ip 访问即可【如果这里打不开，可能是端口 80 没有开启，被防火墙禁用了，需要重新开启，开启方法参考后面的章节】。获取 SSL 证书、配置参数 SSL 证书获取 证书的获取可以参考我的文章：利用阿里云申请免费的 SSL 证书 。我在阿里云获取的证书是免费的、有效期一年的，等证书过期了可以重新申请【不知道能不能自动续期】，因为我有阿里云的帐号，所以就直接使用了。当然，通过其它方式也可以获取 SSL 证书，大家自行选择。 直接下载即可，下载后上传到站点的任意目录，但是要记住文件的位置，因为等一下配置 Nginx 的时候需要指定证书的位置。我把它们放在了 /site/ 目录，一共有 2 个文件：.key 文件时私钥文件，.pem 文件时公钥文件。Nginx 参数配置 更改配置文件，打开文件【使用 vi 命令会自动创建不存在的文件】，进入编辑模式：12# 配置 vi /etc/nginx/nginx.conf填写内容如下【我这里只是配置基本的参数 server 有关内容，大家当然可以根据实际需要配置更为丰富的参数】，留意证书的公钥与私钥这 2 个文件的配置：123456789101112131415161718192021222324252627# 80 端口是用来接收基本的 http 请求，里面做了永久重定向，重定向到 https 的链接 server &#123; listen 80; server_name blog.playpi.org; access_log /site/iplaypi.github.io.http-blog-access.log main; rewrite ^/(.*)$ https://blog.playpi.org/$1 permanent; &#125;# 443 端口是用来接收 https 请求的 server &#123; listen 443 ssl;# 监听端口 server_name blog.playpi.org;# 域名 access_log /site/iplaypi.github.io.https-blog-access.log main; root /site/iplaypi.github.io; ssl_certificate /site/1883927_blog.playpi.org.pem;# 证书路径 ssl_certificate_key /site/1883927_blog.playpi.org.key;#key 路径 ssl_session_cache shared:SSL:1m;# 储存 SSL 会话的缓存类型和大小 ssl_session_timeout 5m;# 配置会话超时时间 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;# 为建立安全连接，服务器所允许的密码格式列表 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on;# 依赖 SSLv3 和 TLSv1 协议的服务器密码将优先于客户端密码 #减少点击劫持 add_header X-Frame-Options DENY; #禁止服务器自动解析资源类型 add_header X-Content-Type-Options nosniff; #防 XSS 攻击 add_header X-Xss-Protection 1; &#125;只要按照如上的配置，就可以同时接收 http 请求与 https 请求【实际上 http 的请求被永久重定向到了 https】，我的配置如下图【请忽略 www 二级域名的配置项】：验证参数是否准确 有时候配置了参数，可能因为字符、参数名问题导致启动失败，然后再回来改配置文件，比较繁琐，所以可以直接使用 Nginx 提供的命令来验证配置文件的内容是否合法，如果有问题可以在输出警告日志中看到，改起来也非常方便。1nginx -t可以看到，配置项正常，接下来就可以启动 Nginx 了。开启端口、启动 Nginx在上面的步骤中，如果在一开始想启动 Nginx，虽然启动成功了，但是却访问不了 Nginx 的主页，那很大可能是服务器的端口没有开启，导致访问请求被拒绝，所以需要适当开启必要的端口【如果没有安装防火墙工具 firewall 请自行安装】。1234567891011121314# 查看已经开启的端口 firewall-cmd --list-ports# 开启端口 80firewall-cmd --permanent --zone=public --add-port=80/tcp# 开启端口 443firewall-cmd --permanent --zone=public --add-port=443/tcp# 重载更新的端口信息 firewall-cmd --reload# 这种方式可以，启动 Nginxservice nginx start# 停止 Nginxservice nginx stop# 如果需要重启，直接使用下面的更方便 nginx -s reload大家看一下我的服务器的端口开启信息：验证站点 打开站点 https://blog.playpi.org ，可以愉快地访问了，可以看到 https 链接的绿锁。接着查看一下 SSL 证书的信息。题外话 重定向问题思考 关于开启 https 的访问，我一开始也配置了 www 的二级域名，但是通过日志发现没有通过 301 重定向访问 https://www.playpi.org 的请求，一直不明白原因。后来发现，因为做重定向的时候还是重定向到 GitHub 上面了。同理，如果使用 ip 直接访问，可以观察到自动跳转到 https://www.playpi.org 了，查看证书还是 GitHub 的证书。所以后来直接把百度爬虫的请求转发到 blog 的二级域名还是明智的【www 的二级域名就不用自己再搞一套了】，否则百度爬虫还是抓取不到。如果百度爬虫直接使用 https 链接抓取还是可以的，但是看百度站长里面的说明，是通过 http 的 301 重定向抓取的。Nginx 的 https 模块安装 由于我使用的是简单小白的安装方式，不需要关心额外用到的模块，例如 http_ssl 模块，因为安装包里面自带了这个模块，可以使用 nginx -V 命令查看。因此，如果大家有使用源码编译安装的方式，注意 https 模块不能缺失，否则不能开启 https 的方式。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>https</tag>
        <tag>ssl</tag>
        <tag>证书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用阿里云申请免费的 SSL 证书]]></title>
    <url>%2F2019030401.html</url>
    <content type="text"><![CDATA[在搭建博客的过程中，一开始是全部使用 GitHub，因为这样做就什么也不用考虑了，例如主机、带宽、SSL 证书，全部都交给 GitHub 了，自己唯一需要做的就是写 Markdown 文档。但是，后来发现 GitHub 把百度爬虫给禁止了，也就是百度爬虫爬取不到 GitHub 的内容，导致我的站点没有被百度收录。后来为了专门给百度爬虫搭建一条线路，自己搭建了一个镜像服务，也就是和 GitHub 上面的内容一模一样站点，是专门给百度爬虫使用的。而且，为了测试方便，在 DNSPod 中还增加了一条 blog 二级域名的解析记录，blog 的访问全导向自己的镜像，这样就可以方便观察部署是否成功。后来还把百度爬虫的 www 访问通过 CNANE 跳转到 blog 去，这样就不用单独再搞一个 www 了，因为挺麻烦的（域名解析线路问题、测试问题、证书确认问题，都挺麻烦）。而在这个过程中，就产生了使用阿里云申请免费的 SSL 证书这一流程（有效期一年），记录下来给大家参考。注册阿里云、开启实名认证 这个步骤就不多说了，需要证书总得注册一个帐号吧，也方便后续管理。此外，国内的证书服务商都要求实名认证，这个也没办法。如果不想实名认证，可以使用开源的 Lets Encrypt ，只不过有效期只能是 3 个月，也就是说每隔 3 个月就要更新一次，GitHub Pages 使用的就是它。阿里云的官网链接：https://www.aliyun.com 。购买 SSL 证书 1、在阿里云系统找到关于 SSL 证书的服务， 产品与服务 -&gt; 安全（云盾）-&gt;SSL 证书（应用安全）。2、进入后，点击右上角的 购买证书 。3、按照我截图中的步骤 1、2、3 选择，这里需要注意，这个免费的选项隐藏的很深，直接勾选是不会出现的，要按照我标识的步骤来勾选才行，这里看到出现的费用很贵不用害怕，等一下接着选择对了就会免费的。最终选择 免费型 DV SSL，按照我下图中的选项，可以看到费用是 0 元。选择后，下单即可，虽然要走购买流程，但是是不用付钱的。绑定证书信息、等待审核 1、下单完成后开始 申请 ，这里的 申请 的意思是申请使用它，要填写一些基本的信息，包括个人信息和网站信息，后续还需要验证身份，看你有没有权限管理你配置的网站。如果不申请 使用 ，证书其实就一直闲置在那里。填写个人信息，主要就是我个人的联系方式。填写网站信息，由于我使用的是自己的服务器上面搭建的 Web 服务，既没有使用阿里云也没有使用其它云服务，所以我选择了 文件验证 ，即需要把验证文件上传到我的域名对应的目录下面，用来证明这个站点是我管理的。当然，验证通过后，这个文件可以删除。2、填写完成后，会生成一个文件 fileauthor.txt，我需要把这个文件下载下来，然后上传到我的服务器对应的目录中，才能点击 验证 按钮，如果通过了，说明这个站点就是我管理的，也就是一个权限验证。由于在验证 www 证书对应的文件的时候，需要把 fileauthor.txt 文件上传到服务器，但是由于在 DNSPod 中设置的域名解析是解析到 GitHub 的（没有专门针对阿里的设置），所以总是验证失败。后来就干脆临时把所有的 www 解析都指向我自己的服务器，等通过了验证再改回去，整个过程很是折腾。折腾了一大圈，最后还发现了更简单的方法，直接放弃 www 证书的申请，在 DNSPod 中把百度的流量通过 CNAME 直接引到 blog 上面去就行了，这样只要维护一个 blog 的 Web 服务就行了。这样只需要增加一条解析，而且 blog 的证书验证过程也方便简单。DNSPod 解析示例 在这个过程中，我还发现验证过程需要一定的时间，一开始显示失败，但是不告诉我原因，还以为是自己的服务器的问题，重试了多种方法，包括重启 Web 服务。我等了十几分钟，证书就莫名其妙审核通过了，然后还发送了短信通知（到这里我猜测阿里云的 Web 界面显示的内容是滞后的，短信通知的内容才是实时的）。证书申请成功，可以使用了。下载证书、上传到自己的服务器 下载证书、上传到自己的服务器这一步骤就不多说了，主要就是复制粘贴的工作。着重要说一下 Nginx 的配置，主要就是 server 属性的配置，由于我把 www、blog 这 2 个二级域名都保留了，所以需要分开配置。其实，这里配置的 www 的二级域名根本没有用，因为不会有流量过来的，重在测试证书的安装。Nginx 的配置内容参考（2 个子域名分开配置，有 2 份 SSL 证书）：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253 server &#123; listen 80; server_name www.playpi.org; access_log /site/iplaypi.github.io.http-www-access.log main; rewrite ^/(.*)$ https://www.playpi.org/$1 permanent; &#125; server &#123; listen 80; server_name blog.playpi.org; access_log /site/iplaypi.github.io.http-blog-access.log main; rewrite ^/(.*)$ https://blog.playpi.org/$1 permanent; &#125; server &#123; listen 443 ssl;# 监听端口 server_name www.playpi.org;# 域名 access_log /site/iplaypi.github.io.https-www-access.log main; root /site/iplaypi.github.io; ssl_certificate /site/1884603_www.playpi.org.pem;# 证书路径 ssl_certificate_key /site/1884603_www.playpi.org.key;#key 路径 ssl_session_cache shared:SSL:1m;# 储存 SSL 会话的缓存类型和大小 ssl_session_timeout 5m;# 配置会话超时时间 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;# 为建立安全连接，服务器所允许的密码格式列表 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on;# 依赖 SSLv3 和 TLSv1 协议的服务器密码将优先于客户端密码 #减少点击劫持 add_header X-Frame-Options DENY; #禁止服务器自动解析资源类型 add_header X-Content-Type-Options nosniff; #防 XSS 攻击 add_header X-Xss-Protection 1; &#125;server &#123; listen 443 ssl;# 监听端口 server_name blog.playpi.org;# 域名 access_log /site/iplaypi.github.io.https-blog-access.log main; root /site/iplaypi.github.io; ssl_certificate /site/1883927_blog.playpi.org.pem;# 证书路径 ssl_certificate_key /site/1883927_blog.playpi.org.key;#key 路径 ssl_session_cache shared:SSL:1m;# 储存 SSL 会话的缓存类型和大小 ssl_session_timeout 5m;# 配置会话超时时间 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;# 为建立安全连接，服务器所允许的密码格式列表 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on;# 依赖 SSLv3 和 TLSv1 协议的服务器密码将优先于客户端密码 #减少点击劫持 add_header X-Frame-Options DENY; #禁止服务器自动解析资源类型 add_header X-Content-Type-Options nosniff; #防 XSS 攻击 add_header X-Xss-Protection 1; &#125;配置完成后重启 Nginx（使用 nginx -s reload），去浏览器查看证书信息，看到有效期一年。打开链接，看到左上角的小绿锁，好了，网站是经过验证的了。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>https</tag>
        <tag>阿里云</tag>
        <tag>SSL证书</tag>
        <tag>Lets Encrypt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年关于技术的未解之谜]]></title>
    <url>%2F2019030101.html</url>
    <content type="text"><![CDATA[由于技术能力的限制，平时会遇到一些自己觉得非常诡异的问题，感觉到莫名其妙。其实到头来发现，归根结底还是自己的认知问题：可能是技术水平不够，或者考虑不周全，甚至是一些低级别的错误判断。总而言之，遇到这些问题后，有时候请教人、查资料之后仍旧不得解，只能先记录下来，留做备注说明，等待以后解决。当然，随着时间的流逝，有些问题可能就被忘记了，有些问题在之后的某一个时间点被解决了。本文就是要记录这些问题，并在遇到新问题或者解决老问题之后，保持更新。常用链接 在这里先列出一些常用的网站链接，方便查看：es-hadoop 官网：https://www.elastic.co/guide/en/elasticsearch/hadoop/5.6/configuration.html ；xes-spark 读取 es 数据后 count 报错 使用 es-hadoop 组件，起 Spark 任务去查询 es 数据，然后过滤，过滤后做一个 count 算子，结果就报错了。而且，在报错后又重试了很多次（5 次以上），一直正常，没法重现问题。这个任务需要经常跑，以前从来没遇到过这样的异常，初步怀疑是 es 集群不稳定，具体原因不得而知。错误截图：完整错误信息如下（重要包名称被替换）：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556572019-02-26_15:01:44 [main] ERROR spokesman3.SpokesAndBrand:510: !!!!Spark 出错: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name at [Source: org.apache.commons.httpclient.AutoCloseInputStream@2687cf14; line: 1, column: 17581]org.elasticsearch.hadoop.rest.EsHadoopParsingException: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name at [Source: org.apache.commons.httpclient.AutoCloseInputStream@2687cf14; line: 1, column: 17581] at org.elasticsearch.hadoop.rest.RestClient.parseContent (RestClient.java:171) at org.elasticsearch.hadoop.rest.RestClient.get (RestClient.java:155) at org.elasticsearch.hadoop.rest.RestClient.targetShards (RestClient.java:357) at org.elasticsearch.hadoop.rest.RestRepository.doGetReadTargetShards (RestRepository.java:306) at org.elasticsearch.hadoop.rest.RestRepository.getReadTargetShards (RestRepository.java:297) at org.elasticsearch.hadoop.rest.RestService.findPartitions (RestService.java:241) at org.elasticsearch.spark.rdd.AbstractEsRDD.esPartitions$lzycompute (AbstractEsRDD.scala:73) at org.elasticsearch.spark.rdd.AbstractEsRDD.esPartitions (AbstractEsRDD.scala:72) at org.elasticsearch.spark.rdd.AbstractEsRDD.getPartitions (AbstractEsRDD.scala:44) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply (RDD.scala:239) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply (RDD.scala:237) at scala.Option.getOrElse (Option.scala:120) at org.apache.spark.rdd.RDD.partitions (RDD.scala:237) at org.apache.spark.SparkContext.runJob (SparkContext.scala:1929) at org.apache.spark.rdd.RDD.count (RDD.scala:1157) at org.apache.spark.api.java.JavaRDDLike$class.count (JavaRDDLike.scala:440) at org.apache.spark.api.java.AbstractJavaRDDLike.count (JavaRDDLike.scala:46) at com.package.to.class.SpokesAndBrand.getMention (SpokesAndBrand.java:508) at com.package.to.class.SpokesAndBrand.runCelebrityByBrand (SpokesAndBrand.java:185) at com.package.to.class.SpokesAndBrand.execute (SpokesAndBrand.java:116) at com.package.to.class.SpokesmanAnalyzer.execute (SpokesmanAnalyzer.java:162) at com.package.to.class.SpokesmanAnalyzeCli.execute (SpokesmanAnalyzeCli.java:154) at com.package.to.class.SpokesmanAnalyzeCli.start (SpokesmanAnalyzeCli.java:75) at com.package.to.class.util.AdvCli.initRunner (AdvCli.java:191) at com.package.to.class.job.client.BasicInputOutputSystemWorker.run (BasicInputOutputSystemWorker.java:79) at com.package.to.class.model.AbstractDataReportWorker.run (AbstractDataReportWorker.java:122) at com.package.to.class.buffalo.job.AbstractBUTaskWorker.runTask (AbstractBUTaskWorker.java:63) at com.package.to.class.report.cli.TaskLocalRunnerCli.start (TaskLocalRunnerCli.java:110) at com.package.to.class.util.AdvCli.initRunner (AdvCli.java:191) at com.package.to.class.report.cli.TaskLocalRunnerCli.main (TaskLocalRunnerCli.java:43)Caused by: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name at [Source: org.apache.commons.httpclient.AutoCloseInputStream@2687cf14; line: 1, column: 17581] at org.codehaus.jackson.JsonParser._constructError (JsonParser.java:1433) at org.codehaus.jackson.impl.JsonParserMinimalBase._reportError (JsonParserMinimalBase.java:521) at org.codehaus.jackson.impl.JsonParserMinimalBase._reportInvalidEOF (JsonParserMinimalBase.java:454) at org.codehaus.jackson.impl.Utf8StreamParser.parseEscapedFieldName (Utf8StreamParser.java:1503) at org.codehaus.jackson.impl.Utf8StreamParser.slowParseFieldName (Utf8StreamParser.java:1404) at org.codehaus.jackson.impl.Utf8StreamParser._parseFieldName (Utf8StreamParser.java:1231) at org.codehaus.jackson.impl.Utf8StreamParser.nextToken (Utf8StreamParser.java:495) at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.mapObject (UntypedObjectDeserializer.java:219) at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.deserialize (UntypedObjectDeserializer.java:47) at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.mapArray (UntypedObjectDeserializer.java:165) at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.deserialize (UntypedObjectDeserializer.java:51) at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.mapArray (UntypedObjectDeserializer.java:165) at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.deserialize (UntypedObjectDeserializer.java:51) at org.codehaus.jackson.map.deser.std.MapDeserializer._readAndBind (MapDeserializer.java:319) at org.codehaus.jackson.map.deser.std.MapDeserializer.deserialize (MapDeserializer.java:249) at org.codehaus.jackson.map.deser.std.MapDeserializer.deserialize (MapDeserializer.java:33) at org.codehaus.jackson.map.ObjectMapper._readValue (ObjectMapper.java:2704) at org.codehaus.jackson.map.ObjectMapper.readValue (ObjectMapper.java:1286) at org.elasticsearch.hadoop.rest.RestClient.parseContent (RestClient.java:166) ... 29 more2019-02-26_15:01:44 [main] INFO rdd.JavaEsRDD:58: Removing RDD 3086 from persistence listHexo 生成 html 静态页面目录锚点失效 我这些所有的博客文档是先写成 Markdown 文件，然后使用 Hexo 渲染生成 html 静态页面，再发布到 GitHub Pages 上面，还有一些是发布到我自己的 VPS 上面（为了百度爬虫）。但是最近我发现一个现象，有一些文章的锚点无效，也就是表现为目录无法跳转，例如想直接查看某一级目录的内容，在右侧的 文章目录 中直接点击对应的标题，不会自动跳转过去。这个问题我发现了很久，但是一直没在意，也没有找到原因。最近才碰巧发现是因为标题内容里面有空格，这才导致生成的 html 静态页面里面的锚点失效，我随机又测试了几次其它的页面，看起来的确是这样。下面列出一些示例：123https://www.playpi.org/2019022501.html ，Hexo 踩坑记录的 https://www.playpi.org/2018121901.html ，js 字符串分割方法 https://www.playpi.org/2019020701.html ，itchat 0 - 初识 但是，我又发现其他人的博客，目录标题内容中也有空格，却可以正常跳转，我很疑惑。现在我猜测是 Hexo 的问题，或者哪里需要配置，等待以后的解决方法吧。别人的博客示例：https://blog.itnote.me/Hexo/hexo-chinese-english-space/ 。邮件依赖的诡异异常 在项目中新引入了邮件相关的依赖【没有其它任何变化】，这样就可以在需要时发送通知邮件，依赖内容如下：123456&lt;!-- 邮件相关依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-email&lt;/artifactId&gt; &lt;version&gt;1.3.3&lt;/version&gt;&lt;/dependency&gt;然后神奇的事情发生了，实际执行时，程序抛出异常【去掉这个依赖则正常】：123456789101112131415161718Exception in thread &quot;main&quot; java.lang.StackOverflowError at sun.nio.cs.UTF_8$Encoder.encodeLoop (UTF_8.java:619) at java.nio.charset.CharsetEncoder.encode (CharsetEncoder.java:561) at sun.nio.cs.StreamEncoder.implWrite (StreamEncoder.java:271) at sun.nio.cs.StreamEncoder.write (StreamEncoder.java:125) at java.io.OutputStreamWriter.write (OutputStreamWriter.java:207) at java.io.BufferedWriter.flushBuffer (BufferedWriter.java:129) at java.io.PrintStream.write (PrintStream.java:526) at java.io.PrintStream.print (PrintStream.java:669) at java.io.PrintStream.println (PrintStream.java:806) at org.slf4j.impl.SimpleLogger.write (SimpleLogger.java:381) at org.slf4j.impl.SimpleLogger.log (SimpleLogger.java:376) at org.slf4j.impl.SimpleLogger.info (SimpleLogger.java:538) at org.apache.maven.cli.logging.Slf4jLogger.info (Slf4jLogger.java:59) at org.codehaus.plexus.archiver.AbstractArchiver$1.hasNext (AbstractArchiver.java:464) at org.codehaus.plexus.archiver.AbstractArchiver$1.hasNext (AbstractArchiver.java:467) at org.codehaus.plexus.archiver.AbstractArchiver$1.hasNext (AbstractArchiver.java:467) at org.codehaus.plexus.archiver.AbstractArchiver$1.hasNext (AbstractArchiver.java:467)而根据这个异常信息，我搜索不到任何有效的信息，一直无法解决。最后，我对比了其它项目的配置，发现 手动设置 maven-assembly-plugin 插件的版本为 2.6 即可。而之前是没有设置这个版本号的，默认去仓库获取的最新版本，这个默认的版本可能刚好有问题。Python 入门踩坑 在一开始使用 Python 的时候，没有使用类似 Anaconda、Winpython 这种套件来帮我自动管理 Python 的第三方工具库，而是从 Python 安装开始，用到什么再用 pip 安装什么。整个过程真的可以把人搞崩溃，工具库之间的传递依赖、版本的不兼容等问题，令人望而却步，下面给出一些难忘的经历。出现错误：12Install packages failed: Installing packages: error occurrednumpy.distutils.system_info.NotFoundError: no lapack/blas resources found需要先手动安装 numpy+mkl，再手动安装 scipy，下载文件链接：http://www.lfd.uci.edu/~gohlke/pythonlibs 。我下载了 2 个文件：numpy-1.11.3+mkl-cp27-cp27m-win32.whl、scipy-0.19.0-cp27-cp27m-win32.whl，然后手动安装。一开始我下载的是 64 位的安装包，结果发现我的 Windows 安装的 Python 是 32 位的，导致不支持【下载时没有选择位数，直接下载的默认的包】。另外，直接进入 Python 的命令行环境时也会打印出版本信息的。使用 import pip; print (pip.pep425tags.get_supported ()); 可以获取到 pip 支持的文件名和版本。注意安装 scipy 之前还需要各种第三方库，官方介绍：Install numpy+mkl before installing scipy.。在 Shell 中验证安装第三方库是否成功，例如 numpy：from numpy import *。scipy 包安装：pip install scipy==0.16.1【不推荐】，成功完成安装，如果缺少第三方包会报很多错误。网上查询后的总结：安装 numpy 后安装 scipy 失败，报错：numpy.distutils.system_info.NotFoundError，一般是缺少一些系统库，需要安装：libopenblas-dev、liblapack-dev、libatlas-dev、libblas-dev。常见第三方库介绍：pandas，分析数据 sklearn，机器学习，各种算法jieba，分词工具gensim nlp word2v，模块训练词向量模型scipy，算法库，数学工具包numpy，数据分析matlptop，图形可视化Python 中的编码：2.X 版本，python 编码过程： 输入 –&gt; str –&gt; decode –&gt; unicode –&gt; encode –&gt; str –&gt; 输出。3.X 版本，不一样，直接是 unicode。Python 中代码有 print u’xx’ + yy，yy 是中文，直接跑的时候打印到 Shell 不报错，但是使用后台挂起跑的时候，重定向到文件时，会报错，因为 Python 获取不到输出流的编码。Spark UI 无法显示 使用 yarn-client 模式起了一个 Spark 任务，在 Driver 端看到异常日志：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394952019-01-16_14:53:31 [qtp192486017-1829 - /static/timeline-view.js] WARN servlet.DefaultServlet:587: EXCEPTION java.lang.IllegalArgumentException: MALFORMED at java.util.zip.ZipCoder.toString (ZipCoder.java:58) at java.util.zip.ZipFile.getZipEntry (ZipFile.java:583) at java.util.zip.ZipFile.access$900 (ZipFile.java:60) at java.util.zip.ZipFile$ZipEntryIterator.next (ZipFile.java:539) at java.util.zip.ZipFile$ZipEntryIterator.nextElement (ZipFile.java:514) at java.util.zip.ZipFile$ZipEntryIterator.nextElement (ZipFile.java:495) at java.util.jar.JarFile$JarEntryIterator.next (JarFile.java:257) at java.util.jar.JarFile$JarEntryIterator.nextElement (JarFile.java:266) at java.util.jar.JarFile$JarEntryIterator.nextElement (JarFile.java:247) at org.spark-project.jetty.util.resource.JarFileResource.exists (JarFileResource.java:189) at org.spark-project.jetty.servlet.DefaultServlet.getResource (DefaultServlet.java:398) at org.spark-project.jetty.servlet.DefaultServlet.doGet (DefaultServlet.java:476) at javax.servlet.http.HttpServlet.service (HttpServlet.java:707) at javax.servlet.http.HttpServlet.service (HttpServlet.java:820) at org.spark-project.jetty.servlet.ServletHolder.handle (ServletHolder.java:684) at org.spark-project.jetty.servlet.ServletHandler$CachedChain.doFilter (ServletHandler.java:1507) at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter (AmIpFilter.java:164) at org.spark-project.jetty.servlet.ServletHandler$CachedChain.doFilter (ServletHandler.java:1478) at org.spark-project.jetty.servlet.ServletHandler.doHandle (ServletHandler.java:499) at org.spark-project.jetty.server.handler.ContextHandler.doHandle (ContextHandler.java:1086) at org.spark-project.jetty.servlet.ServletHandler.doScope (ServletHandler.java:427) at org.spark-project.jetty.server.handler.ContextHandler.doScope (ContextHandler.java:1020) at org.spark-project.jetty.server.handler.ScopedHandler.handle (ScopedHandler.java:135) at org.spark-project.jetty.server.handler.GzipHandler.handle (GzipHandler.java:264) at org.spark-project.jetty.server.handler.ContextHandlerCollection.handle (ContextHandlerCollection.java:255) at org.spark-project.jetty.server.handler.HandlerWrapper.handle (HandlerWrapper.java:116) at org.spark-project.jetty.server.Server.handle (Server.java:366) at org.spark-project.jetty.server.AbstractHttpConnection.handleRequest (AbstractHttpConnection.java:494) at org.spark-project.jetty.server.AbstractHttpConnection.headerComplete (AbstractHttpConnection.java:973) at org.spark-project.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete (AbstractHttpConnection.java:1035) at org.spark-project.jetty.http.HttpParser.parseNext (HttpParser.java:641) at org.spark-project.jetty.http.HttpParser.parseAvailable (HttpParser.java:231) at org.spark-project.jetty.server.AsyncHttpConnection.handle (AsyncHttpConnection.java:82) at org.spark-project.jetty.io.nio.SelectChannelEndPoint.handle (SelectChannelEndPoint.java:696) at org.spark-project.jetty.io.nio.SelectChannelEndPoint$1.run (SelectChannelEndPoint.java:53) at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob (QueuedThreadPool.java:608) at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run (QueuedThreadPool.java:543) at java.lang.Thread.run (Thread.java:748)2019-01-16_14:53:31 [qtp192486017-1829 - /static/timeline-view.js] WARN servlet.ServletHandler:592: Error for /static/timeline-view.jsjava.lang.NoClassDefFoundError: org/spark-project/jetty/server/handler/ErrorHandler$ErrorPageMapper at org.spark-project.jetty.server.handler.ErrorHandler.handle (ErrorHandler.java:71) at org.spark-project.jetty.server.Response.sendError (Response.java:349) at javax.servlet.http.HttpServletResponseWrapper.sendError (HttpServletResponseWrapper.java:118) at org.spark-project.jetty.http.gzip.CompressedResponseWrapper.sendError (CompressedResponseWrapper.java:291) at org.spark-project.jetty.servlet.DefaultServlet.doGet (DefaultServlet.java:589) at javax.servlet.http.HttpServlet.service (HttpServlet.java:707) at javax.servlet.http.HttpServlet.service (HttpServlet.java:820) at org.spark-project.jetty.servlet.ServletHolder.handle (ServletHolder.java:684) at org.spark-project.jetty.servlet.ServletHandler$CachedChain.doFilter (ServletHandler.java:1507) at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter (AmIpFilter.java:164) at org.spark-project.jetty.servlet.ServletHandler$CachedChain.doFilter (ServletHandler.java:1478) at org.spark-project.jetty.servlet.ServletHandler.doHandle (ServletHandler.java:499) at org.spark-project.jetty.server.handler.ContextHandler.doHandle (ContextHandler.java:1086) at org.spark-project.jetty.servlet.ServletHandler.doScope (ServletHandler.java:427) at org.spark-project.jetty.server.handler.ContextHandler.doScope (ContextHandler.java:1020) at org.spark-project.jetty.server.handler.ScopedHandler.handle (ScopedHandler.java:135) at org.spark-project.jetty.server.handler.GzipHandler.handle (GzipHandler.java:264) at org.spark-project.jetty.server.handler.ContextHandlerCollection.handle (ContextHandlerCollection.java:255) at org.spark-project.jetty.server.handler.HandlerWrapper.handle (HandlerWrapper.java:116) at org.spark-project.jetty.server.Server.handle (Server.java:366) at org.spark-project.jetty.server.AbstractHttpConnection.handleRequest (AbstractHttpConnection.java:494) at org.spark-project.jetty.server.AbstractHttpConnection.headerComplete (AbstractHttpConnection.java:973) at org.spark-project.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete (AbstractHttpConnection.java:1035) at org.spark-project.jetty.http.HttpParser.parseNext (HttpParser.java:641) at org.spark-project.jetty.http.HttpParser.parseAvailable (HttpParser.java:231) at org.spark-project.jetty.server.AsyncHttpConnection.handle (AsyncHttpConnection.java:82) at org.spark-project.jetty.io.nio.SelectChannelEndPoint.handle (SelectChannelEndPoint.java:696) at org.spark-project.jetty.io.nio.SelectChannelEndPoint$1.run (SelectChannelEndPoint.java:53) at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob (QueuedThreadPool.java:608) at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run (QueuedThreadPool.java:543) at java.lang.Thread.run (Thread.java:748)2019-01-16_14:53:31 [qtp192486017-1829 - /static/timeline-view.js] WARN server.AbstractHttpConnection:552: /static/timeline-view.jsjava.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted () Z at org.spark-project.jetty.servlet.ServletHandler.doHandle (ServletHandler.java:608) at org.spark-project.jetty.server.handler.ContextHandler.doHandle (ContextHandler.java:1086) at org.spark-project.jetty.servlet.ServletHandler.doScope (ServletHandler.java:427) at org.spark-project.jetty.server.handler.ContextHandler.doScope (ContextHandler.java:1020) at org.spark-project.jetty.server.handler.ScopedHandler.handle (ScopedHandler.java:135) at org.spark-project.jetty.server.handler.GzipHandler.handle (GzipHandler.java:264) at org.spark-project.jetty.server.handler.ContextHandlerCollection.handle (ContextHandlerCollection.java:255) at org.spark-project.jetty.server.handler.HandlerWrapper.handle (HandlerWrapper.java:116) at org.spark-project.jetty.server.Server.handle (Server.java:366) at org.spark-project.jetty.server.AbstractHttpConnection.handleRequest (AbstractHttpConnection.java:494) at org.spark-project.jetty.server.AbstractHttpConnection.headerComplete (AbstractHttpConnection.java:973) at org.spark-project.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete (AbstractHttpConnection.java:1035) at org.spark-project.jetty.http.HttpParser.parseNext (HttpParser.java:641) at org.spark-project.jetty.http.HttpParser.parseAvailable (HttpParser.java:231) at org.spark-project.jetty.server.AsyncHttpConnection.handle (AsyncHttpConnection.java:82) at org.spark-project.jetty.io.nio.SelectChannelEndPoint.handle (SelectChannelEndPoint.java:696) at org.spark-project.jetty.io.nio.SelectChannelEndPoint$1.run (SelectChannelEndPoint.java:53) at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob (QueuedThreadPool.java:608) at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run (QueuedThreadPool.java:543) at java.lang.Thread.run (Thread.java:748)这个日志在反复打印，也就是在任务的运行过程中，一直都有这个错误。它引发了什么问题呢，我检查了一下，对 Spark 任务的实际功能并没有影响，任务跑完后功能正常实现。但是，我发现在任务的运行过程中，Spark UI 页面打开后不正常显示【异常信息的开头就是关于某个 js 文件问题】：点击进去，直接显示 Error 500：服务器的 Driver 端日志截图：日志截图 1日志截图 2日志截图 3等了几天，又遇到同样的问题，除了这 2 次，其它时间点就没遇到过了：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394952019-01-24_22:51:49 [qtp697001207-1591 - /static/spark-dag-viz.js] WARN servlet.DefaultServlet:587: EXCEPTION java.lang.IllegalArgumentException: MALFORMEDat java.util.zip.ZipCoder.toString (ZipCoder.java:58)at java.util.zip.ZipFile.getZipEntry (ZipFile.java:583)at java.util.zip.ZipFile.access$900 (ZipFile.java:60)at java.util.zip.ZipFile$ZipEntryIterator.next (ZipFile.java:539)at java.util.zip.ZipFile$ZipEntryIterator.nextElement (ZipFile.java:514)at java.util.zip.ZipFile$ZipEntryIterator.nextElement (ZipFile.java:495)at java.util.jar.JarFile$JarEntryIterator.next (JarFile.java:257)at java.util.jar.JarFile$JarEntryIterator.nextElement (JarFile.java:266)at java.util.jar.JarFile$JarEntryIterator.nextElement (JarFile.java:247)at org.spark-project.jetty.util.resource.JarFileResource.exists (JarFileResource.java:189)at org.spark-project.jetty.servlet.DefaultServlet.getResource (DefaultServlet.java:398)at org.spark-project.jetty.servlet.DefaultServlet.doGet (DefaultServlet.java:476)at javax.servlet.http.HttpServlet.service (HttpServlet.java:707)at javax.servlet.http.HttpServlet.service (HttpServlet.java:820)at org.spark-project.jetty.servlet.ServletHolder.handle (ServletHolder.java:684)at org.spark-project.jetty.servlet.ServletHandler$CachedChain.doFilter (ServletHandler.java:1507)at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter (AmIpFilter.java:164)at org.spark-project.jetty.servlet.ServletHandler$CachedChain.doFilter (ServletHandler.java:1478)at org.spark-project.jetty.servlet.ServletHandler.doHandle (ServletHandler.java:499)at org.spark-project.jetty.server.handler.ContextHandler.doHandle (ContextHandler.java:1086)at org.spark-project.jetty.servlet.ServletHandler.doScope (ServletHandler.java:427)at org.spark-project.jetty.server.handler.ContextHandler.doScope (ContextHandler.java:1020)at org.spark-project.jetty.server.handler.ScopedHandler.handle (ScopedHandler.java:135)at org.spark-project.jetty.server.handler.GzipHandler.handle (GzipHandler.java:264)at org.spark-project.jetty.server.handler.ContextHandlerCollection.handle (ContextHandlerCollection.java:255)at org.spark-project.jetty.server.handler.HandlerWrapper.handle (HandlerWrapper.java:116)at org.spark-project.jetty.server.Server.handle (Server.java:366)at org.spark-project.jetty.server.AbstractHttpConnection.handleRequest (AbstractHttpConnection.java:494)at org.spark-project.jetty.server.AbstractHttpConnection.headerComplete (AbstractHttpConnection.java:973)at org.spark-project.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete (AbstractHttpConnection.java:1035)at org.spark-project.jetty.http.HttpParser.parseNext (HttpParser.java:641)at org.spark-project.jetty.http.HttpParser.parseAvailable (HttpParser.java:231)at org.spark-project.jetty.server.AsyncHttpConnection.handle (AsyncHttpConnection.java:82)at org.spark-project.jetty.io.nio.SelectChannelEndPoint.handle (SelectChannelEndPoint.java:696)at org.spark-project.jetty.io.nio.SelectChannelEndPoint$1.run (SelectChannelEndPoint.java:53)at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob (QueuedThreadPool.java:608)at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run (QueuedThreadPool.java:543)at java.lang.Thread.run (Thread.java:748)2019-01-24_22:51:49 [qtp697001207-1591 - /static/spark-dag-viz.js] WARN servlet.ServletHandler:592: Error for /static/spark-dag-viz.jsjava.lang.NoClassDefFoundError: org/spark-project/jetty/server/handler/ErrorHandler$ErrorPageMapperat org.spark-project.jetty.server.handler.ErrorHandler.handle (ErrorHandler.java:71)at org.spark-project.jetty.server.Response.sendError (Response.java:349)at javax.servlet.http.HttpServletResponseWrapper.sendError (HttpServletResponseWrapper.java:118)at org.spark-project.jetty.http.gzip.CompressedResponseWrapper.sendError (CompressedResponseWrapper.java:291)at org.spark-project.jetty.servlet.DefaultServlet.doGet (DefaultServlet.java:589)at javax.servlet.http.HttpServlet.service (HttpServlet.java:707)at javax.servlet.http.HttpServlet.service (HttpServlet.java:820)at org.spark-project.jetty.servlet.ServletHolder.handle (ServletHolder.java:684)at org.spark-project.jetty.servlet.ServletHandler$CachedChain.doFilter (ServletHandler.java:1507)at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter (AmIpFilter.java:164)at org.spark-project.jetty.servlet.ServletHandler$CachedChain.doFilter (ServletHandler.java:1478)at org.spark-project.jetty.servlet.ServletHandler.doHandle (ServletHandler.java:499)at org.spark-project.jetty.server.handler.ContextHandler.doHandle (ContextHandler.java:1086)at org.spark-project.jetty.servlet.ServletHandler.doScope (ServletHandler.java:427)at org.spark-project.jetty.server.handler.ContextHandler.doScope (ContextHandler.java:1020)at org.spark-project.jetty.server.handler.ScopedHandler.handle (ScopedHandler.java:135)at org.spark-project.jetty.server.handler.GzipHandler.handle (GzipHandler.java:264)at org.spark-project.jetty.server.handler.ContextHandlerCollection.handle (ContextHandlerCollection.java:255)at org.spark-project.jetty.server.handler.HandlerWrapper.handle (HandlerWrapper.java:116)at org.spark-project.jetty.server.Server.handle (Server.java:366)at org.spark-project.jetty.server.AbstractHttpConnection.handleRequest (AbstractHttpConnection.java:494)at org.spark-project.jetty.server.AbstractHttpConnection.headerComplete (AbstractHttpConnection.java:973)at org.spark-project.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete (AbstractHttpConnection.java:1035)at org.spark-project.jetty.http.HttpParser.parseNext (HttpParser.java:641)at org.spark-project.jetty.http.HttpParser.parseAvailable (HttpParser.java:231)at org.spark-project.jetty.server.AsyncHttpConnection.handle (AsyncHttpConnection.java:82)at org.spark-project.jetty.io.nio.SelectChannelEndPoint.handle (SelectChannelEndPoint.java:696)at org.spark-project.jetty.io.nio.SelectChannelEndPoint$1.run (SelectChannelEndPoint.java:53)at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob (QueuedThreadPool.java:608)at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run (QueuedThreadPool.java:543)at java.lang.Thread.run (Thread.java:748)2019-01-24_22:51:49 [qtp697001207-1591 - /static/spark-dag-viz.js] WARN server.AbstractHttpConnection:552: /static/spark-dag-viz.jsjava.lang.NoSuchMethodError: javax.servlet.http.HttpServletRequest.isAsyncStarted () Zat org.spark-project.jetty.servlet.ServletHandler.doHandle (ServletHandler.java:608)at org.spark-project.jetty.server.handler.ContextHandler.doHandle (ContextHandler.java:1086)at org.spark-project.jetty.servlet.ServletHandler.doScope (ServletHandler.java:427)at org.spark-project.jetty.server.handler.ContextHandler.doScope (ContextHandler.java:1020)at org.spark-project.jetty.server.handler.ScopedHandler.handle (ScopedHandler.java:135)at org.spark-project.jetty.server.handler.GzipHandler.handle (GzipHandler.java:264)at org.spark-project.jetty.server.handler.ContextHandlerCollection.handle (ContextHandlerCollection.java:255)at org.spark-project.jetty.server.handler.HandlerWrapper.handle (HandlerWrapper.java:116)at org.spark-project.jetty.server.Server.handle (Server.java:366)at org.spark-project.jetty.server.AbstractHttpConnection.handleRequest (AbstractHttpConnection.java:494)at org.spark-project.jetty.server.AbstractHttpConnection.headerComplete (AbstractHttpConnection.java:973)at org.spark-project.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete (AbstractHttpConnection.java:1035)at org.spark-project.jetty.http.HttpParser.parseNext (HttpParser.java:641)at org.spark-project.jetty.http.HttpParser.parseAvailable (HttpParser.java:231)at org.spark-project.jetty.server.AsyncHttpConnection.handle (AsyncHttpConnection.java:82)at org.spark-project.jetty.io.nio.SelectChannelEndPoint.handle (SelectChannelEndPoint.java:696)at org.spark-project.jetty.io.nio.SelectChannelEndPoint$1.run (SelectChannelEndPoint.java:53)at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob (QueuedThreadPool.java:608)at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run (QueuedThreadPool.java:543)at java.lang.Thread.run (Thread.java:748)此外，还有一点值得注意，Chrome 浏览器的某些端口是禁止访问的，所以遇到过有一个 Spark 任务使用了 4045 端口【locked】，在 Chrome 浏览器是看不了任务状态的，页面无法打开，被 Chrome 浏览器屏蔽了，此时并不是 Spark 的问题。关于 Git 的小问题 1、本地版本落后，而且又与远程仓库冲突，git pull 报错警告，需要 merge，无法直接更新最新版本。下面的操作直接覆盖本地文件，强制更新到最新版本，本地未提交的更改会丢失。12git fetch --allgit reset --hard origin/master2、在 2018 年 9 月的某一天，发现 Git 的代码推送总是需要输入帐号和密码，哪怕保存下来也不行，每次 push 都需要重新输入，感觉很奇怪。后来发现是版本太旧了，当时的版本是 v2.13.0，升级后的版本是 v2.18.0，升级后就恢复正常了。后来无意间在哪里看到过通知，说是 TSL 协议升级了，所以针对旧版本强制输入用户名密码，升级就可以解决。 备注一下，HTTPS 是在 TCP 和 HTTP 之间增加了 TLS【Transport Layer Security，传输层安全】，提供了内容加密、身份认证和数据完整性三大功能。TLS 的前身是 SSL【Secure Sockets Layer，安全套接字层】，由网景公司开发，后来被 IETF 标准化并改名。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Git</tag>
        <tag>ElasticSearch</tag>
        <tag>es</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 的踩坑经验]]></title>
    <url>%2F2019022501.html</url>
    <content type="text"><![CDATA[大家知道，我是使用 Hexo 来构建我的静态站点的，每次使用 Markdown 语法书写 md 文档即可。写完后在本地使用 hexo g &amp; hexo s 命令【在本地生成并且部署，默认主页是 localhost:4000】来验证一下是否构建正常。如果有问题或者对页面效果不满意就返回重新修改，如果没有问题就准备提交到 GitHub 上面的仓库里面【在某个项目的某个分支】，后续 travid-cli 监控对应的分支变化，然后自动构建，并推送到 master 分支。至此，更新的页面就发布完成了，本人需要做的就是管理书写 md 文档，然后确保没问题就提交到 GitHub 的仓库。问题清单 前言描述的很好，很理想，但是有时候总会出现一些未知的问题，而我又不了解其中的技术，所以解决起来很麻烦，大部分时候都是靠蒙的【当然，也可以直接在 Hexo 的官方项目上提出 Issue，让作者帮忙解决】。下面就记录一些遇到的问题，以及我自己找到的原因。1-Markdown 语法不规范 这个错误有在 travis 上面出现过，在 travis 的 116 号、117 号错误：https://travis-ci.org/iplaypi/iplaypi.github.io/builds/476399853 。在使用 hexo 框架的时候，一定要确保 markdown 文件里面的代码块标识【标记代码的类型，例如：java、bash、html 等】使用正确。否则使用 hexo g 生成静态网页的时候，不会报错，但是却没有成功生成 html 静态网页，虽然 html 静态文件是有的，但是却查看不了，显示一片空白。代码块示例：Java 格式 xml 格式bash 格式 例如我把图一的 java 误写成了 bash，hexo g 的时候没有报错，但是生成的 html 静态网页却是空白一片，打开了什么也看不到。空白页面 但是如果把 java 误写成了 xml，在本地执行 hexo g 的时候不会报错，生成的 html 静态网页也是正常的。而一旦使用 travis-cli 执行自动构建的时候，构建是失败的【在 travis 的 116 号、117 号错误：https://travis-ci.org/iplaypi/iplaypi.github.io/builds/476399853 】，并且可以看到错误信息，图四，但是我看不懂错误原因，只能猜测找到问题所在，比较耗时。travis-cli 报错日志【我看不懂】：travis-cli 日志 1travis-cli 日志 2travis-cli 日志 3此外，写 Markdown 文档，使用代码块标记的时候，使用 3 个反单引号来标记，如果不熟悉代码块里面的编程语言，可以省略类型，例如 java、bash、javascript，不要填写，否则填错了生成的 html 静态文件是空白的。还有就是如果代码块里面放的是一段英文文本，和编程语言无关，也不要填写类型，否则生成的 html 静态文件也是空白的。2-Hexo 报错奇怪 这个错误还没有到 travis 上面，所以 travis 上面没有记录；在本地测试过程中，无论是 hexo s 还是 hexo g 都会报错，错误信息如图：看着这个信息，很像在当前项目的目录中找不到 hexo 命令，和 java 类似，我就怀疑是不是安装的 hexo 被什么时候卸载了，其实不是的，在其它项目中还能用。后来我发现是当前项目使用的模块缺失，为什么会缺失我也不知道，由于这些缺失的模块是通过 hexo 引入的，所以直接报错：hexo not found，给人以误导。总的来说，就是报错有误导性，没有报模块缺失，而我又不懂这些，查了一些资料，手动测试了一些方法，总算找到原因所在。找到原因，那解决办法很简单了，直接安装缺失的模块即可，使用 nmp install 命令安装 package.json 里面的模块。3-Hexo 配置错误引起的误导性 这个错误还没有到 travis 上面，所以 travis 上面没有记录；这个错误和上面的类似，但是如果从报错信息上面看，也具有误导性。在更改了 _config.yml 配置文件后，按照正常步骤去生成、部署的时候【使用 hexo g &amp; hexo s 命令，直接报错了，把我整蒙了，报错信息如下：关键配置部分如下，后续找到问题确实出在这里：从图中看信息，我也看不到什么原因，因为确实不懂。注意，我为了测试，发现 hexo g 是没有问题的，也就是生成没问题，那问题就出在部署步骤了，它会不认这个 hexo s 命令？我查了资料，发现大部分人都说缺失 hexo server 模块，我通过检查可以确保本机有这个模块，而且卸载了重新装，所以不是这个问题。最后发现是配置信息里面的参数【官方定义的关键词】错误了，里面的 Plugins 这个参数应该使用首字母大写，这谁能想到，正确的配置参数如下图：4-travis 配置问题 这个错误有在 travis 上面出现过，在 travis 的 27 号：https://travis-ci.org/iplaypi/iplaypi.github.io/builds/448152737 。在使用 travis 自动构建时，有一次突发奇想，想使用最新版本的 node_js，于是在 travis.yml 配置文件中，把 node_js 设为了 stable，即稳定版本，这样在构建的时候会使用最新稳定版本的 node_js，没想到就出问题了。node_js 的配置如下：travis 报错日志如下：重要部分：12error nunjucks@3.1.3: The engine "node" is incompatible with this module. Expected version "&gt;= 6.9.0 &lt;= 11.0.0-0". Got "11.0.0"error Found incompatible module看来还是在搞清楚新旧版本之间的差异后再想着升级版本，不要随意来，要不然浪费的是自己的时间。后来解决办法就是手动指定 node_js 的版本。5 - 无缘无故出现的问题 这个错误有在 travis 上面出现过，在 travis 的 133 号、134 号错误、135 号错误、136 号错误，举例：https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498318318 ；日志部分截图：这错误信息里面对我来说确实看不到有效的内容，还没找到解决办法，看似是文件路径不存在，但是项目配置也没变过。123456789npm ERR! path /home/travis/.nvm/versions/node/v10.10.0/lib/node_modules/hexo-cli/node_modules/highlight.js/tools/build.jsnpm ERR! code ENOENTnpm ERR! errno -2npm ERR! syscall chmodnpm ERR! enoent ENOENT: no such file or directory, chmod '/home/travis/.nvm/versions/node/v10.10.0/lib/node_modules/hexo-cli/node_modules/highlight.js/tools/build.js'npm ERR! enoent This is related to npm not being able to find a file.npm ERR! enoent npm ERR! A complete log of this run can be found in:npm ERR! /home/travis/.npm/_logs/2019-02-25T18_45_08_713Z-debug.log等待找问题的原因。好，仔细看了日志、找了博客文档，没有解决方法，我也不懂，看到可能是版本原因【我不能升级 nodejs 版本，与 yarn 有关】，可能是权限问题。我用 sudo npm install -g hexo-cli 试了试，明显不行：https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498794142 ，然后我就放弃了，直接改回来提交了，没想到无缘无故就可以了，构建日志：https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498796865 。准备发邮件问问 travis 客服，我现在单方面怀疑是 travis 的环境问题或者构建脚本所依赖的环境问题。由于时差问题，先记录几个时区的缩写，方便查看邮件内容的时候核对时间：UTC【世界标准时间】、EST【东部标准时间，UTC-5】、CET【欧洲中部时间，UTC+1】。我发送的邮件内容如下【发送于北京时间 2019-02-28 14:42:00】：完整文字版供参考 123456789101112131415161718192021Automatic building is failedHello,I hava a repository in GitHub,and i use travis-ci to build it automatically.I configured the correct script,and it has been built successfully more than one hundred times.My script is :https://github.com/iplaypi/iplaypi.github.io/blob/source/.travis.yml ;But it built failed at 2019-02-26,the all log as follows (i retry it three times,but still failed):https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498267014https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498278045https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498297576https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498318318I cannot find any usefuI information in my log.I am very sad and helpless.But i retry it at 2019-02-27,it actually build successfully,amazing.I swear I have not changed any files,the successful log is:https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498796865 ;So i am puzzled,i donnot know why,i suspect it is a problem with the machine.Can you help me?Best wishes. 发送后对方自动有一个回复，告知我他们的工作时间【中国北京时间与对方时差 + 13】：等了好几天，对方终于回复了【回复于北京时间 2019-03-04 11:00:00】，对方回复内容如下：对方回复重要文字内容 123456789Hey there,Thanks for reaching out and I&apos;m sorry for these spurious failures you have experienced.I think it could be an issue with the package itself or the NPM registry on that specific day. I&apos;ve looked at NPM&apos;s status and their was an incident on Feb. 27th. See https://status.npmjs.org/incidents/ptnlj2rtwfwm. Maybe it was already happening on Feb. 26th? Sorry for not having a better explanation.Please let us know if this issue resurfaces again, we would be happy to have another look.Thanks in advance and happy building! 看起来技术支持也没发现是啥问题，只是说有可能是 NPM 的问题，还给了一个链接：https://status.npmjs.org/incidents/ptnlj2rtwfwm，根据链接可以看到 NPM 的状态在某个时间点出问题了【时间点为 2019-02-27 15:46:00 UTC，也就是北京时间 2019-02-27 23:46:00】：但是我那个自动构建的问题是出在北京时间 2019-02-26 凌晨的，时间点也对不上，所以技术支持只是怀疑，也没有结论，那我也就不管了，继续观察以后有没有相同的问题出现。6 - 排版问题 1、在 Markdown 文件中关于链接的，要使用 []、() 这 2 个完整的标记，不要直接放一个链接出来，会导致生成的 html 文件带链接的内容居中对齐，导致文字分散开来，不好看。2、中文括号不要使用，也会导致居中对齐的问题，文字排版不好看，使用方括号吧：【内容示例】。7 - 草稿问题 我在使用 Hexo 的草稿功能时，发现一个问题，操作完成发布时，发现 Markdown 文档的头部描述信息变化了。例如我本来设置的 id 又变回了日期【可以理解，因为模板就是这样设置的】，然后 tags 的中括号中的标签变为了无需列表【不可理解】。暂时还没发现内容的变化，可能是内容中没有特殊符号。导致的问题就是草稿发布后【内容已经变化了】，提交到 source 分支，自动构建时，提交到主分支 master 后，这些文章的链接变为了日期的乱格式【因为是基于错误的 Markdown 文件构建的】。所以以后还是不要使用草稿功能了，没有必要，还麻烦，没写完也可以发布嘛，没啥大问题。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>markdown</tag>
        <tag>java</tag>
        <tag>bash</tag>
        <tag>xml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[鸡蛋饼做法总结]]></title>
    <url>%2F2019021001.html</url>
    <content type="text"><![CDATA[鸡蛋饼算是一种小吃，做法多种多样，可以煎，可以烙，可以蒸；吃法也多种多样，有的地方会卷配菜吃，有的地方会配粥吃，有的地方会直接吃。总而言之，鸡蛋饼算是一种万能美食，全国各地都有，大家也都喜欢吃，本文就记录鸡蛋煎饼的做法总结，本文记录的做法是采用煎的方式，另外还会额外放点葱花。食材准备 以下准备的食材可以煎 15 个鸡蛋饼左右：1、鸡蛋 4 个（喜欢的话多放点也可以）；2、200-300 克面粉（可以煎 15 个左右，面粉不能确定量，是因为如果面糊没有配好，就适量加水或者加面，调整好为止，具体会用到多少看情况了）；3、小葱 5 根（根据个人口味添加，多点少点都行）；适量的食材 制作步骤 制作步骤很简单，只要能煎好一张饼，重复进行就行了，煎好一张饼大概需要 3 分钟。当然，如果是第一次煎饼，可能煎前面几张饼的时候需要练习一下，也可能需要重新调制面糊，所以时间会长一点，但是为了煎饼成功，麻烦一点也值了。调制面糊的过程就不记录了，就是加盐（4 勺）、葱花、面粉、鸡蛋、水（最好可以用凉白开，别直接使用自来水）搅拌即可，如果里面有很多面疙瘩，不用担心，静置 10 分钟搅拌一次，重复 3 次左右面疙瘩即全部溶于水。面糊调制初步，还有很多面疙瘩 面糊调制完成 粘稠度大概这样，不会很粘稠，和液体差不多 1，锅里加油烧热，只要半勺即可（吃饭的那种小汤勺），多了会腻，然后火力转小火，并一直持续小火。 半勺油的量 2，放入一汤勺面糊（烧汤那种大汤勺，或者电饭煲自带的那种粥勺），如果发现煎出来的饼太厚了或者太大了，可以适当少放一点点面糊，具体放多少自己把握。然后适当转动煎锅，让面糊呈圆形（一定要快，10 秒内完成，否则因为受热不均匀，饼可能会散开变成多块，或者是一个圆环饼套着一个小圆饼），等逐渐凝固后就成了圆饼，然后接触锅的那一面就变得金黄，这个过程大概 1 分钟。 一大汤勺面糊 加面糊到锅里 转动成型 3，等凝固后就可以翻身了，这个步骤说简单也简单，说难也难，如果直接用锅不方便翻身的话，可以借助铲子，翻身后，可以看到饼的上一面已经煎好了，金黄的。这个时候注意要适当把饼转动一下，吸收一下油，避免粘锅。 给饼翻身 4，翻身后再煎 1 分钟左右，就可以出锅了，如果看到饼上面哪里煎的不均匀，还没熟，可以再着重煎几十秒。切记不能煎太久，要不然饼就糊了。 翻身后继续煎 1 分钟，再根据实际情况着重煎一下，准备出锅 5，出锅装盘，继续下一张鸡蛋饼。 全部出锅装盘 注意事项1、特别注意，如果调制面糊不是特别熟练的话，可能面糊的粘稠度不适合，或者调味偏淡偏咸，这样都不好，所以最好尝试着煎一个，然后品尝一下，如果味道不对再加调料，如果煎出来的饼不对，再加水或者面粉。多试几次，确保煎出来的饼自己满意。如果一味地煎饼，最后发现不好吃，那就浪费了；2、如果有两个锅可以用，为了节省时间，最好两个锅同时煎，要不然整个过程很枯燥，因为有一半的时间都在等待；3、有时候可能看着好像煎糊了，不用担心，不影响吃，因为出锅后等一会儿，褐色就会变成金黄色，非常好看；4、整个过程一定要确保是小火，否则饼很快就糊了；5、难点在于翻身，只要一出错一张饼就废了（或者变成了一堆碎饼）。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>鸡蛋饼</tag>
        <tag>鸡蛋葱饼</tag>
        <tag>鸡蛋煎饼</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ItChat 系列 0 - 初识 ItChat]]></title>
    <url>%2F2019020701.html</url>
    <content type="text"><![CDATA[微信已经是我们日常生活中常用的 APP 之一，每天都离不开。作为掌握技术的理工科人员，有时候总想着是否可以利用微信的接口完成一些重复的工作，例如群发消息、自动回复、接入机器人自动聊天等。当然，这些都可以实现，而且只要是人工可以做到的事情，基本都可以做到自动化（前提是微信提供了对应的接口，反例就是自动收发红包不行，当然微信不会直接提供 API 接口，需要自己寻找）。本文就讲解为了做到这些，需要的入门知识点，主要就是利用 ItChat 工具（屏蔽了微信的 API 接口，简化了使用微信接口的过程，不懂技术的普通人也可以轻松掌握），当然本文只是一个入门的例子而已（完成后对自己来说很实用而且有成就感），后续会讲解更加深入与广泛的内容。本文基于 Windows 7 操作系统，Python 2.7 版本（为了兼容性与易维护性，我推荐使用 Python 3.x 版本）ItChat 简介 摘录官方文档描述：itchat 是一个开源的微信个人号接口，使用 python 调用微信从未如此简单；使用不到三十行的代码，你就可以完成一个能够处理所有信息的微信机器人；当然，该 api 的使用远不止一个机器人，更多的功能等着你来发现；该接口与公众号接口 itchatmp 共享类似的操作方式，学习一次掌握两个工具；如今微信已经成为了个人社交的很大一部分，希望这个项目能够帮助你扩展你的个人的微信号、方便自己的生活。当然，我是觉得上面的描述有一些语句不通顺，但是不影响我们理解作者的原意。其实微信官方并没有提供详细的 API 接口，ItChat 是利用网页版微信收集了接口信息，然后独立封装一层，屏蔽掉底层的接口信息，提供一套简单的使用接口，方便使用者调用，这不仅提升了效率，还扩展了使用人群。使用入门 以下使用入门包括基础环境的安装、itcaht 的安装、代码的编写、实际运行，当然，为了避免赘述，不会讲解的很详细，如果遇到一些问题，自行利用搜索引擎解决。安装 Python 环境 下载 Python去官网：https://www.python.org/downloads/windows ，选择自己需要的版本，我这里选择 Windows 系统的版本（64 位操作系统），Python 2.7（这是一个很古老的版本了，推荐大家使用 3.x 版本）；我选择的版本 下载过程就和下载普通的文件、视频等一样，根据网速的限制有快有慢。安装 Python就像安装普通程序一样，直接双击下载的程序文件，选择安装即可，这里就不再赘述详细的安装过程了；如果你们的环境不是 Windows 7 系统的，可以自行使用搜索引擎搜索教程；这里一定要注意安装的版本是否适配自己的操作系统（包括系统类型与系统位数）；在 Windows 系统的 程序和功能 中查看已经安装完成的 Python 程序（2.7 版本，我是使用 Anaconda2 安装的，所以看起来有些不一样）：配置环境变量 如果这一步忽略了，使用 Python 或者 Python 自带的插件的时候（比如安装 ItChat 的时候就会用到 pip 工具），会找不到应用程序，只能先进入到 Python 目录或者插件所在的目录再使用对应的工具（例如进入 Python 所在的目录或者 pip 所在的目录），比较麻烦，所以在此建议大家配置一下环境变量；配置环境变量的过程也不再赘述，大家自己利用搜索引擎获取，下图是基于 Windows 7 版本的配置截图示例；系统属性 高级系统设置 环境变量 ，我这里编辑用户环境变量 PATH 的内容（如果不存在就新建，当然编辑系统环境变量 PATH 的内容也是可以的），切记内容一定是英文格式下的，多个使用英文逗号分隔 用户环境变量 ，我这里需要填写 2 条内容，使用英文逗号隔开（如果是直接安装的 Python，pip 和 python 应该在同一个路径下面，所以只需要 1 条就行了）我的环境需要配置 2 条内容 内容解释：1234--pip 所在目录 D:\Anaconda2\Scripts\;--python 所在目录 D:\Anaconda2;安装 ItChat 工具 在 Python 安装完成的情况下，才能进行接下来的操作，因为 ItChat 是基于 Python 环境运行的；为了验证 Python 是否正确安装，可以在命令行中输入 python，如果看到以下内容，就说明 Python 安装成功：接下来利用 pip 工具（Python 自带的）直接安装 itchat，非常简单，使用命令（如果 pip 命令不可用，请检查 Python 的安装目录是否存在 pip.exe 文件）：1pip install itchat安装 ItChat如果看到以下内容，说明 ItChat 安装成功：入门代码示例 一切准备就绪，接下来就可以写代码了，当然，入门代码非常简单实用（我会尽可能多的添加注释说明）：123456789101112131415161718192021222324252627282930313233343536#-*-coding:utf-8 -*-# 从 python 环境中导入 itchat 包，re 正则表达式包 import itchat, re# 从 itchat.content 中导入所有类、常量 (例如代码中的 TEXT 其实就是 itchat.content.TEXT 常量)from itchat.content import *# 导入时间包里面的 sleep 方法 from time import sleep# 导入随机数包 import random# 注册消息类型为文本 (即只监控文本消息，其它的例如语音 / 图片 / 表情包 / 文件都不会监控)# 也就是说只有普通的文字微信消息才能触发以下的代码 # isGroupChat=True 开启群聊模式，即只是监控群聊内容 (如果不开启就监控个人聊天，不监控群聊)@itchat.msg_register ([TEXT], isGroupChat=True)# @itchat.msg_register ([TEXT])def text_reply(msg): # msg 是消息体，msg ['Text'] 用来获取消息内容 # 第一个单引号中的内容是关键词，使用正则匹配，可以自行更改 (我使用.* 表示任意内容), 如果使用中文注意 2.x 版本的 Python 会报错，需要 u 前缀 message = msg ['Text'] print (message) match = re.search ('.*', message) # match = re.search (u'年 | 春 | 快乐', message) # 增加睡眠机制，随机等待一定的秒数 (1-10 秒) 再回复，更像人类 second = random.randint (1,10) sleep (second) if match: # msg ['FromUserName'] 用来获取用户名，发送消息给对方 from_user_name = msg ['FromUserName'] print (from_user_name) itchat.send (('====test message'), from_user_name) # 第一个单引号中的内容是回复的内容，可以自行更改 # 热启动，退出一定时间内重新登录不需要扫码 (其实就是把二维码图片存下来，下次接着使用)itchat.auto_login (hotReload=True)# 开启命令行的二维码 itchat.auto_login (enableCmdQR=True)# 运行 itchat.run ()代码截图如下：演示 登录扫码 登录成功 群聊自动回复（正则是任意内容，所以总是会自动回复）退出 重新登录继续聊天（由于开启了热启动，不需要重新扫码）继续聊天 小问题总结 1、部分系统可能字幅宽度有出入，可以通过将 enableCmdQR 赋值为特定的倍数进行调整：12# 如部分的 linux 系统，块字符的宽度为一个字符 (正常应为两字符), 故赋值为 2itchat.auto_login (enableCmdQR=2)2、Python 2.7 版本的中文报错问题（在 Python 2.7 环境下使用中文需要额外注意，坑比较多）： 例如代码中正则匹配带中文（由于编码问题导致无法匹配，或者会抛出异常）12# 正则搜索带中文，直接单引号在 Python 2.7 环境下是不行的 match = re.search ('年 | 春 | 快乐', message)实际运行时就会报错（报错信息如果不捕捉后台是看不到的）或者匹配结果不是想象中的（仅针对 Python 2.x 环境）需要使用 u 前缀 123# 正则搜索带中文，直接单引号在 Python 2.7 环境下是不行的 # 增加 u 前缀，表示 unicode 编码，才行 match = re.search (u'年 | 春 | 快乐', message)3、如果不开启热启动，每次重新登录时都会生成新的二维码，直接在 Wimdows 的命令行中，可能由于窗口太小显示不完整，此时需要拉伸一下命令行的窗口：4、有些人的电脑设置问题，命令行环境背景为白色，生成的二维码的颜色黑白色是相反的，导致扫码时无法识别，此时需要设置代码：12# 默认控制台背景色为暗色 (黑色)，若背景色为浅色 (白色)，可以将 enableCmdQR 赋值为负值 itchat.auto_login (enableCmdQR=-1) 接入机器人 一般读者做到上面的内容就算入门了，可以实现自动回复，并且关于 ItChat 也了解了一些，可以独自参考文档进行更加深入的开发了。但是，自动回复的内容毕竟太固定了，而且只能覆盖极少的内容，没办法实现真正的自动化。要想做到真正的自动化回复，机器人是少不了了，那么接下来讲解的就是如何接入一个第三方机器人，实现机器人自动回复。当然，代码内容也会稍显复杂，操作步骤也会稍显繁琐。接入机器人代码示例 接入机器人时为了换种方式，先把群聊模式关闭，使用个人聊天监控模式（方便聊天内容的随意性，更能提现机器人的可用性）：1@itchat.msg_register ([TEXT])还要导入网络请求相关的包：1import requests需要使用图灵机器人的核心配置（注册图灵机器人的过程不在此赘述，官网链接：http://www.tuling123.com ）：1234567891011121314151617181920# 封装一个根据内容调用机器人接口，返回回复的方法 def get_response(msg): # 构造了要发送给服务器的数据 apiUrl = 'http://www.tuling123.com/openapi/api' data = &#123; 'key' : APIKEY, 'info' : msg, 'userid' : 'wechat-robot', &#125; try: r = requests.post (apiUrl, data=data).json () # 字典的 get 方法在字典没有 'text' 值的时候会返回 None 而不会抛出异常 return r.get ('text') # 为了防止服务器没有正常响应导致程序异常退出，这里用 try-except 捕获了异常 # 如果服务器没能正常交互 (返回非 json 或无法连接), 那么就会进入下面的 return except Exception,err: # 打印一下错误信息 print (err) # 将会返回一个 None return完整代码示例（代码会封装的更好，格式更加规范，易读）：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#-*-coding:utf-8 -*-# 从 python 环境中导入 itchat 包，requests 网络请求包 import itchat, requests# 从 itchat.content 中导入所有类、常量 (例如代码中的 TEXT 其实就是 itchat.content.TEXT 常量)from itchat.content import *# 导入时间包里面的 sleep 方法 from time import sleep# 导入随机数包 import random# 机器人的 apikeyAPIKEY = '376cb2ca51d542c6b2e660f3c9ea3754'# 封装一个根据内容调用机器人接口，返回回复的方法 def get_response(msg): # 构造了要发送给服务器的数据 apiUrl = 'http://www.tuling123.com/openapi/api' data = &#123; 'key' : APIKEY, 'info' : msg, 'userid' : 'wechat-robot', &#125; try: r = requests.post (apiUrl, data=data).json () # 字典的 get 方法在字典没有 'text' 值的时候会返回 None 而不会抛出异常 return r.get ('text') # 为了防止服务器没有正常响应导致程序异常退出，这里用 try-except 捕获了异常 # 如果服务器没能正常交互 (返回非 json 或无法连接), 那么就会进入下面的 return except Exception,err: # 打印一下错误信息 print (err) # 将会返回一个 None return# 注册消息类型为文本 (即只监控文本消息，其它的例如语音 / 图片 / 表情包 / 文件都不会监控)# 也就是说只有普通的文字微信消息才能触发以下的代码 # isGroupChat=True 开启群聊模式，即只是监控群聊内容 (如果不开启就监控个人聊天，不监控群聊)# @itchat.msg_register ([TEXT], isGroupChat=True)@itchat.msg_register ([TEXT])def tuling_reply(msg): # msg 是消息体，msg ['Text'] 用来获取消息内容 # 第一个单引号中的内容是关键词，使用正则匹配，可以自行更改 (我使用.* 表示任意内容), 如果使用中文注意 2.x 版本的 Python 会报错，需要 u 前缀 message = msg ['Text'] print (message) # 增加睡眠机制，随机等待一定的秒数 (1-10 秒) 再回复，更像人类 second = random.randint (1,10) sleep (second) # 为了保证在图灵 apikey 出现问题的时候仍旧可以回复，这里设置一个默认回复 defaultReply = 'I received:' + message # 如果图灵 apikey 出现问题，那么 reply 将会是 None reply = get_response (message) # a or b 的意思是，如果 a 有内容，那么返回 a, 否则返回 b return reply or defaultReply# 热启动，退出一定时间内重新登录不需要扫码 (其实就是把二维码图片存下来，下次接着使用)itchat.auto_login (hotReload=True)# 开启命令行的二维码 itchat.auto_login (enableCmdQR=True)# 运行 itchat.run ()代码截图（使用工具渲染了一下）：接入机器人演示 演示一下，随便聊了几句：备注 1、ItChat 项目 GitHub 地址：https://github.com/littlecodersh/itchat ；2、ItChat 项目说明文档：https://itchat.readthedocs.io/zh/latest ；3、感谢微博科普博主 灵光灯泡 的科普视频 https://weibo.com/6969849160/HeLhjcKtA 以及文档参考 石墨文档 ；4、Python 下载官网：https://www.python.org/downloads/windows ，大家一定要选择与自己当前环境适配的版本（包括操作系统版本、Python 版本），环境变量最好配置一下；5、图灵机器人官网：http://www.tuling123.com ；]]></content>
      <categories>
        <category>ItChat 系列</category>
      </categories>
      <tags>
        <tag>ItChat</tag>
        <tag>微信接口</tag>
        <tag>自定义接口</tag>
        <tag>自动回复</tag>
        <tag>微信机器人</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微博电影文稿备份 2]]></title>
    <url>%2F2019020101.html</url>
    <content type="text"><![CDATA[在 2019 年 1 月 1 日整理过一篇，包含 10 部电影，这是第二篇。武侠 《武侠》是一部 2011 年的动作悬疑片，豆瓣评分 6.6，好于 48% 的动作片，好于 43% 的悬疑片，可见评分不是很好，但是里面的动作戏还是不错的。这部电影由陈可辛导演，甄子丹、金城武、汤唯主演。1917 年，中国西南边陲的刘家村，刘金喜（甄子丹 饰演）和妻子阿玉（汤唯 饰演）共同抚养两个儿子方正和晓天，日子平淡且幸福。直到某一天，两个不速之客打破了刘家村的平静，也摧毁着金喜一家的生活，这二人企图洗劫村中的钱柜，被刚好在此的金喜撞见，一阵混乱打斗，二匪稀里糊涂被金喜打死。由于其中一人是政府通缉的要犯，因此县官大喜过望，村里人也将金喜奉为大英雄。但是，这看似普普通通的盲打误杀却引起一个人的怀疑，他名叫徐百九（金城武 饰演），是县衙的捕快。从蛛丝马迹上来看，二匪系死于武功高强人之手，徐百九由此留在村里，对金喜展开了连番的观察、调查与试探。 在这一过程中，金喜神秘的真实身份渐渐浮出水面，而刘家村也面临着一场空前的危机。 以下截取的 10 分钟片段（00:05:56 到 00:15:51），是电影开头，刘金喜和两个劫匪的打斗，并稀里糊涂把两个劫匪全部杀死，然后县官带队破案，发现死者之一是通缉犯。在这个片段中，主演悉数出场：汤唯、金城武、甄子丹，而且这一段打斗也是后面剧情发展的主要依据。后面会发现这一段打斗被捕快徐百九解释地合情合理，而刘金喜的身份也因此渐渐浮出水面，为最终的结局埋下了伏笔。让子弹飞 《让子弹飞》是一部 2010 年的剧情喜剧片，豆瓣评分 8.7，好于 97% 的喜剧片，好于 95% 的剧情片，又名《让子弹飞一会》、《火烧云》，由姜文导演。 民国年间，花钱捐得县长的马邦德（葛优 饰演）携妻子（刘嘉玲 饰演）及随从走马上任。途经南国某地，遭劫匪张麻子（姜文 饰演）一伙伏击，随从尽死，只夫妻二人侥幸活命，马为保命，谎称自己是县长的汤师爷。为汤师爷许下的财富所动，张麻子摇身一变化身县长，带着手下赶赴鹅城上任。有道是天高皇帝远，鹅城地处偏僻，一方霸主黄四郎（周润发 饰演）只手遮天，全然不将这个新来的县长放在眼里。张麻子痛打了黄的武教头（姜武 饰演），黄则设计害死张的义子小六（张默 饰演）。原本只想赚钱的马邦德，怎么也想不到竟会被卷入这场土匪和恶霸的角力之中。鹅城上空愁云密布，血雨腥风在所难免……截取的片段一，7 分钟（00:00:32 到 00:07:28），是电影开头的介绍，著名的台词：让子弹飞一会儿。截取的片段二，6 分钟（00:25:45 到 00:31:18），是黄四郎陷害小六子的场景，到底吃了几碗粉，小六子切腹致死，陈坤演技无敌。著名的台词：你不是欺负老实人吗。截取的片段三，12 分钟（00:34:15 到 00:46:17），是六子死后黄四郎约见县长，县长带着师爷奔赴鸿门宴，本来准备杀死黄四郎替六子报仇，后来计划有变。整个聊天过程真的令人大呼过瘾，经典台词：够硬吗？够硬！]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>微博电影</tag>
        <tag>文稿备份</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 异常之 Netty 相关]]></title>
    <url>%2F2019011401.html</url>
    <content type="text"><![CDATA[在做项目的时候，需要新引入一个外部依赖，于是很自然地在项目的 pom.xml 文件中加入了依赖坐标，然后进行编译、打包、运行，没想到直接抛出了异常：122019-01-13_17:18:52 [sparkDriverActorSystem-akka.actor.default-dispatcher-5] ERROR actor.ActorSystemImpl:66: Uncaught fatal error from thread [sparkDriverActorSystem-akka.remote.default-remote-dispatcher-7] shutting down ActorSystem [sparkDriverActorSystem]java.lang.VerifyError: (class: org/jboss/netty/channel/socket/nio/NioWorkerPool, method: createWorker signature: (Ljava/util/concurrent/Executor;) Lorg/jboss/netty/channel/socket/nio/AbstractNioWorker;) Wrong return type in function任务运行失败，仔细看日志觉得很莫名奇妙，是一个 java.lang.VerifyError 错误，以前从来没见过类似的。本文记录这个错误的解决过程。问题出现 在上述错误抛出之后，可以看到 SparkContext 初始化失败，然后进程就终止了；完整日志如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677782019-01-13_17:18:52 [sparkDriverActorSystem-akka.actor.default-dispatcher-5] ERROR actor.ActorSystemImpl:66: Uncaught fatal error from thread [sparkDriverActorSystem-akka.remote.default-remote-dispatcher-7] shutting down ActorSystem [sparkDriverActorSystem]java.lang.VerifyError: (class: org/jboss/netty/channel/socket/nio/NioWorkerPool, method: createWorker signature: (Ljava/util/concurrent/Executor;) Lorg/jboss/netty/channel/socket/nio/AbstractNioWorker;) Wrong return type in function at akka.remote.transport.netty.NettyTransport.(NettyTransport.scala:283) at akka.remote.transport.netty.NettyTransport.(NettyTransport.scala:240) at sun.reflect.NativeConstructorAccessorImpl.newInstance0 (Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance (NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance (DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance (Constructor.java:423) at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$2.apply (DynamicAccess.scala:78) at scala.util.Try$.apply (Try.scala:161) at akka.actor.ReflectiveDynamicAccess.createInstanceFor (DynamicAccess.scala:73) at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply (DynamicAccess.scala:84) at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply (DynamicAccess.scala:84) at scala.util.Success.flatMap (Try.scala:200) at akka.actor.ReflectiveDynamicAccess.createInstanceFor (DynamicAccess.scala:84) at akka.remote.EndpointManager$$anonfun$9.apply (Remoting.scala:711) at akka.remote.EndpointManager$$anonfun$9.apply (Remoting.scala:703) at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply (TraversableLike.scala:722) at scala.collection.Iterator$class.foreach (Iterator.scala:727) at scala.collection.AbstractIterator.foreach (Iterator.scala:1157) at scala.collection.IterableLike$class.foreach (IterableLike.scala:72) at scala.collection.AbstractIterable.foreach (Iterable.scala:54) at scala.collection.TraversableLike$WithFilter.map (TraversableLike.scala:721) at akka.remote.EndpointManager.akka$remote$EndpointManager$$listens (Remoting.scala:703) at akka.remote.EndpointManager$$anonfun$receive$2.applyOrElse (Remoting.scala:491) at akka.actor.Actor$class.aroundReceive (Actor.scala:467) at akka.remote.EndpointManager.aroundReceive (Remoting.scala:394) at akka.actor.ActorCell.receiveMessage (ActorCell.scala:516) at akka.actor.ActorCell.invoke (ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox (Mailbox.scala:238) at akka.dispatch.Mailbox.run (Mailbox.scala:220) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec (AbstractDispatcher.scala:397) at scala.concurrent.forkjoin.ForkJoinTask.doExec (ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask (ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker (ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run (ForkJoinWorkerThread.java:107)2019-01-13_17:18:52 [sparkDriverActorSystem-akka.actor.default-dispatcher-6] INFO remote.RemoteActorRefProvider$RemotingTerminator:74: Shutting down remote daemon.2019-01-13_17:18:52 [sparkDriverActorSystem-akka.actor.default-dispatcher-6] INFO remote.RemoteActorRefProvider$RemotingTerminator:74: Remote daemon shut down; proceeding with flushing remote transports.2019-01-13_17:18:52 [sparkDriverActorSystem-akka.actor.default-dispatcher-6] ERROR Remoting:65: Remoting system has been terminated abrubtly. Attempting to shut down transports2019-01-13_17:18:52 [sparkDriverActorSystem-akka.actor.default-dispatcher-6] INFO remote.RemoteActorRefProvider$RemotingTerminator:74: Remoting shut down.2019-01-13_17:19:02 [main] ERROR spark.SparkContext:95: Error initializing SparkContext.java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds] at scala.concurrent.impl.Promise$DefaultPromise.ready (Promise.scala:219) at scala.concurrent.impl.Promise$DefaultPromise.result (Promise.scala:223) at scala.concurrent.Await$$anonfun$result$1.apply (package.scala:107) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn (BlockContext.scala:53) at scala.concurrent.Await$.result (package.scala:107) at akka.remote.Remoting.start (Remoting.scala:179) at akka.remote.RemoteActorRefProvider.init (RemoteActorRefProvider.scala:184) at akka.actor.ActorSystemImpl.liftedTree2$1(ActorSystem.scala:620) at akka.actor.ActorSystemImpl._start$lzycompute (ActorSystem.scala:617) at akka.actor.ActorSystemImpl._start (ActorSystem.scala:617) at akka.actor.ActorSystemImpl.start (ActorSystem.scala:634) at akka.actor.ActorSystem$.apply (ActorSystem.scala:142) at akka.actor.ActorSystem$.apply (ActorSystem.scala:119) at org.apache.spark.util.AkkaUtils$.org$apache$spark$util$AkkaUtils$$doCreateActorSystem (AkkaUtils.scala:121) at org.apache.spark.util.AkkaUtils$$anonfun$1.apply (AkkaUtils.scala:53) at org.apache.spark.util.AkkaUtils$$anonfun$1.apply (AkkaUtils.scala:52) at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp (Utils.scala:2024) at scala.collection.immutable.Range.foreach$mVc$sp (Range.scala:141) at org.apache.spark.util.Utils$.startServiceOnPort (Utils.scala:2015) at org.apache.spark.util.AkkaUtils$.createActorSystem (AkkaUtils.scala:55) at org.apache.spark.SparkEnv$.create (SparkEnv.scala:266) at org.apache.spark.SparkEnv$.createDriverEnv (SparkEnv.scala:193) at org.apache.spark.SparkContext.createSparkEnv (SparkContext.scala:288) at org.apache.spark.SparkContext.(SparkContext.scala:457) at org.apache.spark.api.java.JavaSparkContext.(JavaSparkContext.scala:59) at com.ds.octopus.job.utils.SparkContextUtil.refresh (SparkContextUtil.java:77) at com.ds.octopus.job.utils.SparkContextUtil.getJsc (SparkContextUtil.java:34) at com.ds.octopus.job.executors.impl.WeiboZPZExporter.action (WeiboZPZExporter.java:95) at com.ds.octopus.job.executors.impl.WeiboZPZExporter.action (WeiboZPZExporter.java:41) at com.ds.octopus.job.executors.SimpleExecutor.execute (SimpleExecutor.java:40) at com.ds.octopus.job.client.OctopusClient.run (OctopusClient.java:162) at com.yeezhao.commons.buffalo.job.AbstractBUTaskWorker.runTask (AbstractBUTaskWorker.java:63) at com.ds.octopus.job.client.TaskLocalRunnerCli.start (TaskLocalRunnerCli.java:109) at com.yeezhao.commons.util.AdvCli.initRunner (AdvCli.java:191) at com.ds.octopus.job.client.TaskLocalRunnerCli.main (TaskLocalRunnerCli.java:41)2019-01-13_17:19:02 [main] INFO spark.SparkContext:58: Successfully stopped SparkContext错误日志截图：根据日志没有看出有关 Java 层面的什么问题，只能根据 JNI 字段描述符：1class: org/jboss/netty/channel/socket/nio/NioWorkerPool猜测是某一个类的问题，根据：1method: createWorker signature: (Ljava/util/concurrent/Executor;) Lorg/jboss/netty/channel/socket/nio/AbstractNioWorker;) Wrong return type in function猜测是某个方法的问题，方法的返回类型错误。然后在项目中使用 ctrl+shift+t 快捷键（全局搜索 Java 类，每个人的开发工具设置的可能不一样）搜索类：NioWorkerPool，发现这个类的来源不是新引入的依赖包，而是原本就有的 netty 相关包，所以此时就可以断定这个莫名其妙的错误的原因就在于这个类的 createWorker 方法返回类型上面了。搜索类 NioWorkerPool日志的 JNI 字段描述符显示返回类型是 AbstractNioWorker，但是这个一看就是抽象类，不是我们要找的，去类里面看源码，发现 createWorker 方法返回类型是 NioWorker：类 NioWorkerPool 源码 继续搜索类 NioWorker好，此时发现问题了，这个类有 2 个，居然存在两个相同的包名，但是依赖坐标不一样，所以这个隐藏的原因在于类冲突，但是并不能算是依赖冲突引起的。也就是说，NioWorker 这个类重复了，但是依赖包坐标不一样，类的包路径却是一模一样的，不会引起版本冲突问题，而在实际运行任务的时候会抛出运行时异常，所以我觉得找问题的过程很艰辛。使用依赖树查看依赖关系，是看不到版本冲突问题的，2 个依赖都存在：io.netty 依赖 org.jboss.netty 依赖 于是又在网上搜索了一下，发现果然是 netty 的问题，也就是新引入的依赖包导致的，但是根本原因令人哭笑：netty 的组织结构变化，发布的依赖坐标名称变化，但是所有的类的包名称并没有变化，导致了这个错误。问题解决 问题找到了，解决方法就简单了，移除传递依赖即可，同时也要注意以后再添加新的依赖一定要慎重，不然找问题的过程很是令人崩溃。移除依赖 移除配置示例 1234567&lt;!-- 移除引发冲突的 jar 包 --&gt;&lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.jboss.netty&lt;/groupId&gt; &lt;artifactId&gt;netty&lt;/artifactId&gt; &lt;/exclusion&gt;&lt;/exclusions&gt; 问题总结 1、参考：https://stackoverflow.com/questions/33573587/apache-spark-wrong-akka-remote-netty-version ；2、netty 的组织结构（影响发布的 jar 包坐标名称）变化了，但是所有的类的包名称仍然是一致的，很奇怪，导致我找问题也觉得莫名其妙，因为这不会引发版本冲突问题（但是本质上又是 2 个一模一样的类被同时使用，引发类冲突）；3、这个错误信息挺有意思的，解决过程也很好玩，边找边学习；4、对于这种重名的类【类的包路径名、类名】，竟然对应的 jar 包不一样，这种极其特殊的情况也可以使用插件检测出来：12&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;&lt;artifactId&gt;maven-enforcer-plugin&lt;/artifactId&gt; 使用 enforcer:enforce 命令即可。当然，这个插件还可以用来校验很多地方，例如代码中引用了 @Deprecated 的方法，也会给出提示信息，可以按照需求给插件配置需要校验的方面。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>netty</tag>
        <tag>nio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven 插件异常之 socket write error]]></title>
    <url>%2F2019011101.html</url>
    <content type="text"><![CDATA[今天在整理代码的时候，在本机（自己的电脑）通过 Maven 的 deploy 插件（org.apache.maven.plugins:maven-deploy-plugin:2.7）进行发布，把代码打包成构件发布到远程的 Maven 仓库（公司的私服），这样方便大家调用。可是，其中有一个项目发布不了（其它类似的 2 个项目都可以，排除了环境的原因），总是报错：1Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error以上错误日志中的项目名称、包名称均被替换。本文就记录从发现问题到解决问题的过程。环境所使用的 Maven 版本为：3.5.0。问题出现 对一个公共项目进行打包发布，部署到公司私服（已经排除环境因素），出现异常；使用命令 Maven：1mvn deploy出现异常：1234567891011121314[INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 02:49 min[INFO] Finished at: 2019-01-12T16:17:21+08:00[INFO] Final Memory: 68M/1253M[INFO] ------------------------------------------------------------------------[ERROR] Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy (default-deploy) on project dt-x-y-z: Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error -&gt; [Help 1][ERROR][ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR][ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException如果使用 -X 参数（完整命令：mvn deploy -X），可以稍微看到更详细的 Maven 部署日志信息：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990[INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 02:49 min[INFO] Finished at: 2019-01-12T16:17:21+08:00[INFO] Final Memory: 68M/1253M[INFO] ------------------------------------------------------------------------[ERROR] Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy (default-deploy) on project dt-x-y-z: Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error -&gt; [Help 1]org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy (default-deploy) on project dt-x-y-z: Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:213) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:154) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:146) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:81) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:309) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:194) at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:107) at org.apache.maven.cli.MavenCli.execute(MavenCli.java:993) at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:345) at org.apache.maven.cli.MavenCli.main(MavenCli.java:191) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289) at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415) at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)Caused by: org.apache.maven.plugin.MojoExecutionException: Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error at org.apache.maven.plugin.deploy.DeployMojo.execute(DeployMojo.java:193) at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208) ... 20 moreCaused by: org.apache.maven.artifact.deployer.ArtifactDeploymentException: Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error at org.apache.maven.artifact.deployer.DefaultArtifactDeployer.deploy(DefaultArtifactDeployer.java:143) at org.apache.maven.plugin.deploy.AbstractDeployMojo.deploy(AbstractDeployMojo.java:167) at org.apache.maven.plugin.deploy.DeployMojo.execute(DeployMojo.java:157) ... 22 moreCaused by: org.eclipse.aether.deployment.DeploymentException: Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error at org.eclipse.aether.internal.impl.DefaultDeployer.deploy(DefaultDeployer.java:326) at org.eclipse.aether.internal.impl.DefaultDeployer.deploy(DefaultDeployer.java:254) at org.eclipse.aether.internal.impl.DefaultRepositorySystem.deploy(DefaultRepositorySystem.java:422) at org.apache.maven.artifact.deployer.DefaultArtifactDeployer.deploy(DefaultArtifactDeployer.java:139) ... 24 moreCaused by: org.eclipse.aether.transfer.ArtifactTransferException: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error at org.eclipse.aether.connector.basic.ArtifactTransportListener.transferFailed(ArtifactTransportListener.java:52) at org.eclipse.aether.connector.basic.BasicRepositoryConnector$TaskRunner.run(BasicRepositoryConnector.java:364) at org.eclipse.aether.connector.basic.BasicRepositoryConnector.put(BasicRepositoryConnector.java:283) at org.eclipse.aether.internal.impl.DefaultDeployer.deploy(DefaultDeployer.java:320) ... 27 moreCaused by: org.apache.maven.wagon.TransferFailedException: Connection reset by peer: socket write error at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:650) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:553) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:535) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:529) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:509) at org.eclipse.aether.transport.wagon.WagonTransporter$PutTaskRunner.run(WagonTransporter.java:653) at org.eclipse.aether.transport.wagon.WagonTransporter.execute(WagonTransporter.java:436) at org.eclipse.aether.transport.wagon.WagonTransporter.put(WagonTransporter.java:419) at org.eclipse.aether.connector.basic.BasicRepositoryConnector$PutTaskRunner.runTask(BasicRepositoryConnector.java:519) at org.eclipse.aether.connector.basic.BasicRepositoryConnector$TaskRunner.run(BasicRepositoryConnector.java:359) ... 29 moreCaused by: java.net.SocketException: Connection reset by peer: socket write error at java.net.SocketOutputStream.socketWrite0(Native Method) at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111) at java.net.SocketOutputStream.write(SocketOutputStream.java:155) at org.apache.maven.wagon.providers.http.httpclient.impl.io.SessionOutputBufferImpl.streamWrite(SessionOutputBufferImpl.java:126) at org.apache.maven.wagon.providers.http.httpclient.impl.io.SessionOutputBufferImpl.flushBuffer(SessionOutputBufferImpl.java:138) at org.apache.maven.wagon.providers.http.httpclient.impl.io.SessionOutputBufferImpl.write(SessionOutputBufferImpl.java:169) at org.apache.maven.wagon.providers.http.httpclient.impl.io.ContentLengthOutputStream.write(ContentLengthOutputStream.java:115) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon$RequestEntityImplementation.writeTo(AbstractHttpClientWagon.java:209) at org.apache.maven.wagon.providers.http.httpclient.impl.DefaultBHttpClientConnection.sendRequestEntity(DefaultBHttpClientConnection.java:158) at org.apache.maven.wagon.providers.http.httpclient.impl.conn.CPoolProxy.sendRequestEntity(CPoolProxy.java:162) at org.apache.maven.wagon.providers.http.httpclient.protocol.HttpRequestExecutor.doSendRequest(HttpRequestExecutor.java:237) at org.apache.maven.wagon.providers.http.httpclient.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:122) at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.MainClientExec.execute(MainClientExec.java:271) at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184) at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.RetryExec.execute(RetryExec.java:88) at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.RedirectExec.execute(RedirectExec.java:110) at org.apache.maven.wagon.providers.http.httpclient.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184) at org.apache.maven.wagon.providers.http.httpclient.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.execute(AbstractHttpClientWagon.java:834) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:596) ... 38 more[ERROR][ERROR][ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException由于对 Maven 构件的原理不清楚，通过日志报错也看不出根本原因是什么，根据最后一行日志的链接：http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException ，我看到描述：123Unlike many other errors, this exception is not generated by the Maven core itself but by a plugin. As a rule of thumb, plugins use this error to signal a problem in their configuration or the information they retrieved from the POM.The concrete meaning of the exception depends on the plugin so please have a look at its documentation. The documentation for many common Maven plugins can be reached via our plugin index.大致意思也就是说这种类型的错误一般不是 Maven 的问题，而是所使用的 Maven 构件的问题，在这里我使用了 deploy 构件，我也知道和 deploy 构件有关，但是具体原因是什么也没说明；于是接下来使用 -e 参数（完整命令：mvn deploy -e），除了上述的报错日志外，还可以打印更为详细的错误日志追踪信息，然后发现一直会有下面的错误输入，而且一直重复，没有要停的迹象，我只能手动停掉 mvn 进程：1java.lang.IllegalArgumentException: progressed file size cannot be greater than size: 59156480 &gt; 58029604异常信息截图 换算一下单位：59156480KB=56.42M（正是需要发布的构件的大小）；58029604KB=55.34M；1、通过搜索引擎对异常信息的搜索，大部分结果显示和 Maven 后台的 Web 服务有关，如果使用的是 Nginx，会有一个参数用来限制上传文件的大小，上传文件的大小超过最大限制，就会上传失败，并且抛出异常。我部署其它的小构件没有问题，怀疑是这个原因，于是我询问运维人员公司的 Maven 私服对上传的公共构件有没有大小限制（即可能是 Nginx 服务有没有限制上传文件的大小），运维说不会。但是我还是怀疑，于是想通过 Web 端的界面来手动上传我的构件，发现 Web 端的界面没有开放，无法完成上传操作，接下来我就想看看 Maven 后台服务的对应参数配置的值是多大（也可能使用的是默认值），但是不知道后台采用的是什么服务（Nginx 还是 Netty 不确定），先放弃这条路；2、也有结果显示是 Maven 的版本问题，有些版本有 bug，所以造成了这个问题。问题解决 既然没有分析出来具体的原因，只能尝试每一种解决方案了。1、既然怀疑是 Maven 私服限制了构件的大小，那就想办法减小构件。先在本地 install，然后去本地仓库看一下生成的构件的大小，结果我惊讶地发现构件居然有 330M 之大，吓死人了，这个打包发布构件的配置肯定有问题，肯定把第三方依赖全部打进去了。我查看了以前生成的正常的构件，也就 60M 左右。以下放出对比图 2 个 这一看就是把第三方各种依赖包都一起发布了，才会造成构件有这么大，于是更改 pom.xml 文件，把第三方依赖去除，deploy 的时候是不需要的，同时也删除了一些 resources 资源文件夹里面的文本文件，删除时发现文本文件竟然有几十 M，怪不得以前发布的构件大小有 60M 左右，原来都是文本文件在占用空间；更新了之后，直接重新 deploy，不报错了，直接 deploy 成功，去私服仓库搜索查看，大概 30M 左右，很正常 2、问题使用方法一已经解决了，也就是和 Maven 版本没有关系了，而且，在我的当前 Maven 环境下，我去 deploy 其它构件也是成功的，不会有任务报错，所以也从侧面反映了这个问题和 Mave 版本无关，和 Maven 环境也无关； 问题总结 参考：http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException ；apache 的官方 jira：http://mail-archives.apache.org/mod_mbox/maven-issues/201808.mbox/%3CJIRA.13182024.1535592594000.205524.1535738700211@Atlassian.JIRA%3E ；https://issues.apache.org/jira/browse/MNG-6469?page=com.atlassian.jira.plugin.system.issuetabpanels%3Aall-tabpanel ；这个问题去网上搜索不到资料，很痛苦，问人也没有能帮到我的，只能自己去慢慢摸索试验，整个过程比较艰难；]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Maven</tag>
        <tag>deploy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Gson 将 = 转为 u003d 的问题]]></title>
    <url>%2F2019010601.html</url>
    <content type="text"><![CDATA[今天遇到一个问题，实现 Web 后台接收 http 请求的一个方法，发现前端传过来的参数值，有一些特殊符号总是使用了 unicode 编码，例如等号 =，后台接收到的就是 \u003d，导致使用这个参数做 JSON 变换的时候就会出错。我看了一下这个参数取值，是前端直接填写的，而填写的人是从其它地方复制过来的，人为没有去改变，前端没有验证转换，导致传入后台的已经是这样了，那么后台只好自己想办法转换。问题解决 其实就是字符串还原操作，把 Java 字符串里面的 unicode 编码子串还原为原本的字符，例如把 \u003d 转为 = 这样。自己实现一个工具类，做编码字符串和普通字符串的转换，可以解决这个问题。单个编码转换，公共方法示例：1234567891011121314151617/** * unicode 转字符串 * * @param unicode 全为 Unicode 的字符串 * @return */public static String unicode2String(String unicode) &#123; StringBuffer string = new StringBuffer (); String [] hex = unicode.split ("\\\\u"); for (int i = 1; i &lt; hex.length; i++) &#123; // 转换出每一个代码点 int data = Integer.parseInt (hex [i], 16); // 追加成 string string.append ((char) data); &#125; return string.toString ();&#125;整个字符串转换，公共方法示例：12345678910111213141516171819202122232425262728293031/** * 含有 unicode 的字符串转一般字符串 * * @param unicodeStr 混有 Unicode 的字符串 * @return */public static String unicodeStr2String(String unicodeStr) &#123; int length = unicodeStr.length (); int count = 0; // 正则匹配条件，可匹配 \\u 1 到 4 位，一般是 4 位可直接使用 String regex = "\\\\u [a-f0-9A-F]&#123;4&#125;"; String regex = "\\\\u [a-f0-9A-F]&#123;1,4&#125;"; Pattern pattern = Pattern.compile (regex); Matcher matcher = pattern.matcher (unicodeStr); StringBuffer sb = new StringBuffer (); while (matcher.find ()) &#123; // 原本的 Unicode 字符 String oldChar = matcher.group (); // 转换为普通字符 String newChar = unicode2String (oldChar); int index = matcher.start (); // 添加前面不是 unicode 的字符 sb.append (unicodeStr.substring (count, index)); // 添加转换后的字符 sb.append (newChar); // 统计下标移动的位置 count = index + oldChar.length (); &#125; // 添加末尾不是 Unicode 的字符 sb.append (unicodeStr.substring (count, length)); return sb.toString ();&#125;调用示例：12String str = "ABCDEFG\\u003d";System.out.println ("====unicode2String 工具转换:" + unicodeStr2String (str));输出结果：1====unicode2String 工具转换：ABCDEFG=截图示例：问题后续 后续我又在想，这个字符串到底是怎么来的，为什么填写的人会复制出来这样一个字符串，一般 unicode 编码不会出现在日常生活中的。我接着发现这个字符串是从另外一个系统导出的，导出的时候是一个类似于 Java 实体类的 JSON 格式字符串，从里面复制出来这个值，就是 \u003d 格式的。那我觉得肯定是这个系统有问题，做 JSON 序列化的时候没有控制好序列化的方式，导致对于特殊字符就会自动转为 unicode 编码，给他人带来麻烦，当然，我无法得知系统内部做了什么，但是猜测可能是使用 Gson 工具做序列化的时候没有正确使用 Gson 的对象，只是简单的生成 JSON 字符串而已，例如看我下面的代码示例（等号 = 会被转为 \u003d）。使用普通的 1Gson gson1 = new Gson (); 会导致后续转换 JSON 字符串的时候出现 unicode 编码子串的情况，而正确生成 Gson 对象 1Gson gson2 = new GsonBuilder ().disableHtmlEscaping ().create (); 则不会出现这种情况。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Gson</tag>
        <tag>等号编码转换</tag>
        <tag>u003d</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub Pages 禁止百度蜘蛛爬取的问题]]></title>
    <url>%2F2019010501.html</url>
    <content type="text"><![CDATA[最近才发现我的静态博客站点，大部分的网页没被百度收录，除了少量的网页是我自动提交（主动推动、自动推送）的，或者手动提交的，其它的网页都不被收录（网页全部是利用自动提交的 sitemap 方式提交的，一个都没收录）。我查看百度的站长工具后台，发现通过 sitemap 方式提交链接这种方式不可行，因为百度蜘蛛采集链接信息之前需要访问 baidusitemap.xml 文件，而这个文件是在 GitHub Pages 里面的，但是 GitHub Pages 是禁止百度蜘蛛爬取的，所以百度蜘蛛在获取 baidusitemap.xml 文件这一步骤就被禁止了，GitHub Pages 返回 403 错误（在 http 协议中表示禁止访问），因此抓取失败（哪怕获取到 baidusitemap.xml 文件也不行，因为后续需要采集的静态网页全部是放在 GitHub Pages 中的，全部都会被禁止）。本文就详细描述这种现象，以及寻找可行的解决方案。问题出现 网页收录对比差距大 利用搜索引擎的 site 搜索可以看到百度与谷歌明显的差别 百度搜索结果（只有少量的收录，仅有的还是通过主动推送与自动推送提交的）上面那个图片被封了，再来一张局部截图 谷歌搜索结果（收录很多，而且很全面）首先在百度站长工具（官方主页：https://ziyuan.baidu.com/ ）后台看到 baidusitemap.xml 抓取失败，查看具体原因是抓取失败（http 状态码 403）。抓取失败 抓取失败原因概述 根据抓取失败原因，我还以为是文件不存在，或者根据链接打不开（链接是：https://www.playpi.org/baidusitemap.xml ），我使用浏览器和 curl 命令都尝试过了，链接没有问题，可以正常打开。然后根据 403 错误发现是拒绝访问，那就有可能是百度爬虫的问题了（被 GitHub Pages 禁止爬取了）。使用浏览器打开 这里需要注意一点，百度站长工具里面显示的链接是 http 开头的（如上面抓取失败原因概述截图中红框圈出的，不是 https 开头的，我觉得百度爬虫抓取使用的就是 http 开头的链接），不过没关系，我在域名解析里面已经配置了所有的域名情况，完全可以支持。但是有时候仍然会遇到打不开上面链接的情况（在某些电脑上面或者某些网络环境中），我猜测这可能是电脑的缓存或者当前网络的 DNS 设置问题，不是我的站点的问题。因为，哪怕你在浏览器中输入以 http 开头的链接，也会自动跳转到以 https 开头的链接去。浏览器打不开链接的情况（其实不是链接的问题）使用命令行打开（如下使用 curl 命令）1curl https://www.playpi.org/baidusitemap.xml执行命令结果截图 通过百度反馈寻找原因 于是接下来，我就给官方提交了反馈，官方只是回复我说是链接问题（意思就是链接无法正常打开，其实使用浏览器或者检测工具都是可以打开的，但是使用百度爬虫就不行）。提交反馈（官方主页：https://ziyuan.baidu.com/feedback/apply ）反馈回复 前面我已经证明了链接没问题，那我就要猜想是百度蜘蛛爬虫的问题了，于是按照官方回复的建议，使用诊断工具看看是否可行。诊断工具测试多次都失败 如果抓取 UA 设置为移动端（即模拟手机、平板之类的设别），会有部分成功的，而使用 PC 端全部都是失败的。失败原因仍旧是拒绝访问（http 403 状态码）我又接着查看文档（文档地址：https://ziyuan.baidu.com/college/courseinfo?id=267&amp;page=9#007 ），发现拒绝访问的原因之一就是托管服务供应商阻止百度 Spider 访问我的网站，所以猜测是 GitHub Pages 拒绝了百度 Spider 的爬取请求，接着就想办法验证一下猜测是否正确。文档说明截取片段 接下来我又查找了资料，发现网上确实有很多这种说法，而且大家都遇到了这种问题，但是并没有官方的说明放出来。于是，接着我又回复了百度站长对方的反馈，直接问是不是因为 GitHub Pages 禁止了百度爬虫，所以百度爬取的结果总是 403 错误。等了 2 天多（赶上周末），对方没有明确回复，说的都是废话，可能是不想承认，那我也不管了。通过 GitHub Pages 找原因 另一方面，我尝试给 GitHub 的技术支持发送邮件询问，得到了确认的答复，GitHub 已经禁止了百度蜘蛛爬虫的访问，并且不保证在未来的时间恢复。主要是因为以前百度爬虫爬取太猛了，导致 GitHub Pages 不可用或者访问速度变慢，影响了其他正常的用户浏览使用 GitHub Pages，所以把百度爬虫给禁止了（当然，这是官方说法）。GitHub Pages 的反馈链接（填写姓名、邮箱、内容描述即可）：https://github.com/contact ；我发送了一封邮件过去，当然是借助谷歌翻译完成的，勉强能看 成功发送邮件后的通知页面 内容全文如下，仅供参考：1234567891011121314151617A doubt with GitHub PagesHello,I created my own homepage with GitHub Pages,it is https://github.com/iplaypi/iplaypi.github.io.If you input https://iplaypi.github.io,it jumps to https://www.playpi.org automatically because of CNAME file.The website is https://www.playpi.org,and my site only contains static pages and pictures.But I have a problem,the following is my detailed description:I use Google Search Console to crawl my pages and include them.I only need to provide a site file named website.xml,and it works fine.But when i use Baidu Webmaster Tools (a tool made by a Chinese search engine company),it doesn&apos;t work properly.I only need to provide a site file named baiduwebsite.xml,Baidu Spider will crawl the link in this file .But Baidu cannot include my pages finally,and the reason is Baidu Spider can&apos;t crawl my html pages.So,I am trying to find the real reason,then I succeeded.The real reason is Github Pages forbids the crawling of Baidu Spider.So when Baidu Spider crawls my pages,it will definitely fail.Here I want to know is this phenomenon real?If yes,why Github Pages forbids Baidu Spider?And what should i do?Thanks.Best regards.Perry没隔几个小时，就有回复了 回复的重点内容如下：1I&apos;ve confirmed that we are currently blocking the Baidu user agent from crawling GitHub Pages sites. We took this action in response to this user agent being responsible for an excessive amount of requests, which was causing availability issues for other GitHub customers. This is unlikely to change any time soon, so if you need the Baidu user agent to be able to crawl your site you will need to host it elsewhere.那么，我们再来回看一下百度站长里面爬取失败原因的页面，里面有一个用户代理的配置，其实就是构造 http 请求使用的消息头，可以看到正是 Baiduspider/2.0，所以才会被 GitHub Pages 给禁止了。解决方案 至此，我已经把问题的原因搞清楚了。本来这个问题是很好解决的（更换静态博客存储的主机即可，例如各种项目托管服务：码市、gitcafe、七牛云等，或者自己购买一台云主机），但是我不能抛弃 GitHub，于是问题变得复杂了。此时，我还有 3 个方案可以参考：使用 CDN 加速，把每个静态页面都缓存下来，这样百度爬虫的请求就可能不会到达 GitHub Pages，但是不知道有没有保证，可以试试 放弃 自动提交 方式里面的 sitemap 推送 ，改为 主动推送 ，hexo 里面有插件可以用。但是我是坚持大道至简的原则，不想再引用插件了，而且我看了那个插件，需要配置百度账号的信息，我不能把这些信息放在公共仓库里面，会暴露给别人，不想用 在更新博客的同时再部署一份相同的博客 （可以理解为镜像，需要在其它主机部署一份，可以自己搭建主机或者使用类似于 GitHub 的代码托管工具），把 master 分支的内容复制过去即可，然后利用域名解析服务，把百度爬虫的流量引到这份服务器上面（只是为了让百度收录），其他的流量仍然去访问 GitHub Pages，就可以让百度的爬虫顺利爬取到我的博客内容了。这个方法看起来虽然很绕，但是明白了细节实现起来就很简单，而且可靠，可以用 CDN 加速 我先不选择这种方式了，因为需要收费或者免费的加广告，或者服务不稳定，我还是愿意选择稳妥的方式。可以选择的产品有：七牛云、又拍云、阿里云、腾讯云等。选择镜像方式 既然选择了使用复制博客的方式，再加上域名解析服务转移流量，那接下来就开始动手部署了。我手里正好还有一台翻墙使用的 VPS，每个月的流量用不完，所以也不打算使用第三方托管服务了，直接部署在我自己的 VPS 上面就行了。只不过还需要动动手搭建一下 Web 服务，当然是使用强大的 Nginx 了。更改域名服务器和相关配置 1、在 DNSPod 中添加域名DNSPod 账号自行注册，我使用免费版本，当然会有一些限制，例如解析的域名 A 记录个数限制为 2 个【GitHub Pages 有 4 个 ip，我在 Godaddy 中都是配置 4 个，但是没影响，配置 2 个也。或者直接配置 CNAME 记录就行了，以前我不懂就配置了 ip，多麻烦，ip 还要通过 ping iplaypi.github.io 获取，每次还不一样，一共得到了 4 个，多此一举。当然，如果域名被墙了而 ip 没被墙，还是需要这样配置的】。2、添加域名解析记录 我把 Godaddy 中的解析记录直接抄过来就行，不同的是由于使用的是 DNSPod 免费版本，A 记录会少配置 2 个，基本不会有啥影响 【其实不配置 A 记录最好，直接配置 CNAME 就行了，会根据域名自动寻找 ip，以前我不懂】。另外还有一个就是需要针对百度爬虫专门配置一条 www 的 A 记录，针对百度的线路指向自己服务器的 ip【截图只是演示，其中 CNAME 记录应该配置域名，A 记录才是配置 ip】，如果使用的是第三方托管服务，直接添加 CNAME 记录，配置域名就行【例如 yoursite.gitcafe.io】。不使用 A 记录的配置方式 3、在 Godaddy 中绑定自定义域名服务器 第 2 个步骤完成，我们回到 DNSPod 的域名界面，可以看到提示我们修改 NS 地址，如果不知道是什么意思，可以点击提示链接查看帮助手册【其实就是去购买域名的服务商那里绑定 DNSPod 的域名服务器】。提示我们修改 NS 地址 帮助手册 我是在 Godaddy 中购买的域名【不需要备案】，所以需要在 Godaddy 中取消默认的 DNS 域名服务器，然后把 DNSPod 分配的域名服务器配置在 Godaddy 中。这里需要注意，在配置了新的域名服务器的时候，以前的配置的解析记录都没用了，因为 Godaddy 直接把域名解析的工作转给了我配置的 DNSPod 域名服务器【配置信息都转到了 DNSPod 中，也就是步骤 1、步骤 2 中的工作】。原有的解析记录与原有的域名服务器 配置完成新的域名服务器【以前的解析记录都消失了】配置完成后使用 域名设置 里面的 自助诊断 功能，可以看到域名存在异常，主要是因为更改配置后的时间太少了，要耐心等待全球递归 DNS 服务器刷新【最多 72 小时】，不过一般 10 分钟就可以访问主页了。设置镜像服务器 我没有使用第三方托管服务器，例如：gitcafe、码市、coding，而是直接使用自己的 VPS，然后搭配 Nginx 使用。安装 Nginx（基于 CentOS 7 X64）CentOS 的安装过程参考：https://gist.github.com/ifels/c8cfdfe249e27ffa9ba1 。但是，不是全部可信，抽取有用的即可。而且这种方式安装的是已经规划好的一个庞大的包，里面包含了一些常用的模块，可能有一些模块没用，而且如果自己想再安装一些新的模块，就不支持了，必须重新下源码编译安装。总而言之，这种安装方式就是给入门级别的人使用的，不能自定义。1、由于 Nginx 的源头问题，先创建配置文件 12cd /etc/yum.repos.d/vim nginx.repo 填写内容 12345[nginx]name=nginx repobaseurl=http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck=0enabled=12、安装配置 Nginx1234# 安装 yum install nginx -y# 配置 vi /etc/nginx/nginx.conf 填写配置内容 1234567891011121314151617181920212223242526272829303132333435363738user nginx;worker_processes 1;error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; access_log /site/nginx.access.log main; server &#123; listen 80; server_name blog.playpi.org www.playpi.org; access_log /site/iplaypi.github.io.access.log main; root /site/iplaypi.github.io; &#125; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf;&#125;3、开启 80 端口（不开启不行），启动 Nginx1234567891011# 查看已经开启的端口 firewall-cmd --list-ports# 开启端口 firewall-cmd --permanent --zone=public --add-port=80/tcp# 重载更新的端口信息 firewall-cmd --reload# 启动 Nginx# 这种方式不行，找不到目录 /etc/init.d/nginx start# 这种方式可以 service nginx start 额外考虑情况 1、关于 https 认证 要不要考虑 https 的情况，如果百度爬虫没用到 https 抓取（除了 sitemap.xml 文件还要考虑文件里面的所有链接格式，也是 https 的），就不考虑。其实一定要考虑，因为百度爬虫用到了 https 链接去抓取，所以还要想办法开启 Nginx 的 https。此外，在百度的 https 认证里面，也是需要开启 https 的，否则申请不通过。我的域名不知道什么时候验证失败了，但是一开始的时候是验证成功的（可能是 GitHub Pages 禁止百度爬虫的原因，因为以前全部都是 GitHub Pages 提供站点支持）我想重新验证一下，没想到有次数限制，还是先把 Nginx 的 https 开启之后再验证吧 开启 Nginx 的 https，并且保证站点全部的链接都是 https 的，但是同时也要支持 http，使用 301 重定向到 https。1-1、查看 Nginx 的 https 模块 先查看我安装的小白版本的 Nginx 里面有没有关于 https 的模块，使用命令 nginx -V，可以看到是有的，这个模块就是 –with-http_ssl_module。1-2、申请证书 可以购买或者从阿里云、腾讯云里面申请免费的，但是我还是觉得使用 OpenSSL 工具自己生成方便，先查看机器有没有安装 OpenSSL 工具，使用 openssl version 命令，如果没有则需要安装 yum install -y openssl openssl-devel，安装完成后开始生成证书。生成证书的命令：1openssl req -x509 -nodes -days 36500 -newkey rsa:2048 -keyout /site/ssl-nginx.key -out /site/ssl-nginx.crt在生成的过程中还需要填写一些参数信息：国家、城市、机构名称、机构单位名称、域名、邮箱等，这里特别注意我为了能让多个子域名公用一个证书，采用了泛域名的方式（星号的模糊匹配：*.playpi.org）。这种生成证书的方式只是为了测试使用，最终的证书肯定是不可信的，浏览器会提示此证书不受信任，所以还是通过其它方式获取证书比较好（后续我会通过阿里云或者 letsencrypt 获取免费的证书，具体博客参考可以使用相关关键词在站内搜索）。完整信息填写 12345678910111213141516171819Generating a 2048 bit RSA private key........+++..............+++writing new private key to &apos;/site/ssl-nginx.key&apos;-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter &apos;.&apos;, the field will be left blank.-----Country Name (2 letter code) [XX]:CNState or Province Name (full name) []:GuangdongLocality Name (eg, city) [Default City]:GuangzhouOrganization Name (eg, company) [Default Company Ltd]:playpiOrganizational Unit Name (eg, section) []:playpiCommon Name (eg, your name or your server&apos;s hostname) []:*.playpi.orgEmail Address []:playpi@qq.com1-3、更改配置并重启 Nginx 重新配置 http 与 https 的参数（只列出 server 的主要部分，blog 二级域名主要是为了测试使用的，blog 的流量全部导入我的 VPS 中），特别注意 rewrite 的正则表达式，只替换域名部分，链接部分不能替换，否则都跳转到主页去了 123456789101112131415161718192021222324252627282930313233server &#123; listen 80; server_name www.playpi.org; access_log /site/iplaypi.github.io.http-www-access.log main; rewrite ^/(.*)$ https://www.playpi.org/$1 permanent; &#125; server &#123; listen 80; server_name blog.playpi.org; access_log /site/iplaypi.github.io.http-blog-access.log main; rewrite ^/(.*)$ https://blog.playpi.org/$1 permanent; &#125; server &#123; listen 443 ssl;# 监听端口 server_name www.playpi.org blog.playpi.org;# 域名 access_log /site/iplaypi.github.io.https-access.log main; root /site/iplaypi.github.io; ssl_certificate /site/ssl-nginx.crt;# 证书路径 ssl_certificate_key /site/ssl-nginx.key;#key 路径 ssl_session_cache shared:SSL:1m;# 储存 SSL 会话的缓存类型和大小 ssl_session_timeout 5m;# 配置会话超时时间 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;# 为建立安全连接，服务器所允许的密码格式列表 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on;# 依赖 SSLv3 和 TLSv1 协议的服务器密码将优先于客户端密码 #减少点击劫持 add_header X-Frame-Options DENY; #禁止服务器自动解析资源类型 add_header X-Content-Type-Options nosniff; #防 XSS 攻击 add_header X-Xss-Protection 1; &#125; 开启 443 端口，重启 Nginx12345678910# 查看已经开启的端口 firewall-cmd --list-ports# 开启端口 firewall-cmd --permanent --zone=public --add-port=443/tcp# 重载更新的端口信息 firewall-cmd --reload# 验证 Nginx 配置是否准确 nginx -t# 重新启动 Nginxnginx -s reload1-4、打开链接查看 使用 blog 二级域名测试（也需要在 DNSPod 中配置一条 A 记录解析规则）或者使用 curl 命令模拟请求，由于有重定向的问题，所以失败 既然开启了 https，可以使用 curl 关闭失效证书的方式（-k 参数）访问 https 链接 去百度站长里面重新提交 https 认证（使用上面的测试证书是认证失败的，我去阿里云重新申请了证书，认证成功了，申请证书的教程可以在本站搜索，为了给 2 个二级域名不同的证书，nginx 还需要重新配置 server 信息）2、端口的问题 为什么在上面配置域名解析记录的时候，百度的 A 记录配置 VPS 的 ip 就行了呢，这是因为在 VPS 上面只有 Nginx 这一种 Web 服务，机器会分配给它一个端口（默认 80，也是 http 的默认端口，可以配置），然后 www 的访问就使用这个端口（在 Nginx 的配置里面有，还有另外一个 blog 的），所以可以忽略端口的信息。但是如果一台机器上面有各种 Web 服务，切记确保端口不要冲突（例如 Tomcat 和 Nginx 同时存在的情况），并且给 Nginx 的就是 80 端口，然后如果有其它服务，可以使用 Nginx 做代理转发（例如把 email 二级域名转到一个端口，blog 二级域名转到另一个端口）。完善自动获取更新脚本，拉取 mater 分支的静态页面 1、先用简单的方式 使用 git 把项目克隆到：/site/iplaypi.github.io 即可。2、利用钩子自动拉取 master 分支内容到指定目录 本来最简单的方式就是在 travis 自动构建的时候，把生成的静态页面直接拷贝到目标主机就行了。也就是把 public 目录里面的内容使用类似 scp 的命令拷贝到我的服务器即可。但是，我觉得这种方式太简易，我还是想利用起来 GitHub 的钩子功能，在项目有 push 发生的时候，自动触发我服务器上面的脚本，然后脚本就会执行 pull 的操作。详情见我的另外一篇博客：使用 Github 的 WebHooks 实现代码自动更新 。 验证结果 以下验证都是在没有开启 https 的情况下，即没有对 http 进行 301 重定向，如果做了 301 重定向截图内容会有一点不一样，curl 也会直接失败（需要访问 https 格式的链接）。使用最简单的方式验证就是在百度站长工具里面使用 抓取诊断 来进行模拟抓取多次，看看成功率是否是 100%。通过测试，可以看到，每次抓取都会成功，那么接下来就等待百度自己抓取了（百度爬虫抓取 sitemap.xml 文件的频率很低，可能要等一周）。使用抓取诊断方式来验证，这个过程有一个插曲，就是无论怎么验证都是失败的，但是使用 curl 模拟请求却是成功的。我看了失败原因概述里面，抓取的 ip 地址仍旧是 GitHub Pages 的，说明百度爬虫的流量没有到我自己的 VPS 上面。我一开始还以为是 DNSPod 配置没生效，但是通过 curl 模拟请求却可以，说明 DNSPod 配置没问题，那就是百度的问题了，应该是缓存。后来，我在移动端 UA 与 PC 端 UA 切换了一下，然后就行了。此外，既然我们知道了百度爬虫设置的用户代理，那么就可以直接使用 curl 命令来模拟百度爬虫的请求，观察返回的 http 结果是否正常。模拟命令如下：1curl -A "Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)" http://blog.playpi.org/baidusitemap.xml模拟请求的结果，可以看到也是正常的（下面的截图在没有开启 https 的情况下，如果开启 301 重定向就不行了，需要直接访问 https 链接）如果开启了 https，即对 http 请求进行 301 重定向，则可以直接访问 https 链接（如果证书是无效的，像我截图中的，则可以使用 curl 关闭无效证书的方式，加一个 -k 参数）我也去看了 VPS 上面的 Nginx 日志，确实百度爬虫的流量都被引入到这里来了，皆大欢喜 后续还需要观察看看百度的收录结果（等待 3 天后更新了，结果如下）问题总结1、这篇博客耗费了我一个多月才完成，当然不是写了一个多月，而是从发现问题到解决问题，最终写成这篇博客，前后经历了一个多月。在这一个多月里，我看了很多别人的博客，问了一些人，也看了一些技术资料，学到了很多以前不了解的知识，而且通过动手去解决问题，整个过程收获颇丰。2、写 Markdown 文档，使用代码块标记的时候，使用 3 个反单引号来标记，如果不熟悉代码块里面的编程语言，可以省略类型（例如 java、bash、javascript），不要填写，否则填错了生成的 html 静态文件是空白的。还有就是如果代码块里面放的是一段英文文本，和编程语言无关，也不要填写类型，否则生成的 html 静态文件也是空白的。3、通过实战学习了一些网络知识，例如：CNAME、A 记录、域名服务器、二级域名等、https 证书，也学习了一些关于 Nginx 的知识。4、关于访问速度的问题，GitHub Pages 的 CDN 还是很强大的，不会出现卡顿的情况。但是有时候貌似 GitHub 会被墙，打不开。此外，我搞这么久就是为了让百度爬虫能收录我的站点文章，所以自己搭建的 VPS 只是为了给百度爬虫爬取用的，其它正常人或者爬虫仍旧是访问 GitHub Pages 的链接。5、关于 https，使用 GitHub Pages 的时候，服务全部是 GitHub Pages 提供的，我无需关心。但是，自己使用 VPS 做了一个镜像，就需要配置一模一样的环境给百度爬虫使用，否则会导致一些失败的现象，例如 htps 认证失败、链接抓取失败。因此，一定要开启 https，并且同时也支持 http。以下是整理的网络请求流程图，清晰明了。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>建站</tag>
        <tag>GitHub Pages</tag>
        <tag>SEO</tag>
        <tag>百度蜘蛛</tag>
        <tag>Baiduspider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微博电影文稿备份]]></title>
    <url>%2F2019010101.html</url>
    <content type="text"><![CDATA[元旦是个好日子，在一年之始，我整理硬盘文件夹的过程中，发现了很多电影的资料，都是以前下载的，有的看过有的没看过。突发奇想，有些觉得好看的电影可以保留下来，剪辑部分片段发到微博，也可以用来记录自己曾经看过那些电影，本文记录随微博发送的文稿，防止被删。双旗镇刀客 《双旗镇刀客》，是一部 1991 年的武侠动作片，获奖很多，豆瓣评分 8.1，好于 95% 的武侠片，好于 92% 的动作片。但是，这部电影里面几乎没有打斗的场面，也没有动作大场面，但依然削弱不了它的武侠内涵。 仅有的几个决斗场面全部都是一对一，看不到出招、闪躲、反杀，细节全部使用蒙太奇的手法表现，只能听到一声风声、一声刀声、一声剑声，然后胜败已分，没有嘶喊、没有大动作，非常干净利落。视频截取的片段（01:20:07 到 01:23:03）是最后一刀仙和孩哥决斗的场面。对了，你们能认出饰演这一刀仙的演员是谁吗？–《双旗镇刀客》：粗粝、苦难、鲁莽的平民式的武侠片。扫毒 扫毒片段之一（01:22:06 到 01:32:16）：泰国一战回来，过了很久，马昊天【刘青云 饰演】、张子伟【张家辉 饰演】、苏建秋【古天乐 饰演】第一次见面，他们并不知道张子伟还活着，看一下大家的反映，三大影帝同场飙戏。扫毒片段之二（01:36:30 到 01:46:42）：三兄弟在天台见面，交换人质，然后引发枪战。这一段也是张家辉的鬼畜视频的素材，台词是：五年，五年，你知道这五年我怎么过的吗？你知道吗？【见视频 3 分 38 秒处】那山那人那狗 那山那人那狗，是一部 1999 年的电影，讲述的是儿子（刘烨 饰演）高考落榜不得已回到大山中的家后，做了大半辈子山村邮递员的父亲（滕汝骏 饰演）提前退休，安排儿子接下自己的工作。儿子上班第一天，父亲千叮咛万嘱咐之后仍不放心，带上长年在其左右的忠实老狗（老二）决定陪儿子再走一趟送信之旅。在这趟旅途之中，父子由于长期隔膜一开始默默不语，后来渐渐打开了话匣子，对彼此有了更深一层的认识和了解，故事自然真实、扣人心弦。以下截取的 11 分钟片段（00:51:52 到 01:03:04）是很重要的一场戏，父子两人一同过河，然后共同休息聊天，在这个过程中，父子的情感逐渐加深了。倚天屠龙记之魔教教主 这是一部 1993 年的武侠片：《倚天屠龙记之魔教教主》，看看这武打场面，动作发挥，虽然当时科技不发达，资源不足，但是拍出来的电影仍然很经典。再看看现在，科技发达了，资金充足，但是动作戏只是做做样子，实在看不下去。截取的 7 分钟片段（01:04:22 到 01:11:24）是围攻光明顶的场面，张无忌出场以及打戏过程，动作非常流畅、到位。叶问外传：张天志 作为《叶问》系列影片，电影《叶问外传：张天志》延续了《叶问 3》的故事，讲述了同为咏春传人的张天志在比武惜败叶问后，决意放下功夫，远离江湖纷争。但面对接踵而至的连番挑衅，面对家国大义遭受的恶意侵犯，决定重拾咏春惩戒毒贩，「以武之道」捍卫民族道义尊严的故事。电影虽然总体评分不高，但是里面的打戏还是值得看的。以下截取的 3 分钟片段（00:08:40 到 00:11:54）是电影开场的矛盾冲突过程以及打戏场面，动作流利，拳拳到肉。屏住呼吸 《屏住呼吸》是一部 2016 年的电影，豆瓣评分 7.0，好于 63% 的惊悚片，好于 82% 的恐怖片。讲述的是三位惯偷（洛奇、艾利克斯、摩尼）潜入一位退伍老兵的家里，老兵因为女儿的车祸获取了巨额赔偿金，他们想偷取这笔钱，从此金盆洗手。 三人潜入了老兵位于底特律的如同鬼宅一般的老房子中，整个社区只有他一个人居住，没想到却就此打开了地狱的大门。老兵拥有着高于常人的嗅觉和听力，以及极为敏捷的行动力，很快就发现了三名不速之客的行踪。在他的枪口之下，一行人犹如瓮中之鳖，无处可逃。在紧张而又激烈的追逃之中，洛奇和艾利克斯误打误撞跌落到了地下室中，却因此发现了一个可怕的秘密。以下截取的 14 分钟视频片段（00:24:51 到 00:39:15）是三人刚刚潜入住宅，本以为被迷晕的老兵却突然出现，及其凶狠地打死小偷一人，去检查保险箱又恰巧被洛奇看到密码，接着他们小偷二人偷取巨额现金准备逃跑。整个过程真的让人屏住呼吸，时刻感觉到危险，此时已经不是偷东西那么简单了，自己的生命已经快保不住了。无间道 《无间道》是一部 2003 年的悬疑犯罪剧情片，获得金马奖最佳剧情片，金像奖最佳电影，豆瓣评分 9.1，好于 99% 的犯罪片，好于 98% 的剧情片。1991 年，香港黑帮三合会会员刘健明（刘德华 饰演）听从老大韩琛（曾志伟 饰演）的吩咐，加入警察部队成为黑帮卧底，韩琛许诺刘健明会帮其在七年后晋升为见习督察。1992 年，警察训练学校优秀学员陈永仁（梁朝伟 饰演）被上级要求深入到三合会做卧底，终极目标是成为韩琛身边的红人。2002 年，两人都不负重望，也都身背重压，但是刘健明渐想成为一个真正的好人，而陈永仁则盼着尽快回归警察的身份。 在后续的较量中，逐渐暴露出双方均有卧底的事实，引发双方高层清除内鬼的决心。命运迥异又相似的刘健明和陈永仁开始在无间道的旅程中接受严峻考验。 以下截取的 7 分钟片段（01:29:28 到 01:36:04），是结局的天台见面，台词、配乐、镜头都很经典，最后的 2 个卧底双双命丧枪口，令人唏嘘。经典台词对话如下：刘建明：给我一个机会。陈永仁：怎么给你机会。刘建明：我以前没得选，我现在想做一个好人。陈永仁：好啊，跟法官说，看给不给你做好人。刘建明：那就是要我死。陈永仁：对不起，我是警察。刘建明：谁知道？毒液：致命守护者 《毒液：致命守护者》是一部 2018 年的科幻、动作、惊悚电影，又名毒魔、猛毒，豆瓣评分 7.2，好于 69% 的科幻片，好于 69% 的动作片。 剧情简介：艾迪是一位深受观众喜爱的新闻记者，和女友安妮相恋多年，彼此之间感情十分要好。安妮是一名律师，接手了生命基金会的案件，在女友的邮箱里，艾迪碰巧发现了基金会老板德雷克不为人知的秘密。为此，艾迪不仅丢了工作，女友也离他而去。之后，生命基金会的朵拉博士找到了艾迪，希望艾迪能够帮助她阻止德雷克疯狂的罪行。在生命基金会的实验室里，艾迪发现了德雷克进行人体实验的证据，并且在误打误撞之中被外星生命体毒液附身。回到家后，艾迪和毒液之间形成了共生关系，他们要应对的是德雷克派出的一波又一波杀手。以下截取的 10 分钟片段（00:50:25 到 01:00:55），是艾迪和毒液共生时第一次遇到德雷克派出的杀手的情景，双方产生了激烈的打斗，此时可以看到艾迪和毒液完美的共生关系。剑雨 《剑雨》是一部 2010 年的武侠、动作电影，豆瓣评分 7.1，好于 58% 的武侠片，好于 66% 的动作片。这部电影由吴宇森监制，有人称它是《卧虎藏龙》之后最好的武侠片。 剧情简介：八百年前，竺人达摩来至中原弘法，其死后尸体被人盗取并分为两部分。传说拿到达摩尸体的人能练成绝世武功，因此江湖上风波骤起。时有转轮王（王学圻 饰演）操控的黑石杀手集团，转轮王率徒众夜袭藏匿半具达摩尸首的首辅张海端（李庆祥 饰演）宅邸，但是他的手下细雨（林熙蕾 饰演）却带着残尸绝走江湖，致令转轮王发出江湖追杀令，引出一阵血雨腥风。为避追杀，细雨易容，更名曾静（杨紫琼 饰演），逃亡期间结识了木讷善良的江阿生（郑宇成 饰演）。一段时间相处，二人渐渐萌生感情，更喜结连理。但是江湖恩怨，怎可轻易了结，后续引发了一系列复仇的情节。截取的 9 分钟片段（01:19:06 到 01:28:15），是曾静被黑石集团追杀，受伤后归家昏厥，而江阿生此时不得不重新拿起剑与敌人厮杀的场景。这一段打斗堪称精彩，使用兵器剑，动作流畅，很符合吴宇森的暴力美学。精彩对白：1、我愿是你杀的最后一人。2、我愿化身石桥，受那五百年风吹，五百年日晒，五百年雨淋，只求她从桥上走过。网络谜踪 《网络谜踪》是一部 2018 年的悬疑、惊悚、犯罪电影，豆瓣评分高达 8.6，好于 96% 的悬疑片，好于 96% 的犯罪片，又名人肉搜寻、人肉搜索、搜索、屏幕搜索。 影片讲述的是工程师大卫・金一直引以为傲的 16 岁乖女玛戈特突然失踪，前来调查此案的警探怀疑女儿离家出走。不满这一结论的父亲为了寻找真相，独自展开调查，他打开了女儿的笔记本电脑，用社交软件开始寻找破案线索，大卫必须在女儿消失之前，沿着她在虚拟世界的足迹找到她。最后经历了各种挫折，总算找到了真相，可惜女儿已经被害离世。没想到，最终在女儿的遗体告别仪式上，父亲又发现了不可告人的秘密，事情得到大反转。整个电影的镜头大部分时间都是由录像、监控、视频、新闻画面组成，反正全部是电子屏幕，很少有摄像机直接拍摄的场景，但是正是由于这样，才体现出整个过程的悬疑、惊悚的效果。看似牢不可破的密码，跳转几次邮箱就可破译；看似无比相熟的女儿，连吸食大麻都未曾知。本来代表身份的头像，竟是无需版权的图库模特；本来孤立寡言的女孩，却在直播软件里袒露心扉。最亲近的女儿却比密码还难破译，最沉默的女孩却比模特还健谈，面前提供海量信息的屏幕，当你在凝视它，它也在吞食你。以下截取的 15 分钟片段（00:42:09 到 00:57:26），是父亲根据女儿在社交网站上的图片、视频发现了女儿的可能去处，并没有一味地听从被 “分配” 过来的警探的建议，没想到真的找到了女儿的遗物和沉入湖底的汽车，这也给警察的搜索带来了方向。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>微博电影</tag>
        <tag>文稿备份</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[蒸水蛋做法总结]]></title>
    <url>%2F2018122901.html</url>
    <content type="text"><![CDATA[蒸水蛋是一道小吃，有时候就简称为水蛋，可以当菜配饭吃，也可以配包子当做早餐，或者晚上蒸一碗当做宵夜，都非常好。吃起来嫩滑爽口，而且营养也丰富，做法非常简单，本文就记录蒸水蛋的过程。食材准备 2 人份的材料（1 人份减半即可，但是我觉得 1 人份的太少了，做起来浪费，不值当）： 鸡蛋 2 只（1 只做成 1 碗）葱花少许 生抽少许 食用盐少许 香油少许 制作步骤 1、准备 2 只小碗（有条件的可以使用带盖子的蒸盅，也就是平时吃快餐盛汤的那种带盖子的小碗），普通的小饭碗即可，一定要是耐热的材料，不要用秸秆环保碗、塑料饭盒、普通玻璃碗等（蒸的时候温度很高，虽然水的沸点是 100 度，但是锅内因为有水蒸气存在，压强变大，同时水蒸气转为液态会放热，锅内实际温度大于 100 度），分别打入 1 只鸡蛋，加少许食用盐，搅拌均匀（下面过程就以 1 份为准，另外 1 份是同样的操作）；2、搅拌均匀后开始加温水（最好是温水），温水的量大概是鸡蛋液的 2 倍，即鸡蛋液比温水等于 1:2，注意加温水的量，少了多了都不好（2-3 倍都行，如果碗大一点可能 1 份水蛋就需要放 2 只鸡蛋，不然显得太少了），继续搅拌，此时搅拌完成后表面应该会有一层小泡沫，可以用勺子把小泡沫都盛出去，保证蒸出来的水蛋表面光滑（怕浪费保留也行）； 搅拌均匀 3、蒸鸡蛋的时候使用保鲜膜封住碗口（有条件的使用整蛊更方便，盖子一盖即可），或者使用小盘子反盖在碗口，这样做是为了保证密封，一方面为了保证蒸出来的水蛋嫩滑，另一方面为了避免液化的水蒸气滴进去，影响水蛋的质量，先大火烧水，等水开后转为小火（火力很重要），再蒸 8 分钟即可出锅（这个时间很重要，太久了鸡蛋就老了）； 开始蒸，我为了省事就不撇小泡沫，也不盖保鲜膜了，所以做出来的成品会有点难看 4、取出后，滴入少许香油、生抽（不加也行，直接吃），撒入一点点葱花，即可食用，入口即化，滑嫩可口； 成品 以下这个我认为是做失败的，加太多水，蒸的过程中不断滴入液化的水蒸气，破坏了美感，也没葱花，但是吃起来绝对美味。注意事项1、食用盐是搅拌鸡蛋的时候就加入的，不是蒸好后再放的，这样才能入味而且分布均匀；2、加水时一定要加温水，不是冷水，也不是热水，温水才能让蒸出来的水蛋保持嫩滑；3、如果使用保鲜膜，一定要用可以蒸的材料，不是随便能用的。如果是 PVC 材料（聚氯乙烯），坚决不行，含有塑化剂释放有毒物质影响健康，如果是 PE 材料（聚乙烯），无毒，但是耐热温度不够，也不行，如果是 PVDC 材料（聚偏氯乙烯），安全温度在 140 度，可以使用。所以还是使用盘子比较好。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>蒸水蛋</tag>
        <tag>水蛋</tag>
        <tag>蒸鸡蛋</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 异常之 java.net.BindException: 地址已在使用]]></title>
    <url>%2F2018122801.html</url>
    <content type="text"><![CDATA[今天查看日志发现，所有的 Spark 程序提交时会抛出异常：1java.net.BindException: 地址已在使用 而且不止一次，会连续有多个这种异常，但是 Spark 程序又能正常运行，不会影响到对应的功能。本文就记录发现问题、分析问题的过程。问题出现 在 Driver 端查看日志，发现连续多次相同的异常（省略了业务相关类信息）：异常截图 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879// 第一次异常 2018-12-28_12:50:56 [main] WARN component.AbstractLifeCycle:204: FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: 地址已在使用 java.net.BindException: 地址已在使用 at sun.nio.ch.Net.bind0 (Native Method) at sun.nio.ch.Net.bind (Net.java:433) at sun.nio.ch.Net.bind (Net.java:425) at sun.nio.ch.ServerSocketChannelImpl.bind (ServerSocketChannelImpl.java:223) at sun.nio.ch.ServerSocketAdaptor.bind (ServerSocketAdaptor.java:74) at org.spark-project.jetty.server.nio.SelectChannelConnector.open (SelectChannelConnector.java:187) at org.spark-project.jetty.server.AbstractConnector.doStart (AbstractConnector.java:316) at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart (SelectChannelConnector.java:265) at org.spark-project.jetty.util.component.AbstractLifeCycle.start (AbstractLifeCycle.java:64) at org.spark-project.jetty.server.Server.doStart (Server.java:293) at org.spark-project.jetty.util.component.AbstractLifeCycle.start (AbstractLifeCycle.java:64) at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:252) at org.apache.spark.ui.JettyUtils$$anonfun$5.apply (JettyUtils.scala:262) at org.apache.spark.ui.JettyUtils$$anonfun$5.apply (JettyUtils.scala:262) at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp (Utils.scala:2024) at scala.collection.immutable.Range.foreach$mVc$sp (Range.scala:141) at org.apache.spark.util.Utils$.startServiceOnPort (Utils.scala:2015) at org.apache.spark.ui.JettyUtils$.startJettyServer (JettyUtils.scala:262) at org.apache.spark.ui.WebUI.bind (WebUI.scala:136) at org.apache.spark.SparkContext$$anonfun$13.apply (SparkContext.scala:481) at org.apache.spark.SparkContext$$anonfun$13.apply (SparkContext.scala:481) at scala.Option.foreach (Option.scala:236) at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:481) at org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:59)......2018-12-28_12:50:56 [main] WARN component.AbstractLifeCycle:204: FAILED org.spark-project.jetty.server.Server@33e434c8: java.net.BindException: 地址已在使用 java.net.BindException: 地址已在使用 at sun.nio.ch.Net.bind0 (Native Method) at sun.nio.ch.Net.bind (Net.java:433) at sun.nio.ch.Net.bind (Net.java:425) at sun.nio.ch.ServerSocketChannelImpl.bind (ServerSocketChannelImpl.java:223) at sun.nio.ch.ServerSocketAdaptor.bind (ServerSocketAdaptor.java:74) at org.spark-project.jetty.server.nio.SelectChannelConnector.open (SelectChannelConnector.java:187) at org.spark-project.jetty.server.AbstractConnector.doStart (AbstractConnector.java:316) at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart (SelectChannelConnector.java:265) at org.spark-project.jetty.util.component.AbstractLifeCycle.start (AbstractLifeCycle.java:64) at org.spark-project.jetty.server.Server.doStart (Server.java:293) at org.spark-project.jetty.util.component.AbstractLifeCycle.start (AbstractLifeCycle.java:64) at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:252) at org.apache.spark.ui.JettyUtils$$anonfun$5.apply (JettyUtils.scala:262) at org.apache.spark.ui.JettyUtils$$anonfun$5.apply (JettyUtils.scala:262) at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp (Utils.scala:2024) at scala.collection.immutable.Range.foreach$mVc$sp (Range.scala:141) at org.apache.spark.util.Utils$.startServiceOnPort (Utils.scala:2015) at org.apache.spark.ui.JettyUtils$.startJettyServer (JettyUtils.scala:262) at org.apache.spark.ui.WebUI.bind (WebUI.scala:136) at org.apache.spark.SparkContext$$anonfun$13.apply (SparkContext.scala:481) at org.apache.spark.SparkContext$$anonfun$13.apply (SparkContext.scala:481) at scala.Option.foreach (Option.scala:236) at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:481) at org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:59)......// 第二次异常 2018-12-28_12:50:56 [main] WARN component.AbstractLifeCycle:204: FAILED SelectChannelConnector@0.0.0.0:4041: java.net.BindException: 地址已在使用 java.net.BindException: 地址已在使用 at sun.nio.ch.Net.bind0 (Native Method) at sun.nio.ch.Net.bind (Net.java:433) at sun.nio.ch.Net.bind (Net.java:425)...... 其它信息都一样 // 第三次异常 2018-12-28_12:50:56 [main] WARN component.AbstractLifeCycle:204: FAILED SelectChannelConnector@0.0.0.0:4042: java.net.BindException: 地址已在使用 java.net.BindException: 地址已在使用 at sun.nio.ch.Net.bind0 (Native Method) at sun.nio.ch.Net.bind (Net.java:433) at sun.nio.ch.Net.bind (Net.java:425)....... 其它信息都一样 // 第一次异常 2018-12-28_12:50:56 [main] WARN component.AbstractLifeCycle:204: FAILED SelectChannelConnector@0.0.0.0:4043: java.net.BindException: 地址已在使用 java.net.BindException: 地址已在使用 at sun.nio.ch.Net.bind0 (Native Method) at sun.nio.ch.Net.bind (Net.java:433) at sun.nio.ch.Net.bind (Net.java:425)....... 其它信息都一样 可以轻易发现核心的地方在于：1FAILED SelectChannelConnector@0.0.0.0: 端口号: java.net.BindException: 地址已在使用 端口号在不断变化，从 4040 一直到 4043，才停止了异常的抛出。问题分析 在 Spark 创建 context 的时候，会使用 4040 端口作为默认的 SparkUI 端口，如果遇到 4040 端口被占用，则会抛出异常。接着会尝试下一个可用的端口，采用累加的方式，则使用 4041 端口，很不巧，这个端口也被占用了，也会抛出异常。接着就是重复上面的过程，直到找到空闲的端口。这个异常其实没什么问题，是正常的，原因可能就是在一台机器上面有多个进程都在使用 Spark，创建 context，有的 Spark 任务正在运行着，占用了 4040 端口；或者就是单纯的端口被某些应用程序占用了而已。此时是不能简单地把这些进程杀掉的，会影响别人的业务。问题解决 既然找到了问题，解决办法就很简单了：1、这本来就不是问题，直接忽略即可，不会影响 Spark 任务的正常运行；2、如果非要不想看到异常日志，那么可以检查机器的 4040 端口被什么进程占用了，看看能不能杀掉，当然这种方法不好了；3、可以自己指定端口（使用 spark.ui.port 配置项），确保使用空闲的端口即可（不建议，因为要确认空闲的端口，如果端口不空闲，Spark 的 context 会创建失败，更麻烦，还不如让 Spark 自己去重试）。参考：hortonworks原文：When a spark context is created, it starts an application UI on port 4040 by default. When the UI starts, it checks to see if the port is in use, if so it should increment to 4041. Looks like you have something running on port 4040 there. The application should show you the warning, then try to start the UI on 4041.This should not stop your application from running. If you really want to get around the WARNING, you can manually specify which port for the UI to start on, but I would strongly advise against doing so.To manually specify the port, add this to your spark-submit:–conf spark.ui.port=your_port]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>BindException</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS 异常之 READ is not supported in state standby]]></title>
    <url>%2F2018122702.html</url>
    <content type="text"><![CDATA[今天查看日志发现，以前正常运行的 Spark 程序会不断抛出异常：1org.apache.hadoop.ipc.RemoteException (org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby但是却没有影响到功能的正常运行，只不过是抛出了大量的上述异常，而且内容都一样，也都是操作 HDFS 产生的，所以猜测与 HDFS 集群（或者配置）有关系。本文就记录发现问题、解决问题的过程。问题出现 按照日常操作，查看 Spark 任务的 Driver 端的日志，结果发现了大量的重复异常，又看了一下对功能的影响，结果发现没有影响，所有功能均正常运行，产生的结果也是期望的。问题分析 详细来看一下 Driver 端的日志异常信息：123456789101112131415161718192021222324252627282930313233342018-12-26_23:25:40 [main] INFO retry.RetryInvocationHandler:140: Exception while invoking getFileInfo of class ClientNamenodeProtocolTranslatorPB over hadoop1/192.168.10.162:8020. Trying to fail over immediately.org.apache.hadoop.ipc.RemoteException (org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation (StandbyState.java:87) at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation (NameNode.java:1722) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation (FSNamesystem.java:1362) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo (FSNamesystem.java:4414) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo (NameNodeRpcServer.java:893) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo (ClientNamenodeProtocolServerSideTranslatorPB.java:835) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod (ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call (ProtobufRpcEngine.java:619) at org.apache.hadoop.ipc.RPC$Server.call (RPC.java:962) at org.apache.hadoop.ipc.Server$Handler$1.run (Server.java:2039) at org.apache.hadoop.ipc.Server$Handler$1.run (Server.java:2035) at java.security.AccessController.doPrivileged (Native Method) at javax.security.auth.Subject.doAs (Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs (UserGroupInformation.java:1628) at org.apache.hadoop.ipc.Server$Handler.run (Server.java:2033) at org.apache.hadoop.ipc.Client.call (Client.java:1468) at org.apache.hadoop.ipc.Client.call (Client.java:1399) at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke (ProtobufRpcEngine.java:232) at com.sun.proxy.$Proxy30.getFileInfo (Unknown Source) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo (ClientNamenodeProtocolTranslatorPB.java:768) at sun.reflect.GeneratedMethodAccessor34.invoke (Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke (Method.java:498) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod (RetryInvocationHandler.java:187) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke (RetryInvocationHandler.java:102) at com.sun.proxy.$Proxy31.getFileInfo (Unknown Source) at org.apache.hadoop.hdfs.DFSClient.getFileInfo (DFSClient.java:2007) at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall (DistributedFileSystem.java:1136) at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall (DistributedFileSystem.java:1132) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve (FileSystemLinkResolver.java:81) at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus (DistributedFileSystem.java:1132) at org.apache.hadoop.fs.FileSystem.isFile (FileSystem.java:1426)注意一下核心异常所在：12Exception while invoking getFileInfo of class ClientNamenodeProtocolTranslatorPB over hadoop1/192.168.10.162:8020. Trying to fail over immediately.org.apache.hadoop.ipc.RemoteException (org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby当去从 hadoop1/192.168.10.162:8020 这里 getFileInfo 的时候，抛出了异常，而且明确告诉我们这台机器处于 standby 状态，不支持读取操作。此时，可以想到，肯定是 hadoop1/192.168.10.162:8020 这台机器已经处于 standby 状态了，无法提供服务，所以抛出此异常。既然问题找到了，那么问题产生的原因是什么呢，以及为什么对功能没有影响，接下来一一分析。首先查看 hdfs-site.xml 配置文件，看看 namenode 相关的配置项：12345678910111213141516171819&lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;r-cluster&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.namenodes.r-cluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.r-cluster.nn1&lt;/name&gt; &lt;value&gt;hadoop1:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.r-cluster.nn2&lt;/name&gt; &lt;value&gt;rocket15:8020&lt;/value&gt;&lt;/property&gt;可以看到，namenode 相关配置有 2 台机器：nn1、nn2，而上述产生异常的信息表明连接 nn1 被拒绝，那么我去看一下 HDFS 集群的状态，发现 nn1 果然是 standby 状态的，而 nn2（rocket15） 才是 active 状态。再仔细查看日志，没有发现连接 nn2 的异常，那就说明是第一次连接 nn1 抛出异常，然后试图连接 nn2，成功连接，没有抛出异常，接下来程序就正常处理数据了，对功能没有任何影响。到这里，我们已经分析出了整个过程，现象表明这个异常只是连接了 standby 状态的 namenode，是正常抛出的。然后会再次连接另外一台 active 状态的 namenode，连接成功。抛异常的流程细节 1、客户端在连接 HDFS 的时候，会从配置文件 hdfs-site.xml 中，读取 nameservices 的配置，获取机器编号，我这里是 nn1 和 nn2，分别对应着 2 台 namenode 机器；2、客户端会首先选择编号较小的 namenode（我这里是 nn1，对应着 hadoop1），试图连接；3、如果这台 namenode 是 active 状态，则客户端可以正常处理请求；但是如果这台 namenode 是 standby 状态，则客户端抛出由服务端返回的异常：Operation category READ is not supported in state standby，同时打印 ip 信息，接着会尝试连接另外一台编号较大的 namenode（我这里是 nn2，即 rocket15）；4、如果连接成功，则客户端可以正常处理请求；如果 nn2 仍然像 nn1 一样，客户端会抛出一样的异常，此时会继续反复重试 nn1 与 nn2（重试次数有配置项，间隔时间有配置项）；如果有成功的，则客户端可以正常处理请求，如果全部失败，则客户端无法正常处理请求，此时应该要关注解决 namenode 为什么全部都处在 standby 状态。 配置参数如下（参考 Hadoop 官方文档 ）：1234567891011121314151617181920212223242526272829&lt;!-- 客户端重试次数，默认 15 --&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.max.attempts&lt;/name&gt; &lt;value&gt;15&lt;/value&gt;&lt;/property&gt;&lt;!-- 客户端 2 次重试间隔时间，默认 500 毫秒 --&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.sleep.base.millis&lt;/name&gt; &lt;value&gt;500&lt;/value&gt;&lt;/property&gt;&lt;!-- 客户端 2 次重试间隔时间，默认 1500 毫秒 --&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.sleep.max.millis&lt;/name&gt; &lt;value&gt;1500&lt;/value&gt;&lt;/property&gt;&lt;!-- 客户端 1 次连接中重试次数，默认 0, 在网络不稳定时建议加大此值 --&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.connection.retries&lt;/name&gt; &lt;value&gt;0&lt;/value&gt;&lt;/property&gt;&lt;!-- 客户端 1 次连接中超时重试次数，仅是指超时重试，默认 0, 在网络不稳定时建议加大此值 --&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.connection.retries.on.timeouts&lt;/name&gt; &lt;value&gt;0&lt;/value&gt;&lt;/property&gt; 问题解决 既然明确了问题，并且分析出了具体原因，解决起来就简单了，对于我这种情况，有 2 种方法：1、不用解决，也无需关心，这个异常没有任何影响，会自动重连另外一台 active 状态的 namenode 机器的；2、如果就是一心想把异常消除掉，那就更改 hdfs-site.xml 配置文件里面的 nameservices 配置项对应的机器，把编号最小的机器设置成状态为 active 的 namenode（例如我这里把 nn1、nn2 的对应的机器 ip 地址交换一下即可，确保 nn1 是 active 状态的），那么连接 HDFS 的时候第一次就会直接连接这台机器，就不会抛出异常了（但是要注意 namenode 以后可能是会挂的，挂了会自动切换，那么到那个时候还要更改这个配置项）。123456789&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.r-cluster.nn1&lt;/name&gt; &lt;value&gt;rocket15:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.r-cluster.nn2&lt;/name&gt; &lt;value&gt;hadoop1:8020&lt;/value&gt;&lt;/property&gt;问题总结1、参考：http://support-it.huawei.com/docs/zh-cn/fusioninsight-all/maintenance-guide/zh-cn_topic_0062904132.html2、这个问题其实不是问题，只不过抛出了异常，我看到有点担心而已，但是如果连接所有的机器都抛出这种异常，并且重试了很多次就有影响了，说明所有的 namenode 都挂了，根本无法正常操作 HDFS 系统；3、根据 2 进行总结：如果只是在操作 HDFS 的时候打印一次（每次操作都会打印一次），说明第一次连接到了 standby 状态的 namenode，是正常的，不用关心；但是，如果出现了大量的异常（比如连续 10 次，连续 20 次），说明 namenode 出问题了，此时应该关心 namenode 的状态，确保正常服务。]]></content>
      <categories>
        <category>Hadoop 从零基础到入门系列</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Spark</tag>
        <tag>HDFS</tag>
        <tag>nameNode</tag>
        <tag>standby</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS 异常之 Filesystem closed]]></title>
    <url>%2F2018122701.html</url>
    <content type="text"><![CDATA[今天通过 Hadoop 的 api 去操作 HDFS 里面的文件，读取文本内容，但是在代码里面总是抛出以下异常：1Caused by: java.io.IOException: Filesystem closed然而文本内容又是正常读取出来的，但是我隐隐觉得读取的文本内容可能不全，应该只是所有文本内容的一部分。本文就记录这个问题的原因、影响以及解决方法。问题出现 通过查看日志发现，有大量的异常日志打印出来，全部都是操作 HDFS 的时候产生的，有的是使用 Spark 连接 HDFS 读取文本数据，有的是使用 Hadoop 的 Java api 通过文件流来读取数据，每次读取操作都会产生一个如下异常信息（会影响实际读取的内容，多个 DataNode 的内容会漏掉）：123456789101112131415161718192021222324252627282018-12-26_23:25:46 [SparkListenerBus] ERROR scheduler.LiveListenerBus:95: Listener EventLoggingListener threw an exceptionjava.lang.reflect.InvocationTargetException at sun.reflect.GeneratedMethodAccessor33.invoke (Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke (Method.java:498) at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply (EventLoggingListener.scala:150) at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply (EventLoggingListener.scala:150) at scala.Option.foreach (Option.scala:236) at org.apache.spark.scheduler.EventLoggingListener.logEvent (EventLoggingListener.scala:150) at org.apache.spark.scheduler.EventLoggingListener.onJobStart (EventLoggingListener.scala:173) at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent (SparkListenerBus.scala:34) at org.apache.spark.scheduler.LiveListenerBus.onPostEvent (LiveListenerBus.scala:31) at org.apache.spark.scheduler.LiveListenerBus.onPostEvent (LiveListenerBus.scala:31) at org.apache.spark.util.ListenerBus$class.postToAll (ListenerBus.scala:55) at org.apache.spark.util.AsynchronousListenerBus.postToAll (AsynchronousListenerBus.scala:37) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp (AsynchronousListenerBus.scala:80) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply (AsynchronousListenerBus.scala:65) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply (AsynchronousListenerBus.scala:65) at scala.util.DynamicVariable.withValue (DynamicVariable.scala:57) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp (AsynchronousListenerBus.scala:64) at org.apache.spark.util.Utils$.tryOrStopSparkContext (Utils.scala:1181) at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run (AsynchronousListenerBus.scala:63)Caused by: java.io.IOException: Filesystem closed at org.apache.hadoop.hdfs.DFSClient.checkOpen (DFSClient.java:795) at org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync (DFSOutputStream.java:1986) at org.apache.hadoop.hdfs.DFSOutputStream.hflush (DFSOutputStream.java:1947) at org.apache.hadoop.fs.FSDataOutputStream.hflush (FSDataOutputStream.java:130) ... 20 more最直接清晰的描述就是：1Caused by: java.io.IOException: Filesystem closed上述异常信息表明 HDFS 的 Filesystem 被关闭了，但是代码仍旧试图打开文件流读取内容。问题解决 分析一下 根据上述信息，查看代码，每次操作 HDFS 都是独立的，会先根据统一的 conf 创建 Filesystem，然后根据文件路径创建 Path，打开输入流，读取内容，读取完成后关闭 Filesystem，没有什么异常的地方。同时，根据异常信息可以发现，异常的抛出点并不是业务逻辑代码，更像是已经开始开启文件流读取文件，读着读着 Filesystem 就被关闭了，然后引发了异常，而业务逻辑中并没有突然关闭 Filesystem 的地方，也没有多线程操作 Filesystem 的地方。1234567891011121314151617181920212223242526272829303132333435363738/** * 获取文件内容 * 纯文本，不做转换 * 如果传入目录，返回空内容 * * @param hdfsFile * @return */public static Set&lt;String&gt; getFileContent(String hdfsFile) &#123; Set&lt;String&gt; dataResult = new HashSet&lt;&gt;(); FileSystem fs = null; try &#123; // 连接 hdfs fs = FileSystem.get (CONF); Path path = new Path (hdfsFile); if (fs.isFile (path)) &#123; FSDataInputStream fsDataInputStream = fs.open (path); BufferedReader bufferedReader = new BufferedReader (new InputStreamReader (fsDataInputStream)); String line = null; while (null != (line = bufferedReader.readLine ())) &#123; dataResult.add (line); &#125; &#125; else &#123; LOGGER.error ("!!!! 当前输入参数为目录，不读取内容:&#123;&#125;", hdfsFile); &#125; &#125; catch (Exception e) &#123; LOGGER.error ("!!!! 处理 hdfs 出错:" + e.getMessage (), e); &#125; finally &#123; if (null != fs) &#123; try &#123; fs.close (); &#125; catch (IOException e) &#123; LOGGER.error ("!!!! 关闭文件流出错:" + e.getMessage (), e); &#125; &#125; &#125; return dataResult;&#125;通过查找文档发现，这个异常是 Filesystem 的缓存导致的。当任务提交到集群上面以后，多个 datanode 在 getFileSystem 过程中，由于 Configuration 一样，会得到同一个 FileSystem。如果有一个 datanode 在使用完关闭连接，其它的 datanode 在访问时就会出现上述异常，导致数据缺失（如果数据恰好只存在一个 datanode 上面，可能没问题）。找到方法 通过上面的分析，找到了原因所在，那么解决方法有 2 种：1、可以在 HDFS 的 core-site.xml 配置文件里面把 fs.hdfs.impl.disable.cache 设置为 true，这样设置会全局生效，所有使用这个配置文件的连接都会使用这种方式，有时候可能不想这样更改，那就使用第 2 种方式；1234&lt;property&gt; &lt;name&gt;fs.hdfs.impl.disable.cache&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;2、在 HDFS 提供的 Java api 里面更改配置信息，则会只针对使用当前 conf 的连接有效，相当于临时参数。12// 缓存 fs, 避免多 datanode 异常：Caused by: java.io.IOException: Filesystem closedCONF.setBoolean ("fs.hdfs.impl.disable.cache", true);上面 2 种方法的目的都是为了关闭缓存 Filesyetem 实例，这样每次获得的 Filesystem 实例都是独立的，不会产生上述的异常，但是缺点就是会增加网络的 I/O，频繁开启、关闭文件流。问题总结1、参考：https://stackoverflow.com/questions/23779186/ioexception-filesystem-closed-exception-when-running-oozie-workflow ；2、保留日志，查看日志很重要；3、FileSytem 类内部有一个 static CACHE，用来保存每种文件系统的实例集合，FileSystem 类中可以通过参数 fs.% s.impl.disable.cache 来指定是否禁用缓存 FileSystem 实例（其中 % s 替换为相应的 scheme，比如 hdfs、local、s3、s3n 等）。如果没禁用，一旦创建了相应的 FileSystem 实例，这个实例将会保存在缓存中，此后每次 get 都会获取同一个实例，但是如果被关闭了，则再次用到就会无法获取（多 datanode 读取数据的时候）；4、源码分析放在以后，留坑。]]></content>
      <categories>
        <category>Hadoop 从零基础到入门系列</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Spark</tag>
        <tag>Filesystem</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[可乐鸡翅做法总结]]></title>
    <url>%2F2018122501.html</url>
    <content type="text"><![CDATA[可乐鸡翅，是一道做法很简单的菜，很巧妙地将饮料和鸡翅结合在一起，做出来的可乐鸡翅既好看又好吃。本文简单介绍可乐鸡翅的做法总结，这是一种偏甜的做法。食材准备 3 人份的材料（8-10 个鸡翅），吃多了也不好吃1、鸡翅 9 个，最好是鸡中翅（一般 2-3 元一个）；2、可乐 1 罐（330 毫升的，如果鸡翅多的话，适当增加可乐）； 选择百事可乐 3、姜片、八角、桂皮（也可以不用）；4、料酒、生抽、老抽、食用盐； 制作步骤 1、在鸡翅背面划几刀（正面保留完整为了摆盘好看而已），更容易入味，用食用盐、料酒、老抽腌制 10 分钟，备用； 划刀腌制 2、锅中加水，放入姜片、少量料酒，鸡翅也下锅（冷水下锅），煮开即可，不用煮透（煮透鸡翅就老了），看到浮沫很多就可以捞出，用温水清洗一下，晾干（晾不干就用厨房纸擦一下，防止煎的时候溅油），此时如果发现有不干净的鸡毛可以拔干净； 鸡翅冷水下锅 鸡翅焯水出浮沫 3、锅中放入少量油（不放也行，鸡翅会自己出油的），放入姜片，开始煎鸡翅，开小火，煎至两面金黄即可，不可以煎太久，否则鸡翅老了不好吃； 小火煎 4、加一罐可乐，适量料酒、生抽、老抽，适量桂皮、八角，开始小火炖煮，炖至可乐还有一小碗水的量的时候，尝尝味道，适量加盐； 加入可乐、配料 小火炖煮 5、炖至汤浓收汁，基本所有的汁都覆盖在鸡翅上面了，鸡翅也有味道，装盘，正面朝上，把锅中剩余的汤汁淋入鸡翅中（大概 1-2 饭勺的量），再撒上少许白芝麻，既好看又好吃。 可以收汁 收汁之前补充食用盐、老抽 收汁完成 装盘 注意事项1、不要再放糖了，一罐可乐里面含糖大概 35 克；2、如果放了那种本身是咸味的生抽，也不用放盐了，或者少量放一点点（放盐之前先尝尝汤水的味道，不容易出差错）；3、焯水的时候冷水下锅，防止肉老了，并且放一点姜片和料酒，去腥味；4、鸡翅焯水后晾干很有必要，否则下一步骤煎的时候水和热油混合一起会溅出油的；5、甜味和咸味的控制依据个人口味调整，此外，可口可乐比百事可乐更甜，即含糖量更高。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>可乐鸡翅</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 错误之 JavaSparkContext not serializable]]></title>
    <url>%2F2018122101.html</url>
    <content type="text"><![CDATA[今天更新代码，对 Spark 里面的 RDD 随便增加了一个 Function，结果遇到了序列化（Serializable）的问题，这个不是普通的自定义类不能序列化问题，而是 JavaSparkContext 的用法问题，由于小看了这个问题，多花了一点时间解决问题，本文就记录下这一过程。问题出现 针对已有的项目改动了一点点，结果直接出现了这个错误：一开始疏忽大意了，以为像往常一样，是某些需要传递的对象对应的类没有序列化，由于对代码不敢改动太大，就想着用最简单的方法，把几个自定义类都序列化了，以为就应该可以了。结果，还是不行，此时虽然不会有自定义类的序列化问题了，但是却出现了终极错误：JavaSparkContext not serializable，这是什么意思呢，是说 JavaSparkContext 不能序列化，总不能把 JavaSparkContext 序列化吧，Spark 是不允许这么干的。那么问题是什么呢？我首先猜测肯定是 Function 里面用到了 JavaSparkContext 对象，导致启动 Spark 任务的时候，需要序列化 Function 用到的所有对象（当然也需要序列化对象所属类里面的所有属性），而这些 Function 所用到的所有对象里面，就有 JavaSparkContext 对象。于是，我耐心看了一下代码，果然，在创建 Function 对象的时候，竟然把 JavaSparkContext 对象作为参数传进去了，还是因为 JavaSparkContext 不能乱用。其实，报错日志里面都已经明显指向说明了，除了自定义的类，错误归结于 1at org.apache.spark.api.java.AbstractJavaRDDLike.mapPartitions (JavaRDDLike.scala:46) 而这里的代码，正是我增加的一部分，为了贪图简单方便，直接把 JavaSparkContext 对象传递给了 mapPartitions 对应的 Function。解决问题 既然找到了问题，接下来就好办了。既然 JavaSparkContext 不能乱用，那就不用，把这个传递参数去掉，即可正常运行，但是这样做太简单粗暴，不是解决问题的思路。仔细分析一下，可以有 2 种解决办法（思路就是避免序列化）：1、如果在 Function 里面非要用到 JavaSparkContext 对象，那就把 JavaSparkContext 对象设置为全局静态的 Java 属性（使用 static 关键字），那么在哪里都可以调用它了，而无需担心序列化的问题（静态属性可以避免从 Driver 端发送到 Executor 端，从而避免了序列化过程）；2、对于 Function 不要使用内部匿名类，这样必然需要序列化 Function 对象，同时也必然需要序列化 Function 对象用到的 JavaSparkContext 对象，其实可以把 Function 类定义为内部静态类，就可以避免序列化了。问题总结 1、出现这种错误，不要想当然地认为就是某种原因造成的，而要先看详细日志，否则会走弯路，浪费一些时间（虽然最终也能解决问题）；2、有时候状态不好，晕乎乎的，找问题又慢又低效，此时应该休息一下，等头脑清醒了再继续找问题，否则可能事倍功半，而且影响心情。 参考：https://stackoverflow.com/questions/27706813/javasparkcontext-not-serializable]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark序列化</tag>
        <tag>serializable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微博 url mid 相互转换算法实现 - Java 版本]]></title>
    <url>%2F2018122001.html</url>
    <content type="text"><![CDATA[对微博数据有了解的人都知道，一条微博内容对应有唯一的微博 url，同时对微博官方来说，又会生成一个 mid，mid 就是一条微博的唯一标识（就像 uid 是微博用户的唯一标识一样），也类似于人的身份证号。其实，微博 url 里面有一串看起来无意义的字符（由字母、数字组成，6-8 个字符长度），可以和 mid 互相转换，本文就根据理论以及 Java 版本的实现，讲解微博 url 与 mid 的互相转换过程。 待整理。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>微博url</tag>
        <tag>微博mid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript 中字符串截取方法总结]]></title>
    <url>%2F2018121901.html</url>
    <content type="text"><![CDATA[最近在处理数据的时候，用到了 JavaScript 编程语言，通过绕弯路来解决 ETL 处理的逻辑，其中就用到了字符串的截取方法，查 JavaScript 的文档看到了 3 个方法，被绕的有点晕，本文就总结一下 JavaScript 中字符串截取的方法。开篇 首先声明，JavaScript 中对方法名字的大小写是敏感的，该是小写就是小写，该是大写就是大写。substring () 方法 定义和用法 substring () 方法用于截取字符串中介于两个指定下标之间的字符 语法 stringObject.substring (start, stop) 上述参数解释：参数名 解释说明 start 必须，一个整数（是负数则被自动置为 0），要截取的子串的第一个字符在 stringObject 中的位置 end 可选（如果省略该参数，则被默认为字符串长度），一个整数（是负数则被自动置为 0），比要截取的子串的最后一个字符在 stringObject 中的位置多 1返回值 一个全新的字符串，其实就是 stringObject 的一个子字符串，其内容是从 start 到 stop-1 的所有字符，其长度为 stop 减 start。注意事项 1、substring () 方法返回的子字符串包括 start 处的字符，但是不包括 stop 处的字符，这一点可能很多人会迷惑，其实很多编程语言都是这个逻辑；2、如果参数 start 与 stop 相等，那么该方法返回的就是一个空串（即长度为 0 的字符串，不是 null，也不是 undefined）；3、如果 start 比 stop 大，那么该方法在截取子串之前会先交换这两个参数，这就会导致参数的顺序不影响截取的结果了；4、参数理论上不能出现负数（在本方法中无特殊意义，在其它方法中就有特殊意义了），如果有，那么在截取子串之前会被置为 0。 举例说明 例子 1（从下标 3 截取到字符串最后）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substring (3))&lt;/script&gt;输出（长度为 10 的子串）：1lo-world!例子 2（从下标 3 截取到下标 8）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substring (3, 8))&lt;/script&gt;输出（长度为 5 的子串）：1lo-wo例子 3（从下标 3 截取到下标 8，但是参数位置反了）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substring (8, 3))&lt;/script&gt;输出（长度为 5 的子串）：1lo-wo例子 4（参数为负数，从下标 0 截取到下标 3）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substring (-1, 3))&lt;/script&gt;输出（长度为 3 的子串）：1Helsubstr () 方法 定义和用法 substr () 方法可在字符串中截取从 start 下标开始的指定长度的子串 语法 stringObject.substr (start, length) 上述参数解释：参数名 解释说明 start 必须，必须是数值（0、正数、负数都可以），表示要截取的子串的起始下标。如果是负数，那么该参数声明的是从字符串的尾部开始计算的位置。也就是说，-1 指字符串中最后一个字符，-2 指倒数第二个字符，以此类推。（参数为负数也可以理解成字符串长度加负数之和即为起始下标）length可选（如果省略该参数，那么默认为从 start 开始一直到 stringObject 的结尾对应的长度），必须是数值（0、正数、负数都可以）。返回值 一个全新的字符串，包含从 stringObject 的 start（包括 start 所指的字符）下标开始的 length 个字符。如果没有指定 length，那么返回的字符串包含从 start 到 stringObject 的结尾的字符。如果 length 指定为负数或者 0，那么返回空串。如果 length 指定为远远大于 stringObject 长度的正数，那么返回的字符串包含从 start 到 stringObject 的结尾的字符。注意事项 1、start 参数为负数是有特殊含义的；2、如果 length 指定为负数或者 0，那么返回空串（即长度为 0 的字符串，不是 null，也不是 undefined）；3、ECMAscript 没有对该方法进行标准化，因此不建议使用它。 举例说明 例子 1（从下标 3 截取到字符串最后）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substr (3))&lt;/script&gt;输出（长度为 9 的子串）：1lo-world!例子 2（从下标 3 截取长度为 5 的子串）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substr (3, 5))&lt;/script&gt;输出（长度为 5 的子串）：1lo-wo例子 3（从下标 3 截取长度为 - 5 的子串，返回空串）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substr (3, -5))&lt;/script&gt;输出（返回空串）：12例子 4（start 参数为负数，即从字符串倒数第 5 个位置截取长度为 3 的子串）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substr (-5, 3))&lt;/script&gt;输出（长度为 3 的子串）：1orlslice () 方法 定义和用法 slice () 方法用于截取字符串中介于两个指定下标之间的字符，与 substring () 方法的功能类似 语法 stringObject.slice (start, end) 上述参数解释：参数名 解释说明 start 必须，一个整数（0、正数、负数，负数有特殊含义），要截取的子串的第一个字符在 stringObject 中的位置。如果是负数，那么该参数声明的是从字符串的尾部开始计算的位置。也就是说，-1 指字符串中最后一个字符，-2 指倒数第二个字符，以此类推。（参数为负数也可以理解成字符串长度加负数之和即为起始下标）end可选（如果省略该参数，则被默认为字符串长度），一个整数（负数含义与 start 相同），比要截取的子串的最后一个字符在 stringObject 中的位置多 1返回值 一个全新的字符串，其实就是 stringObject 的一个子字符串，其内容是从 start 到 stop-1 的所有字符，其长度为 stop 减 start。注意事项 1、slice () 方法返回的子字符串包括 start 处的字符，但是不包括 stop 处的字符，这一点可能很多人会迷惑，其实很多编程语言都是这个逻辑；2、如果参数 start 与 stop 相等，那么该方法返回的就是一个空串（即长度为 0 的字符串，不是 null，也不是 undefined）；3、参数可以出现负数（比 substring () 方法灵活多了）。 举例说明 例子 1（从下标 3 截取到字符串最后）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.slice (3))&lt;/script&gt;输出（长度为 9 的子串）：1lo-world!例子 2（从下标 3 截取到下标 8）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.slice (3, 8))&lt;/script&gt;输出（长度为 5 的子串）：1lo-wo例子 3（从下标 3 截取到下标 8，但是参数使用负数，从下标 - 9 截取到下标 - 4）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.slice (-9, -4))&lt;/script&gt;输出（长度为 5 的子串，（-4）-（-9）=5）：1lo-wo例子 4（从下标 3 截取到下标 2）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.slice (3, 2))&lt;/script&gt;输出返回空串）：12]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>字符串截取</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[西红柿疙瘩汤做法总结]]></title>
    <url>%2F2018121601.html</url>
    <content type="text"><![CDATA[西红柿疙瘩汤，是一道做法非常简单的主食与配菜混为一起的菜品，适合在寒冷的冬天食用，吃一碗热乎乎的，非常暖胃，我知道在中原地区（河南、安徽北部）都有这个做法。本文就讲述西红柿疙瘩汤的做法总结。食材准备 以下的食材份量大约 2 人份：黄心乌菜一颗（实在没有使用其它青菜也可以）西红柿一颗（粉的最好，与脆的对立）鸡蛋 2 颗 面粉 100 克 小葱、香菜各 2 棵 调味料（食用盐、芝麻油）制作步骤 从开火到关火预计耗时 15-20 分钟：0、葱花香菜段；1、西红柿去皮，划十字刀花，放入热水中烫 1 分钟左右，取出直接去皮，不去皮也行，但是会影响口感，去皮后切丁，切小一点，放入碗中备用；粉粉的西红柿 十字花刀 开水烫 1 分钟（30 秒翻身一次），轻易去皮 西红柿去皮 西红柿切丁 2、准备黄心乌菜，洗干净，随便切（手撕也行，无所谓），切成条状或者小块状，别太大就行；3、面粉放入大碗中，放在水龙头下，让水一滴一滴滴下来，迅速搅拌面粉，很快就可以做成面粒；4、鸡蛋打入碗中，搅拌均匀备用；5、锅烧热，倒入油，炒制西红柿丁，中小火炒制 3-5 分钟，此时西红柿的状态就是一半是糊状，一半是小颗粒，混合在一起，倒入开水（注意量的控制，比想象的多倒一点，面粒会吸收大量水分的），大火烧开； 西红柿丁炒制 加开水，煮开 6、烧开后放入面粒，大火煮 5 分钟，面粒基本熟透，汤变得浓稠，放入青菜，中火继续煮 1 分钟左右，鸡蛋液慢慢淋入锅中，搅拌，放入食用盐，中火继续煮 2 分钟； 放入面粒，继续煮 5 分钟 7、开锅，放入芝麻油、香菜段，葱花，搅拌十几秒，关火。 一锅 一碗 做完顺便又加了 2 个菜：花菜回锅肉 辣椒回锅肉 注意事项 1、青菜最好选择黄心乌，因为我一直吃的都是这种，黄心乌这种青菜一般在沿淮地区才播种，因为它比较耐寒，在秋季播种，在冬天收割，一般北方的冬天也看不到其它青菜可以生长了；2、条件允许的话，可以放一点酱肉之类的肉粒进去，更能增加食欲；3、做面粒的时候切记不要直接倒水搅拌，这样是做不成的一粒一粒的效果的，只能用水滴进去然后迅速搅拌，使水滴周围裹上面粉形成一粒，很快就全部都是面粒了，而且很均匀，另外，做好面粒后要立马使用，不要提前做好放那里，因为放久了（10 分钟都不行）面粒会粘连在一起，实在要放的话再多加点面粉进去，让面粒之间隔开；4、西红柿最好选择粉的，就是那种吃起来很柔绵的，更容易做成均匀的汤；5、如果在煮的过程中发现有点粘锅，那是因为水少了，面太多了，这时候用汤勺试着加 1-2 勺水进去，再搅拌一下，如果还是粘锅再加 1-2 勺，千万不要一下子加很多水，面汤最好的状态就是不粘锅但是又很浓稠；6、做回锅肉，肉要煮到什么程度才能回锅，简单的判断方法就是筷子可以轻易穿透肉，一般要煮 20 分钟以上。 补充说明2018 年 12 月 23 日，广州突然降温，降到 17 度左右（前一天的冬至还 25 度呢，短袖都穿起来了），天气冷了，于是又煮了一锅。可惜这次没买到香菜，没买到黄心乌菜，也没买到酱肉，凑活着吃。2018 年 12 月 30 日，广州的温度降到了个位数，最低 5 度，实在是冷。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>西红柿疙瘩汤</tag>
        <tag>疙瘩汤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用正则表达式列表]]></title>
    <url>%2F2018121401.html</url>
    <content type="text"><![CDATA[正则表达式是一种表达式语句。本文记录一些常用的正则表达式，以便使用。 待整理。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[与微博内容分析相关的正则表达式]]></title>
    <url>%2F2018121101.html</url>
    <content type="text"><![CDATA[在分析微博内容时，常常需要进行特殊内容去除与抽取，例如抽取微博话题、微博昵称、微博表情、微博短链接、网址长链接等等。本文依据实际使用情况，记录下了与微博内容分析相关的正则表达式，以便查用。微博表情 表情是使用左右中括号包含的文本（在实际使用时，显示的是 emoji 表情，不是单纯的字符），例如：[爱心]、[微笑]、[笑哭]，分别表示：:heart:、❤️、:smile:、😊、:joy:、😂参考：emoji 百科 。 如果在微博内容中抽取表情，使用正则表达式（1-7 个字符，中文和字母，不排除有的新的表情出现，导致字符更长）：1\[[\u4e00-\u9fa5A-Za-z]&#123;1,7&#125;\]不同字符长度的表情举例（我用了 10 分钟把微博表情全部浏览了一遍，发现 [小黄人] 系列、[文明遛狗] 是最近刚刚发布出来的）：[耶]、[来]、[跪了]、[ok]、[中国赞]、[紫金草]、[doge]、[文明遛狗]、[给你小心心]、[小黄人微笑]、[弗莱见钱眼开]、[小黄人剪刀手]、[哆啦 A 梦害怕]、[带着微博去旅行]。注意，在 2019 年 3 月 21 日，发现微博新增了表情：[大侦探皮卡丘微笑]，这个表情有 8 个字符，所以表情的正则表达式也要做相应的更新。1\[[\u4e00-\u9fa5A-Za-z]&#123;1,8&#125;\]微博昵称 微博昵称是用户填写的昵称，并且在转发或者提到时，会增加 @ 前缀，例如有一个 playpi 微博用户，在实际微博内容中，会以 @playpi 的形式出现，当然，微博昵称的可用字符是有限制的，不是任意字符都行，长度也是有限制的，最少 4 个字符，最多 30 个字符。以及微博客服的回答：微博客服微博 。 但是这个规则是针对修改昵称的限制，如果有些帐号是以前注册的，并且昵称在微博官方限制以前没有修改过，那么就有可能是 2 个字符，3 个字符，例如各个明星、作家、自媒体的个人微博：@阑夕、@王力宏、@韩寒 等等。如果在微博内容中抽取昵称，使用正则表达式（中文、数字、字母、横线、下划线的组合，2-30 个字符）：1@[\u4e00-\u9fa5A-Z0-9a-z_-]&#123;2,30&#125;微博话题 话题是微博定义的一种概念，可以用来标识热门事件、重大新闻、明星、综艺节目等等，发布规则就是使用 2 个 #符号包含话题内容（例如：# 创造 101#），话题即生成，微博还专门有一个实时话题榜单。如果在微博内容中抽取话题，使用正则表达式（2 个 #号之间，非指定的符号，长度在 1-49 之间）：1#[^@&lt;&gt;#"&amp;'\r\n\t]&#123;1,49&#125;#注意，我找到 2014 年的 一篇旧帖子 ，微博小秘书评论说话题不能包含指定的几个特殊字符，还有内容长度限制，但是我在微博页面试了一下，这些特殊字符都可以使用（但是生成的话题页面，&lt; 字符、&gt; 字符被转成了 html 字符实体，换行符后的内容被截断，@符号、’ 单引号、” 双引号被自动替换掉，# 符号根本无法发布，空格符可以正常使用），而且长度限制是 1-49 个字符（中英文、标点都算 1 个字符）。但是为了话题内容的传播，还是使用通俗易懂的中文或者字母比较好。 微博短链接 微博短链接是微博官方提供的网址压缩功能产生的一种只包含少量字符的短网址，例如：http://finance.sina.com.cn ，压缩后为：http://t.cn/RnM1Uti 。这样的话，发微博时链接占用更少的字符长度。如果发微博时，内容中带了链接，例如视频地址、淘宝店地址，会被自动压缩为短链接。微博短链接可以直接在浏览器中访问，会被微博的网址解析服务器转换为原来的正常链接再访问。如果在微博内容中抽取短链接，使用正则表达式（我这里只是抽取 t.cn 域名的，6-8 个字母、数字）：1#https&#123;0,1&#125;://t.cn/[A-Z0-9a-z]&#123;6,8&#125;[/]&#123;0,1&#125;#参考：微博开放平台说明：http://open.weibo.com/wiki/2/short_url/shorten ；免费在线短链接转换工具：http://dwz.wailian.work 。网址长链接 网址长链接也就是普通的网址，有多种可能性。如果在微博内容中抽取网址长链接，使用正则表达式（我这里只考虑 http、https、ftp、file 协议）：1(https?|ftp|file)://[-A-Za-z0-9+&amp;@#/%?=~_|!:,.;]+[-A-Za-z0-9+&amp;@#/%=~_|]]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
        <tag>微博内容</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10 默认程序设置无效]]></title>
    <url>%2F2018120901.html</url>
    <content type="text"><![CDATA[装了 Windows 10 系统（教育版本），用了将近 3 个月了，最近发现一个诡异的现象，我的默认程序设置每次都只是设置后生效一段时间，例如视频播放器、音乐播放器，我分别设置成了迅雷看看、网易云音乐，用了半天之后，发现又变成了 Window 10 系统自带的视频播放器。这个现象也不是重启之后才出现的，而是平时用着用着就会出现，很莫名其妙。后来查阅资料发现这是一个普遍的现象，这个问题的根本原因是 Windows 10 自带的 bug，通常导致这个 bug 出现的原因是开启了系统的自动更新。现象 在 Windows 10 系统（没有打对应补丁的）中，如果开启了系统自动更新，就会触发相应的 bug：默认程序会被系统更改回系统自带的程序，例如视频播放器、音乐播放器等等。这个问题的原因用官方标识来指定就是由于 KB3135173 所致，同时这个 bug 已经有对应的补丁了。按照系统设置，把某些默认程序改为自己需要的，我这里把视频播放器改为迅雷影音，设置特定格式的文件（.mkv，.mp4 等等）使用迅雷影音打开。在桌面右下角打开 所有设置 选项 在 Windows 设置中，选择 应用 选项 选择默认应用，设置视频播放器为 迅雷影音 上述的设置步骤实际上还不够，因为视频类型有很多种，还需要进一步指定每种类型的默认播放器，在默认应用下方有一个 按文件类型指定默认应用 选项 我这里特别关注 .mkv、.mp4 这 2 种格式的文件，默认应用设置为 迅雷影音 上述内容设置完成，就可以使用了，但是用不了多久，系统时不时就弹出提示框，通知默认程序重置，然后又被设置为系统内置的应用了 解决方案 不推荐方案 更改注册表、使用命令行卸载系统默认程序，这些方案是可行的，但是对于普通用户来说太麻烦了一点，根本不懂得如何操作，而且解决方法太粗暴了，当然喜欢折腾的人是可以选择的。以下给出几个命令行示例（需要在管理员模式下执行，打开 Windows PowerShell 的时候选择有管理员的那个）：卸载 “电影和电视” 应用（星号表示通配符，下同）1get-appxpackage *zunevideo* | remove-appxpackage卸载 “Groove 音乐” 应用 1get-appxpackage *zunemusic* | remove-appxpackage 卸载 “照片” 应用 1get-appxpackage *photos* | remove-appxpackage 如果还想恢复已经卸载的系统自带应用，可以使用以下命令（重装所有系统内置的应用）1Get-AppxPacKage -allusers | foreach &#123;Add-AppxPacKage -register "$($_.InstallLocation)appxmanifest.xml" -DisableDevelopmentMode&#125;推荐直接打补丁（更新系统）这个方法很简单，容易操作，直接在系统更新里面更新即可，确保要能更新到 KB3135173 这个补丁才行（或者更高版本的补丁）。我这里是已经更新完成的，等待重启，补丁标识是 KB4469342。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>Win10</tag>
        <tag>默认程序设置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark on Yarn 查看日志]]></title>
    <url>%2F2018120702.html</url>
    <content type="text"><![CDATA[一直一来都是直接在 Yarn 的 UI 界面上面查看 Spark 任务的日志的，感觉看少量的内容还勉强可以，但是如果内容很多，浏览器就没法看了，更没法分析。本文讲述如何使用 Yarn 自带的命令在终端查看 Spark 任务的日志，也可以拷贝出日志文件，便于分析。1、查看某个 Spark 任务的日志，使用 logs 入口：1yarn logs -applicationId application_1542870632001_26426如果日志非常多，直接看会导致刷屏，看不到有用的信息，所以可以重定向到文件中，再查看文件：1yarn logs -applicationId application_1542870632001_26426 &gt; ./application.log2、查看某个 Spark 任务的状态，使用 application 入口：1yarn application -status application_1542870632001_26426同时也可以看到队列、任务类型、日志链接等详细信息 3、kill 掉某个 Spark 任务，有时候是直接在 Driver 端 kill 掉进程，然后 Yarn 的 Spark 任务也会随之失败，但是这种做法是不妥的。其实 kill 掉 Spark 任务有自己的命令：1yarn application -kill application_1542870632001_264264、需要注意的是，步骤 1 中去查看日志，要确保当前 HADOOP_USER_NAME 用户是提交 Spark 任务的用户，否则是看不到日志的，因为日志是放在 HDFS 对应的目录中的，其中路径中会有用户名。此外，步骤 1 中的日志要等 Spark 任务运行完了才能看到，否则日志文件不存在（还没收集到 HDFS 中）。 在 Linux 环境中可以使用 export HADOOP_USER_NAME=xxx 临时伪装用户。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Yarn</tag>
        <tag>日志查看</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[煮鸡蛋做法总结]]></title>
    <url>%2F2018120301.html</url>
    <content type="text"><![CDATA[本文记录水煮鸡蛋的做法总结。介绍 水煮鸡蛋是最常见的吃法之一，做法非常简单，直接将鸡蛋放入开水中煮熟即可。煮熟的鸡蛋营养丰富，水煮鸡蛋的营养可以 100% 被保留，是所有的鸡蛋做法中营养被保留的最好的一种。建议每天食用 1-2 个，因为过量的食用可能会导致营养不良，同时鸡蛋的营养并没有被身体吸收，相当于浪费了。在生活当中，大家几乎每天早上都会吃煮鸡蛋，或者茶叶蛋，但是有一些人卖的煮鸡蛋不算成功的煮鸡蛋，因为剥皮的时候发现不好剥，蛋壳与蛋白紧紧粘在一起，吃起来可麻烦了，这是因为煮鸡蛋的做法错误，遗漏了重要的步骤。做法步骤 1、简单地清洗一下鸡蛋，因为鸡蛋的表面可能会有一些茅草、粪便之类的污垢，这是因为鸡蛋必须是原生的，存储、运输、销售过程都不能清洗，如果非要清洗，水会破坏表面的保护膜，放不了两天鸡蛋就坏了；2、放在冷水中浸泡一会儿，1-2 分钟，这样做的目的是防止沸水煮的时候蛋壳破裂；3、放入锅中，水的高度稍微没过鸡蛋，使用中火煮开水，不要使用大火，大火煮的速度太快，鸡蛋容易裂开，另外中火使水沸腾的时间会长一些，预热了鸡蛋，味道更香；4、水沸腾后，改为小火，煮 7-8 分钟（如果继续使用中火，5 分钟左右即可）；5、如果需要溏心蛋（蛋清凝固，蛋黄成稠液状，软嫩滑润），煮 5 分钟即可；6、煮熟后不要立即捞出，等 1-2 分钟，然后才捞出，切记此时需要放入冷水中，浸泡 1-3 分钟，这一步骤的目的是保证鸡蛋容易剥开，避免蛋白和蛋壳粘在一起。 煮鸡蛋成品 注意事项1、煮鸡蛋前最好放入冷水中浸泡 1-2 分钟，防止煮的过程开裂；2、注意控制火力和时间，鸡蛋不能煮太久，超过 10 分钟会有化学反应，导致营养流失；3、煮熟后不要立即捞出，捞出后也要放在冷水中浸泡，防止蛋白和蛋壳粘在一起；4、每天不要吃太多，1-2 个就够了；5、如果想要保持蛋黄在中间，煮鸡蛋的过程中要适当搅拌让鸡蛋旋转。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>煮鸡蛋</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一条正则表达式引发的惨案]]></title>
    <url>%2F2018120201.html</url>
    <content type="text"><![CDATA[本文讲述由于正则表达式引发的性能惨案，背景就是使用 Java 编程语言进行正则表达式匹配，由于正则表达式很复杂，再加上 Java 使用的是 NFA（非确定型有穷自动机）匹配引擎，导致匹配一条文本内容使用了十几个小时还没完成，一直卡住，同时线上环境的主机 CPU 使用率也居高不下（我猜的，因为我没有权限看）。 整理中。 参考：http://www.cnblogs.com/study-everyday/p/7426862.htmlhttps://www.jianshu.com/p/5c2e893b8d5d]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
        <tag>Java NFA</tag>
        <tag>非确定型有穷自动机</tag>
        <tag>正则无限回溯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jackson 包版本低导致 NoSuchMethodError]]></title>
    <url>%2F2018120101.html</url>
    <content type="text"><![CDATA[本文讲述 Java 项目由 Maven 包冲突或者版本不合适导致的运行时错误：1java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.JavaType.isReferenceType () Z起因 今天在升级 Web 项目的相关接口，更新了所依赖的 SDk 版本，删除了一些旧代码，测试时发现某个功能不可用，直接抛出异常，异常是在运行时抛出的，编译、打包、部署都没有任何问题。我看到第一眼，就知道肯定是 Maven 依赖问题，要么是版本冲突（存在不同版本的 2 个相同依赖），要么是依赖版本不对（太高或者太低），但为了保险起见，我还是先检查了一下 Git 的提交记录，看看有没有对 pom.xml 配置文件做相关改动。检查后发现，除了一些业务逻辑的变动，以及无关 jackson 依赖的版本升级，没有其它对 pom.xml 文件的改动，由此可以断定，某个依赖的升级导致了此问题，问题原因找到了，接下来就是解决问题。解决办法 查看项目的 Maven 依赖树 由于依赖太多，使用可视化的插件查看太繁杂，所以选择直接使用 maven 的 dependency 构件来生成文本，然后再搜索查看：1mvn dependency:tree &gt; tree.txt在 tree.txt 文件中搜索 jackson，可以找到 jackson-databind 相关的依赖包，还有 jackson-annotations、jackson-core 这 2 个依赖包。jackson-databind 的版本为 2.9.3确定了使用的版本，接下来可以在 IDEA 里面搜索一下这个类，然后再找调用的方法，直接去查看源码，看看到底有没有这个方法。搜索 JavaType Java 类，注意包的路径，可能会有很多重名的类出现，我是用 Ctrl + Shift + T 的快捷键搜索，各位根据自己的快捷键设置进行搜索。然后进入类的源代码，搜索方法 isReferenceType，报错信息后面的大写的 Z，是 JNI 字段描述符，表示这个方法的返回值类型，Z 表示 Boolean 类型，我们搜索看看有没有这个方法。我们发现连同名的方法都没有，更不用看返回值类型了，但是注意还是要去父类还有接口里面去搜索一下，保证都没有才是最终的没有。经过查找，没发现这个方法（主要原因是父类 ResolvedType 的版本太低，父类所在的 jackson-core 的版本只有 2.3.3，所以找不到这个方法），到这里就要准备升级 jackson-core 或者降级 jackson-databind 依赖了。去除多余依赖 如果是检查到存在依赖冲突的情况，一般是高低版本之间的冲突（最多的情况是多级传递依赖引起的），然后 Maven 编译打包时会全部打进业务的包。1、导致运行时程序不知道选择哪一个，于是抛出 NoSuchMethodError 异常，此时根据需要，移除多余的依赖包即可；2、步骤 1 操作后，还是一种可能是虽然只存在一个版本，但是由于版本太新或者太旧，无法兼容所有的调用，导致多处需要调用这个依赖包的地方总会有某个地方出现 NoSuchMethodError 异常。此时就比较麻烦，如果能找到一个合适版本的依赖包，兼容所有的调用，当然是好的；或者升级调用处对应的接口版本；如果还是无法解决，就只能通过 Shade 构件解决问题了，此处就不赘述了。经过检查，我这里遇到的就是步骤 2 的情况，虽然只剩下一个依赖包，但是版本太低或者太高，导致调用时找不到 isReferenceType 方法，类其实是存在的，所以要采用升级或者降级的方式。升级降级依赖 如果是检查到只有一个依赖，并没有冲突的情况，就容易了，直接找到最稳定的版本或者适合使用的旧版本，提取依赖的坐标，配置到 pom.xml 文件中即可。经过检查，我这里遇到的就是这种情况，去 Maven 私服中搜索 jackson，找到合适的版本（自己根据需要选择，我这里选择 jackson-databind 的 2.9.7 版本，然后 jackson-core 也指定 2.9.7 版本，就可以了，然后又查资料也发现这个方法是 2.6.0 版本之后才开始加上的），配置到 pom.xml 文件中即可。私服搜索 配置到 pom.xml我这里使用了常量，在 pom.xml 文件的 properties 属性下面配置即可。踩坑总结1、其实 jackson 这个依赖我并没有使用，而是引用的一个第三方依赖内部使用的，但是这个第三方依赖并没有一同打进来，也没有说明需要什么版本的，所以导致我自己在实验，最终找到到底哪一个版本合适。2、为了统一，jackson-core 的版本要与 jackson-databind 的版本一致，jackson-databind 里面是已经自带了 jackson-annotations 的，由于 jackson-databind 里面的类继承了 jackson-core 里面的，所以才都要升级并且保持版本一致。3、搜索类方法时，注意留意父类和接口里面，不一定非要在当前类里面出现。更改版本后同样也去类里面搜索一下，看看有没有需要调用的方法出现，确定版本用对了再继续做测试。4、这种错误在编译、打包、部署阶段是检查不出来的，因为代码并没有实际调用到，属于运行时错误，只有跑起来程序，执行到需要使用该方法的时候，才会报错。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>NoSuchMethodError</tag>
        <tag>jackson</tag>
        <tag>Maven</tag>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub 个人站点绑定独立的域名]]></title>
    <url>%2F2018112701.html</url>
    <content type="text"><![CDATA[随着越来越多的人使用 GitHub，都在里面创建了自己的仓库，或者 clone 了别人的优秀项目，也有很多人想利用 GitHub 自带的 GitHub Pages 来搭建个人博客，此时就可以使用独立的域名 https://www.username.github.io 访问自己的博客，全部的资源都来自于 GitHub，并且是免费的，不需要其它任何配置或者购买，这里面包含域名、流量、带宽、存储空间、Htpps 认证等服务。但是，有的人可能购买了自己的独立域名，例如： https://www.abc.com ，并且想把域名直接绑定到 GitHub 免费的域名上面，这样以后访问博客的时候更容易辨识，本文就描述 GitHub Pages 绑定独立域名的操作过程，前提是 GitHub Pages 已经创建完成。我在 Godaddy 上面购买了域名：playpi.org，选择 Godaddy 主要是不想备案，国内的域名服务商都要求备案，我以前在阿里云上面买过一个，后来没按照要求备案就不能用了，我也放弃了。购买域名 当然，大家可以选择自己喜欢的域名服务商，例如腾讯云、阿里云等，但是这些域名服务商需要给域名备案，有点麻烦【当然不是所有的域名都需要备案】。所以我选择的域名服务商是 Godaddy，主页地址：https://sg.godaddy.com/zh ，在主页中点击左上角的 域名 ，开始搜索域名。我这里输入域名 playpi.org，可以看到被占用了，已经已经被我购买了，可以看到右侧显示出了可以购买的域名列表，并且带有报价。在左侧，可以添加筛选条件，过滤掉自己不想要的域名。如果找到了满意的域名，加入购物车购买就行了。选择域名服务器 有了域名，还没有用，因为还没有把域名用起来，所以接下来需要找域名服务器，把你的域名解析到 GitHub Pages 去。这样，才能保证访问你的域名，自动跳转到 GitHub Pages 去。我一开始选择的 Godaddy 自己的域名服务器，只需要在 我的产品 -&gt; 域名 -&gt;DNS，设置一些解析记录即可。Godaddy 的配置解析规则可以参考下图【可以先忽略解析规则，后面会讲到的】：后来由于 GitHub Pages 屏蔽百度爬虫的问题，我必须设置一条专门的解析规则去解析百度爬虫的请求，引入到我自己的 Web 服务器上面，但是 Godaddy 不支持线路的自定义，比较笼统，所以我就放弃了。转而选择了腾讯的 DNSPod，还是比较好用的，虽然前不久刚出过问题，大量的网络瘫痪，但是解析速度还是挺快的。先在 DNSPod 中添加域名，也就是在 Godaddy 中购买的域名【如果是直接在腾讯云中购买的，就不用配置了，默认就有】。添加完成后，可以看到提示我 NS 地址还未修改，也就是目前仍旧是 Godaddy 负责解析这个域名，所以要把域名服务器给切换过来。如果不知道是什么意思，可以点击提示链接查看帮助手册【其实就是去购买域名的服务商那里绑定 DNSPod 的域名服务器】提示我们修改 NS 地址 帮助手册 我在这就直接查看当前的域名的 NS 地址，选择域名，进入配置页面查看。去 Godaddy 中配置域名服务器，替换掉原本默认的。在 我的产品 -&gt; 域名 -&gt;DNS：我把它修改为我在 DNSPod 中查到的属于我的域名的域名服务器，一般都会有 2 个，保证可靠性。配置完成新的域名服务器【以前的解析记录都消失了】：配置完成，域名解析的工作就完全交给 DNSPod 了，我们可以退出 Godaddy 了【只是在这里买了一个域名】，接下来全程都要在 DNSPod 中配置其它信息。配置域名的解析规则 上一步骤我已经配置完成了域名的基本信息，接下来需要配置的就非常关键了，是域名的解析规则，它会指引着访问域名的请求怎么跳转。这里先提前说一下配置规则：主机记录 为 @表示直接访问域名，例如访问 playpi.org主机记录 为其它字符表示访问二级域名，例如访问 www.playpi.org 、blog.playpi.org记录类型 为 A 表示跳转到 ip 地址，后面的 记录值 就需要填 ip，例如 66.32.122.18记录类型 为 CNAME 表示跳转到域名，后面的 记录值 就需要填域名，例如 blog.playpi.org线路类型 是 DNSPod 自定义的逻辑分类，给访问的请求分类，例如百度爬虫、搜狗爬虫，这个选项对于我来说很有用，可以解决 GitHub Pages 屏蔽爬虫的问题 我把 Godaddy 中的解析记录直接抄过来就行，不同的是由于使用的是 DNSPod 免费版本，A 记录会少配置 2 个，基本不会有啥影响 【其实不配置 A 记录最好，直接配置 CNAME 就行了，会根据域名自动寻找 ip，以前我不懂】。另外还有一个就是需要针对百度爬虫专门配置一条 www 的 A 记录，针对百度的线路指向自己服务器的 ip【截图只是演示，其中 CNAME 记录应该配置域名，A 记录才是配置 ip】。如果使用的是第三方托管服务，直接添加 CNAME 记录，配置域名就行【例如 yoursite.gitcafe.io】。上面的配置里面的 A 记录明显是多余的，而且还要通过 ping 去寻找那几个 ip【我这里是 ping iplaypi.github.io 得到，大家换为自己的 GitHub 用户名即可，每个用户之间的 ip 应该有差别，不会完全一样】。所以建议大家不使用 A 记录的配置方式，直接使用 CNAME 配置。配置完成后使用 域名设置 里面的 自助诊断 功能，可以看到域名存在异常，主要是因为更改配置后的时间太少了，要耐心等待全球递归 DNS 服务器刷新【最多 72 小时】，不过一般 10 分钟就可以访问主页了。但是后来我发现，那个 GitHub 的域名【iplaypi.github.io】被墙了而 ip 没被墙，表现为每天总会有一段时间访问不了【DNSPod 也会给我发告警邮件，说宕机了，当然是他们的域名测试服务器连不上这个域名】，而且我用自己的浏览器也访问不了。而 blog 那个二级域名却可以正常访问，这就说明 GitHub 的那个域名不好使，而我自己给 blog 专门部署 Web 服务是正常的。因此，在主机记录为 @ 的解析规则里面还是配置 A 记录吧，把几个 ip 都配置上去【免费版本的 DNSPod 只能添加 2 条，可怜】。这样做还会引起 GitHub 的警告，因为这个 ip 地址可能会变化，所以 GitHub 建议配置域名。如果想知道域名对应的 ip 地址，除了使用 ping 之外，还有更快捷的方法：dig 命令。在 GitHub 中设置 CNAME关于域名的配置都完成了，最后还有一个重要的步骤，需要在 GitHub 的项目中添加一个文件，文件名称是 CNAME，文件内容就是域名【我这里使用的是二级域名，也可以，就是在直接访问域名的时候多了一次转换】。那这个文件的作用是什么呢，为什么要这么配置呢？其实，CNAME 是一个别名记录，它允许你将多个名字映射到同一台计算机，还决定着主页的链接最终展示的样子，直接是域名【https://playpi.org 】还是带二级域名【https://www.playpi.org 】。这里有 GitHub 的官方说明：https://help.github.com/en/articles/using-a-custom-domain-with-github-pages 。此外，在 GitHub 中还可以开启 https 认证，这样你的每一个文档链接都会有把小绿锁了，GitHub 使用的是 Lets Encrypt 的证书，有效期 3 个月，不过别担心过期问题，GitHub 会自动更新的。开启了 https 认证后，哪怕使用 http 的链接访问，也会自动跳转的。那如果有人想把我的域名访问指向自己的 GitHub，是不是他在自己的仓库里面新建一个 CNAME 文件，并且填上我的域名就行了呢？其实不行，GitHub 是禁止这样做的。即使有人真的在自己的仓库里面新建了 CNAME 文件并且填写了我的域名，GitHub 是不认可的并且会给出警告。当然，如果我自己在 GitHub 中没有使用这个域名，别人当然可以使用。现在突然想到一个问题，我把自己的域名和域名服务器都暴漏了，会不会有人在 DNSPod 中把我的域名解析到其它地方去了【看起来所有的 DNSPod 的域名服务器都是一样的 2 台机器】，然后我就访问不了自己网站了，或者说流量变小了。我觉得 DNSPod 应该会禁止这种行为。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>个人站点</tag>
        <tag>绑定域名</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10 输入法简繁体快捷键与 IDEA 冲突]]></title>
    <url>%2F2018112301.html</url>
    <content type="text"><![CDATA[用了 2 个月的 Windows 10 系统（教育版），又安装了 IDEA 代码集成工具，开发的时候，发现每一次只要我使用快捷键 Ctrl + Shift + F 格式化代码后（主要作用就是代码对齐），不起作用，而且写中文注释时发现输入法的中文就被切换为了繁体，再来一次就被切换为了简体。到这里，我知道 IDEA 的快捷键与输入法的快捷键冲突了。解决方案 1、如前文描述，在写代码的过程中发现这个问题，并且看出是快捷键冲突的问题，接下来就要解决它。作为一名工程师，IDEA 的快捷键是因为使用习惯设置的，是写代码效率的保证，不可能更改的，任何与它有冲突的快捷键都要让步，那肯定是要更改输入法的快捷键的；2、信心满满，打开 搜狗输入法 的 属性设置 界面，找到 高级 选项，选择，可以看到里面有 快捷键 的相关配置；配置所有的快捷键 3、看了半天，也就这么几个快捷键配置，里面根本没有 简体 / 繁体 切换这一个配置选择，去搜索了一下其它资料，发现 简体 / 繁体 切换这一个快捷键是 Windows 10 系统内置的，默认就是 Ctrl + Shift + F，默认是给微软输入法使用的，某些版本的 Windows 10 系统有 bug，无法更改，哪怕卸载微软输入法，安装其它输入法也无效；4、我看了我的 Windows 10 系统版本，已经是新版本了，不会有那个 bug 出现了，所以要从系统设置入手了，应该有地方设置才对，查看了语言里面的设置信息，没找到，只能又返回到搜狗输入法里面，这时突然看到里面有一个 系统功能快捷键 选项；5、就是这里了，点进去，把 简繁切换 关闭（如果需要保留的话，更改快捷键即可），解决问题。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>Win10</tag>
        <tag>输入法</tag>
        <tag>快捷键冲突</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 博客静态资源压缩优化]]></title>
    <url>%2F2018112101.html</url>
    <content type="text"><![CDATA[使用 hexo-cli 生成的静态网页 html 文件，使用文本编辑器打开，可以看到内容中有大量的回车换行等空白符。尽管是空白符，但是也占据着空间大小，而且那么多，导致 html 文件偏大，网页加载时不仅浪费流量，而且还影响速度。同时，最重要的是对于手机端来说，静态页面 html 文件太大了的确不友好。所以要做优化，用术语说是压缩，其实目的就是在生成 html 文件时，尽量去除内容中多余的空白符，减小 html 文件的大小。此外，顺便也把 css 文件、js 文件一起压缩了。当前现象 为了简单起见，只是列举 html 文件来看现象，目前查看生成的 8 个 html 静态页面（为了具有对比性，不包含当前页面），大小为 314 K。打开其中一个 html 文件查看内容，可以看到很多回车换行符。接下来就是要想办法消除这些空白符。压缩方式选择 通过查看 hexo 官网（附上插件库：hexo 插件库 ），搜索资料了解别人的例子，发现有两种方式： 一种是先全局（-g 参数）安装 gulp 模块，根据压缩需求再安装需要的模块，例如 gulp-htmlclean、gulp-htmlmin、gulp-imagemin、gulp-minify-css、gulp-uglify，每个模块都有自己的功能，另外需要单独配置一个 js 脚本（放在站点根目录下），指明使用的模块，文件所在目录或者通配符文件名，然后每次使用 hexo generate 之后再使用 gulp 就可以压缩文件了。这种方式灵活度高，可以自定义，而且 gulp 的功能与 hexo 解耦，如果有其它静态文件，也可以使用 gulp 进行压缩。但是缺点也显而易见，门槛太高了，根据我的折腾经验，如果出了问题肯定要捣鼓半天，对于我这种零基础的人来说不够友好，我不选择；另一种是类似于 hexo 的一个插件，像其它插件或者主题一样，直接安装一个模块，在配置文件中配置你想要的压缩内容，在 hexo generate 的时候就可以实现压缩，无需关心具体流程，也不用配置什么脚本，非常容易，我选择这个，目前我看到有两个类似的插件：hexo-neat、hexo-filter-cleanup，用法都差不多，我选择前者，其实这些插件也是依赖于其它插件，把多种插件的功能整合在一起而已。安装配置 hexo-neat 插件其实是使用 HTMLMinifier、clean-css、UglifyJS 插件实现。 安装（由于网络不稳定因素，可能不是一次就成功，可以多试几次）1npm install hexo-neat --save站点配置 编辑站点的配置文件 &#95;config.yml，开启对应的属性 12345678910111213141516171819202122# 文件压缩，设置一些需要跳过的文件 # hexo-neatneat_enable: true# 压缩 htmlneat_html: enable: true exclude:# 压缩 cssneat_css: enable: true exclude: - '**/*.min.css'# 压缩 jsneat_js: enable: true mangle: true output: compress: exclude: - '**/*.min.js' - '**/jquery.fancybox.pack.js' - '**/index.js' 查看效果 在执行 hexo generate 的命令行中就可以看到压缩率输出。8 个 html 文件被压缩后，大小只有 206 K，和之前的 314 K 比少了 108 K，虽然只是简单的数字，也可以看到压缩效果不错。继续打开先前打开的那个 html 文件，可以看到整个 html 文档被合并成为了一行文本内容，不影响浏览器对 html 文件的解析展示，回车换行的空白符内容肯定没有了。但是这样对于 html 文件的可读性变差了，最好还是使用一些回车换行符的，还好这些 html 文件我不会去看，能接受目前的效果。踩坑记录1、由于牵涉到压缩文件，所以 hexo 生成静态文件的速度会比以前慢一点，但是可以接受。2、不要跳过 .md 文件，也不要跳过 .swig 文件，因为是在 hexo generate 阶段进行压缩的，所以这些文件必须交给 hexo-neat 插件处理，才能保证生成的 html 文件纯净。3、参考博客：1、2、3、4。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>建站</tag>
        <tag>代码压缩</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google 账号开启两步验证与应用专用密码]]></title>
    <url>%2F2018111901.html</url>
    <content type="text"><![CDATA[使用 Google 账号的都知道，带来了很多方便，不仅有强大的免费搜索服务，还有 Google 文档、云主机、云存储等各种服务，但是唯一的缺点是需要翻墙，让一些人望而却步，把很多人挡在了便利门外。本文是针对已经实现翻墙愿望，并在日常工作中会使用到 Google 账号的人，说不定可以给你带来一些冷知识，解决一些小问题。Google 账号的便利性 目前在日常工作与生活中，查找资料时，基本使用的都是 Google 搜索，并且使用非常好用的 Chrome 浏览器。其中我用的最多就是标签收藏，平时偶尔搜到什么有用的知识点或者需要反复查看的网页，来不及看完整理，就先把网页分类收藏了，以便日后查漏补缺。此时，利用 Chrome 浏览器的标签收藏功能，可以很方便地把一切网页收藏起来，并且可以很好地分类存放，清晰明了。可能有人说也有很多其它的工具可以做到这一点，不久收藏吗？但是我觉得还是利用 Chrome 浏览器自带的这个功能比较好，再配合 Google 账号，就可以达到同步更新的效果了，公司的电脑、家里的电脑，只要都登录了 Google 账号，所有收藏的标签都可以实时同步。而且，所有的浏览记录、搜索历史、记住的账号密码等等，都可以同步，跨机器使用也很方便。再配合 Chrome 浏览器的插件，对收藏的网页搜索起来非常方便。Google 账号开启两步验证 为了安全起见，最好给 Google 账号开启两步验证，可以选择绑定手机号、启用身份验证器、安全密钥等方式，为了方便，我选择了绑定手机号。开启两步验证后，在陌生的设备上登录 Google 账号（包括 Google 自家的各种应用，例如邮件、YouTube 等）需要验证码的二次验证，当然，如果把设备设置为可信任的设备，则不需要每次都重复输入验证码。开启的方式非常简单，登录 Google 账号，在” 登录与安全 “中有” 两步验证 “的开启选项，选择自己需要的方式，继续即可。启两步验证 1启两步验证 2如果使用” 身份验证器 “的方式，还需要在手机上安装一个” 身份验证器 “应用，校准时间后，每隔 30 秒更新验证码，登录账号时需要使用当前的验证码，并且在有效期内完成登录的操作，否则验证码过期，需要使用新的验证码，类似于手机收到的验证码只有 1 分钟一样。同时，如果使用 Google 邮箱账号注册了其它平台的账号，例如注册了 Twitter，注册了 Facebook，为了安全起见也可以使用” 身份验证器 “的方式，一种验证方式管理着多种账号的安全。开启两步验证后带来的问题 我遇到的问题之一就是自己手机的邮件客户端无法登录 Google 邮箱了，我使用的时第三方邮件客户端，总是提示我密码错误，其实密码没有错误，是因为 Google 账号开启两步验证后，邮箱的登录也需要对应方式的验证，但是第三方邮件应用并没有做这个验证，所以无法登录。本来是想着单独把 Google 邮箱的两步验证关闭，但是找了半天设置选项也没有找到，看来 Google 账号已经是一个大统一的账号，不允许单独设置涉及安全性的信息，可以理解。同理，使用其它应用客户端也会遇到相同的问题，当然，Google 官方解释说明也解释了有部分设备不需要关注这个问题，其它大部分设备或者应用还是要受到影响的。见：使用应用专用密码登录 此时，需要使用” 应用专用密码 “或者在手机上开发一个” 具有账号访问权限的应用 “用来代理整个 Google 的账号访问。问题的解决方法 应用专用密码方式的使用 1、在 Google 账号的登录和安全中，可以找到” 应用专用密码 “这个选项：2、点击进入后，可以看到选择应用与选择设备，由于我使用的是一种不知名的 Android 手机，所以官方选项中没有可以选择的，只好自定义一种，随便起一个名字标识即可。3、选择完成后，会生成一串 16 位的密码，这个密码就可以在其它设备上登录的时候使用，不需要使用原来的密码，也不需要使用 Google 验证码。4、在使用过程中还可以看到设备的情况。 具有账号访问权限的应用的使用 这种方式就是手机本身有一个后台应用，代理了 Google 账号的一切请求，把信息转发到本地应用（比如 Chrome 浏览器就是这样一个应用，只不过是官方开发的，只要登录了 Google 账号，邮件、YouTube、搜索、Play、相册、日历等等这些应用同步一起使用，不需要额外再登录，这也是我使用 Chrome 浏览器的原因。），所以后台应用如果知道了 Google 账号的用户名、密码，就可以代理所有 Google 应用的请求，无需关心 应用专用密码了。我发现锤子手机的 Smartisan OS 系统（v6.0.3，Android 版本 7.1.1）对邮件就做了这个后台应用 Smartisan Mail，所以在使用内置的邮件客户端时，即使开启了两步验证，也无需关心验证码的问题（第一次登录还是需要验证的）。下面截图则是一步一步设置：1、在邮件客户端设置中添加 Google 邮箱2、输入 Google 账号密码（也是邮箱密码）3、输入验证码（由于开启了两步验证，一定需要），此时切记勾选” 在此计算机上不再询问 “，才能保证邮件客户端正常收发 Goole 邮件，否则不行。4、允许，可以看到 Smartisan Mail 想要访问 Google 账号5、点开 Smartisan Mail，可以看到开发者信息，里面其实设置了代理转发6、此外，在登录成功后，在 Google 账号的登录和安全中，可以看到具有账号访问权限的应用：]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>Google 账号</tag>
        <tag>两步验证</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Vultr 搭建 Shadowsocks（VPS 搭建 SS）]]></title>
    <url>%2F2018111601.html</url>
    <content type="text"><![CDATA[本文讲述通过 Vultr 云主机搭建 Shadowsocks 的过程，图文并茂，非常详细。当然，关于云主机很多 VPS 都可以选择，根据价格、配置、地区等可以自由选择【但是我还是推荐 Vultr，因为它很灵活】。主机购买 使用 Vultr 的云主机，选择洛杉矶地区的或者日本的服务器，我的推广链接【可以获取 10 美元的代金券，只要充值 10 美元就能使用】：我的 10 美元推广链接 ，官网：Vultr。 这里再多说点，如果使用上面的推广链接注册 Vultr 帐号，可以获取 10 美元的代金券，需要在 30 天之内使用，使用的条件就是充值 10 美元以上的钱。例如充值 10 美元就会获取 20 美元的帐号余额，这些钱如果购买 3.5 美元的主机可以使用半年了，挺划算的。此外还有一个限时的大优惠，如果准备长期使用 Vultr，肯定要充值多一点，我这里有一个限时的推广链接：我的 25 美元推广链接 ，可以获取 25 美元的代金券，使用条件就是充值 25 美元以上的金额。假如充值了 25 美元，总共获取 50 美元入账，购买 3.5 美元的主机可以使用 14 个多月，适合长期使用 Vultr 的。而且，Vultr 有一个好处就是主机的费用并不是按照月份扣除的，而是按照天扣除的，每天扣除的费用是 月租 / 30，例如你的主机只用了 10 天，然后销毁不用了，实际只会扣除月租 1/3 的钱，这种方式很是灵活，哪怕主机的 ip 地址被屏蔽了也可以销毁重新生成一个，并不会浪费钱。价格有 &#36;2.5 / 月（只有 IP6 地址）、&#36;3.5 / 月、&#36;5 / 月等等，更贵的也有，一般选择这三个中的一个就够用了，但是要注意便宜的经常售罄，而且最便宜的只支持 IP6，慎用。大家如果看到没有便宜的主机了不用着急，可以先买了贵的用着，反正费用是按照天数扣除的，等后续发现便宜的套餐赶紧购买，同时把贵的主机给销毁，不会亏钱的。Shadowsocks 服务安装 云主机选择 CentOS 7 x64 版本，全程操作使用 Linux 命令（注意，如果选择其它系统命令会不一致，请自己查询，例如：Debian/Ubuntu 系统的安装命令更简洁，先 apt-get install python-pip，再 pip install shadowsocks 即可）。注意如果安装了防火墙（更安全），需要的端口一定要开启，否则启动 Shandowsocks 会失败。安装组件：123yum install m2crypto python-setuptoolseasy_install pippip install shadowsocks过程如图：配置服务器参数：1vi /etc/shadowsocks.json如下列出主要参数解释说明 参数名称 解释说明 server 服务器地址，填 ip 或域名 local_address 本地地址 local_port 本地端口，一般 1080，可任意 server_port 服务器对外开的端口 password 密码，每个端口可以设置不同的密码 port_passwordserver_port + password ，服务器端口加密码的组合timeout 超时重连 method 加密方法，默认：“aes-256-cfb”fast_open开启或关闭 TCP_FASTOPEN，填 true /false，需要服务端支持 配置多端口信息（多个帐号，多人也可用）：12345678910111213&#123; "server": "你的 IP 地址"（例如：192.168.0.1）, "local_address": "127.0.0.1"（默认值）, "local_port":1080（默认值）, "port_password"（开启的端口和密码，自己按需配置，确保端口打开并不被其它程序占用）: &#123; "1227": "pengfeivpn1227", "1226": "pengfeivpn1226", "1225": "pengfeivpn" &#125;, "timeout":300（超时时间，默认值）, "method":"aes-256-cfb"（加密方法，默认值）, "fast_open": false&#125;配置多端口信息（纯净版本，更改 ip、端口等信息直接复制使用）：12345678910111213&#123; "server": "x.x.x.x", "local_address": "127.0.0.1", "local_port":1080, "port_password": &#123; "1227": "vpn1227", "1226": "vpn1226", "1225": "vpn" &#125;, "timeout":300, "method":"aes-256-cfb", "fast_open": false&#125;配置一个端口信息（只有一个帐号，多人也可用）：12345678910&#123; "server":"你的 IP 地址"（例如：192.168.0.1）, "server_port":1225（唯一的端口）, "local_address":"127.0.0.1", "local_port":1080, "password":"pengfeivpn"（唯一的密码）, "timeout":300, "method":"aes-256-cfb", "fast_open":false&#125;配置一个端口信息（纯净版本，更改 ip、端口等信息直接复制使用）：12345678910&#123; "server":"x.x.x.x", "server_port":1225, "local_address":"127.0.0.1", "local_port":1080, "password":"vpn", "timeout":300, "method":"aes-256-cfb", "fast_open":false&#125;Shadowsocks 性能优化：另外还有很多参数可以优化性能，例如设置连接数、字节大小等，比较复杂，在此略过。防火墙安装：123456789101112# 安装防火墙 yum install firewalld# 启动防火墙 systemctl start firewalld# 查看目前已经开启的端口号 firewall-cmd --list-ports# 端口号是你自己设置的端口 firewall-cmd --permanent --zone=public --add-port=1225/tcpfirewall-cmd --permanent --zone=public --add-port=1226/tcpfirewall-cmd --permanent --zone=public --add-port=1227/tcp# 重载更新的端口信息 firewall-cmd --reload过程如图：启动 Shadowsocks：1234# 后台运行 ssserver -c /etc/shadowsocks.json -d start# 调试时使用下面命令，实时查看日志 ssserver -c /etc/shadowsocks.json过程如图：客户端使用 Windows 平台使用 下载 Windows 平台的客户端，下载地址：shadowsocks-windows GitHub，shadowsocks 官网 ，直接解压放入文件夹即可使用，不需要安装。 但是注意配置内容（端口、密码、加密协议等等），另外注意有些 Windows 系统缺失 Shadowsocks 必要的组件（.NET Framework），需要安装，官网也有说明。配置示例：实际上下载程序后，无需安装，直接解压即可，解压后只有一个 exe 文件，双击即可运行（最好放入指定文件夹中，便于程序管理和升级）。第一次启动，需要设置参数，如上图所示，至少配置一台机器，另外还可以设置开机启动，以后不用重新打开。此外，如果有更新版本的程序，会放在 ss_win_temp 文件夹下，直接解压后复制替换掉当前的 exe 文件即可；如果文件夹中有 gui-config.json、statistics-config.json 这 2 个文本文件，它们是程序的配置以及前面设置的翻墙配置，不能删掉；如果使用系统代理的 PAC 模式（推荐使用），会生成 pac.txt 文本文件，存放从 GFWList 获取的被墙的网址，必要时才会通过翻墙代理访问，其它正常的网址则直接访问，这样可以节约流量。如果有切换代理的需求，搭配浏览器的插件来完成，例如 Proxy SwitchyOmega 就可以。关于启动系统代理并使用 PAC 模式（根据条件过滤，不满足的直连），如果是入门级别使用，直接设置完就可以用了，不用再管其它设置，切记要定时更新 GFWList 列表，因为如果某些网站最近刚刚被屏蔽，不在以前的 HFWList 列表里面，就会导致无法连接，只有及时更新才能正常连接。但是还有一种极端情况，就是某些网站 GFWList 迟迟没有收录，怎么更新都不会起作用，别着急，此时可以使用用户自定义规则，模仿 GFWList 填写自己的过滤规则，即可实现灵活的切换，使用用户自定义规则后会在安装文件夹中生成 user-rule.txt 文本文件。其实，PAC 模式的原理就是根据公共的过滤规则（收集被屏蔽的网站列表），自动生成了一个脚本文件，把脚本文件绑定到浏览器的代理设置中，使浏览器访问网站前都会运行这个脚本，根据脚本的结果决定是直接访问还是通过本地代理访问，脚本在 Shadowsocks 的 PAC 设置中可以看到，浏览器的设置信息可以在代理设置中看到（浏览器在 Shadowsocks 开启系统代理的时候会自动设置代理，无需人工干预）。由此可以得知，通过本机访问网络，决定是直接连接还是通过 Shadowsocks 代理连接的是 PAC 脚本，并不是 Shadowsocks 本身，所以如果使用系统的 Ping 命令访问 www.google.com 仍然是不能访问的，因为直接 Ping 没有经过 PAC 脚本，还是直接连接了，不可能访问成功。除了浏览器之外，如果其它程序也想访问被屏蔽的网站（例如 Git、Maven 仓库），只能通过程序自己的代理设置进行配置，完成访问的目的。（如果放弃 PAC 模式，直接使用全局模式，则不需要配置任何信息，本机所有的网络请求会全部经过翻墙代理，当然这样做会导致流量消耗过大，并且国内的正常网站访问速度也会很慢）获取到的 PAC 脚本地址为：http://127.0.0.1:1080/pac?t=20181118030355597&amp;secret=qZKsW49fDFezR4jJQtRDhUVPRqnFu6JC3Nc+vtXDb0g=以上是查看 Chrome 浏览器和 IE 浏览器的代理设置信息，对于 Microsoft Edge（Windows 10 自带）浏览器来说，界面有点不一样，在设置 -&gt; 高级 -&gt; 代理设置里面。此外，如果在浏览器中有更灵活的需求应用， 例如在设置多个代理的情况下，针对公司内网是一套，针对指定的几个网站是一套，针对被屏蔽的网站是一套，剩余的直接连接。在这种情况下仅仅使用代理脚本就不能完成需求了，显得场景很单一，当然也可以把脚本写的复杂一点，但是成本太高，而且不方便维护更新。这个时候就需要浏览器的插件出场了，例如在 Chrome 下我选择了 SwitchyOmega 这个插件，可以设置多种情景模式，根据实际情况自由切换，非常方便。我设置了三种情景模式：hdpProxy（公司内网）、shadowSocks（翻墙代理）、auto switch（根据条件自动切换），前面两种情景模式直接设置完成即可，最后的 auto switch 需要配置得复杂一点，根据正则表达式或者通配符指定某些网站的访问方式必须使用 hdpProxy 代理，另外其它的根据规则列表 （https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt ，和 Shadowsocks 的 GFWList 列表类似）必须通过翻墙代理，剩余的才是直接连接。当然，此时就不需要把 Shadowsocks 设置为系统代理了，保持 Shadowsocks 后台运行就可以了。Android 平台使用Android 平台的安装使用方法就非常简单了，分为 安装、配置、启动 这 3 个步骤，没有其它多余的操作。安装 下载 Android 平台的客户端，一般我们都称之为 影梭 ，在应用商城是找不到的，因为通不过审核，所以只能去官网下载，下载地址：shadowsocks 官网 。切记，千万不要去第三方网站下载，因为下载的安装包可能带有其它的应用，导致给你的手机安装了一堆软件。当然，如果你连官网都不信，可以自己下载源代码，自己打包 apk 文件，也是可以的，懂一点点 Android 开发就行了，源代码全部是开源的，放在了 GitHub 上面：https://github.com/Jigsaw-Code/outline-client/ 。 下载完 apk 文件，安装也就是和安装普通的应用一样，需要注意的是有些 Android 手机会禁止外部来源的 app（不是从应用商店下载安装的）安装，所以需要同意一下，也就是 信任此应用 ，才能顺利完成安装。配置 需要配置的内容和 Windows 平台的一样，把那些必要的参数填进去就行了，其它内容不需要关心。例如我这里配置了 ip、端口、密码、加密方式等。启动 启动只要点击右上角的灰色圆形按钮，里面有一个小飞机，大概等待几秒钟，就会变绿，表示已经连接上 VPN 了，此时手机就可以连接被屏蔽的网站了。唯一的缺点就是，不支持设置类似于 PAC 规则的站点切换（ 路由 默认设置的是绕过中国大陆地址），因为只要一连上 VPN，手机上所有的国外连接都是走 VPN，会导致连某些正常的国外的网站也会慢一点，还浪费 VPS 的流量。当然，如果是在 WIFI 的环境下，通过 Android 系统的网络代理设置也可以设置一些类似于 PAC 的规则，就不细说了。启动后，还可以看到流量发送接收统计信息。在手机的设置里面也可以看到 VPN 的开启 踩坑记录 1、在云主机安装服务端后，又安装了防火墙，但是没有开启 Shadowsocks 需要的端口，导致启动 Shadowsocks 总是失败，但是报错信息又是 Python 和 Linux 的，看不懂，搜索资料也搜不到，后来重装，并且想清楚每一步骤是干什么的，会造成什么影响，通过排除法找到了根本原因。2、在 Windows 平台使用的时候，安装了客户端，也安装了 .NET Framework 组件，配置信息确认无误，但是就是上不了外网，同样的操作使用 Android 客户端却可以，所以有理由怀疑是自己的主机问题。后来，重启系统，检查网络，关闭杀毒软件，还是不行，后来，依靠搜索，找到了是杀毒软件 Avast 的问题，扫描 SSL 连接被开启了，大坑，关闭即可。3、参考： 梯子搭建4、本来以为 Shadowsocks 的系统代理中的 PAC 模式会在接收到网络请求的基础上进行过滤，即 Shadowsocks 能控制所有的网络请求进行过滤判断，然后该翻墙的翻墙，该直连的直连，后来发现不是的，浏览器插件 SwitchyOmega 设置代理规则后，PAC 脚本就不会生效了，全部使用 Shadowsocks 代理的网站都直接翻墙，不会有任何判断了，导致优酷视频消耗了大量的流量，而且速度还很慢。另外，为了保证国内的网站不是经过翻墙代理，能直接连接，就不能使用全局模式。5、使用插件 SwitchyOmega 的过程中，一开始是自己整理一些规则，而没有使用https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt 列表规则，导致配置信息很多，而且自己看着头都大，不好维护与查看，后来就发现了列表规则，解放了劳动力。6、解决了 Chrome 浏览器的收藏跨平台自动更新同步的问题，以前在三台电脑之间添加取消收藏，总是不能更新同步，需要手动开启系统代理设置全局模式（Chrome 浏览器的收藏同步功能被屏蔽了，我又不知道 url 是什么），等一会更新同步之后再关闭（防止其它场景也翻墙了）。目前使用规则列表，收藏可以自动更新同步了，不需要手动来回切换了，也不用担忘记同步的情况了。]]></content>
      <tags>
        <tag>Shadowsocks</tag>
        <tag>Vultr</tag>
        <tag>Avast</tag>
        <tag>VPS</tag>
        <tag>影梭</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Aria2 Web 管理面板使用]]></title>
    <url>%2F2018110902.html</url>
    <content type="text"><![CDATA[如果使用浏览器默认的下载器，下载百度云的文件速度大多数情况下不理想，而且大文件不能下载，同时非客户端下载文件又会严重限速。如果使用插件和脚本配合，突破了百度云的限制，可以下载大文件，但是遇到下载中途失败的情况，有时候不能继续下载，还要重头来，会让人崩溃，体验非常不友好。那么我前面一篇文章描述了这个过程，同时给出了几个解决方案，本文就记录一下其中涉及到 Aria2 的使用以及其中一种管理面板【YAAW for Chrome】的使用。插件的安装 这款插件的全名是 Yet Another Aria2 Web Frontend，简称 YAAW，可以去谷歌的插件商店获取：YAAW 。当然，如果你没有翻墙，是打不开这个链接的，你可以选择去国内的镜像站点下载，例如：chrome-extension-downloader ，或者 getcrx ，至于怎么使用可以参考本博客的 关于页面 。为了方便你们，我把这款插件的插件 id 也放出来：dennnbdlpgjgbcjfgaohdahloollfgoc。 详细的安装过程就不赘述了，不是重点，一般也就是在线安装或者下载 crx 离线文件安装，都不是困难的事情。插件的使用 为了使用这款插件，还需要 Aria2 、baiduexporter 这 2 款工具的协助。Aria2 在前文已经描述过了，是用来下载文件的后台程序，本来直接使用它下载文件就可以了，但是奈何不方便批量下载以及任务管理，所以需要搭配 YAAW 来使用。baiduexporter 这款插件是用来转换百度云盘文件的链接的，转为 Aria2 可以直接使用的方式。这 2 款工具的安装可以参考上一篇博客，在这里仅给出对应的链接和插件 id：Aria2 安装包 、Aria2 介绍 、baiduexporter 插件 、baiduexporter 插件的 id【jgebcefbdjhkhapijgbhkidaegoocbjj】。Aria2 一切准备就绪后，我先在后台启动 Aria2 进程。切记要开启 RPC 模式，否则 YAAW 插件无法监控后台的下载任务，也就无法进行管理了。还要开启断点续传，这样才能在下载出现异常中断之后，还能接着上次的进度继续下载，节约时间。启动 Aria2 进程 YAAW 如果一开始直接打开 YAAW 插件，会显示错误：Error: Internal server error，其实就是没有找到 Aria2 进程。那它们是怎么通信的呢，其实就是依靠一个端口，这个端口我们使用默认的就行了【默认就是 6800】，否则要在 YAAW 和 Aria2 两边都要设置，并且保持一致。YAAW 插件设置端口 Aria2 设置端口 当然，从上面的截图中我们可以看到，还可以设置一些其它参数，例如：自动刷新时间、限速大小、用户代理、基础目录。然而，这些参数都是全局性的，我们没有必要设置，因为等到真正需要下载文件的时候，还可以重新设置，实际应用中不一定每次下载的设置都一致，所以放在每次下载文件的时候重新设置显得更灵活。baiduexporterbaiduexporter 的安装就比较简单了，就是一个浏览器插件而已，安装后打开即可。三者结合协同工作 打开我的百度云网盘，随意找一个文件测试 细心的人可以发现，在选中一个文件后，在本来的 下载 旁边多了一个选择项 导出下载 ，如果移动鼠标到上面， 导出下载 会展开下拉列表，出来 3 个选项：Aria2 RPC、文本导出、设置。如果我选择第一个 Aria2 RPC，则会直接调用后台的 Aria2 进程，直接帮我下载文件了，不需要我使用 Aria2 的原生命令加上必要的参数去启动一个下载任务了。而第二个文本导出，其实就是导出这个百度云文件的 Aria2 完整的命令，这样我们就可以复制使用，在后台起一个 Aria2 的下载任务了【有了第一种的 RPC 方式，更为方便快捷，肯定不用这种方式】。而设置则是导出的参数设置，这个没什么好说的，一般使用默认的就行了。好，接下来重点来了，必将能让你感受到什么叫做方便快捷。前面说那么多操作步骤，是不是发现还没 YAAW 插件什么事情，别着急，接下来的描述都是它的。直接打开插件：有没有发现什么，刚才下载的任务已经在这里可以看到了。不仅如此，还可以在这个控制面板中随意操管理任务：暂定、开始、删除，还可以看到下载的网速和进度百分比，多方便，这已经近似于一个下载管理软件了【虽然很简陋，但是比直接操作 Aria2 后台方便多了】。刚才使用百度云下载文件的时候，是直接在 导出下载 中一键勾选的，很方便，但是如果是别人发给你一个 Aria2 能下载的链接，你该怎么办呢？是使用 Aria2 后台起一个下载任务，还是怎么样，因为此时没有像下载百度云盘文件那么方便的按钮给你选择。别担心，此时又要使用 YAAW 的另外一个功能了：创建任务，也就是相当于在迅雷中创建一个下载任务一样。在 YAAW 插件中有一个 ADD 按钮。点击后弹出一个配置的对话框，在里面填上对应的参数就行了。不知道大家有没有发现，这里面的参数和使用 baiduexporter 插件的 导出下载 里面的 文本导出 导出的文本里面的一些内容很是相似，另外需要自己设置一下文件名字、文件下载目录、用户代理等信息就行了。这里就不再具体演示了。当然，以上只是使用百度云网盘作为示例，大家比较容易理解，演示给大家，纯粹是为了抛砖引玉，其实 Aria2 还支持更多的协议，大家可以自行参考 Aria2 的官方网站。如果下载磁力链接的东西，有时候迅雷更快，因为毕竟它是 p2p 的，可以加速，大家还是要看情况使用。温馨提示 除了突破限速的场景，我还发现一个场景，那就是资源文件被禁用的时候，也可以突破禁用，自由下载资源文件，例如盗版电影的下载。在不久后的 2019 春节档，竟然出现了史无前例的电影高清资源泄漏的事件。重点包括《新喜剧之王》、《疯狂的外星人》、《流浪地球》、《飞驰人生》这几部，电影刚上映 3 天，就有高清资源【画质比较好，不是一般的枪版】流出来了，大家可以下载。接着电影官网就鼓励大家进行举报封禁，导致各大下载软件都屏蔽了资源，显示由于版权问题而禁止下载。这时候各种开源的下载器就派上用场了，例如 Aria2 就是一个，但是缺点就是网速可能没有那么快，因为没有 p2p 加速机制。但是我仅仅是为了学习，测试一下也无妨。可以看到，使用迅雷下载是被禁止的。把 torrent 文件保存下来，转而使用 Aria2 下载，为了方便直接在 YAAW 插件上面建任务，直接上传 torrent 文件即可，其它参数则使用默认的。可以看到，下载速度可以达到 2M 以上。以上下载电影实践中，当然目的只是为了学习使用，给你们演示一下而已，不是提倡下载盗版电影，在下载一些被屏蔽的资源或者被限速的情况下，可以使用这种方式试试。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>aria2</tag>
        <tag>百度云下载</tag>
        <tag>百度云限速</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Aria2 加速百度网盘下载]]></title>
    <url>%2F2018110901.html</url>
    <content type="text"><![CDATA[在日常工作和生活当中，应该有不少人自愿或者被迫使用百度网盘，一是因为其它厂商基本都关停了网盘服务；二是在获取互联网资料的时候，基本都是先获取到百度网盘链接，然后自己再去下载；三是有时候想备份一些文件，也只能想起来有百度网盘可以使用。这样的话，慢慢地总是会碰到需要百度网盘的时候，我们暂且不考虑这家企业的口碑怎么样，百度网盘这个产品本身还是不错的：有免费的大量空间，使用人群多，分享获取资料方便。但是，产品让人诟病的地方也有几个，而且由此造成的用户体验非常差，大家骂声一片。本文就详细讲述百度网盘这个产品让人诟病的地方以及可以使用技术方式绕过它，从而提升自己的体验。当然，如果你的钱到位的话，直接充值会员吧，可以消除一切不好的使用体验，同时也免去了阅读本文的时间。使用中遇到的问题 本文是针对不充会员的免费用户群体的，在 Windows 平台安装，在 Chrome 浏览器中使用。下载速度太慢，慢到反人类 让人诟病的问题之一是下载速度太慢了，对于免费用户基本维持在几 KB/s 到十几 KB/s 之间，也就是说如果你想下载一部 1 G 大小的电影，按照 1000 M 计算，下载速度按照 10 KB/s 算（取这样的数值方便后续计算），下载完需要 1000 个 100 秒，也就是约等于 27.78 个小时（10 万秒），所以在下载列表中经常看到下载任务还需要大于一天才能完成，这怎么让人受得了，不骂才怪呢！但是只要充值会员，下载速度基本就暴增，可以完全利用宽带的带宽，例如 100 M 的宽带，下载速度可达 12.8 MB/s，哪怕只是 10 M 的宽带，下载速度也能到 1.28 MB/s。因此，百度网盘客户端对于免费用户限制速度限制得太严重了，不充值会员根本没法使用。而且，有时候勉强能使用的时候，经常会弹出会员试用 300 秒的提示，只要选择了，下载速度立马飞速提升，300 秒后又急速下降，经常下降到只有 3.14 KB/s，让人抓狂。网页版限制下载大文件，强迫安装百度网盘客户端 既然百度网盘客户端做了下载速度限制，那么大多数人会想到选择使用浏览器直接下载，同时又可以免去安装百度网盘客户端的麻烦，浏览器的下载速度通常在几百 KB/s，不会像百度网盘客户端那样特别地慢。但是，直接使用网页版的百度网盘下载文件，对文件大小有限制，太大的文件会被网页拦截，下载不了，而是弹出安装百度网盘客户端的提示，这样又回到了原点，因为如果用百度网盘客户端下载速度被限制了。解决问题 使用 aria2 突破线程数限制、下载速度限制 简介 Aria2 是一个多平台轻量级的下载工具，支持 Http、Ftp、BitTorrent、Web 资源等多种格式，使用命令行启动任务，更多具体信息查看官网说明：Aria2 介绍。这种工具可以最大程度利用你的网络带宽，实际上你可以自由配置，包括线程数、网络传输速度、RPC 端口、断点续传是否开启等。 安装 去官网下载安装包：Aria2 安装包 ，我的 Widows 系统 64 位，选择对应的安装包下载。 下载完成后，得到一个 zip 格式的文件，其实直接解压即可，不需要安装，解压后会得到一系列文件，为了方便管理，都放在 aria2 文件夹下面，再复制到程序对应的目录。其中，有一个 .exe 文件，就是运行任务时需要的文件。此外，为了方便起见，把 .exe 文件的路径配置到系统的环境变量中去，这样在任何目录都可以执行 aria2 命令了；如果不配置则只能在 aria2 目录中执行相关命令，否则会找不到程序。配置 1、如果单纯使用命令行启动下载任务，可以把参数信息直接跟在命令后面，临时生效，也就是参数只对当前下载任务有效。显然，这样做很麻烦，每次都是一长串的命令，而且当任务非常多的时候也无法管理，所以不建议使用这种方式。当然，如果只是测试折腾一下，或者也不经常使用，只是偶尔下载一个东西，还是用这种方式比较简介，不用管其它复杂的配置，不用管插件的安装。 单命令行启动任务示例，从电影天堂下载《一出好戏》这部电影。如果下载百度网盘的文件，需要使用 baiduexporter 插件生成 url，生成方式见后续步骤。1234567aria2c.exe -c -s32 -k32M -x16 -t1 -m0 --enable-rpc=true 下载 url 取值 -t1 表示的是每隔 1 秒重试一次 -m0 表示的是重试设置 此外，下载 url 中会包含 --header 的信息：User-Agent、Referer、Cookie、url 理论上 User-Agent、Referer 应该时固定的，Cookie、url 每次会生成不一样的 User-Agent: netdisk;5.3.4.5;PC;PC-Windows;5.1.2600;WindowsBaiduYunGuanJiaReferer: http://pan.baidu.com/disk/home这里再给一个完整的下载命令示例：1aria2c -c -s256 -k2M -x256 -t1 -m0 --enable-rpc=true -o &quot;pyspark-part1.zip&quot; --header &quot;User-Agent: netdisk;5.3.4.5;PC;PC-Windows;5.1.2600;WindowsBaiduYunGuanJia&quot; --header &quot;Referer: http://pan.baidu.com/disk/home&quot; --header &quot;Cookie: BDUSS=FFzb2s3Z2NRcnlTRE00WkxLYn5jTzhLdXktflVYbWprdXRpZm5EQ1FnYXlyTzFaSVFBQUFBJCQAAAAAAAAAAAEAAADYoS0veWVhckxQRjEzMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALIfxlmyH8ZZb; pcsett=1506245668-3f7c157ceb2130e195638efdf62944aa&quot; &quot;https://pcs.baidu.com/rest/2.0/pcs/file?method=download&amp;app_id=250528&amp;path=%2FQQ% E7% BE% A4% E5%90%88% E4% B9% B0% E5% A4% A7% E6%95% B0% E6%8D% AE% E8% A7%86% E9% A2%91%2Fxtwy% E4% B9%8Bpyspark% E8% A7%86% E9% A2%91%2F% E5% AD% A6% E5% BE%92% E6%97% A0% E5% BF% A7pyspark% E8% AF% BE% E7% A8%8Bpart1.zip&quot;2、如果是后台启动，通过其它管理插件来创建下载任务，则直接使用配置文件，文件名称为 aria2.conf，并在启动 aria2 时指定配置文件的位置。这样做的好处是使用一个配置文件就可以指定常用的参数配置，不用更改，每次下载文件前启动 aria2 即可。配置文件可选项如下，例如下载文件存放位置、是否开启 RPC、是否开启断点续传，具体更为详细的内容请参考文档：Aria2 配置信息文档 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394## '#' 开头为注释内容，选项都有相应的注释说明，根据需要修改 #### 被注释的选项填写的是默认值，建议在需要修改时再取消注释 #### 文件保存相关 ### 文件的保存路径 (可使用绝对路径或相对路径), 默认：当前启动位置 dir=E:\\aria2download\\# 启用磁盘缓存，0 为禁用缓存，需 1.16 以上版本，默认：16Mdisk-cache=32M# 文件预分配方式，能有效降低磁盘碎片，默认:prealloc# 预分配所需时间: none &lt; falloc &lt; trunc &lt; prealloc# NTFS 建议使用 fallocfile-allocation=none# 断点续传 continue=true## 下载连接相关 ### 最大同时下载任务数，运行时可修改，默认：5max-concurrent-downloads=32# 同一服务器连接数，添加时可指定，默认：1max-connection-per-server=5# 最小文件分片大小，添加时可指定，取值范围 1M -1024M, 默认：20M# 假定 size=10M, 文件为 20MiB 则使用两个来源下载；文件为 15MiB 则使用一个来源下载 min-split-size=16M# 单个任务最大线程数，添加时可指定，默认：5split=32# 整体下载速度限制，运行时可修改，默认：0#max-overall-download-limit=0# 单个任务下载速度限制，默认：0#max-download-limit=0# 整体上传速度限制，运行时可修改，默认：0max-overall-upload-limit=1M# 单个任务上传速度限制，默认：0#max-upload-limit=1000# 禁用 IPv6, 默认:falsedisable-ipv6=false## 进度保存相关 ### 从会话文件中读取下载任务 input-file=aria2.session# 在 Aria2 退出时保存 ` 错误 / 未完成 ` 的下载任务到会话文件 save-session=aria2.session# 定时保存会话，0 为退出时才保存，需 1.16.1 以上版本，默认：0#save-session-interval=60## RPC 相关设置 ### 启用 RPC, 默认:falseenable-rpc=true# 允许所有来源，默认:falserpc-allow-origin-all=true# 允许非外部访问，默认:falserpc-listen-all=true# 事件轮询方式，取值:[epoll, kqueue, port, poll, select], 不同系统默认值不同 #event-poll=select# RPC 监听端口，端口被占用时可以修改，默认：6800#rpc-listen-port=6800# 设置的 RPC 授权令牌，v1.18.4 新增功能，取代 --rpc-user 和 --rpc-passwd 选项 #rpc-secret=mivm.cn# 设置的 RPC 访问用户名，此选项新版已废弃，建议改用 --rpc-secret 选项 #rpc-user=&lt;USER&gt;# 设置的 RPC 访问密码，此选项新版已废弃，建议改用 --rpc-secret 选项 #rpc-passwd=&lt;PASSWD&gt;## BT/PT 下载相关 ### 当下载的是一个种子 (以.torrent 结尾) 时，自动开始 BT 任务，默认:truefollow-torrent=true# BT 监听端口，当端口被屏蔽时使用，默认：6881-6999listen-port=51413# 单个种子最大连接数，默认：55#bt-max-peers=55# 打开 DHT 功能，PT 需要禁用，默认:trueenable-dht=true# 打开 IPv6 DHT 功能，PT 需要禁用 #enable-dht6=false# DHT 网络监听端口，默认：6881-6999#dht-listen-port=6881-6999# 本地节点查找，PT 需要禁用，默认:false#bt-enable-lpd=true# 种子交换，PT 需要禁用，默认:trueenable-peer-exchange=true# 每个种子限速，对少种的 PT 很有用，默认：50K#bt-request-peer-speed-limit=50K# 客户端伪装，PT 需要 peer-id-prefix=-TR2770-user-agent=Transmission/2.77# 当种子的分享率达到这个数时，自动停止做种，0 为一直做种，默认：1.0seed-ratio=0.1# 强制保存会话，即使任务已经完成，默认:false# 较新的版本开启后会在任务完成后依然保留.aria2 文件 #force-save=false# BT 校验相关，默认:true#bt-hash-check-seed=true# 继续之前的 BT 任务时，无需再次校验，默认:falsebt-seed-unverified=true# 保存磁力链接元数据为种子文件 (.torrent 文件), 默认:false#bt-save-metadata=true 配置完成后在启动 aria2 时指定配置文件的位置即可，例如我把 aria.conf 与 aria2c.exe 放在同一个文件夹下，则启动时直接指定 1aria2c.exe --conf-path=aria2.conf 当然，这样做只是启动了 aria2，并没有开始创建下载任务，不像单个命令行那样简单，直接设置参数就起任务了。接下来还需要浏览器插件的配合，才能保证下载任务的创建与监控，虽然配置步骤麻烦一点，但是使用起来更为方便。为了避免启动时还要输入命令行，在 Windows 平台下可以写一个 bat 脚本，每次双击脚本即可，以下脚本内容供参考：12@echo off &amp; title Aria2aria2c.exe --conf-path=aria2.conf使用 1、使用命令行启动单个任务无需多做介绍，直接敲下命令行，等待文件下载就行了。如果需要连续下载多个文件，则唯一的做法就是多敲下几个命令，多等待而已。因此，这种方式不适合任务数量多的情况，那这种情况下显然是需要批量下载的，并且可以对下载任务进行管理，那就要看下面的一项了：后台起 aria2 服务。 生成下载 url 的过程需要借助 baiduexporter、YAAW for Chrome 插件，直接从 Chrome 浏览器的插件商店搜索安装即可，如果无法翻墙，也可以从离线镜像库下载离线文件进行安装，离线库可以参考本站点的 关于页面 给出的工具链接。 接下来描述使用方式，登录百度网盘账号，把需要下载的文件保存在自己的网盘中，选择需要下载的文件，然后可以看到本来的下载按钮旁边又多了导出下载按钮，包含几个选项：ARIA2 RPC、文本导出、设置。选择文本导出就会弹出当前下载文件的下载 url，复制粘贴到命令后即可直接下载该资源。导出的内容格式如下，当然实际使用的时候里面的参数也是可以更改的，但是下载 url 一定不不能变的。1https://pcs.baidu.com/rest/2.0/pcs/file?method=download&amp;app_id=250528&amp;path=%2F% E9%80%86% E5%90%91% E8% B5%84% E6%96%99%2FIDA%20Pro% E6%9D%83% E5% A8%81% E6%8C%87% E5%8D%97.pdf2、根据前面的描述，后台起了 aria2 服务，但是还没真正用起来，想要用起来，必须配合两个插件：baiduexporter、YAAW for Chrome。这 2 个插件中前者的作用是获取百度网盘的文件 url，这个 url 当然不是分享文件产生的 url，而是下载文件产生的 url；后者插件的作用是配合前者自动创建下载任务，实际下载利用的是已经启动的 aria2 后台，并时时监控任务状态，提供任务管理界面。插件的安装不再赘述，接下来直接描述使用流程，要确保以上两个安装的插件都已经启用。根据上一步骤已经知道导出下载这个按钮，里面包含着一个 ARIA2 RPC 选项，这个选项就是直接使用 后台 aria2 服务创建下载任务，然后 YAAW for Chrome 插件监控着所有下载任务。还有一个前提，就是启动 aria2 服务时要开启 RPC 模式。12# 启用 RPC, 默认:falseenable-rpc=true这样做了之后，aria2 后台服务会开启一个端口，一般默认 6800（如果 aria2 更改了端口，YAAW for Chrome 也要做相应的配置），这个端口用来给 YAAW for Chrome 汇报下载任务的情况，并提供管理下载任务的接口，这样的话，直接通过 YAAW for Chrome 就可以通过可视化的方式创建、暂停、查看任务。后台启动 aria2，开启 RPC 模式。打开 YAAW for Chrome 插件查看端口配置信息。通过 baiduexporter 插件，直接选择 PRC 下载，再去 YAAW 界面刷新查看下载任务。可以看到，aria2 参数还没优化（线程数、分块大小设置），下载速度已经有将近 400 Kb/s 了。使用油猴插件绕过浏览器下载大文件的限制 现象 还是刚才那个文件，文件大小只有 149 M，不想通过百度网盘客户端下载，只想通过网页版下载，那就直接点击下载按钮，发现被限制了，必须让你安装百度网盘客户端。本来还在想通过网页版直接下载，速度也不会很慢，但是被限制了，这个时候我们的万能插件要出场了：Tampermonkey，又称油猴、暴力猴。解决方式 使用万能的插件，屏蔽百度网盘网页版原来的网页内容，从而导致百度网盘的限制失效，这个插件就是 Tampermonkey：官网 、Chrome 浏览器插件商店。 这个插件的作用其实就是帮你管理各种自定义脚本，并运用在网页解析渲染中，从而实现对网页内容的改变，例如：去除网页的广告、去除百度搜索内容的广告条目、更改新浪微博展示界面。其中，也包括让百度网盘的下载文件大小限制失效，从而可以自由下载。1、好，现在需要在插件的基础上安装一个脚本：百度网盘直接下载助手。要安装这个脚本，则首先需要找到它，选择获取新脚本，会引导我们进入脚本仓库。2、各种脚本仓库，我们选择 GreasyFork。3、在搜索框中搜索：百度网盘直接下载助手，选择其中一个。4、安装选择的脚本。5、可以看到脚本内容，点击安装。6、安装完成后，选择管理面板可以查看已经安装的脚本以及是否启用，也可以删除或者二次编辑。7、回到百度网盘，选择文件，可以看到多了一个下载助手选项，选择 API 下载，下载，即可使用浏览器直接下载，不会因为文件太大有网页的限制。8、当然，如果自己会写脚本，或者从别处直接复制的源脚本代码，在插件中选择添加新脚本，自己编辑即可。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>Aria2</tag>
        <tag>百度网盘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WPS 关闭广告推送与自动升级]]></title>
    <url>%2F2018110301.html</url>
    <content type="text"><![CDATA[在工作和生活中，很多人使用金山的 WPS 套件，类似于微软的 Office 套件，而且是免费的。但是很多人会遇到 广告推送 或者 WPS 热点推送 ，每隔几天就会出现，有时候可以点击七天内不再出现，便可以清静几天，而且碍于是免费版本，不想购买会员，于是也就忍了。其实，WPS 自身是有设置可以关闭 广告推送 的，当然也可以关闭 WPS 热点推送 。WPS 的弹框 以下现象描述与截图均出自版本：WPS 2019，v11.1.0.8013 - Release 正式版。操作系统为：Windows 2007 专业版。在使用 WPS 的过程中，经常遇到广告推送与 WPS 热点推送，觉得很受打扰，但是碍于使用的是免费软件，又只能忍受。我一直在想以前是有设置可以关闭的，后来升级了就找不到是在哪里设置的了，后来又查阅了资料，发现果然是有地方可以设置的，只不过隐藏的太深了，不好寻找而已。接下来就一步一步说明具体设置步骤。设置关闭 如果你在互联网上搜索 WPS 广告推送相关话题，可以看到大量的帖子（或者说是方法教程）已经整理出了各种方案，可以帮你解决这个问题，例如：直接更改 WPS 安装目录中的某些文件、利用杀毒软件屏蔽广告推送、直接设置 WPS 等等。显然，前 2 种方案是在走弯路，而最后一种方案才是最简单直接的。1、打开 WPS 主页，在右上角找到 设置 按钮；2、点击 设置 ，选择 配置和修复工具 ；3、在弹出的对话框中选择 高级 ；4、选择对话框的 其它选项 标签页，取消截图中的 3 项勾选，即同时关闭 升级完成后推荐精选软件 、 订阅 WPS 热点 、 接受广告推送 ；此外，进入步骤 3 也可以直接通过系统的安装程序列表（开始 –&gt; 所有程序 –&gt;WPS Office–&gt;WPS Office 工具 –&gt; 配置工具），步骤如下图：按照以上步骤设置， WPS 就不再会弹出广告推送和 WPS 热点推送了，亲测有效。注意事项 1、说实话，我是没想到这个设置会隐藏的这么深，但至少暴露出来了；2、要注意版本区别，可能每个版本的设置步骤有所不同，而且也不排除以后更新的版本会取消这些设置选项，或者隐藏的更深。当然，如果 WPS 找到了其它盈利方式，也可能会取消这些广告推送；3、WPS 每次更新后，上述设置会还原，也就是又回到默认开启的状态，此时需要重新设置一次。当然，为了以后不会莫名其妙又弹出广告推送，可以直接关闭自动升级（和前面关闭广告的步骤一致，但是选择的是 升级设置 标签页），以后想升级的时候再手动升级。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>WPS</tag>
        <tag>关闭广告推送</tag>
        <tag>关闭自动升级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微博 URL 短网址生成算法 - Java 版本]]></title>
    <url>%2F2018101501.html</url>
    <content type="text"><![CDATA[微博短链接是微博官方提供的网址压缩功能产生的一种只包含少量字符的短网址，例如：http://finance.sina.com.cn ，压缩后为：http://t.cn/RnM1Uti 。这样的话，发微博时链接占用更少的字符长度。如果发微博时，内容中带了链接，例如视频地址、淘宝店地址，会被自动压缩为短链接。微博短链接可以直接在浏览器中访问，会被微博的网址解析服务器转换为原来的正常链接再访问。本文描述微博 URL 短网址生成算法，编程语言是使用 Java。 待整理。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>微博URL短网址</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android 逆向系列 2-- 破解第一个 Android 程序]]></title>
    <url>%2F2018101101.html</url>
    <content type="text"><![CDATA[本文简单介绍一下对一个简单的 Android 程序的逆向破解，算是对 Android 逆向的入门了解。 待整理。]]></content>
      <categories>
        <category>Android 逆向系列</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>逆向工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android 逆向系列 1-- 编写第一个 Android 程序]]></title>
    <url>%2F2018101001.html</url>
    <content type="text"><![CDATA[本文简单介绍一下 Android 开发的入门程序，编写一个简单的 Android 程序。 待整理。]]></content>
      <categories>
        <category>Android 逆向系列</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>逆向工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android 逆向系列 0-- 初识 Android 以及逆向工程]]></title>
    <url>%2F2018100901.html</url>
    <content type="text"><![CDATA[本文简单介绍一下 Android 开发以及关于 Android 的逆向工程，算是入门了解。 待整理。]]></content>
      <categories>
        <category>Android 逆向系列</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>逆向工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Kryo 异常]]></title>
    <url>%2F2018100801.html</url>
    <content type="text"><![CDATA[本文讲述使用 es-hadoop （版本 v5.6.8）组件，运行 Spark 任务遇到的异常：123Caused by: java.io.EOFExceptionat org.apache.spark.serializer.KryoDeserializationStream.readObject (KryoSerializer.scala:232)at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject (TorrentBroadcast.scala:217)以及通过 Maven 依赖查找分析的解决方法。遇到问题 由于最近的 elasticsearch 集群升级版本，到了 v5.6.8 版本，所用的功能为了兼容处理高版本的 elasticsearch 集群，需要升级 es-hadoop 相关依赖包到版本 v5.6.8，结果就遇到了问题：代码逻辑就是通过 es-spark 直接读取 elasticsearch 里面的数据，生成 RDD，然后简单处理，直接写入 HDFS 里面。本机在测试平台测试一切正常，没有任何问题，但是在线上运行就会抛出异常。解决方法 先分析一下这个问题产生的原因，在代码层面没有任何变动，只是更改了依赖的版本，所以问题在于更改版本之后是不是导致了传递依赖包的缺失，或者版本冲突，所以总体而言，肯定是 Maven 依赖包的问题，这个思路没问题。提前说明下面截图中出现的 Maven 中的常量：12&lt;elasticsearch-hadoop.version&gt;5.6.8&lt;/elasticsearch-hadoop.version&gt;&lt;spark-core_2.10.version&gt;1.6.2&lt;/spark-core_2.10.version&gt;1、local 模式 通过在本机连接测试平台，运行起来没有问题，但是部署到正式环境，运行不起来，直接抛出上图所示的异常信息。首先去依赖树里面查看与 kryo 相关的依赖信息（使用 mvn dependency:tree 命令）：发现两个依赖包（es-hadoop v5.6.8，spark-core_2.10 v1.6.2）里面都有与之相关的传递依赖，而且版本（奇怪的是 groupId 也稍有不同，这导致了我后续判断失误）不一致，这必然导致依赖包的版本冲突，通过 exclusions 方式去除其中一个依赖（其实不是随意去除一个，要经过分析去除错误的那个，保留正确的那个），local 模式可以完美运行。此图是 pom.xml 文件里面的移除信息，是我根据依赖树整理的，可以更加清楚地看到传递依赖的影响。2、yarn-client 模式 通过步骤 1 解决了 local 模式运行的问题，但是当使用 yarn-client 模式向 yarn 集群提交 Spark 任务时，如果移除的是 spark-core_2.10 里面的 kryo 依赖，异常信息仍然存在，无法正常运行。此时，我想到了前面所说的 2 个 kryo 依赖包的 groupId 有一点不一样，所以这 2 个依赖包虽然是同一种依赖包，但是可能由于版本不同的原因，导致名称有些不同。我认为使用的 es-hadoop 依赖的版本比较高，可能没有兼容低版本的 spark-core_2.10，所以需要保留 spark-core_2.10 里面的 kryo 依赖，而是把 es-hadoop 里面的 kryo 依赖移除。果然，再次完美运行。总结说明 这次通过 Maven 依赖找到了问题，但是版本仅仅限定在我使用的版本，其它的版本之间会有什么冲突我无法得知，但是这种处理问题的思路是正确的，避免走冤枉路，浪费不必要的时间。另外，提醒一下大家，更新 pom.xm 文件（包括新增依赖和更新依赖版本）一定要谨慎而行，并且对所要引入的依赖有一个全面的了解，知道要去除什么、保留什么，否则会浪费一些不必要的时间去查找依赖引发的一系列问题。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Kryo</tag>
        <tag>序列化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[依赖包缺失导致 Spark 任务无法启动]]></title>
    <url>%2F2018100701.html</url>
    <content type="text"><![CDATA[本文讲述使用 Spark 的过程中遇到的错误：1class "javax.servlet.FilterRegistration"'s signer information does not match signer information of other classes in the same package最终通过查找分析 Maven 依赖解决问题。遇到问题 由于最近的 elasticsearch 集群升级版本，到了 v5.6.8 版本，所用的功能为了兼容处理高版本的 elasticsearch 集群，需要升级相关依赖包，结果就遇到了问题。使用 es-hadoop 包（v5.6.8）处理 elasticsearch （v5.6.8）里面的数据，具体点就是通过 es-spark 直接读取 elasticsearch 里面的数据，生成 RDD，然后简单处理，直接写入 HDFS 里面。编译、打包的过程正常，运行代码的时候，抛出异常：1class "javax.servlet.FilterRegistration"'s signer information does not match signer information of other classes in the same package一看到这种错误，就知道肯定是 Maven 依赖出现了问题，要么是版本冲突，要么是包缺失，但是从这个错误信息里面来看，无法区分具体是哪一种，因为没有报 ClassNotFound 之类的错误。解决方法 现象已经看到了，问题也找到了，那么第一步就是直接搜索 Maven 项目的依赖，看看有没有 FilterRegistration 这个类，我的 IDEA 直接使用 Ctrl + Shift + T 快捷键，搜索 FilterRegistration，发现有这个类，但是包名对不上，注意包名是：javax.servlet。现在就可以断定，是包缺失，通过搜索引擎查找文档，需要引入 javax.servlet-api 相关的包， pom.xml 文件的具体依赖信息是：12345&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;4.0.1&lt;/version&gt;&lt;/dependency&gt;当然，版本信息根据实际的场景需要进行选择，我这里选择 4.0.1 版本。需要注意的是，有另外一个包，它的 artifactId 是 servlet-api，可能你会因为没看清而配置了这个依赖包，导致还是包缺失，所以一定要看清楚。我这里遇到的问题比较简单，只是包缺失而已，如果遇到的是包版本冲突，需要移除不需要的版本，只保留一个依赖包即可，此时可以借助 Maven 的 dependency 构建来进行分析查找：1mvn dependency:tree这个命令会输出项目的所有依赖树，非常清晰，如果内容太多，可以使用：1mvn dependency:tree &gt; ./tree.txt重定向到文本文件中，再进行搜索查找。总结 1、还遇到一种情况，在正式环境运行正常【没有单独配置这个依赖，使用的是别的依赖包里面的同名类，org.eclipse.jetty.orbit:javax.servlet】，但是在本机跑，创建 SparkContext 的时候就会报错，无法创建成功。其实还是因为包缺失，确保要使用 javax.servlet-api 这个依赖，其它的都不好使。2、在本机连接测试环境的 yarn，创建 SparkContext 的时候无法指定用户名，默认总是当前系统的用户名，导致创建 SparkContext 失败，伪装用户无效，只有打 jar 包执行前使用命令切换用户名：export HADOOP_USER_NAME=xx，而在代码中设置 System.setProperty (“user.name”, “xx”)、System.setProperty (“HADOOP_USER_NAME”, “xx”) 是无效的（这个问题会有一篇文章专门分析，需要查看源代码）；3、针对 2 的情况，简单通过 local 模式解决，暂时不使用 yarn-client 模式；4、针对 2 的情况，还有一种简单的方法，那就是直接设置 IDEA 的环境变量参数（不是设置操作系统的环境变量，我试了无效），如下图（和设置运行参数类似）； 设置 IEDA 的环境变量：5、此外，还有一种情况，当需要操作 HDFS 的时候，发现无论怎么设置环境变量都不可以（配置文件配置、代码设置），总是读取的系统默认用户，就和 2 中讲的一致，其实如果只是单纯地操作 HDFS，还可以在创建文件流的时候指定用户名（不过这种方法要先从 conf 中获取 uri）；12String uri = conf.get ("fs.defaultFS");FileSystem fs = FileSystem.get (new URI (uri), CONF, "zeus");]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Maven</tag>
        <tag>依赖问题</tag>
        <tag>FilterRegistration</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[地锅鸡做法总结（皖北、苏北地区）]]></title>
    <url>%2F2018100601.html</url>
    <content type="text"><![CDATA[地锅鸡是流行于皖北、苏北地区的一道传统名菜，并经过改良产生了地锅鱼、地锅豆腐、地锅牛肉、地锅三鲜等一系列菜品，但是配饼始终不变，一般是面饼或者玉米饼。本文用于记录地锅鸡做法总结。地锅鸡简介 地锅鸡是一道起源于江苏省的北部、山东省的南部的传统名菜，在安徽北部地区也非常流行，主要食材就是鸡肉、面饼、辣椒，做成菜品后既有主食又包含配菜，口味醇香，饼借菜味，菜借饼香，吃起来回味无穷。另外，经过不断的改进创新，还产生了地锅鱼、地锅豆腐、地锅牛肉、地锅三鲜等一系列菜品，它们的核心都在于使用地锅制作，并配以面饼、玉米饼。材料准备 2 人份的材料： 面粉 150 克 鸡肉 300 克 鸡蛋 1 个（和面使用，也可以不用）花椒 10-15 粒 姜片 4 片 大葱 4 段 干辣椒 5 个 桂皮 1 小块 八角 2 个 大蒜 5 瓣，不用切 青椒、红椒各半个，滚刀切好 酵母菌 各种调味料 主要步骤 1、使用面粉 150 克和面，使用温水，加入 0.5 克酵母菌，也可以加一个鸡蛋一起，和完的面很软但是不粘手，使用保鲜膜包住，注意先撒一点面粉再包（或者直接放碗里用保鲜膜盖住密封，也需要先在碗里撒一层面粉），这样是避免最后粘住。大概需要发 20-30 分钟，等待的过程可以去做其它准备工作了。2、取出配料，花椒、姜片、大葱、干辣椒、桂皮、八角、大蒜。锅烧热，放油，多放一点油，先放花椒、桂皮、八角，几秒后再放入葱段、姜片、大蒜、干辣椒，大概 10 秒煸出香味，捞出大蒜备用，其它配料不用捞出。3、鸡肉洗干净，放入锅内中火炒 5-10 分钟，放入老抽、生抽、白糖、料酒、豆瓣酱，混合后加入开水，稍微没过鸡肉一点，转为大火，烧开。烧开后中小火焖 15-20 分钟，此时鸡肉已经熟了，要保证还有一些水汤在锅里，因为等一下还需要贴饼、调味、继续焖、收汁等步骤。4、在步骤 3 的过程中，面已经发好，均匀分成条状，具体做法是先拉伸，变成长条，然后揪断就行了，大概 20 个左右（如果锅小了一次贴不完，就分 2 次，贴 1 次先吃着，吃完再加一点汤继续贴下一锅），放入清水中，主要不要再动了，就让它们浸泡在水中。5、步骤 3 结束后，改为小火，加盐调味，并准备贴饼（如果感觉鸡肉的颜色不够，可以再加一点酱油上色）。步骤 4 的面团在水中浸泡了大概 10 分钟，一个一个取出，是湿漉漉的，用手扯成长条饼状，一半贴在锅沿，用力压一下确保贴紧，一半放入汤中。这样，上半部分会焦脆，下半部分吸收了汤汁很美味。饼贴满后，小火继续焖 10-15 分钟，此时注意如果锅受热不均匀，需要每隔几分钟旋转一下锅。6、在步骤 5 中，几分钟后，饼快熟了，就可以加入步骤 2 中捞出来的大蒜，和滚刀切的青椒、红椒，加点香油，稍微搅拌一下，此时汤汁已经基本没了。饼完全熟透了，开锅，大火稍微翻炒几下，就可以吃了，直接在锅里吃。 注意事项 1、锅、灶的选择，在农村地区、乡镇地区、城市周边的农家乐，才会有地锅这种设备，所以在家里自己做是很难找到地锅的，只能退而求其次使用普通的炒锅，也是可以的，注意尺寸要大一点的（做 2 人份的地锅鸡就选 3-4 人份的炒锅）；另外最好保证灶的火力能大一点，有用；2、面发的时间，和面时可以放鸡蛋也可以不放，最好使用温水，发面的时间不要太久，一般 30 分钟就行了，甚至可以不发，直接使用死面；3、贴饼的时候速度一定要快，不然刚刚贴了半圈就已经熟了，丧失了饼的香味；另外饼要贴紧一点，粘在锅上，如果锅受热不均匀，注意每隔几分钟旋转一下锅，保证饼的上半部分能焦脆，这也是需要灶的火力大一点的原因。 上图 在安徽合肥吃到的 在 2018 年中秋期间，回老家经过合肥，于是在以前的同学的带领下吃了一次地锅鸡，非常满足。这个店的地点在安徽大学（馨苑校区）的西门附近，那条路叫九龙路，一条街都是吃的，又名九龙美食街。自己做的 自己在 2018 年国庆期间做了一次，由于没有地锅可以使用，只好选用了普通的炒菜锅，做起来味道还是不错的，只不过饼没有达到香脆的水平，稍有遗憾。此外，家用煤气灶的火力不行，需要更大火的时候不够，导致温度不够高，间接导致了鸡肉的香味和饼的香味没有充分融合，吃的时候感受不到纯正的地锅鸡的香味。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>地锅鸡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10 取消 Skype for Business 自动登录]]></title>
    <url>%2F2018090701.html</url>
    <content type="text"><![CDATA[最近在使用 Win10 系统，遇到一个问题，每次开机，Skype for Business 都会自动弹出来，提示登录，每次我都会关掉它。遇到多次之后，我想这个应用我不需要，直接卸载掉算了，但是却找不到这个应用的信息，最后只能通过关闭 开机自动启动 的方式来解决问题，本文记录解决问题的过程。问题出现 最近在使用 Win10 的时候，每次开机后，Skype for Business（这个应用不同于 Skype，虽然功能一样）总会弹出来，提示我登录，我每次都会毫不犹豫地关掉它。Skype for Business 登录界面 正常的 Skype 应用登录界面 但是出现多次之后，很麻烦，当我想卸载这个应用的时候，发现从应用列表里面找不到，也就无从卸载。后来就想能不能关闭开机启动，找了一些文档发现可以，那就这么办了（而且还发现 Skype for Business 根本卸载不了）。问题解决 1、Skype for Business 是属于 Office 套件中的一个软件，所以在安装整个 Office 的同时也会自动安装上 Skype for Business。由于是一次安装整个 Office 套件，所以无法单独删除其中的一个软件（Skype for Business）。如果不需要开机自动启动 Skype for Business（也就不会提示我登录了），可以在 Skype for Business 的 设置 菜单中的 个人 选项里将 当我登录到 Windows 时自动启动应用 这个设置取消。设置（在登录界面的右上角，有一个齿轮按钮）取消当我登录到 Windows 时自动启动应用 2、不知道哪一天 Windows 升级到了新的系统后，Skype for Business 不见了（怎么找也找不到），随之而来的是 Skype，尽管它也属于 Office 中的一个应用（还有很多其它一系列应用），但是这个应用可以单独安装卸载，不再与 Office 绑为一个整体。 打开我的 Office查看应用列表，也可以直接安装显示的应用 问题总结 1、我是一开始关闭了 Skype for Business 的登录界面，然后再想打开它，就找不到了，不知道在哪（理论上应该隐藏在某个应用列表里面，目前我还没找到，可能是 Windows 系统升级导致的），但是现在却自动有了 Skype 这个应用（可能是 Windows 系统升级替换了以前的 business 版本），其实这 2 个应用应该差不多。2、现在 Windows 系统升级到最新版本（最新升级时间是 2019-02-17）后，Skype for Business 已经不存在了，替换它的是 Skype，而这个应用是可以单独卸载的。3、参考： 官方回复 、 设置方式、Skype for Business 应用介绍 。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>Skype for Business</tag>
        <tag>自动登录</tag>
        <tag>Win10</tag>
        <tag>Skype</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[踩坑特殊字符之硬空格]]></title>
    <url>%2F2018090601.html</url>
    <content type="text"><![CDATA[最近处理数据的过程中，发现一个奇怪的问题，处理数据逻辑如下：有一个短字符串，我需要从一个长字符串寻找这个短字符串是否出现。这个逻辑很简单，使用任何一种编程语言，基本上都会有 包含 这种方法，直接就可以判断了，但是我遇到的情况明明就是包含了，子串就是存在，但是判断结果却不包含。另外我又直接把 2 个字符串单独取出来，肉眼去看，也是包含的。愁眉苦展之际，突然灵光闪现：会不会字符串中包含一些奇怪的特殊字符，并且肉眼难以发现。于是立马去验证一下，果然是这样，解决了我的问题，本文记录解决问题的过程。问题出现 在使用 Java 编程语言处理数据的时候，有一个字符串包含判断的逻辑，明明是包含关系，判断的结果却是不包含，代码示例如下：1234@Testpublic void containsTest() &#123; System.out.println ("每当 #每月 28 日京东企业会员日 #来临，就会有优惠".contains ("# 每月 28 日京东企业会员日 #".trim ()));&#125;运行结果：尽管加上了 trim () 方法，但是返回结果仍然是 false，足以说明子串中头尾有某个符号很特殊，并没有被清除掉，所以我觉得很蹊跷。虽然在代码和截图中，那个大大的空格（其实不是空格字符）看起来很显眼，但是在实际运用中是在文本中存放的，而且不止这一个字符串，有很多，所以在检查时使用搜索替换功能（把空格替换为空白），也是没把这个特殊符号清除掉。同时，也没有料想到文本中会出现这样特殊的符号，所以只是简单地使用 trim () 方法来清除头尾空白符（还以为生效了，其实遇到这种特殊的符号就没作用了）。把内容复制到 Notepad++ 文本编辑器中，并且设置文本编辑器的视图显示所有的字符（会用带有颜色的特殊图标来表示文本中的特殊字符，例如肉眼看不到的字符：空格、换行等），然后可以看到空格（橙色点点）、Tab 符号（橙色箭头）、回车换行（黑块）都会显示出来，但是唯独这个特殊字符没有显示出来，仍然是空白一片。使用文本编辑器打开 问题解决 已经知道这是个特殊字符了，下一步只要搞清楚这是个什么字符就行了，问题就会迎刃而解。先把特殊字符复制出来，找一个转码器，把字符转为十六进制编码，看看它的编码是什么，在线编码转换工具参考：http://ctf.ssleye.com/jinzhi.html 。在 文本 这个文本框中输入特殊字符（–&gt; &lt;–），然后在 十六进制 文本框中可以看到编码是 a0。可以看到十六进制的结果是 a0好，接下来去 Unicode 字符列表（维基百科里面的：https://zh.wikipedia.org/wiki/Unicode% E5% AD%97% E7% AC% A6% E5%88%97% E8% A1% A8 ）查看这到底是个什么特殊字符。直接搜索 00A0，就可以找到，发现这是 不换行空格 ，在 拉丁字符 - 1 辅助 里面。不换行空格 更进一步，我去维基百科查看这一特殊符号的介绍，发现这个符号还是挺有用的，附上链接：https://zh.wikipedia.org/wiki/% E4% B8%8D% E6%8D% A2% E8% A1%8C% E7% A9% BA% E6% A0% BC 。不换行空格介绍，一般用在网页排版中 至此，问题原因找到了，我竟然被一个特殊符号坑了（以前也被输入法的全角、半角问题坑过），那解决办法就很简单了，针对这种符号做替换清除就行了。我仍在思考，这个符号是怎么被输进文本文件给我的，因为正常人通过输入法不可能打出这个符号。后来询问相关人，得知他们是从网页上直接复制的内容粘贴到文本文件中，这就可以解释了，因为这个符号就是针对网页的自动压缩空白符问题，量身定做的，怪不得。问题总结 1、这个特殊字符（–&gt; &lt;–，编码 a0）有些编辑器不一定支持，会产生丢失，例如我把这条字符串内容放在了 Tower 上面的任务回复中，想记录一下，结果再复制下来就变成了空格字符，说明丢失了（并且被转为了空格字符）。但是没关系，我们记住它的 Unicode 编码就行了，找一个工具（例如：http://ctf.ssleye.com/jinzhi.html ）就可以转换了，Unicode 编码是：U+00A0，把 a0 复制到 十六进制 的文本框中，则 文本 这个文本框中出现的就是它的字符，当然，直接肉眼看不出来，要选中变为蓝色才会发现有一个字符。编码恢复到字符2、以后处理字符串问题的时候，一定要区分场景，真的是自己知识面之外的情况都可能出现，而且是正常的，自己千万不要怀疑人生，而是要抽丝剥茧，一步一步找问题。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>硬空格</tag>
        <tag>不换行空格</tag>
        <tag>hard-space</tag>
        <tag>fixed-space</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[归来]]></title>
    <url>%2F2018090501.html</url>
    <content type="text"><![CDATA[好，整理完成后，重新出发。二级标题，自动创建锚点和目录 三级标题 - 开始 1$ hexo new "My New Post"More info: Writing 三级标题 - 中间 1$ hexo serverMore info: Server 三级标题 - 结束 1$ hexo generateMore info: Generating 三级标题 - 备注 1$ hexo deployMore info: Deployment 踩坑记录 注意使用 hexo 对 Markdown 文件进行解析时，有一些转义字符是会失败的（使用反斜杠 \ 进行转义的，例如美元符号 &#36;，成对出现有特殊含义，所以需要转义，在 Markdown 中可以使用 \$ 进行转义，但是 hexo 解析完成 html 文件是失败的），所以最好使用编码解决，例如美元符号使用 &amp;#36; 替代。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>建站</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven 使用中遇到的证书问题以及 JDK8 问题]]></title>
    <url>%2F2018082501.html</url>
    <content type="text"><![CDATA[在 Java 开发过程中，使用 Maven 往私服 deploy 构件【Java 打成的 jar 包】的时候，原本正常的流程突然出问题了，报错信息：1[WARNING] Could not transfer metadata org.leapframework:leap:0.4.0b-SNAPSHOT/maven-metadata.xml from/to bingo-maven-repository-hosted ($bingo.maven$): sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target看到里面的 security 和 certification 关键词就猜测是安全与证书的问题。恰好最近私服的域名访问从 http 升级为了 https，也就是增加了 SSL 证书，我想可能和这个有关。本文就记录解决这个问题的流程，以及后续由此又引发了其它的问题，例如 JDK8 导致的注解问题、IDEA 的乱码问题。问题出现 在分析问题现象之前，首先需要了解一个基本事实，Maven 是依赖于 JDK 环境运行的，所以使用 Maven 之前必须安装 JDK，并且配置好 JAVA_HOME 。在使用 Maven 过程中遇到的一些问题可能会和 JDK 环境有关，例如 SSL 证书问题、JDK 版本问题。以下内容现象基于 Windoes7 X64 操作系统，JDK 版本为 1.7u89，Maven 版本为 3.5。好，言归正传，继续关注遇到的问题。在某一天，发现公司的 Maven 私服仓库更新了管理系统，并且增加了 SSL 证书【证书类型是 Let’s Encrypt ，有效期三个月，这为后续的各种问题埋下了伏笔】，这样访问的链接全部变为 https 开头的了。但是我又注意到一个现象，我发现仓库里的很多 SNAPSHOT 类型的 jar 包消失了，这应该是管理系统设置了自动清除机制，把没用的快照版本的 jar 包全部清除，节约空间。但是，其中有一些 jar 包对于我来说是有用的，更麻烦的是这些 jar 包根本没有 RELEASE 正式版，都是前人留下的坑，为了图方便临时打了一个快照版本 jar 包给别人使用，竟然把资源文件也打进去，导致一个 jar 包有将近 100M 大小。这种操作显然是违背 Maven 的理念的，面对这种情况，再想申请发布一个 RELEASE 版本的 jar 包也麻烦，而且这种业务类型的代码就不应该打成 jar 包给别人使用。思考了半天，我决定采用一个折中的办法：取得代码阅读权限，移除没用的资源文件，仅仅发布代码，仍旧发布 SNAPSHOT 版本，以后有时间再把业务代码复制出来，不再使用 jar 的形式。思路确定了，就开始行动。一开始我发现开发环境本地仓库有这些 jar 包，还想手动上传到 Maven 私服仓库的，把坐标定义准确就行了。但是后续发现 Maven 私服仓库的管理系统不支持手动上传 jar 包，只支持通过账号、密码认证的方式，从源代码发布 jar 包到仓库，而且个人账号只能发布 SNAPSHOT 版本的，管理员账号才能发布 RELEASE 版本的。路走到了这里，那我只能先获取项目代码的权限，然后新开分支，删掉无用的配置文件，仅仅发布源代码到私服仓库，先发布 SNAPSHOT 版本使用。准备工作做好后，接着开始 deploy，就遇到了一连串的问题，在确认账号密码没有问题的前提下，deploy 失败，错误信息如下：1[WARNING] Could not transfer metadata org.leapframework:leap:0.4.0b-SNAPSHOT/maven-metadata.xml from/to bingo-maven-repository-hosted ($bingo.maven$): sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target看到里面的 security 和 certification 关键词就猜测是安全与证书的问题，公司私服的其它环境并没有变化，只能从 SSL 证书入手解决问题，然而一开始没有头绪。 但是又问了同事，发现他们可以正常 deploy，又测试了一下线上的发布系统，也可以正常发布 jar 包到公司私服仓库。那只有 2 个怀疑方向了，一个是本地 Maven 的版本问题，一个是本地 JDK 的版本问题。后来通过对比发现，的确是 JDK 版本的问题，某些低版本的 JDK 不会自动导入 Let’s Encrypt 的证书，才会导致 Maven 进行 deploy 时认证失败【Maven 底层是依赖于 JDK 的】，自然而然 delpoy 也就失败了。参考：stackoverflow 讨论一例 。The Let’s Encrypt certificate is just a regular public key certificate. Java supports it (according to Let’s Encrypt Certificate Compatibility , for Java 7 &gt;= 7u111 and Java 8 &gt;= 8u101). 在找原因的过程中还一度怀疑是公司私服仓库的 SSL 证书问题，后面发现是本地环境的问题，但是背后的根本原因还是 SSL 证书的问题。恰好遇到了 Let’s Encrypt 类型的证书，又恰好 JDK 版本过低，引起一系列连锁反应。问题解决 既然找到了问题，那就容易解决了，直接升级 JDK 即可，JDK7 需要升级到 &gt;=7u111，JDK8 需要升级到 &gt;=8u101。手动导入证书 其实，如果不升级 JDK，还有一种繁琐的解决办法，那就是手动导入证书。解决思路就是从需要访问的 https 站点下载证书，然后导入本地的 Java 环境证书库，缺点就是每次证书更新都需要重新导入，显得麻烦。其实这种做法更有助于我们理解这个问题的核心所在，高版本的 JDK 会自动帮我们导入 Let’s Encrypt 证书，但是低版本的不会，我们不仅能知其然，也知其所以然。下载证书 证书可以在访问网站时，在 url 文本框的左侧，有一把小绿锁，选中点击，接着查看证书，下载即可。点击小绿锁 下载证书 此外也可以通过浏览器调试工具的 Security 标签查看下载证书，下图是使用 Chrome 浏览器的效果，其它浏览器可能会略有不同。把证书文件导入证书库 先要清楚本地 Java 的证书库的位置，一般在 {JAVA_HOME}/jre/lib/security/ 目录下面，里面有一个 cacerts 文件，它就是所有证书的集合组成的文件。另外还要清楚 keytool 工具，它是 JDK 提供的可以操作证书的工具，可以直接使用。Java 证书库的位置 以下命令供参考，特别需要注意 命令执行权限 、 文件写权限 ：12345678-- 把证书文件 maven.datastory.cer 导入 Java 证书库 cacerts 中，别名为 maven.datastory，密码是 changeitkeytool -import -alias maven.datastory -keystore cacerts -file maven.datastory.cer -trustcacerts -storepass changeit-- 在证书库中查看指定别名的证书信息，需要输入密码 keytool -list -keystore cacerts -alias maven.datastory-- 删除证书库中指定别名的证书 keytool -delete -keystore cacerts -alias maven.datastory后续维护问题 这种手动导入证书的方式，只能确保一时可以使用，因为证书是会过期的，特别是 Let’s Encrypt 证书，有效期只有 3 个月。所以后期维护起来会很麻烦，如果某一天发现 deploy 又报一样的错，那估计是证书过期了，也有可能是站点的证书被更换了。因此，还是升级高版本的 JDK 比较好，把证书的维护更新工作都交给 JDK 来执行，自己安心写代码就行了。我后期就经历了这一过程，用了没多久发现 deploy 还是失败，只好把证书下载下来重复了导入的过程。而且一开始没有往证书过期上面怀疑，浪费了一些找问题的时间。自定义证书库 其实还有一种更为繁琐的做法，那就是自定义证书库，思路就是把 Java 的证书库复制一份，并且把自己的证书添加进去，然后为 Maven 指定这个自己的证书库。指定证书库的参数【例如路径、密码、证书库类型】需要在 Maven 命令执行之前配置，就像设置一些环境变量一样。更为详细的做法就不演示了，毕竟一般都没有必要这样做，可以参考：自定义 Maven 证书库 。 题外话 如果 IDEA 因为 Maven 的依赖问题，有红色的线条提醒，可能是没有及时更新 UI 界面导致的，其实依赖都完整了，此时重新启动 IDEA 即可，我一直怀疑这是 IDEA 的 bug，有时候明明缺少依赖，IDEA 也不提示错误。而判断是否真的缺失 Maven 依赖，应该使用 Maven 命令编译、打包【mvn compile package】，看看日志是否正常，不能以 IDEA 的显示为依据【有时候 IDEA 会抽风】。如果真的缺少 Maven 依赖，使用 Maven 命令编译、打包是会失败的，并且有提示。而如果明明不缺少依赖，但是代码中报错一大堆，此时强制更新【reimport 重新导入】 Maven 依赖即可，必要时也需要重启 IDEA。另外有一个很好用的小技巧，如果本地仓库存在一个可以使用的 jar 包，可以直接复制给别人使用，按照相同的目录放在指定的路径下面即可，这样 Maven 就会认为本地已经存在 jar 包了，不再去私服仓库下载。这种做法虽然很低级，但是却实用，可以快速解决私服仓库没有 jar 包，初始化环境时无法下载依赖的情况。其它问题记录 在同一时期，同事的环境已经升级到 JDK8 以上，并且 &gt;=8u101，Maven 版本是 3.2.1，可以正常 deploy 构件。但是突然有一天就出错，从日志来看是认证问题，切换为别人的账号密码就正常，使用他自己的账号密码却报错，我猜测是账号的问题，找运维解决。报错信息如下，留意关键部分【Not authorized , ReasonPhrase:Unauthorized.】：123456789101112131415161718192021[INFO] --- maven-install-plugin:2.4:install (default-install) @ project-name ---[INFO] Installing D:\datastory\workspace\study\project-name\target\project-name-1.1.11-SNAPSHOT.jar to D:\pro\env\maven\repository\com\datastory\radar\project-name\1.1.11-SNAPSHOT\project-name-1.1.11-SNAPSHOT.jar[INFO] Installing D:\datastory\workspace\study\project-name\pom.xml to D:\pro\env\maven\repository\com\datastory\radar\project-name\1.1.11-SNAPSHOT\project-name-1.1.11-SNAPSHOT.pom[INFO] [INFO] --- maven-deploy-plugin:2.7:deploy (default-deploy) @ project-name ---Downloading: http://maven.domain/nexus/content/repositories/snapshots/com/datastory/radar/project-name/1.1.11-SNAPSHOT/maven-metadata.xml[WARNING] Could not transfer metadata com.datastory.radar:project-name:1.1.11-SNAPSHOT/maven-metadata.xml from/to snapshots (http://maven.domain/nexus/content/repositories/snapshots): Not authorized , ReasonPhrase:Unauthorized.[INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 5.860 s[INFO] Finished at: 2018-08-10T21:48:24+08:00[INFO] Final Memory: 24M/326M[INFO] ------------------------------------------------------------------------[ERROR] Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy (default-deploy) on project project-name: Failed to retrieve remote metadata com.datastory.radar:project-name:1.1.11-SNAPSHOT/maven-metadata.xml: Could not transfer metadata com.datastory.radar:project-name:1.1.11-SNAPSHOT/maven-metadata.xml from/to snapshots (http://maven.domain/nexus/content/repositories/snapshots): Not authorized , ReasonPhrase:Unauthorized. -&gt; [Help 1][ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionExceptionJDK8 注解问题 切换到 JDK8 并且升级之后，在 deploy 构件到私服仓库的时候，出现了另外一个问题，直接 deploy 失败，报错信息如下：12Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:2.7:jar (attach-javadocs) on project [projectname]: MavenReportException: Error while generating Javadoc:Exit code: 1 - [path-to-file]:[linenumber]: warning: no description for @paramdeploy 失败日志截图 查看里面的关键信息，可以找出 maven-javadoc-plugin 这个插件，说明是这个插件在生成 Javadoc 的时候出问题了。而我回想了一下，最近的插件版本、代码结构并没有变化，唯一变化的就是开发环境，JDK 由 1.7 版本切换为了 1.8 版本，那就往这方面找问题了。查了一下资料，由于 JDK8 的 Javadoc 生成机制比之前的版本要严谨许多，在 Javadoc 中添加了 doclint，而这个工具的主要目的是获得符合 W3C HTML 4.01 标准规范的 HTML 文档。所以使用 maven-javadoc-plugin 插件 deploy 的时候，JDK8 环境触发了 Javadoc 验证，验证自然不能通过，Maven 插件直接报错，deploy 不成功。为了验证这个过程，我又把本地环境的 JDK 切回到了 1.7 版本，可以正常 deploy，成功发布 SNAPSHOT 版本的构件到私服仓库。而由于线上发布系统的 JDK 版本强制设置为了 1.8，无法更改，所以无法在线上做验证，只能发现在线上发布的 RELEASE 版本构件一定是失败的。既然找到了原因所在，接下来就容易操作了，可以选择关闭 Javadoc 验证，或者直接不使用 maven-javadoc-plugin 这个插件。而我选择继续使用这个插件，但是可以选择是跳过生成 Javadoc 还是关闭 Javadoc 验证，根据自己的需要了，具体步骤与效果我会在下面演示。先来看一下这个插件的 pom.xml 文件配置：12345678910111213141516171819202122232425&lt;!-- javadoc 打包插件 --&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt; &lt;version&gt;2.9&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-javadocs&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;!-- 直接跳过 Javadoc 生成 --&gt; &lt;skip&gt;true&lt;/skip&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;!-- 此参数针对 jdk8 环境使用，如果本机是 jdk7 环境会报错，所以增加上述 skip 配置，保证线上和本地都可以部署 (install/deploy), 当然最好使用 profile 激活灵活的配置 --&gt; &lt;!-- add this to disable checking, 禁用 Javadoc 检查 --&gt; &lt;additionalparam&gt;-Xdoclint:none&lt;/additionalparam&gt; &lt;!-- 使用 profile 激活灵活的配置 --&gt; &lt;additionalparam&gt;$&#123;javadoc.opts&#125;&lt;/additionalparam&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt;以上配置是非常完整的，把所有的重要配置项都列出来了，并给出了注释，实际使用中选择自己需要的即可，如果看不懂没有关系，接着往下看，详细解释了参数的使用以及最终的优化配置方案。在这里需要注意一个问题，如果你的 pom.xml 文件中根本没有配置 maven-javadoc-plugin 插件，但是这些错误仍旧存在，那是为什么呢？其实是因为 Maven 已经默认给每个生命周期都绑定了对应的插件，如果没有在 pom.xml 中配置自定义的插件，则使用 Maven 默认的【这里的默认有 2 层意思，一个是插件类型默认，一个是版本号默认】。例如当前项目如果没有配置 javadoc 插件，则会默认使用仓库里版本最高的稳定版 maven-javadoc-plugin 插件，插件的配置也都是默认的，无法更改。而使用 Maven 默认的插件，很可能会引发莫名的问题，根本原因就在于对于某个插件、某个版本的插件、插件的默认配置，我们都是未知的，出了问题也比较难定位，所以在一些重要的插件上面还是手动显式配置出来比较好，这样问题都能在自己的掌握之中。跳过 Javadoc 的生成 如果直接配置了跳过 Javadoc 的生成【使用 skip 参数】，configuration 下面的内容都不需要配置了，配置了也不会用到。由于插件会直接跳过 Javadoc 的生成，所以也就不存在验证的过程了。然而，这种做法对于构件的使用方是不友好的，因为缺失了 Javadoc，当查看源码遇到问题时就无法寻求有效的帮助了。123&lt;configuration&gt; &lt;skip&gt;true&lt;/skip&gt;&lt;/configuration&gt;开启 Javadoc 生成但是关闭 Javadoc 验证 因此，还是要开启 Javadoc 的生成，但是关闭 JDK8 对于 Javadoc 的严格验证，此时需要在 configuration 里面增加参数：123&lt;configuration&gt; &lt;additionalparam&gt;-Xdoclint:none&lt;/additionalparam&gt;&lt;/configuration&gt;一般为了方便他人查看项目的参数，最好把这种重要的参数值设置为全局变量，在 pom.xml 文件的 properties 节点下面声明即可，例如：123456789-- 设置全局变量 &lt;properties&gt; &lt;additionalparam.val&gt;-Xdoclint:none&lt;/additionalparam.val&gt;&lt;/properties&gt;-- 使用全局变量 &lt;configuration&gt; &lt;additionalparam&gt;$&#123;additionalparam.val&#125;&lt;/additionalparam&gt;&lt;/configuration&gt;潜在的问题 难道这样配置就完了吗，显然有潜在的问题，作为经历过的人，我告诉你，附加参数 -Xdoclint:none 是只有 JDK8 及以上版本才会支持的，如果有人在构建项目时使用了 JDK7 的环境，最终的结果还是失败，失败的原因是参数不合法，无法支持。报错信息举例：12Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:2.7:jar (attach-javadocs) on project [projectname]: MavenReportException: Error while generating Javadoc:Exit code: 1 - javadoc: 错误 - 无效的标记: -Xdoclint:none所以接下来还要想一个更好的办法，不仅能关闭 Javadoc 的验证，还要根据当前的实际 JDK 环境来自动切换参数的取值，这样就可以兼容所有的环境了。显然，没有什么比 profile 更适合这个情况了，配置一个 profile 激活信息，根据 JDK 的版本激活全局变量，参数值传入给 additionalparam 使用，比起上面的固定的全局变量，这种可变的全局变量更灵活。详细配置如下：1234567891011121314151617-- 全局变量 javadoc.opts 在 JDK8 及以上版本才激活 &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;doclint-java8-disable&lt;/id&gt; &lt;activation&gt; &lt;jdk&gt;[1.8,)&lt;/jdk&gt; &lt;/activation&gt; &lt;properties&gt; &lt;javadoc.opts&gt;-Xdoclint:none&lt;/javadoc.opts&gt; &lt;/properties&gt; &lt;/profile&gt;&lt;/profiles&gt;-- 使用激活的全局变量，如果没有激活则为空 &lt;configuration&gt; &lt;additionalparam&gt;$&#123;javadoc.opts&#125;&lt;/additionalparam&gt;&lt;/configuration&gt;插件版本的踩坑 在解决问题的过程中还遇到了一个典型的问题，由插件版本引起。一开始在项目的 pom.xml 文件中没有配置插件 maven-javadoc-plugin 的版本号，即 version 参数，导致项目使用的是公司私服仓库最新的版本：v3.0.0，而在这个版本中使用 -Xdoclint:none 关闭验证是无效的，不知道是插件本身的问题还是参数 -Xdoclint:none 对 3.0.0 版本的插件无效。后来指定版本为 2.9，就没有这个问题了。由于一开始没有指定插件 maven-javadoc-plugin 的版本号，出错了也不知道为啥，在 deploy 的输出日志中看到使用的 v3.0.0 版本的插件，猜测可能和插件版本有关系，于是更换了版本，就没有问题了。因此这种重要的插件还是要手动指定自己认为稳定的版本，这样有问题也能在自己的掌握之中。参考 在 JDK8 中禁用 Javadoc 验证 stackoverflow 中的问答一例IDEA 乱码问题 在解决上面的 JDK8 注解的问题过程中，遇到了一个乱码问题，系统环境是 Windows7 X64。当在 JDK7 的环境中配置了以下内容时：123&lt;configuration&gt; &lt;additionalparam&gt;-Xdoclint:none&lt;/additionalparam&gt;&lt;/configuration&gt;本意是想测试这个参数在 JDK7 环境中的效果【前面已经验证过在 JDK8 中是完美运行的】，发现报错了，但是错误信息是乱码的，导致看不出来错误信息是什么，也就没法解决问题。在 JDK7 中 deploy 报错乱码 其实，这只是 IDEA 的编码设置问题，更改一下编码就行了。在 setting –&gt; maven –&gt; rumnner –&gt; VMoptions ，添加参数：-Dfile.encoding=GB2312 ，就可以正常输出了。当然，Windows 系统配置编码为 GBK 也行。为什么要这么配置呢，因为 Maven 是依赖于当前系统的编码的，可以使用 mvn -version 命令查看编码的信息，查看 Default locale 那一项，可以看到是 GBK。配置完成后，报错信息正常显示 问题总结1、通过手动导入证书的方式，一开始解决了问题，后来过了一段时间突然又不能使用，这时候我很是疑惑的。问了问同事却都能正常使用，我还以为是我的 Maven 的版本问题，换了 Maven 版本也不行，最后折腾了很久发现是私服域名的 SSL 证书失效了，再重新导入一份就行了。因为私服域名的 Let’s Encrypt 证书有效期只有三个月，所以每次证书续期或者更换的时候，都要手动重新导入，旧证书会自动失效。这样多麻烦，所以还是直接升级 JDK 比较好，一劳永逸。2、在与同事的开发环境对比的过程中，仔细对比了 Maven 的版本和 JDK 的版本，发现都是 Maven 3.5 与 JDK1.7，但是别人能用我的就不能用，一度怀疑人生。最终才发现根本原因是没有对比小版本号，同样是 JDK1.7，没有 &gt;=7u111 也不行。3、关于 Maven 的插件版本问题，切记要手动指定自己认为可靠的版本，不要让 Maven 使用仓库最新的稳定版本，哪怕的确是使用最新的版本，也要指明，确保出了问题自己可控，否则就像无头的苍蝇乱打乱撞。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Maven</tag>
        <tag>JDK</tag>
        <tag>证书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[武功山两天徒步登山总结]]></title>
    <url>%2F2018071601.html</url>
    <content type="text"><![CDATA[公司部门组织团建，在 2018-07-13 这天开始，一行二十多人傍晚从公司出发，踏上了去往江西武功山的旅程。本文就详细讲述 2018-07-13 晚上从广州出发，2018-07-16 凌晨到达广州的整个旅程。 图文并茂，等待整理。]]></content>
      <categories>
        <category>游玩</category>
      </categories>
      <tags>
        <tag>武功山</tag>
        <tag>徒步</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[辣椒炒肉做法总结]]></title>
    <url>%2F2018063001.html</url>
    <content type="text"><![CDATA[辣椒炒肉，农家小炒肉，青椒肉片，很多类似的菜品，有的是川菜，有的是湘菜，但是它们有一个共同点，都是食材简单、可口下饭，本文就讲述辣椒炒肉的步骤以及需要注意的地方，本文讲述的做法是湘菜的做法。食材准备 辣椒炒肉的食材比较简单：辣椒 4 个（最好是螺丝椒，次之也可以用普通的青椒）二刀肉 300 克（或者是普通的后腿肉也行，实在没有用前腿肉也勉强可以，确保肥瘦比例是 2：3 就行了，一定要有肥肉）老抽、生抽、料酒（用来腌制瘦肉使用，最后也要使用生抽提鲜）食用盐 大蒜 5 瓣 炒制步骤 1、螺丝椒 4 个，脆嫩皮薄，买的时候一定要挑选颜色比较鲜亮的（颜色发暗的不能要，不新鲜了），同时捏起来比较脆硬（软的不能要），说明新鲜。此外尽量挑大个的，比较容易去籽、容易滚刀切。买回来后去除头部，去除籽粒（一定要去除干净，否则炒制的时候辣椒籽容易变黑变苦，影响颜色与口感，当然稍微有个别的辣椒籽可以忽略），滚刀切成小块，沥干水分（尽量让它保持干燥）； 认准螺丝椒 螺丝椒去籽洗干净 滚刀切螺丝椒 2、将二刀肉去皮，肥瘦分离（这个步骤自己处理比较麻烦，最好让卖肉的大叔大姐帮忙处理了），肥肉切片，稍微切大一点，方便后续炼油，沥干水分，不做任何处理，直接放入碗中备用。瘦肉也是切片，放入碗中，接着放入生抽、老抽、食用盐、料酒，用手搅拌 1 分钟，再腌制 10 分钟； 肥肉沥干水分装碗备用 腌制瘦肉 3、大蒜切片，每一瓣切 3-5 片，尽量厚一点，备用； 忽略掉旁边的葱花和蒜粒，那是炒其它菜使用的 4、干锅煸炒辣椒（这个过程大概 3 分钟），先将炒锅烧干烧热，不放油、不放水，直接将步骤 1 中切好的辣椒片扔进去（这也是辣椒切好后必须沥干水分的原因），就是干煸，这是决定这道菜口感的第一步。控制中小火（大火容易糊），让辣椒块水分逐渐挥发，表皮起皱，即俗称起虎皮，这时候会有少量烟冒出，并伴随着独特的呛香辣味（像我这样家里没有抽油烟机的，只能在旁边准备好湿毛巾，不时去捂一下口鼻）。此时可以稍微放一点食用盐，这样辣椒才能入味，看到辣椒表皮起皱了，就可以了，切记不能熟透（熟透严重影响口感，因为最后还要回锅调味）。此时大概七八分熟，俗称断生，盛出放入碗中备用；5、炒制肥肉（这个过程大概 6 分钟），大火将炒锅烧热，放入少量花生油（少量就可以，只是为了防止肥肉下锅时粘锅，不是为了炒菜），下肥肉，先煸炒 1 分钟，待煸炒出少量猪油，火力转中小火，开始炼油。这里耗费时间长一点，大概 5 分钟，肥肉慢慢消失，本来的肥肉块越来越小，越来越干，炼出了很多猪油，很香； 肥肉炼油，很香 6、炒制瘦肉（这个过程大概 1 分钟），等步骤 5 中的肥肉炼成了肉干（这里自己把握，也可以炼油时间短一点，保留多一点肥肉，吃起来更香），火力转大火，将步骤 2 中腌制的瘦肉倒进来翻炒，大概 1 分钟，炒到瘦肉变色，基本熟了；7、调味翻炒，辣椒回锅（这个过程大概 2 分钟，根据放调味料和辣椒的速度而定），等瘦肉基本熟了，放入大蒜片（不是一开始和瘦肉一起放，否则大蒜片都炒烂了），此时按照湖南的做法，还要放点豆豉进来（注意这是几秒内就要完成的动作，如果大蒜片和豆豉没放在边上，记得先关小火，等放完了再开大火）。接着把步骤 4 中的干煸后的辣椒倒进来，先翻炒几下，然后不停地用锅铲戳，戳 1 分钟左右，倒点生抽提鲜，放点食用盐，再翻炒 10 秒，关火。 辣椒炒肉成品，不过辣椒有点炒过头 注意事项 1、所谓 二刀肉 ，是指屠户旋掉猪尾巴那圈肉以后，靠近后腿的那块肉，因为它是第二刀，顾名思义，就称为二刀肉。那地方的肉有肥有瘦，肥瘦搭配，一刀肉肥的多，二刀肉是 肥四瘦六 ，比较适合做这道菜，同时也适合做 回锅肉 这道菜；2、腌制瘦肉，煸炒辣椒块，都已经放过少量的盐了，所以最后千万不要又放了很多盐，根据个人口味添加；3、购买辣椒时，一定要选择新鲜的，否则口感不好；4、步骤 4 中的干煸辣椒千万不要把辣椒炒熟了，否则最后的成品辣椒不够脆嫩，而且辣味会全部丢失；5、这道菜看起来注定非常油腻，因为肥肉炼出了大量的猪油，但是吃起来不会腻，而且菜底的这个油非常香，甚至可以用来拌饭吃；6、为了让成品的颜色好看一点，腌制瘦肉时需要老抽，如果在炒制瘦肉的最后发现颜色不够，可以再补加一点老抽。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>辣椒炒肉</tag>
        <tag>川菜</tag>
        <tag>湘菜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[红烧肉做法总结]]></title>
    <url>%2F2018060301.html</url>
    <content type="text"><![CDATA[红烧肉，是一道做法非常简单的传统菜品，而且有多种版本，也有多种口味，同时基于红烧肉再补充其它配菜，又创新出了很多菜品。本文就讲述红烧肉的做法总结，口味偏甜，江南一带的做法。食材准备 以下的食材份量大约 2 人份（实在吃多了也会腻的，要和其它菜配在一起吃）：1、五花肉 500 克；2、冰糖 30 克；3、大葱 3 段、姜片 3 片、香叶 2 片、八角 2 个、桂皮 2 小块、大蒜 2 瓣（可以不用）；4、老抽（糖色不够的时候加老抽补充颜色）、料酒、食用盐（看口味，有时候不需要加盐了）；制作步骤 1、配料准备，装盘备用，实际上只需要少量即可，比我想象中的少很多； 几种大料准备 2、五花肉切块，本来应该切大块的，我的炒锅小，同时五花肉的量也少，不方便做，就切小块了（如果买的五花肉质量不好，肥肉肥油比较多的话，不适合直接下锅，最好焯水一下，把肥油过滤掉一些，如果肉的质量好，直接沥干水分就可以）； 原始的五花肉 3、炒糖色，下五花肉，炒糖色其实就是放少量油，把冰糖融化，然后在高温下冰糖渐渐变色，类似棕色或者深红色，此时放进去的五花肉小块就有颜色了（如果炒糖色成功了，后面就不用放老抽了，但是要注意不能炒太久，否则糖会变苦），此步骤同时也可以把五花肉小块定型，防止煮的过久烂掉； 给五花肉小块上色 4、加水（水量很重要，漫过所有的肉块再多一点，否则最后无法完成收汁操作），放大料，开大火煮开，然后转为小火，煮 50 分钟（中途可以晃一下锅，最好不要打开锅盖，还要注意一下水够不够）； 加水，放大料 煮了 30 分钟 5、50 分钟后，开锅，拣出大料，扔掉，开中火，收汁（一般 10 分钟就够，如果水量少了可能 3 分钟水就干了，看情况根据需要及时放老抽、食用盐等调料），我放的糖不多，于是加了一点食用盐，收汁完成盛出（可以看到我这份颜色还是不够啊），成品看起来虽然有点油腻，但是吃起来绝对肥而不腻，入口即化。 拣出大料 收汁完成，成品 附加一份以前做的另外一份红烧肉，当时忘记炒糖色，只好使用老抽上色，并且加了很多盐，味道也是非常棒（由于变成了咸口味，感觉味道和卤肉差不多）注意事项 1、炒糖色这一步骤是为了替代生抽给红烧肉上色，那种棕色或者深红色是糖遇热产生的颜色，很好看（一开始炒完颜色很好看，然后加水煮的时候可能看不出来了，没关系，等最后收汁的时候颜色会回来的），此外，炒糖色这一步骤也可以不进行，直接在煮的时候放糖（增甜增鲜），然后最后加老抽上色即可；2、大火煮开之后一定要转小火，慢慢煮（小火才能把肉煮的又透又软，达到入口即化的地步），就和普通的煲汤的火力一样，煮够 50 分钟；3、其实只有在五花肉的质量非常好的情况下，才能免除焯水，一般的五花肉肥油太多，不提前处理一下做出来的红烧肉非常油腻。 而且，很多人看到五花肉那么肥，就不想吃，或者觉得不好吃。其实不是的，正是由于肥肉的存在才能煮出那种香味，如果大部分都是瘦肉，也做不出来红烧肉，那种三肥七瘦的五花肉最好。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>红烧肉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[玉米胡萝卜排骨汤做法总结]]></title>
    <url>%2F2018053001.html</url>
    <content type="text"><![CDATA[排骨汤，是一道做法非常简单的汤，需要的只是新鲜的食材与足够的耐心而已。除了排骨，还可以增加玉米、胡萝卜这两种配菜，以增加排骨汤的甘甜与鲜美。本文就讲述玉米胡萝卜排骨汤的做法总结。食材准备 用最少的食材做排骨汤，才能做成真正的排骨汤，不需要各种配料，也不需要各种调味料，最终炖出来的排骨汤才能有排骨的鲜美。以下的食材分量大约 3 人份：排骨 300 克（排骨越好汤越好，我用过各种价格的排骨，12 元 / 斤 - 40 元 / 斤）甜玉米 1 根（稍微大一点嫩一点最好，买玉米的时候可以品尝一粒生的）姜片 2-3 片（不能太多，或者也可以不放）胡萝卜一根 食用盐适量 制作步骤 不计算食材的准备，从开火到关火总耗时预计 70 分钟；1、清洗排骨，稍微用水冲洗一下，然后在冷水中浸泡 10 分钟 - 20 分钟，目的是去除杂质与血水，但是油脂仍然保留，一般购买排骨的时候会让卖菜的帮忙剁好，否则自己用菜刀处理很麻烦；2、清洗玉米，切片，可以切薄一点，大概每片的厚度是 3 粒玉米的距离，玉米很难切，所以菜刀一定要使用锋利点的，否则会损坏玉米粒的，如果购买的玉米很新鲜，玉米棒里面也是很甜的，有助于增加排骨汤的甘甜；3、清洗胡萝卜，去皮，切片，注意最好去皮，否则最终部分胡萝卜皮会混在汤里，影响汤的品质；4、姜片，准备 2-3 片即可；5、汤锅准备好，加水，加姜片，冷水就下排骨，开大火煮，水开后立马转小火，把水表面的浮沫撇去，小火继续煮 30 分钟；6、步骤 5 的 30 分钟后，加玉米片，继续小火煮 20 分钟；7、步骤 6 的 20 分钟后，加胡萝卜片，继续小火煮 10 分钟；8、加盐调味，关火；一锅排骨汤 一碗排骨汤 以前做的另外一锅排骨汤，当时买的排骨 38 元 / 斤 注意事项1、排骨不能洗的太彻底，比如洗排骨的时候用力搓，不仅洗掉了血水，还把油脂洗没了，这样会导致汤里面丧失了香味；2、如果选择了排骨焯水（或者是选择了先在水里煮一下），切记不要焯太久（或者煮太久），10-30 秒即可，目的只是去除血水，否则也会流失油脂；3、为了省事，我一般不会对排骨怎么清洗，稍微用水冲一下，确保没有杂质与血水即可，然后直接煮，但是切记煮开后立即用勺子撇掉浮沫（油脂与碎渣），否则最终的汤会浑浊，而且没有香味；4、玉米一定要选择甜玉米，嫩的最好；5、时间一定要控制好，总计 70 分钟，也要注意火力的控制，除了一开始是大火，后面的 60 分钟全部是小火；6、如果只想喝汤，排骨可以用来做红烧排骨、糖醋排骨、酱排骨等等。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>排骨汤</tag>
        <tag>玉米排骨汤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[注册 Facebook Twitter Tumblr 遇到的问题]]></title>
    <url>%2F2018020101.html</url>
    <content type="text"><![CDATA[本文讲述注册使用 Facebook、Twitter、Tumblr 等社交账号的过程、遇到的问题、解决的办法，给自己留一个备份，同时也可能给大家带去一丝方便。FacebookTwitter注册 注册 Twitter 帐号，首先需要一个邮箱帐号，或者手机号，进入注册首页，进行信息填写 注册页 ，填写完成后，接下来也就是常规流程，发送短信验证码、语音验证码、邮箱激活链接等，基本没什么问题。 绑定手机号问题 由于我选择的是 Google 邮箱注册，注册完成之后正常登录，但是进入不到主页面，就被绑定手机号页面拦截了，一直提示需要添加一个手机号，要不然就在当前页面，什么也做不了，除非退出。但是呢，诡异的是我使用自己的手机号进行绑定时，提示错误：由于技术问题，无法完成当前的请求，请重试（Due to a technical issue, we couldn’t complete this request. Please try again.），我怀疑是因为中国的手机号无法进行绑定。上网搜索了一下资料，果然是这个原因，大家都建议一开始直接使用手机号注册，不要使用邮箱注册，就不会有这个问题了。接下来没有办法了，只能尝试寻找可行的办法，毕竟邮箱已经注册过了，不想浪费。绑定手机号解决方案尝试 官方说当前帐号疑似是机器人（不是一个真实的人类），所以被冻结了，必须添加一个可用的手机号，用来接收验证码，才能证明当前帐号是人为注册的，才能进行接下来的操作。1、利用 Chrome 浏览器的开发者工具更改下拉列表的值，把日本的编号 81 改为 86，应用在页面上，实际操作发现不行，Twitter 验证的时候还会重新刷新下拉列表。在 Chrome 浏览器的对应页面，按下键盘的 F12 按键，就可以打开调试工具（或者点击鼠标的右键，选择检查），在 “Elements” 选项中可以看到源代码，更改表单里面的下拉列表的值，即可。2、去帮助中心，找客服，发送申诉邮件，说明你是一个真实的人，现在注册帐号被冻结了，Twitter 帮助中心 。在帮助中心选择 “Contact us”，进一步选择 “View all support topics”。 进入选择页面后，进一步选择 “Suspended or locked account”，对冻结或者锁定的帐号进行申诉处理。最终进入的页面就是这样的：申诉信息填写 。 这里面最主要的内容就是问题描述，请描述清楚你的问题，另外设备的选择按照自己的实际情况填写，全名和手机号也按照实际情况填写。此外，注意填写信息前需要登录帐号，否则页面是锁定状态，无法填写任何信息，而且登录后，大部分信息都是自动填充完成的，无需填写，只需要填写重要的几项内容。例如我填写的问题描述：1Account suspended.Could not unsuspend it through phone number.Pls help to unsuspend the account.Thanks.提交后会收到一封由 Twitter 官方技术支持（Twitter Support &#115;&#117;&#112;&#x70;&#x6f;&#114;&#x74;&#64;&#x74;&#119;&#105;&#x74;&#116;&#x65;&#114;&#46;&#x63;&#x6f;&#109;）发送的邮件，告诉你应该怎么做，邮件内容如下。但是看内容也看不出来什么，只是说疑似机器人帐号，需要绑定手机号码，列出一系列步骤。其实我也是想做这一步，但是奈何中国的手机号码不支持。仔细看最后一句话，如果还有问题，可以直接回复此邮件并说明问题详细。接下来我又回复了一封邮件，说明遇到的问题，内容大概如下，解释说明自己是一个真实的人，但是由于手机号码是中国的，无法接收到验证码，请求解决：1234Hello, I try in this way,But i am in China,i can not receive messages. I am a human indeed,and my phone number is +86 1********06. best wishes.接下来就是等待官方的回复了（希望晚上睡一觉后明天有好消息）。在等待了一夜后，又过了半天时间（总共大概 17 个小时），收到了 Twitter 官方的回复，说我的帐号已经解冻，并解释了原因。这次回复等待了这么长时间，不像上次申诉回复那么快，说明很大可能是人工审核的，然后解冻了你的帐号，再回复这封通知邮件给我。不管怎样，帐号可以使用了。接下来为了保证不被封号，最好重新设置一下昵称，并且填写一些必要的信息：用户名（id）、头像、生日、国家、描述等，也可以关注一些其他推主。更改用户名在 “Settings and Privacy” 里面，由于用户名是唯一的（和 GitHub 的策略一样），所以常用的都被别人注册过了，自己要注意寻找，否则更改不了，显示被占用。更改昵称、头像、背景墙、描述等在 “Profile” 里面。流程总结：1、只针对使用 Google 邮箱注册的情况，注册后帐号被冻结，什么也做不了，绑定手机号又说不支持，只能通过申诉来解决；2、申诉的目的是为了解冻帐号，但是官方是自动回复，让绑定手机号，又回到了原地；3、在步骤 2 的基础上可以直接回复邮件（邮件中有提示），说明遇到的问题，等待将近一天就行了；（如果没有步骤 2，直接给官方技术支持发邮件，不知道行不行）4、步骤 3 官方回复的邮件中，提示说不要回复此邮件。（回复了应该也没人理）Tumblr]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>Facebook</tag>
        <tag>Twitter</tag>
        <tag>Tumblr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017102901.html</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.Quick StartCreate a new post1$ hexo new "My New Post"More info: WritingRun server1$ hexo serverMore info: ServerGenerate static files1$ hexo generateMore info: GeneratingDeploy to remote sites1$ hexo deployMore info: Deployment]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>建站</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VLOOKUP 函数跨工作表跨文件使用方式]]></title>
    <url>%2F2017051401.html</url>
    <content type="text"><![CDATA[今天在处理 Excel 文件的时候，需要使用 VLOOKUP 函数，感觉很方便。内心有一种掌握了一个小技巧就可以节省很多时间的骄傲感，同时，除了入门级别的使用，还进一步发现了可以跨工作表、跨文件使用这个函数，顿时觉得更加方便了。我觉得这个函数在日常工作中应该很常用，而且很好用，所以本文就记录这个函数的使用方式，以及简单介绍 Excel 中的函数概念。基础概念介绍 什么是函数 在 Excel 中，函数就是官方定义的一种通用公式，例如求和、求平均数、计数、查找，可以帮助用户方便快捷地处理数据。掌握了这些函数，用起来 Excel 才算是合格水平，有时候遇到难以处理的数据，可能使用几个公式就解决了。Excel 的基础功能就是存数据，然后基于这些数据再做进一步处理，此时为了方便高效，就产生了很多函数。以下内容中提及的 函数 或者 公式 都是同一个概念。函数的优点 各个行业的人使用 Excel 肯定有自己的感悟，以及觉得 Excel 某些函数特别好用，也就是可以说，一千个使用 Excel 函数的人眼里，有一千个函数的优点。我在这里只是泛泛地列举几个例子。1、常用的四则运算函数完全可以替代计算器。2、财务方面的函数可以帮助财务工作人员快速处理数据。3、逻辑方面的函数可以处理一些简单的逻辑，即使用简单的函数脚本解决。4、查找引用方面的函数，可以高效地进行搜索替换。在 Excel 中可以看到常用的函数分类：函数使用 先简单介绍一下求和函数【SUM】，以保证有个基本的认知。例如有一个 Excel 文件，有三列数字，分别是语文、数学、英语的成绩，现在需要计算每一行的第一列到第三列的和，求和结果放在第四列，也就是总成绩。只要在第四列中，输入函数以及参数：1=SUM (A2:C2)截图如下：其中，=SUM () 是函数名称，表示累加求和，括号里面的 A2:C2 是单元格的位置，: 表示从 A2 到 C2，总结起来就是把第二行的 A 列到 C 列的值累加求和，得出结果。按回车键，就会触发计算，得出结果，结果存放在写有函数的那个单元格。如果需要接着计算第三行、第四行、第五行，是不需要重复输入函数以及参数的，直接选中已经有结果的单元格，鼠标的光标放在单元格右下角，光标会变成一个黑色的十字，鼠标左键长按往下拖拽即可。选中单元格，注意观察单元格右下角的大点 往下拖拽，自动计算 注意，使用复制下拉功能时，行号参数是会自动变化的，也就是说 每一行 的求和结果都是 当前行 的第一列到第三列的数值之和。可以任意选择一行的结果单元格，查看单元格的内容：那这个是怎么做到自动化的呢？其实这是 Excel 自带的功能，术语称为 自动填充 ，不知道你有没有看到，在往下拖拽完成后，可以点开右下角的三角下拉列表，看到里面有三种模式选择，默认的就是 复制单元格 【你也可以试玩一下其它的两种模式】。 复制单元格 对于普通的单元格来说，直接复制内容，对于函数单元格来说，还会自动变更里面的参数。结果已经完全出来，可以正常使用。另外再说一个隐藏的注意点，这种通过函数产生的结果，是不能复制粘贴到别的地方使用的，因为复制粘贴过去的内容是函数公式，不是那个计算结果值。因此如果直接复制粘贴到别的地方，它还会用这个函数计算，得到的结果就与单元格数据当前所在的地方有关，结果肯定和以前不一样，或者根本没有结果。例如我把总成绩和姓名这两列复制粘贴到别的 Excel 文件里面，可以看到得到的结果都是 0，这是因为通过函数计算得出的结果就是 0，表格的第一列到第三列根本没有值。那怎么解决这个问题呢，其实方法是有的：复制时还是正常的复制函数，粘贴时不能默认了，要选择 粘贴为数值 ，或者 选择性粘贴 。这样，粘贴结果单元格里面就是真实的数值了，函数公式已经不见了。此时的单元格就是普通的单元格，里面是文本内容，可以随意复制粘贴使用。粘贴为数值 选择性粘贴 接下来介绍本文的重点：VLOOKUP 函数。先提前说明， 工作表 就是指 Excel 文件中的 Sheet 概念，新建的 Excel 文件一般默认有 3 个 Sheet。以下内容基于两份数据：学生表、成绩表。基本使用方式 VLOOKUP 是一个文本类型的函数，用来匹配搜索的。如果在单一的工作表中使用，例如根据姓名搜索总成绩，可以使用：1=VLOOKUP (H2,A:E,5,FALSE) 截图如下：其中，VLOOKUP 是函数名称，H2 表示需要查找的数据列，A:E 表示搜索的数据范围【此时不需要指定单元格的行号】，5 表示搜索命中的的结果列，5 一定是在 A:E 之间，FALSE 表示关闭模糊搜索，即精确搜索。这里总结起来就是根据 H2 的值，在 A:E 之间搜索【会搜索所有的行】，如果命中了结果，把 A:E 之间的命中那一行的第 5 列单元格【也就是 E 列】的值返回，搜索时使用精确匹配。结合上面的截图，更通俗地说，就是根据 H2 的姓名，在成绩表 A:E 的所有行中搜索同姓名的人，把命中行 E 列的成绩返回，匹配姓名时必须完全相等才算命中。也可以选中结果下拉，自动填充其他人的总成绩。跨工作表使用 上面的内容是在同一个工作表中搜索，如果是跨工作表使用怎么办呢？，例如一个 Excel 文件有两个工作表：学生表、成绩表，现在要根据学生表的姓名从成绩表中搜索总成绩。其实做法也是很简单，函数都是同一个函数，只不过在指定数据范围这个参数的时候，需要加上工作表的名称：1=VLOOKUP (A2, 成绩表！A:E,5,FALSE)成绩表信息 学生表信息 这里除了额外指定了参数 成绩表！A:E 来指定 成绩表 这个 Sheet，其它的参数仍旧与前面一致。跨文件使用 接着又有问题了，如果两个表是分开两个文件呢，聪明人已经可以想到，肯定是继续更改参数的值：1=VLOOKUP (A2,[成绩表.xlsx] 成绩表！A:E,5,FALSE)其中，[成绩表.xlsx] 成绩表！A:E 就是用来指定文件、工作表、数据列的。但是这里需要同时打开两个文件，这样编辑器才能找到文件的内容。这里也可以看出来，为什么在 Excel 中不能同时打开两个同名的文件了，因为 Excel 要以文件名作为一份数据的唯一标识，哪怕两份同名的文件存放在不同的目录，也会被 Excel 当做同一份文件。此外，还有一个小特性，只要函数生成完毕，就可以把成绩表关闭了，不会影响已经搜索出来的结果。而且，如果继续在学生表中增加姓名，还可以继续完成搜索，也就是说 Excel 是把成绩表做了缓存，可以一直使用。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>VLOOKUP</tag>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 框架 Next 主题添加自定义 Page]]></title>
    <url>%2F2017050701.html</url>
    <content type="text"><![CDATA[在整理博客的过程中，发现需要新增一些页面，对于 Hexo 框架来说是 page 的概念，例如 首页 、 关于 、 分类 、 搜索 等页面。这种页面不同于每一篇博客文章那种发表的内容，对于 Hexo 框架来说是 post，而是可以交互的页面，例如可以在 搜索 页面中搜索博客的内容，可以在 分类 页面中查看博客文章的分类统计。当然，类似于 关于 这种页面也是静态的，没有交互的概念。上面提到的这些页面都是 Next 主题自带的，只要在 _config.yml 配置文件中开启相关配置即可，不需要关心它是怎么实现的，例如开启了 分类 页面，它会自动把博客的分类统计好，展示出来。但是我的想法其实是新增一个页面，并且自定义图标、名称、内容，其实也可以实现，本文记录这个过程。自带的页面 Hexo 自带的页面有好几种，例如：关于、首页、分类、搜索、站点地图、404 页面等，可以在主题的配置文件中查看 menu 选项 。例如我使用的是 Next 主题，在 themes/next/_config.yml 中查看 menu 选项，我这里已经配置好 home、about、tags、categories、archives，此外还有没有开启的 schedule、sitemap、commonweal 等，先忽略我新增的 books 页面。 这里面的配置有固定的格式，一共有四列：第一列是展示的名字以及页面标识、第二列是 url 地址、第三列是固定的双竖线、第四列是图标名称。我这里使用 about: /about/ || user 举例，about 就是页面的名字【虽然配置的是英文，但是有汉化字典转为中文，汉化字典文件为：themes/next/languages/zh-Hans.yml】，/about/ 是页面的 url 地址，表示从主页跳转的地址，前面加上域名可以直接访问，|| 双竖线是固定标识符，user 是图标名称，来自于一个图标库：https://fontawesome.com 。只要开启这个配置，就可以看到关于的页面。这些页面都不需要特殊的处理，直接配置完成就可以直接使用，可以在项目的 source 目录里面查看子文件夹，每个子文件夹都会对应一个页面，文件夹里面有一个 index.md 文件，就是页面的原始数据。但是对于搜索、分类、归档等可以交互的页面，Hexo 在渲染时还会重新计算，这里面的 index.md 文件没有内容，只是表示开启了这个页面。而对于静态页面，直接在相应的 index.md 文件里面写上内容就行了，Hexo 值了渲染不会再重新计算内容。例如关于页面，就可以使用 Markdown 语法在 about/index.md 文件里面写上关于作者的简介，我下面要新增的页面也是类似这种格式。各种页面对应的子文件夹 新增页面 了解完了自带的页面，接下来准备新增自定义页面。我需要新增的是一个静态页面，名称为 书籍 ，里面会列出我的读书清单，并给出书籍的部分信息。生成页面并编辑 经过查询 Hexo 的语法，生成新页面的命令为：hexo new page name，page 是关键字，name 表示页面的名字，我直接使用 hexo new page books 即可。执行完命令后，可以在 source 目录看到生成了一个 books 目录，里面有一个 index.md 文件，直接编辑这个页面即可。简单编辑内容如图：这里需要注意文件头的内容，有固定的格式：123456---title: 书籍 date: 2019-04-25 00:16:58type: bookscomments: false---其中，title 就是渲染后 html 网页的居中标题以及网页的 title 标签值，会在浏览器的 tab 页上面显示【这里也可以使用英文名称 books，但是需要在汉化文件的 title 选项下面增加中英文配置，和后面的 menu 汉化类似】。type 就是页面的类别，与自定义页面名称保持一致。此外 comments 切记关闭，因为博客如果开启了评论功能，会默认在所有的页面都开启评论框，而这种自定义页面是不需要评论框的，因此选择关闭，即设置为 false。开启页面配置 在主题的配置文件 themes/next/_config.yml 中，配置自定义页面，在 menu 选项下面，配置内容如下：1books: /books/ || book截图如下：其中，books 是新建的页面名称，/books/ 是链接，book 是图标【因为没有 books 图标可以使用，只能使用 book 图标了，原因在最后会描述，主要是收费问题】。汉化页面名称 配置 themes/next/languages/zh-Hans.yml 文件，也是在 menu 选项下面，配置内容如下：1books: 书籍 截图如下：打开页面预览 在博客点击书籍页面或者直接输入 域名 /books/ 链接，打开页面。注意事项 注意，图标是来自于图标库：https://fontawesome.com ，只要提供图标的名字即可，Hexo 会自动匹配对应的图标展示。需要特别注意的是，这里面的图标有大部分是收费的【搜索时会显示灰色状态，能免费使用的才会显示黑色状态】，所以不能使用，即使配置了名称 Hexo 也不会展示出来。例如我想使用一个名字为 books 的图标，是收费的，发现 Hexo 不会展示，我换成了另外一个名字为 book 的免费图标，Hexo 就可以正常展示了。搜索图标结果]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
        <tag>page</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 输出重定向的问题]]></title>
    <url>%2F2017050401.html</url>
    <content type="text"><![CDATA[最近遇到一个好玩的事，在使用 Linux 命令执行任务【Java 程序或者 Python 程序】时，需要把输出内容日志重定向到文件中，并且保持任务后台执行，这样就可以继续执行其它的命令，不占用 SSH 的 session。而且，如果等不及，直接退出 SSH 登录即可，任务会在后台继续执行，下次重新登录时可以继续查看任务的状态、分析日志的内容。这里面会涉及到重定向、设备文件、输出类型的概念，本文记录这个问题以及涉及的相关知识点。基础概念 https://blog.csdn.net/ithomer/article/details/9288353 详细举例 在这里如果只使用 nohup 不使用 &amp; 把程序后台挂起，表面上看把输出日志重定向文件中了，屏幕不再滚动打印出来大量的文本内容。其实，程序此时仍旧在占用着键盘的输入流，你的命令行在等待着输入，你无法使用键盘进行其它的命令操作，而且你不能使用 ctrl + c 的方式中断，否则程序会退出。注意事项 1、在执行 Python 脚本时，发现日志内容并没有及时更新【在 Python 脚本中 print 的内容】，实时查看日志文件的内容【使用 tail -f name.log 命令】，发现不会像 Java 程序那样实时刷出来新的内容，而是会卡住一段时间，然后突然一大段日志出来。造成这种现象的原因是 Python 有输出流缓存机制，不会把输出内容实时写入输出流，而是等待缓冲区积累一定的内容再操作，这样一来，重定向到文件中的内容总是一批一批的。 当然，可以选择关闭这个选项，在执行 Python 脚本时，使用 -u 参数就可以把输出内容实时写入到输出流，也就可以实时重定向到日志文件了。]]></content>
      <categories>
        <category>Linux 命令系列</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>redirect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于 Spark 或者 mapreduce 的累加器]]></title>
    <url>%2F2017043001.html</url>
    <content type="text"><![CDATA[在 Spark 和 mapreduce 中都有累加器的概念，使用方式也是大同小异，但是还是有一点点不同的地方。累加器基础概念 mr 程序的累加器不用单独打印出来最终结果，在运行日志中可以看到统计值，但是在 yarn 中却看不到。而 spark 程序可以在 yarn 中查看累加器。 累加器的使用 1、Spark 中的使用 注意版本的影响，使用方式不一致。在程序中对累加器只能增加，不能取值，在 driver 端等待任务结束时才能取值。2、mapreduce 中的使用 在 driver 端会自动打印出自定义的累加器取值。参考：https://www.cnblogs.com/cc11001100/p/9901606.htmlhttps://www.jianshu.com/p/9d6111fc6303]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>mapreduce</tag>
        <tag>Accumulator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch 中的 429 错误 es_rejected_execution_exception]]></title>
    <url>%2F2017042601.html</url>
    <content type="text"><![CDATA[今天在处理数据，处理逻辑是从 HBase 中扫描读取数据，经过转换后写入 Elasticsearch 中，程序的整体方案使用的是 mapreduce 结构。map 负责扫描 HBase 数据，并转换为 Map 结构，reduce 负责把 Map 结构的数据转为 JSON 格式，并验证合法性、补充缺失的字段、过滤非法数据等，最后使用 elasticsearch 官方发布的 BulkProcessor 把数据批量写入 elasticsearch。在处理数据的过程中，遇到了一个诡异的问题，说它诡异是因为一开始不知道 BulkProcessor 存在的坑。关于这个问题，表面现象就是漏数，写入 elasticsearch 中的数据总是少于 HBase 中的数据，而且差距巨大。当然，如果是有经验的工程师，可以猜测好几个原因：扫描读取 HBase 的数据时设置过滤器过滤掉了不该过滤的数据、ETL 的处理逻辑中有误过滤数据的 bug、写入 elasticsearch 时数据不合法导致写入失败、由于 BulkProcessor 潜在的问题导致写入漏数。本文就记录解决这个问题的过程。问题出现 问题其实就是漏数，HBase 里面的数据写入到 Elasticsearch 后发现数据量对不上，而且重跑了几次作业，每次重跑都会有多一点点的数据写入 Elasticsearch，这就很诡异了，不像普通的漏数。这个漏数现象复现不了，虽然每次重跑作业都会漏数，但是数据量对不上，说明背后有一只无形的手在操控着这一切，而且操控过程随心所欲，让人疑惑不解。漏数现象出现后，作为一个有经验的工程师，我先初步怀疑了几个关键点，然后逐步分析，抽丝剥茧，找到了问题所在。Elasticsearch 版本为 v5.6.8。问题解决 怀疑点排查 1、扫描读取 HBase 的数据时设置过滤器过滤掉了不该过滤的数据，经过查看，扫描过滤器只是设置了某个时间字段的范围，并且提交作业时设置的参数属于正常范围，不会影响数据量，排除此种可能。2、ETL 的处理逻辑中有误过滤数据的 bug，仔细查看了 ETL 的处理逻辑，里面有多处过滤数据的处理逻辑，例如发表时间、id 等必要的字段必须存在，但是不会过滤掉正常的数据，而且给对应的过滤指标设置了累加器。一旦有数据被正常过滤掉，累加器会记录数据量的，在作业的日志中可以查看，排除此种可能。3、写入 elasticsearch 时数据不合法导致写入失败，在作业运行中，如果出现这种情况，一定会抛出异常【使用 BulkProcessor 不会抛出异常，但是有回调方法可以使用，从而检测异常情况】，所以在业务代码中，考虑了异常情况的发生，把对应的数据格式输出到日志中，方便查看。我仔细搜索检查了日志文件，没有发现数据不合法的异常日志内容，排除此种可能。4、由于 BulkProcessor 潜在的问题导致写入漏数，这个怀疑点就比较有意思了，使用 BulkProcessor 来批量把数据写入 elasticsearch 时，会有两个隐藏的坑：一是写入失败不会抛出异常，注意，批量的内容全部失败或者部分失败都不会抛出异常，只能在它提供的回调方法【afterBulk ()】中捕捉异常信息，二是资源紧张会导致 elasticsearch 拒绝请求，导致写入数据失败，注意，此时也不会抛出异常，只能通过回调方法捕捉错误信息。所以有可能是这个原因。 重点排查 好了，已经逐条分析了可能的原因，并初步定位了最有可能的原因，接下来就是利用 BulkProcessor 提供的回调方法，把异常信息捕捉，并在日志中输出所有必要的信息，以方便发现问题后排查具体原因。代码更新完成后重跑作业，为了速度快一点，先筛选少量的数据进行重跑，然后观察日志。查看日志，发现有大量的错误信息，就是从 BulkProcessor 的回调方法 afterBulk () 里面捕捉打印的【以下日志片段本来是一行，为了友好地显示，我把它格式化多行了】：12345678910111213...... 省略 2019-04-23 17:37:22,738 ERROR [I/O dispatcher 68] org.playpi.blog.client.es.ESBulkProcessor: bulk [43 : 1556012242738] - &#123; &quot;cause&quot;: &#123; &quot;reason&quot;: &quot;Elasticsearch exception [type=es_rejected_execution_exception, reason=rejected execution of org.elasticsearch.transport.TransportService$7@45c00a5f on EsThreadPoolExecutor [bulk, queue capacity = 1500, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@53ffdb18 [Running, pool size = 32, active threads = 32, queued tasks = 4527, completed tasks = 26531491]]]&quot;, &quot;type&quot;: &quot;exception&quot; &#125;, &quot;id&quot;: &quot;f176fd6b68d22ad357a61714313d2748&quot;, &quot;index&quot;: &quot;org-playpi-datatype-post-year-2018-v1&quot;, &quot;status&quot;: 429, &quot;type&quot;: &quot;post&quot;&#125;...... 省略 更多错误内容如截图所示 找到里面的关键信息：es_rejected_execution_exception、“status”: 429，到这里，可以确定这个错误不是由于数据格式不合法导致写入 Elasticsearch 失败，否则错误信息应该携带 source invalid 字样。可惜，进一步，我看不懂这个异常错误，只能借助搜索引擎了。经过搜索，发现这个问题的原因在于 Elasticsearch 集群的资源不足，处理请求的线程池队列全部被占用，无法接收新的请求，于是拒绝，这也就导致了数据漏掉。在这里先提前说明一下，以下内容的配置信息是基于 数据索引所在的集群、节点 ，例如索引 A 在某个集群，分配了 3 个节点，那就只看这个集群的这 3 个节点，可能还有其它几百个节点存放的是其它的数据索引，不用关心。这样才能准确找到问题所在，否则如果看到配置信息对不上，就会感到疑惑。另外在使用 API 接口时，可以在 url 结尾增加 ?pretty 协助格式化结果数据，查看更容易，?v 参数可以协助返回结果增加表头，显示更为友好。其实，Elasticsearch 分别对不同的操作【例如：index、bulk、get 等】提供不同的线程池，并设置线程池的线程个数与排队任务上限。可以在数据索引所在节点的 settings 中查看，如果有 head 插件【或者 kopf 插件】，在 概览 -&gt; 选择节点 -&gt; 集群节点信息 中查看详细配置。其中在 settings -&gt; thread_pool 里面有各个操作的线程池配置。这里面，有两种类型的线程池，一种是 fixing，一种是 scaling，其中 fixing 是固定大小的线程池，默认是 core 个数的 5 倍，也可以指定大小，scaling 是动态变化的线程池，可以设置最大值、最小值。如果不使用 head 插件，直接通过 Elasticsearch 集群的 http 接口【前提是开放 http 端口或者设置了转发端口，否则无法访问】也可以获取这个数据，例如通过 /_nodes / 节点唯一标识 /settings/ 查看某个节点的配置信息。这个节点唯一标识【uuid】可以通过 head 插件获取，我这里使用 q6GpFsnCSOOfLoLl72MVAg 演示。使用 head 插件获取节点的唯一标识。使用 API 接口查看节点的配置信息 可以看到数据所在节点的线程池配置，对于 bulk 类型的操作，线程池的大小为 32【由于 min 和 max 都设置为了 32，并且线程池类型为 fixing，所以是 32】，队列上限为 1500。好，至此，再结合上面错误日志中的信息：bulk, queue capacity = 1500、Running, pool size = 32, active threads = 32, queued tasks = 4527，可以发现，当前节点【某个 node，不能说整个集群】处理数据时线程的队列已经超过了上限 1500，而且我惊讶地发现已经到达了 4527，这种情况下 Elasticsearch 显然是要拒绝请求的。此外，使用集群的 API 接口也可以看到节点的线程池使用情况，包括拒绝请求量，/_cat/thread_pool?v，查看详情如下图所示。不妨再次探索一下 mapreduce 的日志，搜索关于 bulk 的错误，可以看到大量的错误都是这种，超过队列上限而被拒绝请求。解决方案 原因找到了，解决方案也可以定下来了。1、给 Elasticsearch 的索引增加更多的节点，这样就可以把线程池扩大了，但是需要消耗资源，一般无法实现。2、优化批量的请求，尽量不要发送多个小批量的请求，而是发送少量的大批量请求。这个方法还是适合的，把 bulk 请求的数据量增大一点，收集多一点数据再发送请求。3、改善索引性能，让文档编制索引速度更快，这样处理请求就更快，批量队列就不太容易阻塞了。这个方法说起来容易，做起来有点难，需要优化整个索引设计，例如取消某些字段的索引、删除冗余的字段等。4、在不增加节点的情况下，把节点的线程池设置大一点、队列上限设置大一点，就可以处理更多的请求了。这个方法需要改变 Elasticsearch 集群的配置，然后重启集群，但是一般情况下会有风险，因为节点的硬件配置【内存、CPU】没有变化，单纯增加线程池，会给节点带来压力，可能会宕机，谨慎采用。配置信息参考如下：1234-- 修改 elasticsearch.yml 配置文件 threadpool.bulk.type: fixedthreadpool.bulk.size: 64threadpool.bulk.queue_size: 15005、如果确实在硬件、集群方面都无法改变，那就直接在使用方式上优化吧，例如把并发设置的小一点，请求一批后休眠一段时间，保障 Elasticsearch 可以把请求处理完，接着再进行下一批数据的请求。这种做法立竿见影，不会再影响到 Elasticsearch 的线程池，但是缺点就是牺牲了时间，运行作业的时间会大大增加。迫于资源紧张，我只能选择第 5 种方式了，减小并发数，数据慢慢写入 Elasticsearch，只要不再漏数，时间可以接受。问题总结 除了上面的排查总结，再描述一下一开始针对业务逻辑的具体的思路。拿到错误日志后，简单搜索统计了一下，一个 reduce 任务的错误信息有 16 万次，也就是有 16 万条数据没有成功写入 Elasticsearch。而整个 mapreduce 作业的 reduce 个数为 43，可以预估一下有 688 万次错误信息，也就是有 688 万条数据没有成功写入 Elasticsearch，这可是个大数目。再查看作业日志的统计值，累加器统计结果，在 driver 端的日志中，发现一共处理了 1413 万数据，这样一计算，漏掉了接近 49% 的数据，太严重了。再对比一下我文章开头的描述，每次重跑作业，总是有一部分数据可以重新写入 Elasticsearch，但是成功的数据量仅仅限于几十条、几条。最终还差 500 多条数据的时候，已经重跑了 5 次以上了，所以我才会更加怀疑是程序写入 Elasticsearch 方式的问题。]]></content>
      <categories>
        <category>踩坑系列</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
        <tag>Elasticsearch</tag>
        <tag>HBase</tag>
        <tag>BulkProcessor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决 IDEA 无法创建子包的问题]]></title>
    <url>%2F2017042201.html</url>
    <content type="text"><![CDATA[最近在使用 IDEA 的时候，发现一个奇怪的问题，如果新建了一个多层的包，再想新建一个和除第一层包之外的包等级别的子包就不行。说的这么绕口，什么意思呢？举例来说，比如我新建了一个包，完整路径为：a.b.c.d，如果再想新建一个和 d 等级别的子包 e：a.b.c.e，就不行，IDEA 会默认在 d 下面新建一个子包，那整个包就变成了：a.b.c.d.e，这显然是不合常理的，也不是我需要的。本文记录这个问题的解决方案。问题出现 当在 IDEA 中新建一个 Java 项目的包时，完整路径为：org.playpi.blog，再想新建一个和 blog 等级别的包：www，结果发现 www 是建在了 blog 下面，那就变成了 org.playpi.blog.www，这不是我想要的结果。新建一个包 新建一个子包 注意，上面的现象是在包只有一个的情况下，还没有创建等级别的其它包，如果创建了等级别的其它包就不会有这种现象了。例如如果已经有了 org.playpi.blog、org.playpi.www，再想创建一个 org.playpi.doc，是可以做到的。问题解决 有一种粗暴并且略微繁琐的方法，那就是在新建多层的包时，不要单独创建，而是一层一层创建，并且和类文件【或者其它类型的文件也行，只要不是单纯的包即可】一起创建，这样就可以稳妥地创建多个等级别的包了。但是这种做法显然很傻，而且有时候根本不需要每个包都有等级别的包存在。其实，IDEA 有自己的设置方式，可以看到在项目树形结构的右上方，有一个设置按钮【齿轮形状】，点开，可以看到 Hide Empty Middle Packages，意思就是 隐藏空白的中间包 ，这个选项默认是开启的。注意，这里的 Empty Packages 并不是严格意义上的空包【对应对操作系统的空文件夹】，而是指包里面只有一个子包，并没有其它的类文件或者任意文件。所以，新建的多层的包都会被隐藏，再新建子包时，默认是从最深处的包下面创建，这样也就发生了问题出现的那一幕。解决起来就很容易，把这个选项去掉，不要隐藏，全部的包都显示，这样就可以轻松地新建多个子包了。这样做有一个缺点，对于那些有多个空白包的情况， 都显示出来很难看，不友好，所以最好还是在需要时临时关闭这个选项，等不需要了再打开，毕竟隐藏空白包的效果看起来还是很清爽的。注意，当去掉隐藏选项时，选项的名字会变为 Compact Empty Middle Packages，收起空白包，其实意思和隐藏空白包一样。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>IDEA</tag>
        <tag>package</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 入门系列 0-- 初识 Hadoop]]></title>
    <url>%2F2017040101.html</url>
    <content type="text"><![CDATA[待开始整理 开始 今天是愚人节。]]></content>
      <categories>
        <category>Hadoop 从零基础到入门系列</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
</search>
