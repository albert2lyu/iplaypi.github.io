<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Nginx 配置 SSL 证书实现 HTTPS 访问]]></title>
    <url>%2F2019030501.html</url>
    <content type="text"><![CDATA[由于 GitHub Pages 把百度爬虫屏蔽了，导致百度爬虫爬取不到我的个人主页，所以被百度收录的内容很少，能收录的基本都是我手动提交的。后来我的解决办法就是自己搭建了一台 Web 服务器，然后在 DNSPod 中把百度爬虫的访问流量引到我的 Web 服务器上面，服务器主机是我自己购买的 VPS，服务器应用我选择的是强大的 Nginx。本文就记录 Web 服务器搭建以及配置 SSL 证书这个过程。安装 Nginx我的 VPS 是 CentOS 7 X64 版本的，所以安装 Nginx 的过程比较麻烦一点，需要自己下载源码、编译、安装，如果需要用到附加模块【例如 http_ssl 证书模块】，还需要重新编译，整个过程比较耗时。如果不熟悉的话，遇到问题也要折腾半天才能解决。所以，我在不熟悉的 Nginx 的情况下选择了一种简单的方式，直接自动安装，并自带了一些常用的模块，例如 ssl 证书模块。但是缺点就是安装过程稍微长一点，在网络好的情况下可能需要 3-5 分钟。我还参考了别人的文档：https://gist.github.com/ifels/c8cfdfe249e27ffa9ba1 ，但是仅供参考，因为我发现也有一些不能使用的地方。创建源配置文件 在 /etc/yum.repos.d/ 目录下创建一个源配置文件 nginx.repo，如果不存在这个目录，先使用 mkdir 命令创建目录，然后在目录中添加一个文件 nginx.repo，使用命令：1vi nginx.repo进入编辑模式，填写如下内容：12345[nginx]name=nginx repobaseurl=http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck=0enabled=1编辑完成后保存即可。自动安装 Nginx接下来就是使用命令自动安装 Nginx 了【敲下命令，看着就行了，会有刷屏的日志输出】：1yum install nginx -y安装完成后，使用以下命令启动：1service nginx start可以使用命令 service nginx status 查看 Nginx 是否启动：然后你就能看到 Nginx 的主页了，默认是 80 端口，直接使用 ip 访问即可【如果这里打不开，可能是端口 80 没有开启，被防火墙禁用了，需要重新开启，开启方法参考后面的章节】。获取 SSL 证书、配置参数 SSL 证书获取 证书的获取可以参考我的文章：利用阿里云申请免费的 SSL 证书 。我在阿里云获取的证书是免费的、有效期一年的，等证书过期了可以重新申请【不知道能不能自动续期】，因为我有阿里云的帐号，所以就直接使用了。当然，通过其它方式也可以获取 SSL 证书，大家自行选择。 直接下载即可，下载后上传到站点的任意目录，但是要记住文件的位置，因为等一下配置 Nginx 的时候需要指定证书的位置。我把它们放在了 /site/ 目录，一共有 2 个文件：.key 文件时私钥文件，.pem 文件时公钥文件。Nginx 参数配置 更改配置文件，打开文件【使用 vi 命令会自动创建不存在的文件】，进入编辑模式：12# 配置 vi /etc/nginx/nginx.conf填写内容如下【我这里只是配置基本的参数 server 有关内容，大家当然可以根据实际需要配置更为丰富的参数】，留意证书的公钥与私钥这 2 个文件的配置：123456789101112131415161718192021222324252627# 80 端口是用来接收基本的 http 请求，里面做了永久重定向，重定向到 https 的链接 server &#123; listen 80; server_name blog.playpi.org; access_log /site/iplaypi.github.io.http-blog-access.log main; rewrite ^/(.*)$ https://blog.playpi.org/$1 permanent; &#125;# 443 端口是用来接收 https 请求的 server &#123; listen 443 ssl;# 监听端口 server_name blog.playpi.org;# 域名 access_log /site/iplaypi.github.io.https-blog-access.log main; root /site/iplaypi.github.io; ssl_certificate /site/1883927_blog.playpi.org.pem;# 证书路径 ssl_certificate_key /site/1883927_blog.playpi.org.key;#key 路径 ssl_session_cache shared:SSL:1m;# 储存 SSL 会话的缓存类型和大小 ssl_session_timeout 5m;# 配置会话超时时间 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;# 为建立安全连接，服务器所允许的密码格式列表 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on;# 依赖 SSLv3 和 TLSv1 协议的服务器密码将优先于客户端密码 #减少点击劫持 add_header X-Frame-Options DENY; #禁止服务器自动解析资源类型 add_header X-Content-Type-Options nosniff; #防 XSS 攻击 add_header X-Xss-Protection 1; &#125;只要按照如上的配置，就可以同时接收 http 请求与 https 请求【实际上 http 的请求被永久重定向到了 https】，我的配置如下图【请忽略 www 二级域名的配置项】：验证参数是否准确 有时候配置了参数，可能因为字符、参数名问题导致启动失败，然后再回来改配置文件，比较繁琐，所以可以直接使用 Nginx 提供的命令来验证配置文件的内容是否合法，如果有问题可以在输出警告日志中看到，改起来也非常方便。1nginx -t可以看到，配置项正常，接下来就可以启动 Nginx 了。开启端口、启动 Nginx在上面的步骤中，如果在一开始想启动 Nginx，虽然启动成功了，但是却访问不了 Nginx 的主页，那很大可能是服务器的端口没有开启，导致访问请求被拒绝，所以需要适当开启必要的端口【如果没有安装防火墙工具 firewall 请自行安装】。1234567891011121314# 查看已经开启的端口 firewall-cmd --list-ports# 开启端口 80firewall-cmd --permanent --zone=public --add-port=80/tcp# 开启端口 443firewall-cmd --permanent --zone=public --add-port=443/tcp# 重载更新的端口信息 firewall-cmd --reload# 这种方式可以，启动 Nginxservice nginx start# 停止 Nginxservice nginx stop# 如果需要重启，直接使用下面的更方便 nginx -s reload大家看一下我的服务器的端口开启信息：验证站点 打开站点 https://blog.playpi.org ，可以愉快地访问了，可以看到 https 链接的绿锁。接着查看一下 SSL 证书的信息。题外话 重定向问题思考 关于开启 https 的访问，我一开始也配置了 www 的二级域名，但是通过日志发现没有通过 301 重定向访问 https://www.playpi.org 的请求，一直不明白原因。后来发现，因为做重定向的时候还是重定向到 GitHub 上面了。同理，如果使用 ip 直接访问，可以观察到自动跳转到 https://www.playpi.org 了，查看证书还是 GitHub 的证书。所以后来直接把百度爬虫的请求转发到 blog 的二级域名还是明智的【www 的二级域名就不用自己再搞一套了】，否则百度爬虫还是抓取不到。如果百度爬虫直接使用 https 链接抓取还是可以的，但是看百度站长里面的说明，是通过 http 的 301 重定向抓取的。Nginx 的 https 模块安装 由于我使用的是简单小白的安装方式，不需要关心额外用到的模块，例如 http_ssl 模块，因为安装包里面自带了这个模块，可以使用 nginx -V 命令查看。因此，如果大家有使用源码编译安装的方式，注意 https 模块不能缺失，否则不能开启 https 的方式。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>https</tag>
        <tag>ssl</tag>
        <tag>证书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用阿里云申请免费的 SSL 证书]]></title>
    <url>%2F2019030401.html</url>
    <content type="text"><![CDATA[在搭建博客的过程中，一开始是全部使用 GitHub，因为这样做就什么也不用考虑了，例如主机、带宽、SSL 证书，全部都交给 GitHub 了，自己唯一需要做的就是写 Markdown 文档。但是，后来发现 GitHub 把百度爬虫给禁止了，也就是百度爬虫爬取不到 GitHub 的内容，导致我的站点没有被百度收录。后来为了专门给百度爬虫搭建一条线路，自己搭建了一个镜像服务，也就是和 GitHub 上面的内容一模一样站点，是专门给百度爬虫使用的。而且，为了测试方便，在 DNSPod 中还增加了一条 blog 二级域名的解析记录，blog 的访问全导向自己的镜像，这样就可以方便观察部署是否成功。后来还把百度爬虫的 www 访问通过 CNANE 跳转到 blog 去，这样就不用单独再搞一个 www 了，因为挺麻烦的（域名解析线路问题、测试问题、证书确认问题，都挺麻烦）。而在这个过程中，就产生了使用阿里云申请免费的 SSL 证书这一流程（有效期一年），记录下来给大家参考。注册阿里云、开启实名认证 这个步骤就不多说了，需要证书总得注册一个帐号吧，也方便后续管理。此外，国内的证书服务商都要求实名认证，这个也没办法。如果不想实名认证，可以使用开源的 Lets Encrypt ，只不过有效期只能是 3 个月，也就是说每隔 3 个月就要更新一次，GitHub Pages 使用的就是它。阿里云的官网链接：(https://www.aliyun.com)[https://www.aliyun.com]。购买 SSL 证书 1、在阿里云系统找到关于 SSL 证书的服务， 产品与服务 -&gt; 安全（云盾）-&gt;SSL 证书（应用安全）。2、进入后，点击右上角的 购买证书 。3、按照我截图中的步骤 1、2、3 选择，这里需要注意，这个免费的选项隐藏的很深，直接勾选是不会出现的，要按照我标识的步骤来勾选才行，这里看到出现的费用很贵不用害怕，等一下接着选择对了就会免费的。最终选择 免费型 DV SSL，按照我下图中的选项，可以看到费用是 0 元。选择后，下单即可，虽然要走购买流程，但是是不用付钱的。绑定证书信息、等待审核 1、下单完成后开始 申请 ，这里的 申请 的意思是申请使用它，要填写一些基本的信息，包括个人信息和网站信息，后续还需要验证身份，看你有没有权限管理你配置的网站。如果不申请 使用 ，证书其实就一直闲置在那里。填写个人信息，主要就是我个人的联系方式。填写网站信息，由于我使用的是自己的服务器上面搭建的 Web 服务，既没有使用阿里云也没有使用其它云服务，所以我选择了 文件验证 ，即需要把验证文件上传到我的域名对应的目录下面，用来证明这个站点是我管理的。当然，验证通过后，这个文件可以删除。2、填写完成后，会生成一个文件 fileauthor.txt，我需要把这个文件下载下来，然后上传到我的服务器对应的目录中，才能点击 验证 按钮，如果通过了，说明这个站点就是我管理的，也就是一个权限验证。由于在验证 www 证书对应的文件的时候，需要把 fileauthor.txt 文件上传到服务器，但是由于在 DNSPod 中设置的域名解析是解析到 GitHub 的（没有专门针对阿里的设置），所以总是验证失败。后来就干脆临时把所有的 www 解析都指向我自己的服务器，等通过了验证再改回去，整个过程很是折腾。折腾了一大圈，最后还发现了更简单的方法，直接放弃 www 证书的申请，在 DNSPod 中把百度的流量通过 CNAME 直接引到 blog 上面去就行了，这样只要维护一个 blog 的 Web 服务就行了。这样只需要增加一条解析，而且 blog 的证书验证过程也方便简单。DNSPod 解析示例 在这个过程中，我还发现验证过程需要一定的时间，一开始显示失败，但是不告诉我原因，还以为是自己的服务器的问题，重试了多种方法，包括重启 Web 服务。我等了十几分钟，证书就莫名其妙审核通过了，然后还发送了短信通知（到这里我猜测阿里云的 Web 界面显示的内容是滞后的，短信通知的内容才是实时的）。证书申请成功，可以使用了。下载证书、上传到自己的服务器 下载证书、上传到自己的服务器这一步骤就不多说了，主要就是复制粘贴的工作。着重要说一下 Nginx 的配置，主要就是 server 属性的配置，由于我把 www、blog 这 2 个二级域名都保留了，所以需要分开配置。其实，这里配置的 www 的二级域名根本没有用，因为不会有流量过来的，重在测试证书的安装。Nginx 的配置内容参考（2 个子域名分开配置，有 2 份 SSL 证书）：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253 server &#123; listen 80; server_name www.playpi.org; access_log /site/iplaypi.github.io.http-www-access.log main; rewrite ^/(.*)$ https://www.playpi.org/$1 permanent; &#125; server &#123; listen 80; server_name blog.playpi.org; access_log /site/iplaypi.github.io.http-blog-access.log main; rewrite ^/(.*)$ https://blog.playpi.org/$1 permanent; &#125; server &#123; listen 443 ssl;# 监听端口 server_name www.playpi.org;# 域名 access_log /site/iplaypi.github.io.https-www-access.log main; root /site/iplaypi.github.io; ssl_certificate /site/1884603_www.playpi.org.pem;# 证书路径 ssl_certificate_key /site/1884603_www.playpi.org.key;#key 路径 ssl_session_cache shared:SSL:1m;# 储存 SSL 会话的缓存类型和大小 ssl_session_timeout 5m;# 配置会话超时时间 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;# 为建立安全连接，服务器所允许的密码格式列表 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on;# 依赖 SSLv3 和 TLSv1 协议的服务器密码将优先于客户端密码 #减少点击劫持 add_header X-Frame-Options DENY; #禁止服务器自动解析资源类型 add_header X-Content-Type-Options nosniff; #防 XSS 攻击 add_header X-Xss-Protection 1; &#125;server &#123; listen 443 ssl;# 监听端口 server_name blog.playpi.org;# 域名 access_log /site/iplaypi.github.io.https-blog-access.log main; root /site/iplaypi.github.io; ssl_certificate /site/1883927_blog.playpi.org.pem;# 证书路径 ssl_certificate_key /site/1883927_blog.playpi.org.key;#key 路径 ssl_session_cache shared:SSL:1m;# 储存 SSL 会话的缓存类型和大小 ssl_session_timeout 5m;# 配置会话超时时间 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;# 为建立安全连接，服务器所允许的密码格式列表 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on;# 依赖 SSLv3 和 TLSv1 协议的服务器密码将优先于客户端密码 #减少点击劫持 add_header X-Frame-Options DENY; #禁止服务器自动解析资源类型 add_header X-Content-Type-Options nosniff; #防 XSS 攻击 add_header X-Xss-Protection 1; &#125;配置完成后重启 Nginx（使用 nginx -s reload），去浏览器查看证书信息，看到有效期一年。打开链接，看到左上角的小绿锁，好了，网站是经过验证的了。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>https</tag>
        <tag>阿里云</tag>
        <tag>SSL证书</tag>
        <tag>Lets Encrypt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年关于技术的未解之谜]]></title>
    <url>%2F2019030101.html</url>
    <content type="text"><![CDATA[由于技术能力的限制，平时会遇到一些自己觉得非常诡异的问题，感觉到莫名其妙。其实到头来发现，归根结底还是自己的认知问题：可能是技术水平不够，或者考虑不周全，甚至是一些低级别的错误判断。总而言之，遇到这些问题后，有时候请教人、查资料之后仍旧不得解，只能先记录下来，留做备注说明，等待以后解决。当然，随着时间的流逝，有些问题可能就被忘记了，有些问题在之后的某一个时间点被解决了。本文就是要记录这些问题，并在遇到新问题或者解决老问题之后，保持更新。常用链接 在这里先列出一些常用的网站链接，方便查看：es-hadoop 官网：https://www.elastic.co/guide/en/elasticsearch/hadoop/5.6/configuration.html ；xes-spark 读取 es 数据后 count 报错 使用 es-hadoop 组件，起 Spark 任务去查询 es 数据，然后过滤，过滤后做一个 count 算子，结果就报错了。而且，在报错后又重试了很多次（5 次以上），一直正常，没法重现问题。这个任务需要经常跑，以前从来没遇到过这样的异常，初步怀疑是 es 集群不稳定，具体原因不得而知。错误截图：完整错误信息如下（重要包名称被替换）：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556572019-02-26_15:01:44 [main] ERROR spokesman3.SpokesAndBrand:510: !!!!Spark 出错: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name at [Source: org.apache.commons.httpclient.AutoCloseInputStream@2687cf14; line: 1, column: 17581]org.elasticsearch.hadoop.rest.EsHadoopParsingException: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name at [Source: org.apache.commons.httpclient.AutoCloseInputStream@2687cf14; line: 1, column: 17581] at org.elasticsearch.hadoop.rest.RestClient.parseContent (RestClient.java:171) at org.elasticsearch.hadoop.rest.RestClient.get (RestClient.java:155) at org.elasticsearch.hadoop.rest.RestClient.targetShards (RestClient.java:357) at org.elasticsearch.hadoop.rest.RestRepository.doGetReadTargetShards (RestRepository.java:306) at org.elasticsearch.hadoop.rest.RestRepository.getReadTargetShards (RestRepository.java:297) at org.elasticsearch.hadoop.rest.RestService.findPartitions (RestService.java:241) at org.elasticsearch.spark.rdd.AbstractEsRDD.esPartitions$lzycompute (AbstractEsRDD.scala:73) at org.elasticsearch.spark.rdd.AbstractEsRDD.esPartitions (AbstractEsRDD.scala:72) at org.elasticsearch.spark.rdd.AbstractEsRDD.getPartitions (AbstractEsRDD.scala:44) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply (RDD.scala:239) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply (RDD.scala:237) at scala.Option.getOrElse (Option.scala:120) at org.apache.spark.rdd.RDD.partitions (RDD.scala:237) at org.apache.spark.SparkContext.runJob (SparkContext.scala:1929) at org.apache.spark.rdd.RDD.count (RDD.scala:1157) at org.apache.spark.api.java.JavaRDDLike$class.count (JavaRDDLike.scala:440) at org.apache.spark.api.java.AbstractJavaRDDLike.count (JavaRDDLike.scala:46) at com.package.to.class.SpokesAndBrand.getMention (SpokesAndBrand.java:508) at com.package.to.class.SpokesAndBrand.runCelebrityByBrand (SpokesAndBrand.java:185) at com.package.to.class.SpokesAndBrand.execute (SpokesAndBrand.java:116) at com.package.to.class.SpokesmanAnalyzer.execute (SpokesmanAnalyzer.java:162) at com.package.to.class.SpokesmanAnalyzeCli.execute (SpokesmanAnalyzeCli.java:154) at com.package.to.class.SpokesmanAnalyzeCli.start (SpokesmanAnalyzeCli.java:75) at com.package.to.class.util.AdvCli.initRunner (AdvCli.java:191) at com.package.to.class.job.client.BasicInputOutputSystemWorker.run (BasicInputOutputSystemWorker.java:79) at com.package.to.class.model.AbstractDataReportWorker.run (AbstractDataReportWorker.java:122) at com.package.to.class.buffalo.job.AbstractBUTaskWorker.runTask (AbstractBUTaskWorker.java:63) at com.package.to.class.report.cli.TaskLocalRunnerCli.start (TaskLocalRunnerCli.java:110) at com.package.to.class.util.AdvCli.initRunner (AdvCli.java:191) at com.package.to.class.report.cli.TaskLocalRunnerCli.main (TaskLocalRunnerCli.java:43)Caused by: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name at [Source: org.apache.commons.httpclient.AutoCloseInputStream@2687cf14; line: 1, column: 17581] at org.codehaus.jackson.JsonParser._constructError (JsonParser.java:1433) at org.codehaus.jackson.impl.JsonParserMinimalBase._reportError (JsonParserMinimalBase.java:521) at org.codehaus.jackson.impl.JsonParserMinimalBase._reportInvalidEOF (JsonParserMinimalBase.java:454) at org.codehaus.jackson.impl.Utf8StreamParser.parseEscapedFieldName (Utf8StreamParser.java:1503) at org.codehaus.jackson.impl.Utf8StreamParser.slowParseFieldName (Utf8StreamParser.java:1404) at org.codehaus.jackson.impl.Utf8StreamParser._parseFieldName (Utf8StreamParser.java:1231) at org.codehaus.jackson.impl.Utf8StreamParser.nextToken (Utf8StreamParser.java:495) at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.mapObject (UntypedObjectDeserializer.java:219) at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.deserialize (UntypedObjectDeserializer.java:47) at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.mapArray (UntypedObjectDeserializer.java:165) at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.deserialize (UntypedObjectDeserializer.java:51) at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.mapArray (UntypedObjectDeserializer.java:165) at org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.deserialize (UntypedObjectDeserializer.java:51) at org.codehaus.jackson.map.deser.std.MapDeserializer._readAndBind (MapDeserializer.java:319) at org.codehaus.jackson.map.deser.std.MapDeserializer.deserialize (MapDeserializer.java:249) at org.codehaus.jackson.map.deser.std.MapDeserializer.deserialize (MapDeserializer.java:33) at org.codehaus.jackson.map.ObjectMapper._readValue (ObjectMapper.java:2704) at org.codehaus.jackson.map.ObjectMapper.readValue (ObjectMapper.java:1286) at org.elasticsearch.hadoop.rest.RestClient.parseContent (RestClient.java:166) ... 29 more2019-02-26_15:01:44 [main] INFO rdd.JavaEsRDD:58: Removing RDD 3086 from persistence listHexo 生成 html 静态页面目录锚点失效 我这些所有的博客文档是先写成 Markdown 文件，然后使用 Hexo 渲染生成 html 静态页面，再发布到 GitHub Pages 上面，还有一些是发布到我自己的 VPS 上面（为了百度爬虫）。但是最近我发现一个现象，有一些文章的锚点无效，也就是表现为目录无法跳转，例如想直接查看某一级目录的内容，在右侧的 文章目录 中直接点击对应的标题，不会自动跳转过去。这个问题我发现了很久，但是一直没在意，也没有找到原因。最近才碰巧发现是因为标题内容里面有空格，这才导致生成的 html 静态页面里面的锚点失效，我随机又测试了几次其它的页面，看起来的确是这样。下面列出一些示例：123https://www.playpi.org/2019022501.html ，Hexo 踩坑记录的 https://www.playpi.org/2018121901.html ，js 字符串分割方法 https://www.playpi.org/2019020701.html ，itchat 0 - 初识 但是，我又发现其他人的博客，目录标题内容中也有空格，却可以正常跳转，我很疑惑。现在我猜测是 Hexo 的问题，或者哪里需要配置，等待以后的解决方法吧。别人的博客示例：https://blog.itnote.me/Hexo/hexo-chinese-english-space/ 。xx]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>ElasticSearch</tag>
        <tag>es</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 的踩坑经验]]></title>
    <url>%2F2019022501.html</url>
    <content type="text"><![CDATA[大家知道，我是使用 Hexo 来构建我的静态站点的，每次使用 Markdown 语法书写 md 文档即可。写完后在本地使用 hexo g &amp; hexo s 命令【在本地生成并且部署，默认主页是 localhost:4000】来验证一下是否构建正常。如果有问题或者对页面效果不满意就返回重新修改，如果没有问题就准备提交到 GitHub 上面的仓库里面【在某个项目的某个分支】，后续 travid-cli 监控对应的分支变化，然后自动构建，并推送到 master 分支。至此，更新的页面就发布完成了，本人需要做的就是管理书写 md 文档，然后确保没问题就提交到 GitHub 的仓库。问题清单 前言描述的很好，很理想，但是有时候总会出现一些未知的问题，而我又不了解其中的技术，所以解决起来很麻烦，大部分时候都是靠蒙的【当然，也可以直接在 Hexo 的官方项目上提出 Issue，让作者帮忙解决】。下面就记录一些遇到的问题，以及我自己找到的原因。1-Markdown 语法不规范 这个错误有在 travis 上面出现过，在 travis 的 116 号、117 号错误：https://travis-ci.org/iplaypi/iplaypi.github.io/builds/476399853 。在使用 hexo 框架的时候，一定要确保 markdown 文件里面的代码块标识【标记代码的类型，例如：java、bash、html 等】使用正确。否则使用 hexo g 生成静态网页的时候，不会报错，但是却没有成功生成 html 静态网页，虽然 html 静态文件是有的，但是却查看不了，显示一片空白。代码块示例：Java 格式 xml 格式bash 格式 例如我把图一的 java 误写成了 bash，hexo g 的时候没有报错，但是生成的 html 静态网页却是空白一片，打开了什么也看不到。空白页面 但是如果把 java 误写成了 xml，在本地执行 hexo g 的时候不会报错，生成的 html 静态网页也是正常的。而一旦使用 travis-cli 执行自动构建的时候，构建是失败的【在 travis 的 116 号、117 号错误：https://travis-ci.org/iplaypi/iplaypi.github.io/builds/476399853 】，并且可以看到错误信息，图四，但是我看不懂错误原因，只能猜测找到问题所在，比较耗时。travis-cli 报错日志【我看不懂】：travis-cli 日志 1travis-cli 日志 2travis-cli 日志 3此外，写 Markdown 文档，使用代码块标记的时候，使用 3 个反单引号来标记，如果不熟悉代码块里面的编程语言，可以省略类型，例如 java、bash、javascript，不要填写，否则填错了生成的 html 静态文件是空白的。还有就是如果代码块里面放的是一段英文文本，和编程语言无关，也不要填写类型，否则生成的 html 静态文件也是空白的。2-Hexo 报错奇怪 这个错误还没有到 travis 上面，所以 travis 上面没有记录；在本地测试过程中，无论是 hexo s 还是 hexo g 都会报错，错误信息如图：看着这个信息，很像在当前项目的目录中找不到 hexo 命令，和 java 类似，我就怀疑是不是安装的 hexo 被什么时候卸载了，其实不是的，在其它项目中还能用。后来我发现是当前项目使用的模块缺失，为什么会缺失我也不知道，由于这些缺失的模块是通过 hexo 引入的，所以直接报错：hexo not found，给人以误导。总的来说，就是报错有误导性，没有报模块缺失，而我又不懂这些，查了一些资料，手动测试了一些方法，总算找到原因所在。找到原因，那解决办法很简单了，直接安装缺失的模块即可，使用 nmp install 命令安装 package.json 里面的模块。3-Hexo 配置错误引起的误导性 这个错误还没有到 travis 上面，所以 travis 上面没有记录；这个错误和上面的类似，但是如果从报错信息上面看，也具有误导性。在更改了 _config.yml 配置文件后，按照正常步骤去生成、部署的时候【使用 hexo g &amp; hexo s 命令，直接报错了，把我整蒙了，报错信息如下：关键配置部分如下，后续找到问题确实出在这里：从图中看信息，我也看不到什么原因，因为确实不懂。注意，我为了测试，发现 hexo g 是没有问题的，也就是生成没问题，那问题就出在部署步骤了，它会不认这个 hexo s 命令？我查了资料，发现大部分人都说缺失 hexo server 模块，我通过检查可以确保本机有这个模块，而且卸载了重新装，所以不是这个问题。最后发现是配置信息里面的参数【官方定义的关键词】错误了，里面的 Plugins 这个参数应该使用首字母大写，这谁能想到，正确的配置参数如下图：4-travis 配置问题 这个错误有在 travis 上面出现过，在 travis 的 27 号：https://travis-ci.org/iplaypi/iplaypi.github.io/builds/448152737 。在使用 travis 自动构建时，有一次突发奇想，想使用最新版本的 node_js，于是在 travis.yml 配置文件中，把 node_js 设为了 stable，即稳定版本，这样在构建的时候会使用最新稳定版本的 node_js，没想到就出问题了。node_js 的配置如下：travis 报错日志如下：重要部分：12error nunjucks@3.1.3: The engine "node" is incompatible with this module. Expected version "&gt;= 6.9.0 &lt;= 11.0.0-0". Got "11.0.0"error Found incompatible module看来还是在搞清楚新旧版本之间的差异后再想着升级版本，不要随意来，要不然浪费的是自己的时间。后来解决办法就是手动指定 node_js 的版本。5 - 无缘无故出现的问题 这个错误有在 travis 上面出现过，在 travis 的 133 号、134 号错误、135 号错误、136 号错误，举例：https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498318318 ；日志部分截图：这错误信息里面对我来说确实看不到有效的内容，还没找到解决办法，看似是文件路径不存在，但是项目配置也没变过。123456789npm ERR! path /home/travis/.nvm/versions/node/v10.10.0/lib/node_modules/hexo-cli/node_modules/highlight.js/tools/build.jsnpm ERR! code ENOENTnpm ERR! errno -2npm ERR! syscall chmodnpm ERR! enoent ENOENT: no such file or directory, chmod '/home/travis/.nvm/versions/node/v10.10.0/lib/node_modules/hexo-cli/node_modules/highlight.js/tools/build.js'npm ERR! enoent This is related to npm not being able to find a file.npm ERR! enoent npm ERR! A complete log of this run can be found in:npm ERR! /home/travis/.npm/_logs/2019-02-25T18_45_08_713Z-debug.log等待找问题的原因。好，仔细看了日志、找了博客文档，没有解决方法，我也不懂，看到可能是版本原因【我不能升级 nodejs 版本，与 yarn 有关】，可能是权限问题。我用 sudo npm install -g hexo-cli 试了试，明显不行：https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498794142 ，然后我就放弃了，直接改回来提交了，没想到无缘无故就可以了，构建日志：https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498796865 。准备发邮件问问 travis 客服，我现在单方面怀疑是 travis 的环境问题或者构建脚本所依赖的环境问题。由于时差问题，先记录几个时区的缩写，方便查看邮件内容的时候核对时间：UTC【世界标准时间】、EST【东部标准时间，UTC-5】、CET【欧洲中部时间，UTC+1】。我发送的邮件内容如下【发送于北京时间 2019-02-28 14:42:00】：完整文字版供参考 123456789101112131415161718192021Automatic building is failedHello,I hava a repository in GitHub,and i use travis-ci to build it automatically.I configured the correct script,and it has been built successfully more than one hundred times.My script is :https://github.com/iplaypi/iplaypi.github.io/blob/source/.travis.yml ;But it built failed at 2019-02-26,the all log as follows (i retry it three times,but still failed):https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498267014https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498278045https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498297576https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498318318I cannot find any usefuI information in my log.I am very sad and helpless.But i retry it at 2019-02-27,it actually build successfully,amazing.I swear I have not changed any files,the successful log is:https://travis-ci.org/iplaypi/iplaypi.github.io/builds/498796865 ;So i am puzzled,i donnot know why,i suspect it is a problem with the machine.Can you help me?Best wishes. 发送后对方自动有一个回复，告知我他们的工作时间【中国北京时间与对方时差 + 13】：等了好几天，对方终于回复了【回复于北京时间 2019-03-04 11:00:00】，对方回复内容如下：对方回复重要文字内容 123456789Hey there,Thanks for reaching out and I&apos;m sorry for these spurious failures you have experienced.I think it could be an issue with the package itself or the NPM registry on that specific day. I&apos;ve looked at NPM&apos;s status and their was an incident on Feb. 27th. See https://status.npmjs.org/incidents/ptnlj2rtwfwm. Maybe it was already happening on Feb. 26th? Sorry for not having a better explanation.Please let us know if this issue resurfaces again, we would be happy to have another look.Thanks in advance and happy building! 看起来技术支持也没发现是啥问题，只是说有可能是 NPM 的问题，还给了一个链接：https://status.npmjs.org/incidents/ptnlj2rtwfwm，根据链接可以看到 NPM 的状态在某个时间点出问题了【时间点为 2019-02-27 15:46:00 UTC，也就是北京时间 2019-02-27 23:46:00】：但是我那个自动构建的问题是出在北京时间 2019-02-26 凌晨的，时间点也对不上，所以技术支持只是怀疑，也没有结论，那我也就不管了，继续观察以后有没有相同的问题出现。6 - 排版问题 1、在 Markdown 文件中关于链接的，要使用 []、() 这 2 个完整的标记，不要直接放一个链接出来，会导致生成的 html 文件带链接的内容居中对齐，导致文字分散开来，不好看。2、中文括号不要使用，也会导致居中对齐的问题，文字排版不好看，使用方括号吧：【内容示例】。7 - 草稿问题 我在使用 Hexo 的草稿功能时，发现一个问题，操作完成发布时，发现 Markdown 文档的头部描述信息变化了。例如我本来设置的 id 又变回了日期【可以理解，因为模板就是这样设置的】，然后 tags 的中括号中的标签变为了无需列表【不可理解】。暂时还没发现内容的变化，可能是内容中没有特殊符号。导致的问题就是草稿发布后【内容已经变化了】，提交到 source 分支，自动构建时，提交到主分支 master 后，这些文章的链接变为了日期的乱格式【因为是基于错误的 Markdown 文件构建的】。所以以后还是不要使用草稿功能了，没有必要，还麻烦，没写完也可以发布嘛，没啥大问题。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>markdown</tag>
        <tag>java</tag>
        <tag>bash</tag>
        <tag>xml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[鸡蛋饼做法总结]]></title>
    <url>%2F2019021001.html</url>
    <content type="text"><![CDATA[鸡蛋饼算是一种小吃，做法多种多样，可以煎，可以烙，可以蒸；吃法也多种多样，有的地方会卷配菜吃，有的地方会配粥吃，有的地方会直接吃。总而言之，鸡蛋饼算是一种万能美食，全国各地都有，大家也都喜欢吃，本文就记录鸡蛋煎饼的做法总结，本文记录的做法是采用煎的方式，另外还会额外放点葱花。食材准备 以下准备的食材可以煎 15 个鸡蛋饼左右：1、鸡蛋 4 个（喜欢的话多放点也可以）；2、200-300 克面粉（可以煎 15 个左右，面粉不能确定量，是因为如果面糊没有配好，就适量加水或者加面，调整好为止，具体会用到多少看情况了）；3、小葱 5 根（根据个人口味添加，多点少点都行）；适量的食材 制作步骤 制作步骤很简单，只要能煎好一张饼，重复进行就行了，煎好一张饼大概需要 3 分钟。当然，如果是第一次煎饼，可能煎前面几张饼的时候需要练习一下，也可能需要重新调制面糊，所以时间会长一点，但是为了煎饼成功，麻烦一点也值了。调制面糊的过程就不记录了，就是加盐（4 勺）、葱花、面粉、鸡蛋、水（最好可以用凉白开，别直接使用自来水）搅拌即可，如果里面有很多面疙瘩，不用担心，静置 10 分钟搅拌一次，重复 3 次左右面疙瘩即全部溶于水。面糊调制初步，还有很多面疙瘩 面糊调制完成 粘稠度大概这样，不会很粘稠，和液体差不多 1，锅里加油烧热，只要半勺即可（吃饭的那种小汤勺），多了会腻，然后火力转小火，并一直持续小火。 半勺油的量 2，放入一汤勺面糊（烧汤那种大汤勺，或者电饭煲自带的那种粥勺），如果发现煎出来的饼太厚了或者太大了，可以适当少放一点点面糊，具体放多少自己把握。然后适当转动煎锅，让面糊呈圆形（一定要快，10 秒内完成，否则因为受热不均匀，饼可能会散开变成多块，或者是一个圆环饼套着一个小圆饼），等逐渐凝固后就成了圆饼，然后接触锅的那一面就变得金黄，这个过程大概 1 分钟。 一大汤勺面糊 加面糊到锅里 转动成型 3，等凝固后就可以翻身了，这个步骤说简单也简单，说难也难，如果直接用锅不方便翻身的话，可以借助铲子，翻身后，可以看到饼的上一面已经煎好了，金黄的。这个时候注意要适当把饼转动一下，吸收一下油，避免粘锅。 给饼翻身 4，翻身后再煎 1 分钟左右，就可以出锅了，如果看到饼上面哪里煎的不均匀，还没熟，可以再着重煎几十秒。切记不能煎太久，要不然饼就糊了。 翻身后继续煎 1 分钟，再根据实际情况着重煎一下，准备出锅 5，出锅装盘，继续下一张鸡蛋饼。 全部出锅装盘 注意事项1、特别注意，如果调制面糊不是特别熟练的话，可能面糊的粘稠度不适合，或者调味偏淡偏咸，这样都不好，所以最好尝试着煎一个，然后品尝一下，如果味道不对再加调料，如果煎出来的饼不对，再加水或者面粉。多试几次，确保煎出来的饼自己满意。如果一味地煎饼，最后发现不好吃，那就浪费了；2、如果有两个锅可以用，为了节省时间，最好两个锅同时煎，要不然整个过程很枯燥，因为有一半的时间都在等待；3、有时候可能看着好像煎糊了，不用担心，不影响吃，因为出锅后等一会儿，褐色就会变成金黄色，非常好看；4、整个过程一定要确保是小火，否则饼很快就糊了；5、难点在于翻身，只要一出错一张饼就废了（或者变成了一堆碎饼）。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>鸡蛋饼</tag>
        <tag>鸡蛋葱饼</tag>
        <tag>鸡蛋煎饼</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ItChat 系列 0 - 初识 ItChat]]></title>
    <url>%2F2019020701.html</url>
    <content type="text"><![CDATA[微信已经是我们日常生活中常用的 APP 之一，每天都离不开。作为掌握技术的理工科人员，有时候总想着是否可以利用微信的接口完成一些重复的工作，例如群发消息、自动回复、接入机器人自动聊天等。当然，这些都可以实现，而且只要是人工可以做到的事情，基本都可以做到自动化（前提是微信提供了对应的接口，反例就是自动收发红包不行，当然微信不会直接提供 API 接口，需要自己寻找）。本文就讲解为了做到这些，需要的入门知识点，主要就是利用 ItChat 工具（屏蔽了微信的 API 接口，简化了使用微信接口的过程，不懂技术的普通人也可以轻松掌握），当然本文只是一个入门的例子而已（完成后对自己来说很实用而且有成就感），后续会讲解更加深入与广泛的内容。本文基于 Windows 7 操作系统，Python 2.7 版本（为了兼容性与易维护性，我推荐使用 Python 3.x 版本）ItChat 简介 摘录官方文档描述：itchat 是一个开源的微信个人号接口，使用 python 调用微信从未如此简单；使用不到三十行的代码，你就可以完成一个能够处理所有信息的微信机器人；当然，该 api 的使用远不止一个机器人，更多的功能等着你来发现；该接口与公众号接口 itchatmp 共享类似的操作方式，学习一次掌握两个工具；如今微信已经成为了个人社交的很大一部分，希望这个项目能够帮助你扩展你的个人的微信号、方便自己的生活。当然，我是觉得上面的描述有一些语句不通顺，但是不影响我们理解作者的原意。其实微信官方并没有提供详细的 API 接口，ItChat 是利用网页版微信收集了接口信息，然后独立封装一层，屏蔽掉底层的接口信息，提供一套简单的使用接口，方便使用者调用，这不仅提升了效率，还扩展了使用人群。使用入门 以下使用入门包括基础环境的安装、itcaht 的安装、代码的编写、实际运行，当然，为了避免赘述，不会讲解的很详细，如果遇到一些问题，自行利用搜索引擎解决。安装 Python 环境 下载 Python去官网：https://www.python.org/downloads/windows ，选择自己需要的版本，我这里选择 Windows 系统的版本（64 位操作系统），Python 2.7（这是一个很古老的版本了，推荐大家使用 3.x 版本）；我选择的版本 下载过程就和下载普通的文件、视频等一样，根据网速的限制有快有慢。安装 Python就像安装普通程序一样，直接双击下载的程序文件，选择安装即可，这里就不再赘述详细的安装过程了；如果你们的环境不是 Windows 7 系统的，可以自行使用搜索引擎搜索教程；这里一定要注意安装的版本是否适配自己的操作系统（包括系统类型与系统位数）；在 Windows 系统的 程序和功能 中查看已经安装完成的 Python 程序（2.7 版本，我是使用 Anaconda2 安装的，所以看起来有些不一样）：配置环境变量 如果这一步忽略了，使用 Python 或者 Python 自带的插件的时候（比如安装 ItChat 的时候就会用到 pip 工具），会找不到应用程序，只能先进入到 Python 目录或者插件所在的目录再使用对应的工具（例如进入 Python 所在的目录或者 pip 所在的目录），比较麻烦，所以在此建议大家配置一下环境变量；配置环境变量的过程也不再赘述，大家自己利用搜索引擎获取，下图是基于 Windows 7 版本的配置截图示例；系统属性 高级系统设置 环境变量 ，我这里编辑用户环境变量 PATH 的内容（如果不存在就新建，当然编辑系统环境变量 PATH 的内容也是可以的），切记内容一定是英文格式下的，多个使用英文逗号分隔 用户环境变量 ，我这里需要填写 2 条内容，使用英文逗号隔开（如果是直接安装的 Python，pip 和 python 应该在同一个路径下面，所以只需要 1 条就行了）我的环境需要配置 2 条内容 内容解释：1234--pip 所在目录 D:\Anaconda2\Scripts\;--python 所在目录 D:\Anaconda2;安装 ItChat 工具 在 Python 安装完成的情况下，才能进行接下来的操作，因为 ItChat 是基于 Python 环境运行的；为了验证 Python 是否正确安装，可以在命令行中输入 python，如果看到以下内容，就说明 Python 安装成功：接下来利用 pip 工具（Python 自带的）直接安装 itchat，非常简单，使用命令（如果 pip 命令不可用，请检查 Python 的安装目录是否存在 pip.exe 文件）：1pip install itchat安装 ItChat如果看到以下内容，说明 ItChat 安装成功：入门代码示例 一切准备就绪，接下来就可以写代码了，当然，入门代码非常简单实用（我会尽可能多的添加注释说明）：123456789101112131415161718192021222324252627282930313233343536#-*-coding:utf-8 -*-# 从 python 环境中导入 itchat 包，re 正则表达式包 import itchat, re# 从 itchat.content 中导入所有类、常量 (例如代码中的 TEXT 其实就是 itchat.content.TEXT 常量)from itchat.content import *# 导入时间包里面的 sleep 方法 from time import sleep# 导入随机数包 import random# 注册消息类型为文本 (即只监控文本消息，其它的例如语音 / 图片 / 表情包 / 文件都不会监控)# 也就是说只有普通的文字微信消息才能触发以下的代码 # isGroupChat=True 开启群聊模式，即只是监控群聊内容 (如果不开启就监控个人聊天，不监控群聊)@itchat.msg_register ([TEXT], isGroupChat=True)# @itchat.msg_register ([TEXT])def text_reply(msg): # msg 是消息体，msg ['Text'] 用来获取消息内容 # 第一个单引号中的内容是关键词，使用正则匹配，可以自行更改 (我使用.* 表示任意内容), 如果使用中文注意 2.x 版本的 Python 会报错，需要 u 前缀 message = msg ['Text'] print (message) match = re.search ('.*', message) # match = re.search (u'年 | 春 | 快乐', message) # 增加睡眠机制，随机等待一定的秒数 (1-10 秒) 再回复，更像人类 second = random.randint (1,10) sleep (second) if match: # msg ['FromUserName'] 用来获取用户名，发送消息给对方 from_user_name = msg ['FromUserName'] print (from_user_name) itchat.send (('====test message'), from_user_name) # 第一个单引号中的内容是回复的内容，可以自行更改 # 热启动，退出一定时间内重新登录不需要扫码 (其实就是把二维码图片存下来，下次接着使用)itchat.auto_login (hotReload=True)# 开启命令行的二维码 itchat.auto_login (enableCmdQR=True)# 运行 itchat.run ()代码截图如下：演示 登录扫码 登录成功 群聊自动回复（正则是任意内容，所以总是会自动回复）退出 重新登录继续聊天（由于开启了热启动，不需要重新扫码）继续聊天 小问题总结 1、部分系统可能字幅宽度有出入，可以通过将 enableCmdQR 赋值为特定的倍数进行调整：12# 如部分的 linux 系统，块字符的宽度为一个字符 (正常应为两字符), 故赋值为 2itchat.auto_login (enableCmdQR=2)2、Python 2.7 版本的中文报错问题（在 Python 2.7 环境下使用中文需要额外注意，坑比较多）： 例如代码中正则匹配带中文（由于编码问题导致无法匹配，或者会抛出异常）12# 正则搜索带中文，直接单引号在 Python 2.7 环境下是不行的 match = re.search ('年 | 春 | 快乐', message)实际运行时就会报错（报错信息如果不捕捉后台是看不到的）或者匹配结果不是想象中的（仅针对 Python 2.x 环境）需要使用 u 前缀 123# 正则搜索带中文，直接单引号在 Python 2.7 环境下是不行的 # 增加 u 前缀，表示 unicode 编码，才行 match = re.search (u'年 | 春 | 快乐', message)3、如果不开启热启动，每次重新登录时都会生成新的二维码，直接在 Wimdows 的命令行中，可能由于窗口太小显示不完整，此时需要拉伸一下命令行的窗口：4、有些人的电脑设置问题，命令行环境背景为白色，生成的二维码的颜色黑白色是相反的，导致扫码时无法识别，此时需要设置代码：12# 默认控制台背景色为暗色 (黑色)，若背景色为浅色 (白色)，可以将 enableCmdQR 赋值为负值 itchat.auto_login (enableCmdQR=-1) 接入机器人 一般读者做到上面的内容就算入门了，可以实现自动回复，并且关于 ItChat 也了解了一些，可以独自参考文档进行更加深入的开发了。但是，自动回复的内容毕竟太固定了，而且只能覆盖极少的内容，没办法实现真正的自动化。要想做到真正的自动化回复，机器人是少不了了，那么接下来讲解的就是如何接入一个第三方机器人，实现机器人自动回复。当然，代码内容也会稍显复杂，操作步骤也会稍显繁琐。接入机器人代码示例 接入机器人时为了换种方式，先把群聊模式关闭，使用个人聊天监控模式（方便聊天内容的随意性，更能提现机器人的可用性）：1@itchat.msg_register ([TEXT])还要导入网络请求相关的包：1import requests需要使用图灵机器人的核心配置（注册图灵机器人的过程不在此赘述，官网链接：http://www.tuling123.com ）：1234567891011121314151617181920# 封装一个根据内容调用机器人接口，返回回复的方法 def get_response(msg): # 构造了要发送给服务器的数据 apiUrl = 'http://www.tuling123.com/openapi/api' data = &#123; 'key' : APIKEY, 'info' : msg, 'userid' : 'wechat-robot', &#125; try: r = requests.post (apiUrl, data=data).json () # 字典的 get 方法在字典没有 'text' 值的时候会返回 None 而不会抛出异常 return r.get ('text') # 为了防止服务器没有正常响应导致程序异常退出，这里用 try-except 捕获了异常 # 如果服务器没能正常交互 (返回非 json 或无法连接), 那么就会进入下面的 return except Exception,err: # 打印一下错误信息 print (err) # 将会返回一个 None return完整代码示例（代码会封装的更好，格式更加规范，易读）：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#-*-coding:utf-8 -*-# 从 python 环境中导入 itchat 包，requests 网络请求包 import itchat, requests# 从 itchat.content 中导入所有类、常量 (例如代码中的 TEXT 其实就是 itchat.content.TEXT 常量)from itchat.content import *# 导入时间包里面的 sleep 方法 from time import sleep# 导入随机数包 import random# 机器人的 apikeyAPIKEY = '376cb2ca51d542c6b2e660f3c9ea3754'# 封装一个根据内容调用机器人接口，返回回复的方法 def get_response(msg): # 构造了要发送给服务器的数据 apiUrl = 'http://www.tuling123.com/openapi/api' data = &#123; 'key' : APIKEY, 'info' : msg, 'userid' : 'wechat-robot', &#125; try: r = requests.post (apiUrl, data=data).json () # 字典的 get 方法在字典没有 'text' 值的时候会返回 None 而不会抛出异常 return r.get ('text') # 为了防止服务器没有正常响应导致程序异常退出，这里用 try-except 捕获了异常 # 如果服务器没能正常交互 (返回非 json 或无法连接), 那么就会进入下面的 return except Exception,err: # 打印一下错误信息 print (err) # 将会返回一个 None return# 注册消息类型为文本 (即只监控文本消息，其它的例如语音 / 图片 / 表情包 / 文件都不会监控)# 也就是说只有普通的文字微信消息才能触发以下的代码 # isGroupChat=True 开启群聊模式，即只是监控群聊内容 (如果不开启就监控个人聊天，不监控群聊)# @itchat.msg_register ([TEXT], isGroupChat=True)@itchat.msg_register ([TEXT])def tuling_reply(msg): # msg 是消息体，msg ['Text'] 用来获取消息内容 # 第一个单引号中的内容是关键词，使用正则匹配，可以自行更改 (我使用.* 表示任意内容), 如果使用中文注意 2.x 版本的 Python 会报错，需要 u 前缀 message = msg ['Text'] print (message) # 增加睡眠机制，随机等待一定的秒数 (1-10 秒) 再回复，更像人类 second = random.randint (1,10) sleep (second) # 为了保证在图灵 apikey 出现问题的时候仍旧可以回复，这里设置一个默认回复 defaultReply = 'I received:' + message # 如果图灵 apikey 出现问题，那么 reply 将会是 None reply = get_response (message) # a or b 的意思是，如果 a 有内容，那么返回 a, 否则返回 b return reply or defaultReply# 热启动，退出一定时间内重新登录不需要扫码 (其实就是把二维码图片存下来，下次接着使用)itchat.auto_login (hotReload=True)# 开启命令行的二维码 itchat.auto_login (enableCmdQR=True)# 运行 itchat.run ()代码截图（使用工具渲染了一下）：接入机器人演示 演示一下，随便聊了几句：备注 1、ItChat 项目 GitHub 地址：https://github.com/littlecodersh/itchat ；2、ItChat 项目说明文档：https://itchat.readthedocs.io/zh/latest ；3、感谢微博科普博主 灵光灯泡 的科普视频 https://weibo.com/6969849160/HeLhjcKtA 以及文档参考 石墨文档 ；4、Python 下载官网：https://www.python.org/downloads/windows ，大家一定要选择与自己当前环境适配的版本（包括操作系统版本、Python 版本），环境变量最好配置一下；5、图灵机器人官网：http://www.tuling123.com ；]]></content>
      <categories>
        <category>ItChat 系列</category>
      </categories>
      <tags>
        <tag>ItChat</tag>
        <tag>微信接口</tag>
        <tag>自定义接口</tag>
        <tag>自动回复</tag>
        <tag>微信机器人</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 异常之 Netty 相关]]></title>
    <url>%2F2019011401.html</url>
    <content type="text"><![CDATA[在做项目的时候，需要新引入一个外部依赖，于是很自然地在项目的 pom.xml 文件中加入了依赖坐标，然后进行编译、打包、运行，没想到直接抛出了异常：122019-01-13_17:18:52 [sparkDriverActorSystem-akka.actor.default-dispatcher-5] ERROR actor.ActorSystemImpl:66: Uncaught fatal error from thread [sparkDriverActorSystem-akka.remote.default-remote-dispatcher-7] shutting down ActorSystem [sparkDriverActorSystem]java.lang.VerifyError: (class: org/jboss/netty/channel/socket/nio/NioWorkerPool, method: createWorker signature: (Ljava/util/concurrent/Executor;) Lorg/jboss/netty/channel/socket/nio/AbstractNioWorker;) Wrong return type in function任务运行失败，仔细看日志觉得很莫名奇妙，是一个 java.lang.VerifyError 错误，以前从来没见过类似的。本文记录这个错误的解决过程。问题出现 在上述错误抛出之后，可以看到 SparkContext 初始化失败，然后进程就终止了；完整日志如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677782019-01-13_17:18:52 [sparkDriverActorSystem-akka.actor.default-dispatcher-5] ERROR actor.ActorSystemImpl:66: Uncaught fatal error from thread [sparkDriverActorSystem-akka.remote.default-remote-dispatcher-7] shutting down ActorSystem [sparkDriverActorSystem]java.lang.VerifyError: (class: org/jboss/netty/channel/socket/nio/NioWorkerPool, method: createWorker signature: (Ljava/util/concurrent/Executor;) Lorg/jboss/netty/channel/socket/nio/AbstractNioWorker;) Wrong return type in function at akka.remote.transport.netty.NettyTransport.(NettyTransport.scala:283) at akka.remote.transport.netty.NettyTransport.(NettyTransport.scala:240) at sun.reflect.NativeConstructorAccessorImpl.newInstance0 (Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance (NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance (DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance (Constructor.java:423) at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$2.apply (DynamicAccess.scala:78) at scala.util.Try$.apply (Try.scala:161) at akka.actor.ReflectiveDynamicAccess.createInstanceFor (DynamicAccess.scala:73) at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply (DynamicAccess.scala:84) at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply (DynamicAccess.scala:84) at scala.util.Success.flatMap (Try.scala:200) at akka.actor.ReflectiveDynamicAccess.createInstanceFor (DynamicAccess.scala:84) at akka.remote.EndpointManager$$anonfun$9.apply (Remoting.scala:711) at akka.remote.EndpointManager$$anonfun$9.apply (Remoting.scala:703) at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply (TraversableLike.scala:722) at scala.collection.Iterator$class.foreach (Iterator.scala:727) at scala.collection.AbstractIterator.foreach (Iterator.scala:1157) at scala.collection.IterableLike$class.foreach (IterableLike.scala:72) at scala.collection.AbstractIterable.foreach (Iterable.scala:54) at scala.collection.TraversableLike$WithFilter.map (TraversableLike.scala:721) at akka.remote.EndpointManager.akka$remote$EndpointManager$$listens (Remoting.scala:703) at akka.remote.EndpointManager$$anonfun$receive$2.applyOrElse (Remoting.scala:491) at akka.actor.Actor$class.aroundReceive (Actor.scala:467) at akka.remote.EndpointManager.aroundReceive (Remoting.scala:394) at akka.actor.ActorCell.receiveMessage (ActorCell.scala:516) at akka.actor.ActorCell.invoke (ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox (Mailbox.scala:238) at akka.dispatch.Mailbox.run (Mailbox.scala:220) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec (AbstractDispatcher.scala:397) at scala.concurrent.forkjoin.ForkJoinTask.doExec (ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask (ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker (ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run (ForkJoinWorkerThread.java:107)2019-01-13_17:18:52 [sparkDriverActorSystem-akka.actor.default-dispatcher-6] INFO remote.RemoteActorRefProvider$RemotingTerminator:74: Shutting down remote daemon.2019-01-13_17:18:52 [sparkDriverActorSystem-akka.actor.default-dispatcher-6] INFO remote.RemoteActorRefProvider$RemotingTerminator:74: Remote daemon shut down; proceeding with flushing remote transports.2019-01-13_17:18:52 [sparkDriverActorSystem-akka.actor.default-dispatcher-6] ERROR Remoting:65: Remoting system has been terminated abrubtly. Attempting to shut down transports2019-01-13_17:18:52 [sparkDriverActorSystem-akka.actor.default-dispatcher-6] INFO remote.RemoteActorRefProvider$RemotingTerminator:74: Remoting shut down.2019-01-13_17:19:02 [main] ERROR spark.SparkContext:95: Error initializing SparkContext.java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds] at scala.concurrent.impl.Promise$DefaultPromise.ready (Promise.scala:219) at scala.concurrent.impl.Promise$DefaultPromise.result (Promise.scala:223) at scala.concurrent.Await$$anonfun$result$1.apply (package.scala:107) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn (BlockContext.scala:53) at scala.concurrent.Await$.result (package.scala:107) at akka.remote.Remoting.start (Remoting.scala:179) at akka.remote.RemoteActorRefProvider.init (RemoteActorRefProvider.scala:184) at akka.actor.ActorSystemImpl.liftedTree2$1(ActorSystem.scala:620) at akka.actor.ActorSystemImpl._start$lzycompute (ActorSystem.scala:617) at akka.actor.ActorSystemImpl._start (ActorSystem.scala:617) at akka.actor.ActorSystemImpl.start (ActorSystem.scala:634) at akka.actor.ActorSystem$.apply (ActorSystem.scala:142) at akka.actor.ActorSystem$.apply (ActorSystem.scala:119) at org.apache.spark.util.AkkaUtils$.org$apache$spark$util$AkkaUtils$$doCreateActorSystem (AkkaUtils.scala:121) at org.apache.spark.util.AkkaUtils$$anonfun$1.apply (AkkaUtils.scala:53) at org.apache.spark.util.AkkaUtils$$anonfun$1.apply (AkkaUtils.scala:52) at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp (Utils.scala:2024) at scala.collection.immutable.Range.foreach$mVc$sp (Range.scala:141) at org.apache.spark.util.Utils$.startServiceOnPort (Utils.scala:2015) at org.apache.spark.util.AkkaUtils$.createActorSystem (AkkaUtils.scala:55) at org.apache.spark.SparkEnv$.create (SparkEnv.scala:266) at org.apache.spark.SparkEnv$.createDriverEnv (SparkEnv.scala:193) at org.apache.spark.SparkContext.createSparkEnv (SparkContext.scala:288) at org.apache.spark.SparkContext.(SparkContext.scala:457) at org.apache.spark.api.java.JavaSparkContext.(JavaSparkContext.scala:59) at com.ds.octopus.job.utils.SparkContextUtil.refresh (SparkContextUtil.java:77) at com.ds.octopus.job.utils.SparkContextUtil.getJsc (SparkContextUtil.java:34) at com.ds.octopus.job.executors.impl.WeiboZPZExporter.action (WeiboZPZExporter.java:95) at com.ds.octopus.job.executors.impl.WeiboZPZExporter.action (WeiboZPZExporter.java:41) at com.ds.octopus.job.executors.SimpleExecutor.execute (SimpleExecutor.java:40) at com.ds.octopus.job.client.OctopusClient.run (OctopusClient.java:162) at com.yeezhao.commons.buffalo.job.AbstractBUTaskWorker.runTask (AbstractBUTaskWorker.java:63) at com.ds.octopus.job.client.TaskLocalRunnerCli.start (TaskLocalRunnerCli.java:109) at com.yeezhao.commons.util.AdvCli.initRunner (AdvCli.java:191) at com.ds.octopus.job.client.TaskLocalRunnerCli.main (TaskLocalRunnerCli.java:41)2019-01-13_17:19:02 [main] INFO spark.SparkContext:58: Successfully stopped SparkContext错误日志截图：根据日志没有看出有关 Java 层面的什么问题，只能根据 JNI 字段描述符：1class: org/jboss/netty/channel/socket/nio/NioWorkerPool猜测是某一个类的问题，根据：1method: createWorker signature: (Ljava/util/concurrent/Executor;) Lorg/jboss/netty/channel/socket/nio/AbstractNioWorker;) Wrong return type in function猜测是某个方法的问题，方法的返回类型错误。然后在项目中使用 ctrl+shift+t 快捷键（全局搜索 Java 类，每个人的开发工具设置的可能不一样）搜索类：NioWorkerPool，发现这个类的来源不是新引入的依赖包，而是原本就有的 netty 相关包，所以此时就可以断定这个莫名其妙的错误的原因就在于这个类的 createWorker 方法返回类型上面了。搜索类 NioWorkerPool日志的 JNI 字段描述符显示返回类型是 AbstractNioWorker，但是这个一看就是抽象类，不是我们要找的，去类里面看源码，发现 createWorker 方法返回类型是 NioWorker：类 NioWorkerPool 源码 继续搜索类 NioWorker好，此时发现问题了，这个类有 2 个，居然存在两个相同的包名，但是依赖坐标不一样，所以这个隐藏的原因在于类冲突，但是并不能算是依赖冲突引起的。也就是说，NioWorker 这个类重复了，但是依赖包坐标不一样，类的包路径却是一模一样的，不会引起版本冲突问题，而在实际运行任务的时候会抛出运行时异常，所以我觉得找问题的过程很艰辛。使用依赖树查看依赖关系，是看不到版本冲突问题的，2 个依赖都存在：io.netty 依赖 org.jboss.netty 依赖 于是又在网上搜索了一下，发现果然是 netty 的问题，也就是新引入的依赖包导致的，但是根本原因令人哭笑：netty 的组织结构变化，发布的依赖坐标名称变化，但是所有的类的包名称并没有变化，导致了这个错误。问题解决 问题找到了，解决方法就简单了，移除传递依赖即可，同时也要注意以后再添加新的依赖一定要慎重，不然找问题的过程很是令人崩溃。移除依赖 移除配置示例 1234567&lt;!-- 移除引发冲突的 jar 包 --&gt;&lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.jboss.netty&lt;/groupId&gt; &lt;artifactId&gt;netty&lt;/artifactId&gt; &lt;/exclusion&gt;&lt;/exclusions&gt; 问题总结1、参考：https://stackoverflow.com/questions/33573587/apache-spark-wrong-akka-remote-netty-version ；2、netty 的组织结构（影响发布的 jar 包坐标名称）变化了，但是所有的类的包名称仍然是一致的，很奇怪，导致我找问题也觉得莫名其妙，因为这不会引发版本冲突问题（但是本质上又是 2 个一模一样的类被同时使用，引发类冲突）；3、这个错误信息挺有意思的，解决过程也很好玩，边找边学习；]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>netty</tag>
        <tag>nio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven 插件异常之 socket write error]]></title>
    <url>%2F2019011101.html</url>
    <content type="text"><![CDATA[今天在整理代码的时候，在本机（自己的电脑）通过 Maven 的 deploy 插件（org.apache.maven.plugins:maven-deploy-plugin:2.7）进行发布，把代码打包成构件发布到远程的 Maven 仓库（公司的私服），这样方便大家调用。可是，其中有一个项目发布不了（其它类似的 2 个项目都可以，排除了环境的原因），总是报错：1Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error以上错误日志中的项目名称、包名称均被替换。本文就记录从发现问题到解决问题的过程。环境所使用的 Maven 版本为：3.5.0。问题出现 对一个公共项目进行打包发布，部署到公司私服（已经排除环境因素），出现异常；使用命令 Maven：1mvn deploy出现异常：1234567891011121314[INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 02:49 min[INFO] Finished at: 2019-01-12T16:17:21+08:00[INFO] Final Memory: 68M/1253M[INFO] ------------------------------------------------------------------------[ERROR] Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy (default-deploy) on project dt-x-y-z: Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error -&gt; [Help 1][ERROR][ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR][ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException如果使用 -X 参数（完整命令：mvn deploy -X），可以稍微看到更详细的 Maven 部署日志信息：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990[INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 02:49 min[INFO] Finished at: 2019-01-12T16:17:21+08:00[INFO] Final Memory: 68M/1253M[INFO] ------------------------------------------------------------------------[ERROR] Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy (default-deploy) on project dt-x-y-z: Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error -&gt; [Help 1]org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy (default-deploy) on project dt-x-y-z: Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:213) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:154) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:146) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:81) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:309) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:194) at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:107) at org.apache.maven.cli.MavenCli.execute(MavenCli.java:993) at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:345) at org.apache.maven.cli.MavenCli.main(MavenCli.java:191) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289) at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415) at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)Caused by: org.apache.maven.plugin.MojoExecutionException: Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error at org.apache.maven.plugin.deploy.DeployMojo.execute(DeployMojo.java:193) at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208) ... 20 moreCaused by: org.apache.maven.artifact.deployer.ArtifactDeploymentException: Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error at org.apache.maven.artifact.deployer.DefaultArtifactDeployer.deploy(DefaultArtifactDeployer.java:143) at org.apache.maven.plugin.deploy.AbstractDeployMojo.deploy(AbstractDeployMojo.java:167) at org.apache.maven.plugin.deploy.DeployMojo.execute(DeployMojo.java:157) ... 22 moreCaused by: org.eclipse.aether.deployment.DeploymentException: Failed to deploy artifacts: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error at org.eclipse.aether.internal.impl.DefaultDeployer.deploy(DefaultDeployer.java:326) at org.eclipse.aether.internal.impl.DefaultDeployer.deploy(DefaultDeployer.java:254) at org.eclipse.aether.internal.impl.DefaultRepositorySystem.deploy(DefaultRepositorySystem.java:422) at org.apache.maven.artifact.deployer.DefaultArtifactDeployer.deploy(DefaultArtifactDeployer.java:139) ... 24 moreCaused by: org.eclipse.aether.transfer.ArtifactTransferException: Could not transfer artifact xxx.yyy.zzz:dt-x-y-z:jar:0.0.6-20190112.081518-1 from/to snapshots (http://maven.myself.com/nexus/content/repositories/snapshots): Connection reset by peer: socket write error at org.eclipse.aether.connector.basic.ArtifactTransportListener.transferFailed(ArtifactTransportListener.java:52) at org.eclipse.aether.connector.basic.BasicRepositoryConnector$TaskRunner.run(BasicRepositoryConnector.java:364) at org.eclipse.aether.connector.basic.BasicRepositoryConnector.put(BasicRepositoryConnector.java:283) at org.eclipse.aether.internal.impl.DefaultDeployer.deploy(DefaultDeployer.java:320) ... 27 moreCaused by: org.apache.maven.wagon.TransferFailedException: Connection reset by peer: socket write error at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:650) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:553) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:535) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:529) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:509) at org.eclipse.aether.transport.wagon.WagonTransporter$PutTaskRunner.run(WagonTransporter.java:653) at org.eclipse.aether.transport.wagon.WagonTransporter.execute(WagonTransporter.java:436) at org.eclipse.aether.transport.wagon.WagonTransporter.put(WagonTransporter.java:419) at org.eclipse.aether.connector.basic.BasicRepositoryConnector$PutTaskRunner.runTask(BasicRepositoryConnector.java:519) at org.eclipse.aether.connector.basic.BasicRepositoryConnector$TaskRunner.run(BasicRepositoryConnector.java:359) ... 29 moreCaused by: java.net.SocketException: Connection reset by peer: socket write error at java.net.SocketOutputStream.socketWrite0(Native Method) at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111) at java.net.SocketOutputStream.write(SocketOutputStream.java:155) at org.apache.maven.wagon.providers.http.httpclient.impl.io.SessionOutputBufferImpl.streamWrite(SessionOutputBufferImpl.java:126) at org.apache.maven.wagon.providers.http.httpclient.impl.io.SessionOutputBufferImpl.flushBuffer(SessionOutputBufferImpl.java:138) at org.apache.maven.wagon.providers.http.httpclient.impl.io.SessionOutputBufferImpl.write(SessionOutputBufferImpl.java:169) at org.apache.maven.wagon.providers.http.httpclient.impl.io.ContentLengthOutputStream.write(ContentLengthOutputStream.java:115) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon$RequestEntityImplementation.writeTo(AbstractHttpClientWagon.java:209) at org.apache.maven.wagon.providers.http.httpclient.impl.DefaultBHttpClientConnection.sendRequestEntity(DefaultBHttpClientConnection.java:158) at org.apache.maven.wagon.providers.http.httpclient.impl.conn.CPoolProxy.sendRequestEntity(CPoolProxy.java:162) at org.apache.maven.wagon.providers.http.httpclient.protocol.HttpRequestExecutor.doSendRequest(HttpRequestExecutor.java:237) at org.apache.maven.wagon.providers.http.httpclient.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:122) at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.MainClientExec.execute(MainClientExec.java:271) at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184) at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.RetryExec.execute(RetryExec.java:88) at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.RedirectExec.execute(RedirectExec.java:110) at org.apache.maven.wagon.providers.http.httpclient.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184) at org.apache.maven.wagon.providers.http.httpclient.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.execute(AbstractHttpClientWagon.java:834) at org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.put(AbstractHttpClientWagon.java:596) ... 38 more[ERROR][ERROR][ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException由于对 Maven 构件的原理不清楚，通过日志报错也看不出根本原因是什么，根据最后一行日志的链接：http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException ，我看到描述：123Unlike many other errors, this exception is not generated by the Maven core itself but by a plugin. As a rule of thumb, plugins use this error to signal a problem in their configuration or the information they retrieved from the POM.The concrete meaning of the exception depends on the plugin so please have a look at its documentation. The documentation for many common Maven plugins can be reached via our plugin index.大致意思也就是说这种类型的错误一般不是 Maven 的问题，而是所使用的 Maven 构件的问题，在这里我使用了 deploy 构件，我也知道和 deploy 构件有关，但是具体原因是什么也没说明；于是接下来使用 -e 参数（完整命令：mvn deploy -e），除了上述的报错日志外，还可以打印更为详细的错误日志追踪信息，然后发现一直会有下面的错误输入，而且一直重复，没有要停的迹象，我只能手动停掉 mvn 进程：1java.lang.IllegalArgumentException: progressed file size cannot be greater than size: 59156480 &gt; 58029604异常信息截图 换算一下单位：59156480KB=56.42M（正是需要发布的构件的大小）；58029604KB=55.34M；1、通过搜索引擎对异常信息的搜索，大部分结果显示和 Maven 后台的 Web 服务有关，如果使用的是 Nginx，会有一个参数用来限制上传文件的大小，上传文件的大小超过最大限制，就会上传失败，并且抛出异常。我部署其它的小构件没有问题，怀疑是这个原因，于是我询问运维人员公司的 Maven 私服对上传的公共构件有没有大小限制（即可能是 Nginx 服务有没有限制上传文件的大小），运维说不会。但是我还是怀疑，于是想通过 Web 端的界面来手动上传我的构件，发现 Web 端的界面没有开放，无法完成上传操作，接下来我就想看看 Maven 后台服务的对应参数配置的值是多大（也可能使用的是默认值），但是不知道后台采用的是什么服务（Nginx 还是 Netty 不确定），先放弃这条路；2、也有结果显示是 Maven 的版本问题，有些版本有 bug，所以造成了这个问题。问题解决 既然没有分析出来具体的原因，只能尝试每一种解决方案了。1、既然怀疑是 Maven 私服限制了构件的大小，那就想办法减小构件。先在本地 install，然后去本地仓库看一下生成的构件的大小，结果我惊讶地发现构件居然有 330M 之大，吓死人了，这个打包发布构件的配置肯定有问题，肯定把第三方依赖全部打进去了。我查看了以前生成的正常的构件，也就 60M 左右。以下放出对比图 2 个 这一看就是把第三方各种依赖包都一起发布了，才会造成构件有这么大，于是更改 pom.xml 文件，把第三方依赖去除，deploy 的时候是不需要的，同时也删除了一些 resources 资源文件夹里面的文本文件，删除时发现文本文件竟然有几十 M，怪不得以前发布的构件大小有 60M 左右，原来都是文本文件在占用空间；更新了之后，直接重新 deploy，不报错了，直接 deploy 成功，去私服仓库搜索查看，大概 30M 左右，很正常 2、问题使用方法一已经解决了，也就是和 Maven 版本没有关系了，而且，在我的当前 Maven 环境下，我去 deploy 其它构件也是成功的，不会有任务报错，所以也从侧面反映了这个问题和 Mave 版本无关，和 Maven 环境也无关； 问题总结 参考：http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException ；apache 的官方 jira：http://mail-archives.apache.org/mod_mbox/maven-issues/201808.mbox/%3CJIRA.13182024.1535592594000.205524.1535738700211@Atlassian.JIRA%3E ；https://issues.apache.org/jira/browse/MNG-6469?page=com.atlassian.jira.plugin.system.issuetabpanels%3Aall-tabpanel ；这个问题去网上搜索不到资料，很痛苦，问人也没有能帮到我的，只能自己去慢慢摸索试验，整个过程比较艰难；]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Maven</tag>
        <tag>deploy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Gson 将 = 转为 u003d 的问题]]></title>
    <url>%2F2019010601.html</url>
    <content type="text"><![CDATA[今天遇到一个问题，实现 Web 后台接收 http 请求的一个方法，发现前端传过来的参数值，有一些特殊符号总是使用了 unicode 编码，例如等号 =，后台接收到的就是 \u003d，导致使用这个参数做 JSON 变换的时候就会出错。我看了一下这个参数取值，是前端直接填写的，而填写的人是从其它地方复制过来的，人为没有去改变，前端没有验证转换，导致传入后台的已经是这样了，那么后台只好自己想办法转换。问题解决 其实就是字符串还原操作，把 Java 字符串里面的 unicode 编码子串还原为原本的字符，例如把 \u003d 转为 = 这样。自己实现一个工具类，做编码字符串和普通字符串的转换，可以解决这个问题。单个编码转换，公共方法示例：1234567891011121314151617/** * unicode 转字符串 * * @param unicode 全为 Unicode 的字符串 * @return */public static String unicode2String(String unicode) &#123; StringBuffer string = new StringBuffer (); String [] hex = unicode.split ("\\\\u"); for (int i = 1; i &lt; hex.length; i++) &#123; // 转换出每一个代码点 int data = Integer.parseInt (hex [i], 16); // 追加成 string string.append ((char) data); &#125; return string.toString ();&#125;整个字符串转换，公共方法示例：12345678910111213141516171819202122232425262728293031/** * 含有 unicode 的字符串转一般字符串 * * @param unicodeStr 混有 Unicode 的字符串 * @return */public static String unicodeStr2String(String unicodeStr) &#123; int length = unicodeStr.length (); int count = 0; // 正则匹配条件，可匹配 \\u 1 到 4 位，一般是 4 位可直接使用 String regex = "\\\\u [a-f0-9A-F]&#123;4&#125;"; String regex = "\\\\u [a-f0-9A-F]&#123;1,4&#125;"; Pattern pattern = Pattern.compile (regex); Matcher matcher = pattern.matcher (unicodeStr); StringBuffer sb = new StringBuffer (); while (matcher.find ()) &#123; // 原本的 Unicode 字符 String oldChar = matcher.group (); // 转换为普通字符 String newChar = unicode2String (oldChar); int index = matcher.start (); // 添加前面不是 unicode 的字符 sb.append (unicodeStr.substring (count, index)); // 添加转换后的字符 sb.append (newChar); // 统计下标移动的位置 count = index + oldChar.length (); &#125; // 添加末尾不是 Unicode 的字符 sb.append (unicodeStr.substring (count, length)); return sb.toString ();&#125;调用示例：12String str = "ABCDEFG\\u003d";System.out.println ("====unicode2String 工具转换:" + unicodeStr2String (str));输出结果：1====unicode2String 工具转换：ABCDEFG=截图示例：问题后续 后续我又在想，这个字符串到底是怎么来的，为什么填写的人会复制出来这样一个字符串，一般 unicode 编码不会出现在日常生活中的。我接着发现这个字符串是从另外一个系统导出的，导出的时候是一个类似于 Java 实体类的 JSON 格式字符串，从里面复制出来这个值，就是 \u003d 格式的。那我觉得肯定是这个系统有问题，做 JSON 序列化的时候没有控制好序列化的方式，导致对于特殊字符就会自动转为 unicode 编码，给他人带来麻烦，当然，我无法得知系统内部做了什么，但是猜测可能是使用 Gson 工具做序列化的时候没有正确使用 Gson 的对象，只是简单的生成 JSON 字符串而已，例如看我下面的代码示例（等号 = 会被转为 \u003d）。使用普通的 1Gson gson1 = new Gson (); 会导致后续转换 JSON 字符串的时候出现 unicode 编码子串的情况，而正确生成 Gson 对象 1Gson gson2 = new GsonBuilder ().disableHtmlEscaping ().create (); 则不会出现这种情况。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Gson</tag>
        <tag>等号编码转换</tag>
        <tag>u003d</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub Pages 禁止百度蜘蛛爬取的问题]]></title>
    <url>%2F2019010501.html</url>
    <content type="text"><![CDATA[最近才发现我的静态博客站点，大部分的网页没被百度收录，除了少量的网页是我自动提交（主动推动、自动推送）的，或者手动提交的，其它的网页都不被收录（网页全部是利用自动提交的 sitemap 方式提交的，一个都没收录）。我查看百度的站长工具后台，发现通过 sitemap 方式提交链接这种方式不可行，因为百度蜘蛛采集链接信息之前需要访问 baidusitemap.xml 文件，而这个文件是在 GitHub Pages 里面的，但是 GitHub Pages 是禁止百度蜘蛛爬取的，所以百度蜘蛛在获取 baidusitemap.xml 文件这一步骤就被禁止了，GitHub Pages 返回 403 错误（在 http 协议中表示禁止访问），因此抓取失败（哪怕获取到 baidusitemap.xml 文件也不行，因为后续需要采集的静态网页全部是放在 GitHub Pages 中的，全部都会被禁止）。本文就详细描述这种现象，以及寻找可行的解决方案。问题出现 网页收录对比差距大 利用搜索引擎的 site 搜索可以看到百度与谷歌明显的差别 百度搜索结果（只有少量的收录，仅有的还是通过主动推送与自动推送提交的）谷歌搜索结果（收录很多，而且很全面）首先在百度站长工具（官方主页：https://ziyuan.baidu.com/ ）后台看到 baidusitemap.xml 抓取失败，查看具体原因是抓取失败（http 状态码 403）。抓取失败 抓取失败原因概述 根据抓取失败原因，我还以为是文件不存在，或者根据链接打不开（链接是：https://www.playpi.org/baidusitemap.xml ），我使用浏览器和 curl 命令都尝试过了，链接没有问题，可以正常打开。然后根据 403 错误发现是拒绝访问，那就有可能是百度爬虫的问题了（被 GitHub Pages 禁止爬取了）。使用浏览器打开 这里需要注意一点，百度站长工具里面显示的链接是 http 开头的（如上面抓取失败原因概述截图中红框圈出的，不是 https 开头的，我觉得百度爬虫抓取使用的就是 http 开头的链接），不过没关系，我在域名解析里面已经配置了所有的域名情况，完全可以支持。但是有时候仍然会遇到打不开上面链接的情况（在某些电脑上面或者某些网络环境中），我猜测这可能是电脑的缓存或者当前网络的 DNS 设置问题，不是我的站点的问题。因为，哪怕你在浏览器中输入以 http 开头的链接，也会自动跳转到以 https 开头的链接去。浏览器打不开链接的情况（其实不是链接的问题）使用命令行打开（如下使用 curl 命令）1curl https://www.playpi.org/baidusitemap.xml执行命令结果截图 通过百度反馈寻找原因 于是接下来，我就给官方提交了反馈，官方只是回复我说是链接问题（意思就是链接无法正常打开，其实使用浏览器或者检测工具都是可以打开的，但是使用百度爬虫就不行）。提交反馈（官方主页：https://ziyuan.baidu.com/feedback/apply ）反馈回复 前面我已经证明了链接没问题，那我就要猜想是百度蜘蛛爬虫的问题了，于是按照官方回复的建议，使用诊断工具看看是否可行。诊断工具测试多次都失败 如果抓取 UA 设置为移动端（即模拟手机、平板之类的设别），会有部分成功的，而使用 PC 端全部都是失败的。失败原因仍旧是拒绝访问（http 403 状态码）我又接着查看文档（文档地址：https://ziyuan.baidu.com/college/courseinfo?id=267&amp;page=9#007 ），发现拒绝访问的原因之一就是托管服务供应商阻止百度 Spider 访问我的网站，所以猜测是 GitHub Pages 拒绝了百度 Spider 的爬取请求，接着就想办法验证一下猜测是否正确。文档说明截取片段 接下来我又查找了资料，发现网上确实有很多这种说法，而且大家都遇到了这种问题，但是并没有官方的说明放出来。于是，接着我又回复了百度站长对方的反馈，直接问是不是因为 GitHub Pages 禁止了百度爬虫，所以百度爬取的结果总是 403 错误。等了 2 天多（赶上周末），对方没有明确回复，说的都是废话，可能是不想承认，那我也不管了。通过 GitHub Pages 找原因 另一方面，我尝试给 GitHub 的技术支持发送邮件询问，得到了确认的答复，GitHub 已经禁止了百度蜘蛛爬虫的访问，并且不保证在未来的时间恢复。主要是因为以前百度爬虫爬取太猛了，导致 GitHub Pages 不可用或者访问速度变慢，影响了其他正常的用户浏览使用 GitHub Pages，所以把百度爬虫给禁止了（当然，这是官方说法）。GitHub Pages 的反馈链接（填写姓名、邮箱、内容描述即可）：https://github.com/contact ；我发送了一封邮件过去，当然是借助谷歌翻译完成的，勉强能看 成功发送邮件后的通知页面 内容全文如下，仅供参考：1234567891011121314151617A doubt with GitHub PagesHello,I created my own homepage with GitHub Pages,it is https://github.com/iplaypi/iplaypi.github.io.If you input https://iplaypi.github.io,it jumps to https://www.playpi.org automatically because of CNAME file.The website is https://www.playpi.org,and my site only contains static pages and pictures.But I have a problem,the following is my detailed description:I use Google Search Console to crawl my pages and include them.I only need to provide a site file named website.xml,and it works fine.But when i use Baidu Webmaster Tools (a tool made by a Chinese search engine company),it doesn&apos;t work properly.I only need to provide a site file named baiduwebsite.xml,Baidu Spider will crawl the link in this file .But Baidu cannot include my pages finally,and the reason is Baidu Spider can&apos;t crawl my html pages.So,I am trying to find the real reason,then I succeeded.The real reason is Github Pages forbids the crawling of Baidu Spider.So when Baidu Spider crawls my pages,it will definitely fail.Here I want to know is this phenomenon real?If yes,why Github Pages forbids Baidu Spider?And what should i do?Thanks.Best regards.Perry没隔几个小时，就有回复了 回复的重点内容如下：1I&apos;ve confirmed that we are currently blocking the Baidu user agent from crawling GitHub Pages sites. We took this action in response to this user agent being responsible for an excessive amount of requests, which was causing availability issues for other GitHub customers. This is unlikely to change any time soon, so if you need the Baidu user agent to be able to crawl your site you will need to host it elsewhere.那么，我们再来回看一下百度站长里面爬取失败原因的页面，里面有一个用户代理的配置，其实就是构造 http 请求使用的消息头，可以看到正是 Baiduspider/2.0，所以才会被 GitHub Pages 给禁止了。解决方案 至此，我已经把问题的原因搞清楚了。本来这个问题是很好解决的（更换静态博客存储的主机即可，例如各种项目托管服务：码市、gitcafe、七牛云等，或者自己购买一台云主机），但是我不能抛弃 GitHub，于是问题变得复杂了。此时，我还有 3 个方案可以参考：使用 CDN 加速，把每个静态页面都缓存下来，这样百度爬虫的请求就可能不会到达 GitHub Pages，但是不知道有没有保证，可以试试 放弃 自动提交 方式里面的 sitemap 推送 ，改为 主动推送 ，hexo 里面有插件可以用。但是我是坚持大道至简的原则，不想再引用插件了，而且我看了那个插件，需要配置百度账号的信息，我不能把这些信息放在公共仓库里面，会暴露给别人，不想用 在更新博客的同时再部署一份相同的博客 （可以理解为镜像，需要在其它主机部署一份，可以自己搭建主机或者使用类似于 GitHub 的代码托管工具），把 master 分支的内容复制过去即可，然后利用域名解析服务，把百度爬虫的流量引到这份服务器上面（只是为了让百度收录），其他的流量仍然去访问 GitHub Pages，就可以让百度的爬虫顺利爬取到我的博客内容了。这个方法看起来虽然很绕，但是明白了细节实现起来就很简单，而且可靠，可以用 CDN 加速 我先不选择这种方式了，因为需要收费或者免费的加广告，或者服务不稳定，我还是愿意选择稳妥的方式。可以选择的产品有：七牛云、又拍云、阿里云、腾讯云等。选择镜像方式 既然选择了使用复制博客的方式，再加上域名解析服务转移流量，那接下来就开始动手部署了。我手里正好还有一台翻墙使用的 VPS，每个月的流量用不完，所以也不打算使用第三方托管服务了，直接部署在我自己的 VPS 上面就行了。只不过还需要动动手搭建一下 Web 服务，当然是使用强大的 Nginx 了。更改域名服务器和相关配置 1、在 DNSPod 中添加域名DNSPod 账号自行注册，我使用免费版本，当然会有一些限制，例如解析的域名 A 记录个数限制为 2 个【GitHub Pages 有 4 个 ip，我在 Godaddy 中都是配置 4 个，但是没影响，配置 2 个也。或者直接配置 CNAME 记录就行了，以前我不懂就配置了 ip，多麻烦，ip 还要通过 ping iplaypi.github.io 获取，每次还不一样，一共得到了 4 个，多此一举。当然，如果域名被墙了而 ip 没被墙，还是需要这样配置的】。2、添加域名解析记录 我把 Godaddy 中的解析记录直接抄过来就行，不同的是由于使用的是 DNSPod 免费版本，A 记录会少配置 2 个，基本不会有啥影响 【其实不配置 A 记录最好，直接配置 CNAME 就行了，会根据域名自动寻找 ip，以前我不懂】。另外还有一个就是需要针对百度爬虫专门配置一条 www 的 A 记录，针对百度的线路指向自己服务器的 ip【截图只是演示，其中 CNAME 记录应该配置域名，A 记录才是配置 ip】，如果使用的是第三方托管服务，直接添加 CNAME 记录，配置域名就行【例如 yoursite.gitcafe.io】。不使用 A 记录的配置方式 3、在 Godaddy 中绑定自定义域名服务器 第 2 个步骤完成，我们回到 DNSPod 的域名界面，可以看到提示我们修改 NS 地址，如果不知道是什么意思，可以点击提示链接查看帮助手册【其实就是去购买域名的服务商那里绑定 DNSPod 的域名服务器】。提示我们修改 NS 地址 帮助手册 我是在 Godaddy 中购买的域名【不需要备案】，所以需要在 Godaddy 中取消默认的 DNS 域名服务器，然后把 DNSPod 分配的域名服务器配置在 Godaddy 中。这里需要注意，在配置了新的域名服务器的时候，以前的配置的解析记录都没用了，因为 Godaddy 直接把域名解析的工作转给了我配置的 DNSPod 域名服务器【配置信息都转到了 DNSPod 中，也就是步骤 1、步骤 2 中的工作】。原有的解析记录与原有的域名服务器 配置完成新的域名服务器【以前的解析记录都消失了】配置完成后使用 域名设置 里面的 自助诊断 功能，可以看到域名存在异常，主要是因为更改配置后的时间太少了，要耐心等待全球递归 DNS 服务器刷新【最多 72 小时】，不过一般 10 分钟就可以访问主页了。设置镜像服务器 我没有使用第三方托管服务器，例如：gitcafe、码市、coding，而是直接使用自己的 VPS，然后搭配 Nginx 使用。安装 Nginx（基于 CentOS 7 X64）CentOS 的安装过程参考：https://gist.github.com/ifels/c8cfdfe249e27ffa9ba1 。但是，不是全部可信，抽取有用的即可。而且这种方式安装的是已经规划好的一个庞大的包，里面包含了一些常用的模块，可能有一些模块没用，而且如果自己想再安装一些新的模块，就不支持了，必须重新下源码编译安装。总而言之，这种安装方式就是给入门级别的人使用的，不能自定义。1、由于 Nginx 的源头问题，先创建配置文件 12cd /etc/yum.repos.d/vim nginx.repo 填写内容 12345[nginx]name=nginx repobaseurl=http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck=0enabled=12、安装配置 Nginx1234# 安装 yum install nginx -y# 配置 vi /etc/nginx/nginx.conf 填写配置内容 1234567891011121314151617181920212223242526272829303132333435363738user nginx;worker_processes 1;error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; access_log /site/nginx.access.log main; server &#123; listen 80; server_name blog.playpi.org www.playpi.org; access_log /site/iplaypi.github.io.access.log main; root /site/iplaypi.github.io; &#125; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf;&#125;3、开启 80 端口（不开启不行），启动 Nginx1234567891011# 查看已经开启的端口 firewall-cmd --list-ports# 开启端口 firewall-cmd --permanent --zone=public --add-port=80/tcp# 重载更新的端口信息 firewall-cmd --reload# 启动 Nginx# 这种方式不行，找不到目录 /etc/init.d/nginx start# 这种方式可以 service nginx start 额外考虑情况 1、关于 https 认证 要不要考虑 https 的情况，如果百度爬虫没用到 https 抓取（除了 sitemap.xml 文件还要考虑文件里面的所有链接格式，也是 https 的），就不考虑。其实一定要考虑，因为百度爬虫用到了 https 链接去抓取，所以还要想办法开启 Nginx 的 https。此外，在百度的 https 认证里面，也是需要开启 https 的，否则申请不通过。我的域名不知道什么时候验证失败了，但是一开始的时候是验证成功的（可能是 GitHub Pages 禁止百度爬虫的原因，因为以前全部都是 GitHub Pages 提供站点支持）我想重新验证一下，没想到有次数限制，还是先把 Nginx 的 https 开启之后再验证吧 开启 Nginx 的 https，并且保证站点全部的链接都是 https 的，但是同时也要支持 http，使用 301 重定向到 https。1-1、查看 Nginx 的 https 模块 先查看我安装的小白版本的 Nginx 里面有没有关于 https 的模块，使用命令 nginx -V，可以看到是有的，这个模块就是 –with-http_ssl_module。1-2、申请证书 可以购买或者从阿里云、腾讯云里面申请免费的，但是我还是觉得使用 OpenSSL 工具自己生成方便，先查看机器有没有安装 OpenSSL 工具，使用 openssl version 命令，如果没有则需要安装 yum install -y openssl openssl-devel，安装完成后开始生成证书。生成证书的命令：1openssl req -x509 -nodes -days 36500 -newkey rsa:2048 -keyout /site/ssl-nginx.key -out /site/ssl-nginx.crt在生成的过程中还需要填写一些参数信息：国家、城市、机构名称、机构单位名称、域名、邮箱等，这里特别注意我为了能让多个子域名公用一个证书，采用了泛域名的方式（星号的模糊匹配：*.playpi.org）。这种生成证书的方式只是为了测试使用，最终的证书肯定是不可信的，浏览器会提示此证书不受信任，所以还是通过其它方式获取证书比较好（后续我会通过阿里云或者 letsencrypt 获取免费的证书，具体博客参考可以使用相关关键词在站内搜索）。完整信息填写 12345678910111213141516171819Generating a 2048 bit RSA private key........+++..............+++writing new private key to &apos;/site/ssl-nginx.key&apos;-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter &apos;.&apos;, the field will be left blank.-----Country Name (2 letter code) [XX]:CNState or Province Name (full name) []:GuangdongLocality Name (eg, city) [Default City]:GuangzhouOrganization Name (eg, company) [Default Company Ltd]:playpiOrganizational Unit Name (eg, section) []:playpiCommon Name (eg, your name or your server&apos;s hostname) []:*.playpi.orgEmail Address []:playpi@qq.com1-3、更改配置并重启 Nginx 重新配置 http 与 https 的参数（只列出 server 的主要部分，blog 二级域名主要是为了测试使用的，blog 的流量全部导入我的 VPS 中），特别注意 rewrite 的正则表达式，只替换域名部分，链接部分不能替换，否则都跳转到主页去了 123456789101112131415161718192021222324252627282930313233server &#123; listen 80; server_name www.playpi.org; access_log /site/iplaypi.github.io.http-www-access.log main; rewrite ^/(.*)$ https://www.playpi.org/$1 permanent; &#125; server &#123; listen 80; server_name blog.playpi.org; access_log /site/iplaypi.github.io.http-blog-access.log main; rewrite ^/(.*)$ https://blog.playpi.org/$1 permanent; &#125; server &#123; listen 443 ssl;# 监听端口 server_name www.playpi.org blog.playpi.org;# 域名 access_log /site/iplaypi.github.io.https-access.log main; root /site/iplaypi.github.io; ssl_certificate /site/ssl-nginx.crt;# 证书路径 ssl_certificate_key /site/ssl-nginx.key;#key 路径 ssl_session_cache shared:SSL:1m;# 储存 SSL 会话的缓存类型和大小 ssl_session_timeout 5m;# 配置会话超时时间 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;# 为建立安全连接，服务器所允许的密码格式列表 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on;# 依赖 SSLv3 和 TLSv1 协议的服务器密码将优先于客户端密码 #减少点击劫持 add_header X-Frame-Options DENY; #禁止服务器自动解析资源类型 add_header X-Content-Type-Options nosniff; #防 XSS 攻击 add_header X-Xss-Protection 1; &#125; 开启 443 端口，重启 Nginx12345678910# 查看已经开启的端口 firewall-cmd --list-ports# 开启端口 firewall-cmd --permanent --zone=public --add-port=443/tcp# 重载更新的端口信息 firewall-cmd --reload# 验证 Nginx 配置是否准确 nginx -t# 重新启动 Nginxnginx -s reload1-4、打开链接查看 使用 blog 二级域名测试（也需要在 DNSPod 中配置一条 A 记录解析规则）或者使用 curl 命令模拟请求，由于有重定向的问题，所以失败 既然开启了 https，可以使用 curl 关闭失效证书的方式（-k 参数）访问 https 链接 去百度站长里面重新提交 https 认证（使用上面的测试证书是认证失败的，我去阿里云重新申请了证书，认证成功了，申请证书的教程可以在本站搜索，为了给 2 个二级域名不同的证书，nginx 还需要重新配置 server 信息）2、端口的问题 为什么在上面配置域名解析记录的时候，百度的 A 记录配置 VPS 的 ip 就行了呢，这是因为在 VPS 上面只有 Nginx 这一种 Web 服务，机器会分配给它一个端口（默认 80，也是 http 的默认端口，可以配置），然后 www 的访问就使用这个端口（在 Nginx 的配置里面有，还有另外一个 blog 的），所以可以忽略端口的信息。但是如果一台机器上面有各种 Web 服务，切记确保端口不要冲突（例如 Tomcat 和 Nginx 同时存在的情况），并且给 Nginx 的就是 80 端口，然后如果有其它服务，可以使用 Nginx 做代理转发（例如把 email 二级域名转到一个端口，blog 二级域名转到另一个端口）。完善自动获取更新脚本，拉取 mater 分支的静态页面 1、先用简单的方式 使用 git 把项目克隆到：/site/iplaypi.github.io 即可。2、利用钩子自动拉取 master 分支内容到指定目录 。。。 验证结果 以下验证都是在没有开启 https 的情况下，即没有对 http 进行 301 重定向，如果做了 301 重定向截图内容会有一点不一样，curl 也会直接失败（需要访问 https 格式的链接）。使用最简单的方式验证就是在百度站长工具里面使用 抓取诊断 来进行模拟抓取多次，看看成功率是否是 100%。通过测试，可以看到，每次抓取都会成功，那么接下来就等待百度自己抓取了（百度爬虫抓取 sitemap.xml 文件的频率很低，可能要等一周）。使用抓取诊断方式来验证，这个过程有一个插曲，就是无论怎么验证都是失败的，但是使用 curl 模拟请求却是成功的。我看了失败原因概述里面，抓取的 ip 地址仍旧是 GitHub Pages 的，说明百度爬虫的流量没有到我自己的 VPS 上面。我一开始还以为是 DNSPod 配置没生效，但是通过 curl 模拟请求却可以，说明 DNSPod 配置没问题，那就是百度的问题了，应该是缓存。后来，我在移动端 UA 与 PC 端 UA 切换了一下，然后就行了。此外，既然我们知道了百度爬虫设置的用户代理，那么就可以直接使用 curl 命令来模拟百度爬虫的请求，观察返回的 http 结果是否正常。模拟命令如下：1curl -A "Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)" http://blog.playpi.org/baidusitemap.xml模拟请求的结果，可以看到也是正常的（下面的截图在没有开启 https 的情况下，如果开启 301 重定向就不行了，需要直接访问 https 链接）如果开启了 https，即对 http 请求进行 301 重定向，则可以直接访问 https 链接（如果证书是无效的，像我截图中的，则可以使用 curl 关闭无效证书的方式，加一个 -k 参数）我也去看了 VPS 上面的 Nginx 日志，确实百度爬虫的流量都被引入到这里来了，皆大欢喜 后续还需要观察看看百度的收录结果（等待 3 天后更新了，结果如下）问题总结1、这篇博客耗费了我一个多月才完成，当然不是写了一个多月，而是从发现问题到解决问题，最终写成这篇博客，前后经历了一个多月。在这一个多月里，我看了很多别人的博客，问了一些人，也看了一些技术资料，学到了很多以前不了解的知识，而且通过动手去解决问题，整个过程收获颇丰。2、写 Markdown 文档，使用代码块标记的时候，使用 3 个反单引号来标记，如果不熟悉代码块里面的编程语言，可以省略类型（例如 java、bash、javascript），不要填写，否则填错了生成的 html 静态文件是空白的。还有就是如果代码块里面放的是一段英文文本，和编程语言无关，也不要填写类型，否则生成的 html 静态文件也是空白的。3、通过实战学习了一些网络知识，例如：CNAME、A 记录、域名服务器、二级域名等、https 证书，也学习了一些关于 Nginx 的知识。4、关于访问速度的问题，GitHub Pages 的 CDN 还是很强大的，不会出现卡顿的情况。但是有时候貌似 GitHub 会被墙，打不开。此外，我搞这么久就是为了让百度爬虫能收录我的站点文章，所以自己搭建的 VPS 只是为了给百度爬虫爬取用的，其它正常人或者爬虫仍旧是访问 GitHub Pages 的链接。5、关于 https，使用 GitHub Pages 的时候，服务全部是 GitHub Pages 提供的，我无需关心。但是，自己使用 VPS 做了一个镜像，就需要配置一模一样的环境给百度爬虫使用，否则会导致一些失败的现象，例如 htps 认证失败、链接抓取失败。因此，一定要开启 https，并且同时也支持 http。以下是整理的网络请求流程图，清晰明了。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>建站</tag>
        <tag>GitHub Pages</tag>
        <tag>SEO</tag>
        <tag>百度蜘蛛</tag>
        <tag>Baiduspider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[蒸水蛋做法总结]]></title>
    <url>%2F2018122901.html</url>
    <content type="text"><![CDATA[蒸水蛋是一道小吃，有时候就简称为水蛋，可以当菜配饭吃，也可以配包子当做早餐，或者晚上蒸一碗当做宵夜，都非常好。吃起来嫩滑爽口，而且营养也丰富，做法非常简单，本文就记录蒸水蛋的过程。食材准备 2 人份的材料（1 人份减半即可，但是我觉得 1 人份的太少了，做起来浪费，不值当）： 鸡蛋 2 只（1 只做成 1 碗）葱花少许 生抽少许 食用盐少许 香油少许 制作步骤 1、准备 2 只小碗（有条件的可以使用带盖子的蒸盅，也就是平时吃快餐盛汤的那种带盖子的小碗），普通的小饭碗即可，一定要是耐热的材料，不要用秸秆环保碗、塑料饭盒、普通玻璃碗等（蒸的时候温度很高，虽然水的沸点是 100 度，但是锅内因为有水蒸气存在，压强变大，同时水蒸气转为液态会放热，锅内实际温度大于 100 度），分别打入 1 只鸡蛋，加少许食用盐，搅拌均匀（下面过程就以 1 份为准，另外 1 份是同样的操作）；2、搅拌均匀后开始加温水（最好是温水），温水的量大概是鸡蛋液的 2 倍，即鸡蛋液比温水等于 1:2，注意加温水的量，少了多了都不好（2-3 倍都行，如果碗大一点可能 1 份水蛋就需要放 2 只鸡蛋，不然显得太少了），继续搅拌，此时搅拌完成后表面应该会有一层小泡沫，可以用勺子把小泡沫都盛出去，保证蒸出来的水蛋表面光滑（怕浪费保留也行）； 搅拌均匀 3、蒸鸡蛋的时候使用保鲜膜封住碗口（有条件的使用整蛊更方便，盖子一盖即可），或者使用小盘子反盖在碗口，这样做是为了保证密封，一方面为了保证蒸出来的水蛋嫩滑，另一方面为了避免液化的水蒸气滴进去，影响水蛋的质量，先大火烧水，等水开后转为小火（火力很重要），再蒸 8 分钟即可出锅（这个时间很重要，太久了鸡蛋就老了）； 开始蒸，我为了省事就不撇小泡沫，也不盖保鲜膜了，所以做出来的成品会有点难看 4、取出后，滴入少许香油、生抽（不加也行，直接吃），撒入一点点葱花，即可食用，入口即化，滑嫩可口； 成品 以下这个我认为是做失败的，加太多水，蒸的过程中不断滴入液化的水蒸气，破坏了美感，也没葱花，但是吃起来绝对美味。注意事项1、食用盐是搅拌鸡蛋的时候就加入的，不是蒸好后再放的，这样才能入味而且分布均匀；2、加水时一定要加温水，不是冷水，也不是热水，温水才能让蒸出来的水蛋保持嫩滑；3、如果使用保鲜膜，一定要用可以蒸的材料，不是随便能用的。如果是 PVC 材料（聚氯乙烯），坚决不行，含有塑化剂释放有毒物质影响健康，如果是 PE 材料（聚乙烯），无毒，但是耐热温度不够，也不行，如果是 PVDC 材料（聚偏氯乙烯），安全温度在 140 度，可以使用。所以还是使用盘子比较好。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>蒸水蛋</tag>
        <tag>水蛋</tag>
        <tag>蒸鸡蛋</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 异常之 java.net.BindException: 地址已在使用]]></title>
    <url>%2F2018122801.html</url>
    <content type="text"><![CDATA[今天查看日志发现，所有的 Spark 程序提交时会抛出异常：1java.net.BindException: 地址已在使用 而且不止一次，会连续有多个这种异常，但是 Spark 程序又能正常运行，不会影响到对应的功能。本文就记录发现问题、分析问题的过程。问题出现 在 Driver 端查看日志，发现连续多次相同的异常（省略了业务相关类信息）：异常截图 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879// 第一次异常 2018-12-28_12:50:56 [main] WARN component.AbstractLifeCycle:204: FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: 地址已在使用 java.net.BindException: 地址已在使用 at sun.nio.ch.Net.bind0 (Native Method) at sun.nio.ch.Net.bind (Net.java:433) at sun.nio.ch.Net.bind (Net.java:425) at sun.nio.ch.ServerSocketChannelImpl.bind (ServerSocketChannelImpl.java:223) at sun.nio.ch.ServerSocketAdaptor.bind (ServerSocketAdaptor.java:74) at org.spark-project.jetty.server.nio.SelectChannelConnector.open (SelectChannelConnector.java:187) at org.spark-project.jetty.server.AbstractConnector.doStart (AbstractConnector.java:316) at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart (SelectChannelConnector.java:265) at org.spark-project.jetty.util.component.AbstractLifeCycle.start (AbstractLifeCycle.java:64) at org.spark-project.jetty.server.Server.doStart (Server.java:293) at org.spark-project.jetty.util.component.AbstractLifeCycle.start (AbstractLifeCycle.java:64) at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:252) at org.apache.spark.ui.JettyUtils$$anonfun$5.apply (JettyUtils.scala:262) at org.apache.spark.ui.JettyUtils$$anonfun$5.apply (JettyUtils.scala:262) at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp (Utils.scala:2024) at scala.collection.immutable.Range.foreach$mVc$sp (Range.scala:141) at org.apache.spark.util.Utils$.startServiceOnPort (Utils.scala:2015) at org.apache.spark.ui.JettyUtils$.startJettyServer (JettyUtils.scala:262) at org.apache.spark.ui.WebUI.bind (WebUI.scala:136) at org.apache.spark.SparkContext$$anonfun$13.apply (SparkContext.scala:481) at org.apache.spark.SparkContext$$anonfun$13.apply (SparkContext.scala:481) at scala.Option.foreach (Option.scala:236) at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:481) at org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:59)......2018-12-28_12:50:56 [main] WARN component.AbstractLifeCycle:204: FAILED org.spark-project.jetty.server.Server@33e434c8: java.net.BindException: 地址已在使用 java.net.BindException: 地址已在使用 at sun.nio.ch.Net.bind0 (Native Method) at sun.nio.ch.Net.bind (Net.java:433) at sun.nio.ch.Net.bind (Net.java:425) at sun.nio.ch.ServerSocketChannelImpl.bind (ServerSocketChannelImpl.java:223) at sun.nio.ch.ServerSocketAdaptor.bind (ServerSocketAdaptor.java:74) at org.spark-project.jetty.server.nio.SelectChannelConnector.open (SelectChannelConnector.java:187) at org.spark-project.jetty.server.AbstractConnector.doStart (AbstractConnector.java:316) at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart (SelectChannelConnector.java:265) at org.spark-project.jetty.util.component.AbstractLifeCycle.start (AbstractLifeCycle.java:64) at org.spark-project.jetty.server.Server.doStart (Server.java:293) at org.spark-project.jetty.util.component.AbstractLifeCycle.start (AbstractLifeCycle.java:64) at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:252) at org.apache.spark.ui.JettyUtils$$anonfun$5.apply (JettyUtils.scala:262) at org.apache.spark.ui.JettyUtils$$anonfun$5.apply (JettyUtils.scala:262) at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp (Utils.scala:2024) at scala.collection.immutable.Range.foreach$mVc$sp (Range.scala:141) at org.apache.spark.util.Utils$.startServiceOnPort (Utils.scala:2015) at org.apache.spark.ui.JettyUtils$.startJettyServer (JettyUtils.scala:262) at org.apache.spark.ui.WebUI.bind (WebUI.scala:136) at org.apache.spark.SparkContext$$anonfun$13.apply (SparkContext.scala:481) at org.apache.spark.SparkContext$$anonfun$13.apply (SparkContext.scala:481) at scala.Option.foreach (Option.scala:236) at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:481) at org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:59)......// 第二次异常 2018-12-28_12:50:56 [main] WARN component.AbstractLifeCycle:204: FAILED SelectChannelConnector@0.0.0.0:4041: java.net.BindException: 地址已在使用 java.net.BindException: 地址已在使用 at sun.nio.ch.Net.bind0 (Native Method) at sun.nio.ch.Net.bind (Net.java:433) at sun.nio.ch.Net.bind (Net.java:425)...... 其它信息都一样 // 第三次异常 2018-12-28_12:50:56 [main] WARN component.AbstractLifeCycle:204: FAILED SelectChannelConnector@0.0.0.0:4042: java.net.BindException: 地址已在使用 java.net.BindException: 地址已在使用 at sun.nio.ch.Net.bind0 (Native Method) at sun.nio.ch.Net.bind (Net.java:433) at sun.nio.ch.Net.bind (Net.java:425)....... 其它信息都一样 // 第一次异常 2018-12-28_12:50:56 [main] WARN component.AbstractLifeCycle:204: FAILED SelectChannelConnector@0.0.0.0:4043: java.net.BindException: 地址已在使用 java.net.BindException: 地址已在使用 at sun.nio.ch.Net.bind0 (Native Method) at sun.nio.ch.Net.bind (Net.java:433) at sun.nio.ch.Net.bind (Net.java:425)....... 其它信息都一样 可以轻易发现核心的地方在于：1FAILED SelectChannelConnector@0.0.0.0: 端口号: java.net.BindException: 地址已在使用 端口号在不断变化，从 4040 一直到 4043，才停止了异常的抛出。问题分析 在 Spark 创建 context 的时候，会使用 4040 端口作为默认的 SparkUI 端口，如果遇到 4040 端口被占用，则会抛出异常。接着会尝试下一个可用的端口，采用累加的方式，则使用 4041 端口，很不巧，这个端口也被占用了，也会抛出异常。接着就是重复上面的过程，直到找到空闲的端口。这个异常其实没什么问题，是正常的，原因可能就是在一台机器上面有多个进程都在使用 Spark，创建 context，有的 Spark 任务正在运行着，占用了 4040 端口；或者就是单纯的端口被某些应用程序占用了而已。此时是不能简单地把这些进程杀掉的，会影响别人的业务。问题解决 既然找到了问题，解决办法就很简单了：1、这本来就不是问题，直接忽略即可，不会影响 Spark 任务的正常运行；2、如果非要不想看到异常日志，那么可以检查机器的 4040 端口被什么进程占用了，看看能不能杀掉，当然这种方法不好了；3、可以自己指定端口（使用 spark.ui.port 配置项），确保使用空闲的端口即可（不建议，因为要确认空闲的端口，如果端口不空闲，Spark 的 context 会创建失败，更麻烦，还不如让 Spark 自己去重试）。参考：hortonworks原文：When a spark context is created, it starts an application UI on port 4040 by default. When the UI starts, it checks to see if the port is in use, if so it should increment to 4041. Looks like you have something running on port 4040 there. The application should show you the warning, then try to start the UI on 4041.This should not stop your application from running. If you really want to get around the WARNING, you can manually specify which port for the UI to start on, but I would strongly advise against doing so.To manually specify the port, add this to your spark-submit:–conf spark.ui.port=your_port]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>BindException</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS 异常之 READ is not supported in state standby]]></title>
    <url>%2F2018122702.html</url>
    <content type="text"><![CDATA[今天查看日志发现，以前正常运行的 Spark 程序会不断抛出异常：1org.apache.hadoop.ipc.RemoteException (org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby但是却没有影响到功能的正常运行，只不过是抛出了大量的上述异常，而且内容都一样，也都是操作 HDFS 产生的，所以猜测与 HDFS 集群（或者配置）有关系。本文就记录发现问题、解决问题的过程。问题出现 按照日常操作，查看 Spark 任务的 Driver 端的日志，结果发现了大量的重复异常，又看了一下对功能的影响，结果发现没有影响，所有功能均正常运行，产生的结果也是期望的。问题分析 详细来看一下 Driver 端的日志异常信息：123456789101112131415161718192021222324252627282930313233342018-12-26_23:25:40 [main] INFO retry.RetryInvocationHandler:140: Exception while invoking getFileInfo of class ClientNamenodeProtocolTranslatorPB over hadoop1/192.168.10.162:8020. Trying to fail over immediately.org.apache.hadoop.ipc.RemoteException (org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation (StandbyState.java:87) at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation (NameNode.java:1722) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation (FSNamesystem.java:1362) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo (FSNamesystem.java:4414) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo (NameNodeRpcServer.java:893) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo (ClientNamenodeProtocolServerSideTranslatorPB.java:835) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod (ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call (ProtobufRpcEngine.java:619) at org.apache.hadoop.ipc.RPC$Server.call (RPC.java:962) at org.apache.hadoop.ipc.Server$Handler$1.run (Server.java:2039) at org.apache.hadoop.ipc.Server$Handler$1.run (Server.java:2035) at java.security.AccessController.doPrivileged (Native Method) at javax.security.auth.Subject.doAs (Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs (UserGroupInformation.java:1628) at org.apache.hadoop.ipc.Server$Handler.run (Server.java:2033) at org.apache.hadoop.ipc.Client.call (Client.java:1468) at org.apache.hadoop.ipc.Client.call (Client.java:1399) at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke (ProtobufRpcEngine.java:232) at com.sun.proxy.$Proxy30.getFileInfo (Unknown Source) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo (ClientNamenodeProtocolTranslatorPB.java:768) at sun.reflect.GeneratedMethodAccessor34.invoke (Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke (Method.java:498) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod (RetryInvocationHandler.java:187) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke (RetryInvocationHandler.java:102) at com.sun.proxy.$Proxy31.getFileInfo (Unknown Source) at org.apache.hadoop.hdfs.DFSClient.getFileInfo (DFSClient.java:2007) at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall (DistributedFileSystem.java:1136) at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall (DistributedFileSystem.java:1132) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve (FileSystemLinkResolver.java:81) at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus (DistributedFileSystem.java:1132) at org.apache.hadoop.fs.FileSystem.isFile (FileSystem.java:1426)注意一下核心异常所在：12Exception while invoking getFileInfo of class ClientNamenodeProtocolTranslatorPB over hadoop1/192.168.10.162:8020. Trying to fail over immediately.org.apache.hadoop.ipc.RemoteException (org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby当去从 hadoop1/192.168.10.162:8020 这里 getFileInfo 的时候，抛出了异常，而且明确告诉我们这台机器处于 standby 状态，不支持读取操作。此时，可以想到，肯定是 hadoop1/192.168.10.162:8020 这台机器已经处于 standby 状态了，无法提供服务，所以抛出此异常。既然问题找到了，那么问题产生的原因是什么呢，以及为什么对功能没有影响，接下来一一分析。首先查看 hdfs-site.xml 配置文件，看看 namenode 相关的配置项：12345678910111213141516171819&lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;r-cluster&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.namenodes.r-cluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.r-cluster.nn1&lt;/name&gt; &lt;value&gt;hadoop1:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.r-cluster.nn2&lt;/name&gt; &lt;value&gt;rocket15:8020&lt;/value&gt;&lt;/property&gt;可以看到，namenode 相关配置有 2 台机器：nn1、nn2，而上述产生异常的信息表明连接 nn1 被拒绝，那么我去看一下 HDFS 集群的状态，发现 nn1 果然是 standby 状态的，而 nn2（rocket15） 才是 active 状态。再仔细查看日志，没有发现连接 nn2 的异常，那就说明是第一次连接 nn1 抛出异常，然后试图连接 nn2，成功连接，没有抛出异常，接下来程序就正常处理数据了，对功能没有任何影响。到这里，我们已经分析出了整个过程，现象表明这个异常只是连接了 standby 状态的 namenode，是正常抛出的。然后会再次连接另外一台 active 状态的 namenode，连接成功。抛异常的流程细节 1、客户端在连接 HDFS 的时候，会从配置文件 hdfs-site.xml 中，读取 nameservices 的配置，获取机器编号，我这里是 nn1 和 nn2，分别对应着 2 台 namenode 机器；2、客户端会首先选择编号较小的 namenode（我这里是 nn1，对应着 hadoop1），试图连接；3、如果这台 namenode 是 active 状态，则客户端可以正常处理请求；但是如果这台 namenode 是 standby 状态，则客户端抛出由服务端返回的异常：Operation category READ is not supported in state standby，同时打印 ip 信息，接着会尝试连接另外一台编号较大的 namenode（我这里是 nn2，即 rocket15）；4、如果连接成功，则客户端可以正常处理请求；如果 nn2 仍然像 nn1 一样，客户端会抛出一样的异常，此时会继续反复重试 nn1 与 nn2（重试次数有配置项，间隔时间有配置项）；如果有成功的，则客户端可以正常处理请求，如果全部失败，则客户端无法正常处理请求，此时应该要关注解决 namenode 为什么全部都处在 standby 状态。 配置参数如下（参考 Hadoop 官方文档 ）：1234567891011121314151617181920212223242526272829&lt;!-- 客户端重试次数，默认 15 --&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.max.attempts&lt;/name&gt; &lt;value&gt;15&lt;/value&gt;&lt;/property&gt;&lt;!-- 客户端 2 次重试间隔时间，默认 500 毫秒 --&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.sleep.base.millis&lt;/name&gt; &lt;value&gt;500&lt;/value&gt;&lt;/property&gt;&lt;!-- 客户端 2 次重试间隔时间，默认 1500 毫秒 --&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.sleep.max.millis&lt;/name&gt; &lt;value&gt;1500&lt;/value&gt;&lt;/property&gt;&lt;!-- 客户端 1 次连接中重试次数，默认 0, 在网络不稳定时建议加大此值 --&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.connection.retries&lt;/name&gt; &lt;value&gt;0&lt;/value&gt;&lt;/property&gt;&lt;!-- 客户端 1 次连接中超时重试次数，仅是指超时重试，默认 0, 在网络不稳定时建议加大此值 --&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.connection.retries.on.timeouts&lt;/name&gt; &lt;value&gt;0&lt;/value&gt;&lt;/property&gt; 问题解决 既然明确了问题，并且分析出了具体原因，解决起来就简单了，对于我这种情况，有 2 种方法：1、不用解决，也无需关心，这个异常没有任何影响，会自动重连另外一台 active 状态的 namenode 机器的；2、如果就是一心想把异常消除掉，那就更改 hdfs-site.xml 配置文件里面的 nameservices 配置项对应的机器，把编号最小的机器设置成状态为 active 的 namenode（例如我这里把 nn1、nn2 的对应的机器 ip 地址交换一下即可，确保 nn1 是 active 状态的），那么连接 HDFS 的时候第一次就会直接连接这台机器，就不会抛出异常了（但是要注意 namenode 以后可能是会挂的，挂了会自动切换，那么到那个时候还要更改这个配置项）。123456789&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.r-cluster.nn1&lt;/name&gt; &lt;value&gt;rocket15:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.r-cluster.nn2&lt;/name&gt; &lt;value&gt;hadoop1:8020&lt;/value&gt;&lt;/property&gt;问题总结1、参考：http://support-it.huawei.com/docs/zh-cn/fusioninsight-all/maintenance-guide/zh-cn_topic_0062904132.html2、这个问题其实不是问题，只不过抛出了异常，我看到有点担心而已，但是如果连接所有的机器都抛出这种异常，并且重试了很多次就有影响了，说明所有的 namenode 都挂了，根本无法正常操作 HDFS 系统；3、根据 2 进行总结：如果只是在操作 HDFS 的时候打印一次（每次操作都会打印一次），说明第一次连接到了 standby 状态的 namenode，是正常的，不用关心；但是，如果出现了大量的异常（比如连续 10 次，连续 20 次），说明 namenode 出问题了，此时应该关心 namenode 的状态，确保正常服务。]]></content>
      <categories>
        <category>Hadoop 从零基础到入门系列</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Spark</tag>
        <tag>HDFS</tag>
        <tag>nameNode</tag>
        <tag>standby</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS 异常之 Filesystem closed]]></title>
    <url>%2F2018122701.html</url>
    <content type="text"><![CDATA[今天通过 Hadoop 的 api 去操作 HDFS 里面的文件，读取文本内容，但是在代码里面总是抛出以下异常：1Caused by: java.io.IOException: Filesystem closed然而文本内容又是正常读取出来的，但是我隐隐觉得读取的文本内容可能不全，应该只是所有文本内容的一部分。本文就记录这个问题的原因、影响以及解决方法。问题出现 通过查看日志发现，有大量的异常日志打印出来，全部都是操作 HDFS 的时候产生的，有的是使用 Spark 连接 HDFS 读取文本数据，有的是使用 Hadoop 的 Java api 通过文件流来读取数据，每次读取操作都会产生一个如下异常信息（会影响实际读取的内容，多个 DataNode 的内容会漏掉）：123456789101112131415161718192021222324252627282018-12-26_23:25:46 [SparkListenerBus] ERROR scheduler.LiveListenerBus:95: Listener EventLoggingListener threw an exceptionjava.lang.reflect.InvocationTargetException at sun.reflect.GeneratedMethodAccessor33.invoke (Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke (Method.java:498) at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply (EventLoggingListener.scala:150) at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply (EventLoggingListener.scala:150) at scala.Option.foreach (Option.scala:236) at org.apache.spark.scheduler.EventLoggingListener.logEvent (EventLoggingListener.scala:150) at org.apache.spark.scheduler.EventLoggingListener.onJobStart (EventLoggingListener.scala:173) at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent (SparkListenerBus.scala:34) at org.apache.spark.scheduler.LiveListenerBus.onPostEvent (LiveListenerBus.scala:31) at org.apache.spark.scheduler.LiveListenerBus.onPostEvent (LiveListenerBus.scala:31) at org.apache.spark.util.ListenerBus$class.postToAll (ListenerBus.scala:55) at org.apache.spark.util.AsynchronousListenerBus.postToAll (AsynchronousListenerBus.scala:37) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp (AsynchronousListenerBus.scala:80) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply (AsynchronousListenerBus.scala:65) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply (AsynchronousListenerBus.scala:65) at scala.util.DynamicVariable.withValue (DynamicVariable.scala:57) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp (AsynchronousListenerBus.scala:64) at org.apache.spark.util.Utils$.tryOrStopSparkContext (Utils.scala:1181) at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run (AsynchronousListenerBus.scala:63)Caused by: java.io.IOException: Filesystem closed at org.apache.hadoop.hdfs.DFSClient.checkOpen (DFSClient.java:795) at org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync (DFSOutputStream.java:1986) at org.apache.hadoop.hdfs.DFSOutputStream.hflush (DFSOutputStream.java:1947) at org.apache.hadoop.fs.FSDataOutputStream.hflush (FSDataOutputStream.java:130) ... 20 more最直接清晰的描述就是：1Caused by: java.io.IOException: Filesystem closed上述异常信息表明 HDFS 的 Filesystem 被关闭了，但是代码仍旧试图打开文件流读取内容。问题解决 分析一下 根据上述信息，查看代码，每次操作 HDFS 都是独立的，会先根据统一的 conf 创建 Filesystem，然后根据文件路径创建 Path，打开输入流，读取内容，读取完成后关闭 Filesystem，没有什么异常的地方。同时，根据异常信息可以发现，异常的抛出点并不是业务逻辑代码，更像是已经开始开启文件流读取文件，读着读着 Filesystem 就被关闭了，然后引发了异常，而业务逻辑中并没有突然关闭 Filesystem 的地方，也没有多线程操作 Filesystem 的地方。1234567891011121314151617181920212223242526272829303132333435363738/** * 获取文件内容 * 纯文本，不做转换 * 如果传入目录，返回空内容 * * @param hdfsFile * @return */public static Set&lt;String&gt; getFileContent(String hdfsFile) &#123; Set&lt;String&gt; dataResult = new HashSet&lt;&gt;(); FileSystem fs = null; try &#123; // 连接 hdfs fs = FileSystem.get (CONF); Path path = new Path (hdfsFile); if (fs.isFile (path)) &#123; FSDataInputStream fsDataInputStream = fs.open (path); BufferedReader bufferedReader = new BufferedReader (new InputStreamReader (fsDataInputStream)); String line = null; while (null != (line = bufferedReader.readLine ())) &#123; dataResult.add (line); &#125; &#125; else &#123; LOGGER.error ("!!!! 当前输入参数为目录，不读取内容:&#123;&#125;", hdfsFile); &#125; &#125; catch (Exception e) &#123; LOGGER.error ("!!!! 处理 hdfs 出错:" + e.getMessage (), e); &#125; finally &#123; if (null != fs) &#123; try &#123; fs.close (); &#125; catch (IOException e) &#123; LOGGER.error ("!!!! 关闭文件流出错:" + e.getMessage (), e); &#125; &#125; &#125; return dataResult;&#125;通过查找文档发现，这个异常是 Filesystem 的缓存导致的。当任务提交到集群上面以后，多个 datanode 在 getFileSystem 过程中，由于 Configuration 一样，会得到同一个 FileSystem。如果有一个 datanode 在使用完关闭连接，其它的 datanode 在访问时就会出现上述异常，导致数据缺失（如果数据恰好只存在一个 datanode 上面，可能没问题）。找到方法 通过上面的分析，找到了原因所在，那么解决方法有 2 种：1、可以在 HDFS 的 core-site.xml 配置文件里面把 fs.hdfs.impl.disable.cache 设置为 true，这样设置会全局生效，所有使用这个配置文件的连接都会使用这种方式，有时候可能不想这样更改，那就使用第 2 种方式；1234&lt;property&gt; &lt;name&gt;fs.hdfs.impl.disable.cache&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;2、在 HDFS 提供的 Java api 里面更改配置信息，则会只针对使用当前 conf 的连接有效，相当于临时参数。12// 缓存 fs, 避免多 datanode 异常：Caused by: java.io.IOException: Filesystem closedCONF.setBoolean ("fs.hdfs.impl.disable.cache", true);上面 2 种方法的目的都是为了关闭缓存 Filesyetem 实例，这样每次获得的 Filesystem 实例都是独立的，不会产生上述的异常，但是缺点就是会增加网络的 I/O，频繁开启、关闭文件流。问题总结1、参考：https://stackoverflow.com/questions/23779186/ioexception-filesystem-closed-exception-when-running-oozie-workflow ；2、保留日志，查看日志很重要；3、FileSytem 类内部有一个 static CACHE，用来保存每种文件系统的实例集合，FileSystem 类中可以通过参数 fs.% s.impl.disable.cache 来指定是否禁用缓存 FileSystem 实例（其中 % s 替换为相应的 scheme，比如 hdfs、local、s3、s3n 等）。如果没禁用，一旦创建了相应的 FileSystem 实例，这个实例将会保存在缓存中，此后每次 get 都会获取同一个实例，但是如果被关闭了，则再次用到就会无法获取（多 datanode 读取数据的时候）；4、源码分析放在以后，留坑。]]></content>
      <categories>
        <category>Hadoop 从零基础到入门系列</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Spark</tag>
        <tag>Filesystem</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[可乐鸡翅做法总结]]></title>
    <url>%2F2018122501.html</url>
    <content type="text"><![CDATA[可乐鸡翅，是一道做法很简单的菜，很巧妙地将饮料和鸡翅结合在一起，做出来的可乐鸡翅既好看又好吃。本文简单介绍可乐鸡翅的做法总结，这是一种偏甜的做法。食材准备 3 人份的材料（8-10 个鸡翅），吃多了也不好吃1、鸡翅 9 个，最好是鸡中翅（一般 2-3 元一个）；2、可乐 1 罐（330 毫升的，如果鸡翅多的话，适当增加可乐）； 选择百事可乐 3、姜片、八角、桂皮（也可以不用）；4、料酒、生抽、老抽、食用盐； 制作步骤 1、在鸡翅背面划几刀（正面保留完整为了摆盘好看而已），更容易入味，用食用盐、料酒、老抽腌制 10 分钟，备用； 划刀腌制 2、锅中加水，放入姜片、少量料酒，鸡翅也下锅（冷水下锅），煮开即可，不用煮透（煮透鸡翅就老了），看到浮沫很多就可以捞出，用温水清洗一下，晾干（晾不干就用厨房纸擦一下，防止煎的时候溅油），此时如果发现有不干净的鸡毛可以拔干净； 鸡翅冷水下锅 鸡翅焯水出浮沫 3、锅中放入少量油（不放也行，鸡翅会自己出油的），放入姜片，开始煎鸡翅，开小火，煎至两面金黄即可，不可以煎太久，否则鸡翅老了不好吃； 小火煎 4、加一罐可乐，适量料酒、生抽、老抽，适量桂皮、八角，开始小火炖煮，炖至可乐还有一小碗水的量的时候，尝尝味道，适量加盐； 加入可乐、配料 小火炖煮 5、炖至汤浓收汁，基本所有的汁都覆盖在鸡翅上面了，鸡翅也有味道，装盘，正面朝上，把锅中剩余的汤汁淋入鸡翅中（大概 1-2 饭勺的量），再撒上少许白芝麻，既好看又好吃。 可以收汁 收汁之前补充食用盐、老抽 收汁完成 装盘 注意事项1、不要再放糖了，一罐可乐里面含糖大概 35 克；2、如果放了那种本身是咸味的生抽，也不用放盐了，或者少量放一点点（放盐之前先尝尝汤水的味道，不容易出差错）；3、焯水的时候冷水下锅，防止肉老了，并且放一点姜片和料酒，去腥味；4、鸡翅焯水后晾干很有必要，否则下一步骤煎的时候水和热油混合一起会溅出油的；5、甜味和咸味的控制依据个人口味调整，此外，可口可乐比百事可乐更甜，即含糖量更高。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>可乐鸡翅</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 错误之 JavaSparkContext not serializable]]></title>
    <url>%2F2018122101.html</url>
    <content type="text"><![CDATA[今天更新代码，对 Spark 里面的 RDD 随便增加了一个 Function，结果遇到了序列化（Serializable）的问题，这个不是普通的自定义类不能序列化问题，而是 JavaSparkContext 的用法问题，由于小看了这个问题，多花了一点时间解决问题，本文就记录下这一过程。问题出现 针对已有的项目改动了一点点，结果直接出现了这个错误：一开始疏忽大意了，以为像往常一样，是某些需要传递的对象对应的类没有序列化，由于对代码不敢改动太大，就想着用最简单的方法，把几个自定义类都序列化了，以为就应该可以了。结果，还是不行，此时虽然不会有自定义类的序列化问题了，但是却出现了终极错误：JavaSparkContext not serializable，这是什么意思呢，是说 JavaSparkContext 不能序列化，总不能把 JavaSparkContext 序列化吧，Spark 是不允许这么干的。那么问题是什么呢？我首先猜测肯定是 Function 里面用到了 JavaSparkContext 对象，导致启动 Spark 任务的时候，需要序列化 Function 用到的所有对象（当然也需要序列化对象所属类里面的所有属性），而这些 Function 所用到的所有对象里面，就有 JavaSparkContext 对象。于是，我耐心看了一下代码，果然，在创建 Function 对象的时候，竟然把 JavaSparkContext 对象作为参数传进去了，还是因为 JavaSparkContext 不能乱用。其实，报错日志里面都已经明显指向说明了，除了自定义的类，错误归结于 1at org.apache.spark.api.java.AbstractJavaRDDLike.mapPartitions (JavaRDDLike.scala:46) 而这里的代码，正是我增加的一部分，为了贪图简单方便，直接把 JavaSparkContext 对象传递给了 mapPartitions 对应的 Function。解决问题 既然找到了问题，接下来就好办了。既然 JavaSparkContext 不能乱用，那就不用，把这个传递参数去掉，即可正常运行，但是这样做太简单粗暴，不是解决问题的思路。仔细分析一下，可以有 2 种解决办法（思路就是避免序列化）：1、如果在 Function 里面非要用到 JavaSparkContext 对象，那就把 JavaSparkContext 对象设置为全局静态的 Java 属性（使用 static 关键字），那么在哪里都可以调用它了，而无需担心序列化的问题（静态属性可以避免从 Driver 端发送到 Executor 端，从而避免了序列化过程）；2、对于 Function 不要使用内部匿名类，这样必然需要序列化 Function 对象，同时也必然需要序列化 Function 对象用到的 JavaSparkContext 对象，其实可以把 Function 类定义为内部静态类，就可以避免序列化了。问题总结 1、出现这种错误，不要想当然地认为就是某种原因造成的，而要先看详细日志，否则会走弯路，浪费一些时间（虽然最终也能解决问题）；2、有时候状态不好，晕乎乎的，找问题又慢又低效，此时应该休息一下，等头脑清醒了再继续找问题，否则可能事倍功半，而且影响心情。 参考：https://stackoverflow.com/questions/27706813/javasparkcontext-not-serializable]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark序列化</tag>
        <tag>serializable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微博 url mid 相互转换算法实现 - Java 版本]]></title>
    <url>%2F2018122001.html</url>
    <content type="text"><![CDATA[对微博数据有了解的人都知道，一条微博内容对应有唯一的微博 url，同时对微博官方来说，又会生成一个 mid，mid 就是一条微博的唯一标识（就像 uid 是微博用户的唯一标识一样），也类似于人的身份证号。其实，微博 url 里面有一串看起来无意义的字符（由字母、数字组成，6-8 个字符长度），可以和 mid 互相转换，本文就根据理论以及 Java 版本的实现，讲解微博 url 与 mid 的互相转换过程。 待整理。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>微博url</tag>
        <tag>微博mid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript 中字符串截取方法总结]]></title>
    <url>%2F2018121901.html</url>
    <content type="text"><![CDATA[最近在处理数据的时候，用到了 JavaScript 编程语言，通过绕弯路来解决 ETL 处理的逻辑，其中就用到了字符串的截取方法，查 JavaScript 的文档看到了 3 个方法，被绕的有点晕，本文就总结一下 JavaScript 中字符串截取的方法。开篇 首先声明，JavaScript 中对方法名字的大小写是敏感的，该是小写就是小写，该是大写就是大写。substring () 方法 定义和用法 substring () 方法用于截取字符串中介于两个指定下标之间的字符 语法 stringObject.substring (start, stop) 上述参数解释：参数名 解释说明 start 必须，一个整数（是负数则被自动置为 0），要截取的子串的第一个字符在 stringObject 中的位置 end 可选（如果省略该参数，则被默认为字符串长度），一个整数（是负数则被自动置为 0），比要截取的子串的最后一个字符在 stringObject 中的位置多 1返回值 一个全新的字符串，其实就是 stringObject 的一个子字符串，其内容是从 start 到 stop-1 的所有字符，其长度为 stop 减 start。注意事项 1、substring () 方法返回的子字符串包括 start 处的字符，但是不包括 stop 处的字符，这一点可能很多人会迷惑，其实很多编程语言都是这个逻辑；2、如果参数 start 与 stop 相等，那么该方法返回的就是一个空串（即长度为 0 的字符串，不是 null，也不是 undefined）；3、如果 start 比 stop 大，那么该方法在截取子串之前会先交换这两个参数，这就会导致参数的顺序不影响截取的结果了；4、参数理论上不能出现负数（在本方法中无特殊意义，在其它方法中就有特殊意义了），如果有，那么在截取子串之前会被置为 0。 举例说明 例子 1（从下标 3 截取到字符串最后）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substring (3))&lt;/script&gt;输出（长度为 10 的子串）：1lo-world!例子 2（从下标 3 截取到下标 8）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substring (3, 8))&lt;/script&gt;输出（长度为 5 的子串）：1lo-wo例子 3（从下标 3 截取到下标 8，但是参数位置反了）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substring (8, 3))&lt;/script&gt;输出（长度为 5 的子串）：1lo-wo例子 4（参数为负数，从下标 0 截取到下标 3）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substring (-1, 3))&lt;/script&gt;输出（长度为 3 的子串）：1Helsubstr () 方法 定义和用法 substr () 方法可在字符串中截取从 start 下标开始的指定长度的子串 语法 stringObject.substr (start, length) 上述参数解释：参数名 解释说明 start 必须，必须是数值（0、正数、负数都可以），表示要截取的子串的起始下标。如果是负数，那么该参数声明的是从字符串的尾部开始计算的位置。也就是说，-1 指字符串中最后一个字符，-2 指倒数第二个字符，以此类推。（参数为负数也可以理解成字符串长度加负数之和即为起始下标）length可选（如果省略该参数，那么默认为从 start 开始一直到 stringObject 的结尾对应的长度），必须是数值（0、正数、负数都可以）。返回值 一个全新的字符串，包含从 stringObject 的 start（包括 start 所指的字符）下标开始的 length 个字符。如果没有指定 length，那么返回的字符串包含从 start 到 stringObject 的结尾的字符。如果 length 指定为负数或者 0，那么返回空串。如果 length 指定为远远大于 stringObject 长度的正数，那么返回的字符串包含从 start 到 stringObject 的结尾的字符。注意事项 1、start 参数为负数是有特殊含义的；2、如果 length 指定为负数或者 0，那么返回空串（即长度为 0 的字符串，不是 null，也不是 undefined）；3、ECMAscript 没有对该方法进行标准化，因此不建议使用它。 举例说明 例子 1（从下标 3 截取到字符串最后）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substr (3))&lt;/script&gt;输出（长度为 9 的子串）：1lo-world!例子 2（从下标 3 截取长度为 5 的子串）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substr (3, 5))&lt;/script&gt;输出（长度为 5 的子串）：1lo-wo例子 3（从下标 3 截取长度为 - 5 的子串，返回空串）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substr (3, -5))&lt;/script&gt;输出（返回空串）：12例子 4（start 参数为负数，即从字符串倒数第 5 个位置截取长度为 3 的子串）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.substr (-5, 3))&lt;/script&gt;输出（长度为 3 的子串）：1orlslice () 方法 定义和用法 slice () 方法用于截取字符串中介于两个指定下标之间的字符，与 substring () 方法的功能类似 语法 stringObject.slice (start, end) 上述参数解释：参数名 解释说明 start 必须，一个整数（0、正数、负数，负数有特殊含义），要截取的子串的第一个字符在 stringObject 中的位置。如果是负数，那么该参数声明的是从字符串的尾部开始计算的位置。也就是说，-1 指字符串中最后一个字符，-2 指倒数第二个字符，以此类推。（参数为负数也可以理解成字符串长度加负数之和即为起始下标）end可选（如果省略该参数，则被默认为字符串长度），一个整数（负数含义与 start 相同），比要截取的子串的最后一个字符在 stringObject 中的位置多 1返回值 一个全新的字符串，其实就是 stringObject 的一个子字符串，其内容是从 start 到 stop-1 的所有字符，其长度为 stop 减 start。注意事项 1、slice () 方法返回的子字符串包括 start 处的字符，但是不包括 stop 处的字符，这一点可能很多人会迷惑，其实很多编程语言都是这个逻辑；2、如果参数 start 与 stop 相等，那么该方法返回的就是一个空串（即长度为 0 的字符串，不是 null，也不是 undefined）；3、参数可以出现负数（比 substring () 方法灵活多了）。 举例说明 例子 1（从下标 3 截取到字符串最后）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.slice (3))&lt;/script&gt;输出（长度为 9 的子串）：1lo-world!例子 2（从下标 3 截取到下标 8）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.slice (3, 8))&lt;/script&gt;输出（长度为 5 的子串）：1lo-wo例子 3（从下标 3 截取到下标 8，但是参数使用负数，从下标 - 9 截取到下标 - 4）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.slice (-9, -4))&lt;/script&gt;输出（长度为 5 的子串，（-4）-（-9）=5）：1lo-wo例子 4（从下标 3 截取到下标 2）：1234&lt;script type="text/javascript"&gt;var str="Hello-world!"document.write (str.slice (3, 2))&lt;/script&gt;输出返回空串）：12]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>字符串截取</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[西红柿疙瘩汤做法总结]]></title>
    <url>%2F2018121601.html</url>
    <content type="text"><![CDATA[西红柿疙瘩汤，是一道做法非常简单的主食与配菜混为一起的菜品，适合在寒冷的冬天食用，吃一碗热乎乎的，非常暖胃，我知道在中原地区（河南、安徽北部）都有这个做法。本文就讲述西红柿疙瘩汤的做法总结。食材准备 以下的食材份量大约 2 人份：黄心乌菜一颗（实在没有使用其它青菜也可以）西红柿一颗（粉的最好，与脆的对立）鸡蛋 2 颗 面粉 100 克 小葱、香菜各 2 棵 调味料（食用盐、芝麻油）制作步骤 从开火到关火预计耗时 15-20 分钟：0、葱花香菜段；1、西红柿去皮，划十字刀花，放入热水中烫 1 分钟左右，取出直接去皮，不去皮也行，但是会影响口感，去皮后切丁，切小一点，放入碗中备用；粉粉的西红柿 十字花刀 开水烫 1 分钟（30 秒翻身一次），轻易去皮 西红柿去皮 西红柿切丁 2、准备黄心乌菜，洗干净，随便切（手撕也行，无所谓），切成条状或者小块状，别太大就行；3、面粉放入大碗中，放在水龙头下，让水一滴一滴滴下来，迅速搅拌面粉，很快就可以做成面粒；4、鸡蛋打入碗中，搅拌均匀备用；5、锅烧热，倒入油，炒制西红柿丁，中小火炒制 3-5 分钟，此时西红柿的状态就是一半是糊状，一半是小颗粒，混合在一起，倒入开水（注意量的控制，比想象的多倒一点，面粒会吸收大量水分的），大火烧开； 西红柿丁炒制 加开水，煮开 6、烧开后放入面粒，大火煮 5 分钟，面粒基本熟透，汤变得浓稠，放入青菜，中火继续煮 1 分钟左右，鸡蛋液慢慢淋入锅中，搅拌，放入食用盐，中火继续煮 2 分钟； 放入面粒，继续煮 5 分钟 7、开锅，放入芝麻油、香菜段，葱花，搅拌十几秒，关火。 一锅 一碗 做完顺便又加了 2 个菜：花菜回锅肉 辣椒回锅肉 注意事项 1、青菜最好选择黄心乌，因为我一直吃的都是这种，黄心乌这种青菜一般在沿淮地区才播种，因为它比较耐寒，在秋季播种，在冬天收割，一般北方的冬天也看不到其它青菜可以生长了；2、条件允许的话，可以放一点酱肉之类的肉粒进去，更能增加食欲；3、做面粒的时候切记不要直接倒水搅拌，这样是做不成的一粒一粒的效果的，只能用水滴进去然后迅速搅拌，使水滴周围裹上面粉形成一粒，很快就全部都是面粒了，而且很均匀，另外，做好面粒后要立马使用，不要提前做好放那里，因为放久了（10 分钟都不行）面粒会粘连在一起，实在要放的话再多加点面粉进去，让面粒之间隔开；4、西红柿最好选择粉的，就是那种吃起来很柔绵的，更容易做成均匀的汤；5、如果在煮的过程中发现有点粘锅，那是因为水少了，面太多了，这时候用汤勺试着加 1-2 勺水进去，再搅拌一下，如果还是粘锅再加 1-2 勺，千万不要一下子加很多水，面汤最好的状态就是不粘锅但是又很浓稠；6、做回锅肉，肉要煮到什么程度才能回锅，简单的判断方法就是筷子可以轻易穿透肉，一般要煮 20 分钟以上。 补充说明2018 年 12 月 23 日，广州突然降温，降到 17 度左右（前一天的冬至还 25 度呢，短袖都穿起来了），天气冷了，于是又煮了一锅。可惜这次没买到香菜，没买到黄心乌菜，也没买到酱肉，凑活着吃。2018 年 12 月 30 日，广州的温度降到了个位数，最低 5 度，实在是冷。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>西红柿疙瘩汤</tag>
        <tag>疙瘩汤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用正则表达式列表]]></title>
    <url>%2F2018121401.html</url>
    <content type="text"><![CDATA[正则表达式是一种表达式语句。本文记录一些常用的正则表达式，以便使用。 待整理。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[与微博内容分析相关的正则表达式]]></title>
    <url>%2F2018121101.html</url>
    <content type="text"><![CDATA[在分析微博内容时，常常需要进行特殊内容去除与抽取，例如抽取微博话题、微博昵称、微博表情、微博短链接、网址长链接等等。本文依据实际使用情况，记录下了与微博内容分析相关的正则表达式，以便查用。微博表情 表情是使用左右中括号包含的文本（在实际使用时，显示的是 emoji 表情，不是单纯的字符），例如：[爱心]、[微笑]、[笑哭]，分别表示：:heart:、❤️、:smile:、😊、:joy:、😂参考：emoji 百科 。 如果在微博内容中抽取表情，使用正则表达式（1-7 个字符，中文和字母，不排除有的新的表情出现，导致字符更长）：1\[[\u4e00-\u9fa5A-Za-z]&#123;1,7&#125;\]不同字符长度的表情举例（我用了 10 分钟把微博表情全部浏览了一遍，发现 [小黄人] 系列、[文明遛狗] 是最近刚刚发布出来的）：[耶]、[来]、[跪了]、[ok]、[中国赞]、[紫金草]、[doge]、[文明遛狗]、[给你小心心]、[小黄人微笑]、[弗莱见钱眼开]、[小黄人剪刀手]、[哆啦 A 梦害怕]、[带着微博去旅行]。微博昵称 微博昵称是用户填写的昵称，并且在转发或者提到时，会增加 @ 前缀，例如有一个 playpi 微博用户，在实际微博内容中，会以 @playpi 的形式出现，当然，微博昵称的可用字符是有限制的，不是任意字符都行，长度也是有限制的，最少 4 个字符，最多 30 个字符。以及微博客服的回答：微博客服微博 。 但是这个规则是针对修改昵称的限制，如果有些帐号是以前注册的，并且昵称在微博官方限制以前没有修改过，那么就有可能是 2 个字符，3 个字符，例如各个明星、作家、自媒体的个人微博：@阑夕、@王力宏、@韩寒 等等。如果在微博内容中抽取昵称，使用正则表达式（中文、数字、字母、横线、下划线的组合，2-30 个字符）：1@[\u4e00-\u9fa5A-Z0-9a-z_-]&#123;2,30&#125;微博话题 话题是微博定义的一种概念，可以用来标识热门事件、重大新闻、明星、综艺节目等等，发布规则就是使用 2 个 #符号包含话题内容（例如：# 创造 101#），话题即生成，微博还专门有一个实时话题榜单。如果在微博内容中抽取话题，使用正则表达式（2 个 #号之间，非指定的符号，长度在 1-49 之间）：1#[^@&lt;&gt;#"&amp;'\r\n\t]&#123;1,49&#125;#注意，我找到 2014 年的 一篇旧帖子 ，微博小秘书评论说话题不能包含指定的几个特殊字符，还有内容长度限制，但是我在微博页面试了一下，这些特殊字符都可以使用（但是生成的话题页面，&lt; 字符、&gt; 字符被转成了 html 字符实体，换行符后的内容被截断，@符号、’ 单引号、” 双引号被自动替换掉，# 符号根本无法发布，空格符可以正常使用），而且长度限制是 1-49 个字符（中英文、标点都算 1 个字符）。但是为了话题内容的传播，还是使用通俗易懂的中文或者字母比较好。 微博短链接 微博短链接是微博官方提供的网址压缩功能产生的一种只包含少量字符的短网址，例如：http://finance.sina.com.cn ，压缩后为：http://t.cn/RnM1Uti 。这样的话，发微博时链接占用更少的字符长度。如果发微博时，内容中带了链接，例如视频地址、淘宝店地址，会被自动压缩为短链接。微博短链接可以直接在浏览器中访问，会被微博的网址解析服务器转换为原来的正常链接再访问。如果在微博内容中抽取短链接，使用正则表达式（我这里只是抽取 t.cn 域名的，6-8 个字母、数字）：1#https&#123;0,1&#125;://t.cn/[A-Z0-9a-z]&#123;6,8&#125;[/]&#123;0,1&#125;#参考：微博开放平台说明：http://open.weibo.com/wiki/2/short_url/shorten ；免费在线短链接转换工具：http://dwz.wailian.work 。网址长链接 网址长链接也就是普通的网址，有多种可能性。如果在微博内容中抽取网址长链接，使用正则表达式（我这里只考虑 http、https、ftp、file 协议）：1(https?|ftp|file)://[-A-Za-z0-9+&amp;@#/%?=~_|!:,.;]+[-A-Za-z0-9+&amp;@#/%=~_|]]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
        <tag>微博内容</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10 默认程序设置无效]]></title>
    <url>%2F2018120901.html</url>
    <content type="text"><![CDATA[装了 Windows 10 系统（教育版本），用了将近 3 个月了，最近发现一个诡异的现象，我的默认程序设置每次都只是设置后生效一段时间，例如视频播放器、音乐播放器，我分别设置成了迅雷看看、网易云音乐，用了半天之后，发现又变成了 Window 10 系统自带的视频播放器。这个现象也不是重启之后才出现的，而是平时用着用着就会出现，很莫名其妙。后来查阅资料发现这是一个普遍的现象，这个问题的根本原因是 Windows 10 自带的 bug，通常导致这个 bug 出现的原因是开启了系统的自动更新。现象 在 Windows 10 系统（没有打对应补丁的）中，如果开启了系统自动更新，就会触发相应的 bug：默认程序会被系统更改回系统自带的程序，例如视频播放器、音乐播放器等等。这个问题的原因用官方标识来指定就是由于 KB3135173 所致，同时这个 bug 已经有对应的补丁了。按照系统设置，把某些默认程序改为自己需要的，我这里把视频播放器改为迅雷影音，设置特定格式的文件（.mkv，.mp4 等等）使用迅雷影音打开。在桌面右下角打开 所有设置 选项 在 Windows 设置中，选择 应用 选项 选择默认应用，设置视频播放器为 迅雷影音 上述的设置步骤实际上还不够，因为视频类型有很多种，还需要进一步指定每种类型的默认播放器，在默认应用下方有一个 按文件类型指定默认应用 选项 我这里特别关注 .mkv、.mp4 这 2 种格式的文件，默认应用设置为 迅雷影音 上述内容设置完成，就可以使用了，但是用不了多久，系统时不时就弹出提示框，通知默认程序重置，然后又被设置为系统内置的应用了 解决方案 不推荐方案 更改注册表、使用命令行卸载系统默认程序，这些方案是可行的，但是对于普通用户来说太麻烦了一点，根本不懂得如何操作，而且解决方法太粗暴了，当然喜欢折腾的人是可以选择的。以下给出几个命令行示例（需要在管理员模式下执行，打开 Windows PowerShell 的时候选择有管理员的那个）：卸载 “电影和电视” 应用（星号表示通配符，下同）1get-appxpackage *zunevideo* | remove-appxpackage卸载 “Groove 音乐” 应用 1get-appxpackage *zunemusic* | remove-appxpackage 卸载 “照片” 应用 1get-appxpackage *photos* | remove-appxpackage 如果还想恢复已经卸载的系统自带应用，可以使用以下命令（重装所有系统内置的应用）1Get-AppxPacKage -allusers | foreach &#123;Add-AppxPacKage -register "$($_.InstallLocation)appxmanifest.xml" -DisableDevelopmentMode&#125;推荐直接打补丁（更新系统）这个方法很简单，容易操作，直接在系统更新里面更新即可，确保要能更新到 KB3135173 这个补丁才行（或者更高版本的补丁）。我这里是已经更新完成的，等待重启，补丁标识是 KB4469342。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>Win10</tag>
        <tag>默认程序设置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark on Yarn 查看日志]]></title>
    <url>%2F2018120702.html</url>
    <content type="text"><![CDATA[一直一来都是直接在 Yarn 的 UI 界面上面查看 Spark 任务的日志的，感觉看少量的内容还勉强可以，但是如果内容很多，浏览器就没法看了，更没法分析。本文讲述如何使用 Yarn 自带的命令在终端查看 Spark 任务的日志，也可以拷贝出日志文件，便于分析。1、查看某个 Spark 任务的日志，使用 logs 入口：1yarn logs -applicationId application_1542870632001_26426 如果日志非常多，直接看会导致刷屏，看不到有用的信息，所以可以重定向到文件中，再查看文件：1yarn logs -applicationId application_1542870632001_26426 &gt; ./application.log2、查看某个 Spark 任务的状态，使用 application 入口：1yarn application -status application_1542870632001_26426 同时也可以看到队列、任务类型、日志链接等详细信息 3、kill 掉某个 Spark 任务，有时候是直接在 Driver 端 kill 掉进程，然后 Yarn 的 Spark 任务也会随之失败，但是这种做法是不妥的。其实 kill 掉 Spark 任务有自己的命令：1yarn application -kill application_1542870632001_264264、需要注意的是，步骤 1 中去查看日志，要确保当前 HADOOP_USER_NAME 用户是提交 Spark 任务的用户，否则是看不到日志的，因为日志是放在 HDFS 对应的目录中的，其中路径中会有用户名。此外，步骤 1 中的日志要等 Spark 任务运行完了才能看到，否则日志文件不存在（还没收集到 HDFS 中）。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Yarn</tag>
        <tag>日志查看</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[煮鸡蛋做法总结]]></title>
    <url>%2F2018120301.html</url>
    <content type="text"><![CDATA[本文记录水煮鸡蛋的做法总结。介绍 水煮鸡蛋是最常见的吃法之一，做法非常简单，直接将鸡蛋放入开水中煮熟即可。煮熟的鸡蛋营养丰富，水煮鸡蛋的营养可以 100% 被保留，是所有的鸡蛋做法中营养被保留的最好的一种。建议每天食用 1-2 个，因为过量的食用可能会导致营养不良，同时鸡蛋的营养并没有被身体吸收，相当于浪费了。在生活当中，大家几乎每天早上都会吃煮鸡蛋，或者茶叶蛋，但是有一些人卖的煮鸡蛋不算成功的煮鸡蛋，因为剥皮的时候发现不好剥，蛋壳与蛋白紧紧粘在一起，吃起来可麻烦了，这是因为煮鸡蛋的做法错误，遗漏了重要的步骤。做法步骤 1、简单地清洗一下鸡蛋，因为鸡蛋的表面可能会有一些茅草、粪便之类的污垢，这是因为鸡蛋必须是原生的，存储、运输、销售过程都不能清洗，如果非要清洗，水会破坏表面的保护膜，放不了两天鸡蛋就坏了；2、放在冷水中浸泡一会儿，1-2 分钟，这样做的目的是防止沸水煮的时候蛋壳破裂；3、放入锅中，水的高度稍微没过鸡蛋，使用中火煮开水，不要使用大火，大火煮的速度太快，鸡蛋容易裂开，另外中火使水沸腾的时间会长一些，预热了鸡蛋，味道更香；4、水沸腾后，改为小火，煮 7-8 分钟（如果继续使用中火，5 分钟左右即可）；5、如果需要溏心蛋（蛋清凝固，蛋黄成稠液状，软嫩滑润），煮 5 分钟即可；6、煮熟后不要立即捞出，等 1-2 分钟，然后才捞出，切记此时需要放入冷水中，浸泡 1-3 分钟，这一步骤的目的是保证鸡蛋容易剥开，避免蛋白和蛋壳粘在一起。 注意事项1、煮鸡蛋前最好放入冷水中浸泡 1-2 分钟，防止煮的过程开裂；2、注意控制火力和时间，鸡蛋不能煮太久，超过 10 分钟会有化学反应，导致营养流失；3、煮熟后不要立即捞出，捞出后也要放在冷水中浸泡，防止蛋白和蛋壳粘在一起；4、每天不要吃太多，1-2 个就够了；5、如果想要保持蛋黄在中间，煮鸡蛋的过程中要适当搅拌让鸡蛋旋转。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>煮鸡蛋</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一条正则表达式引发的惨案]]></title>
    <url>%2F2018120201.html</url>
    <content type="text"><![CDATA[本文讲述由于正则表达式引发的性能惨案，背景就是使用 Java 编程语言进行正则表达式匹配，由于正则表达式很复杂，再加上 Java 使用的是 NFA（非确定型有穷自动机）匹配引擎，导致匹配一条文本内容使用了十几个小时还没完成，一直卡住，同时线上环境的主机 CPU 使用率也居高不下（我猜的，因为我没有权限看）。 整理中。 参考：http://www.cnblogs.com/study-everyday/p/7426862.htmlhttps://www.jianshu.com/p/5c2e893b8d5d]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
        <tag>Java NFA</tag>
        <tag>非确定型有穷自动机</tag>
        <tag>正则无限回溯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jackson 包版本低导致 NoSuchMethodError]]></title>
    <url>%2F2018120101.html</url>
    <content type="text"><![CDATA[本文讲述 Java 项目由 Maven 包冲突或者版本不合适导致的运行时错误：1java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.JavaType.isReferenceType () Z起因 今天在升级 Web 项目的相关接口，更新了所依赖的 SDk 版本，删除了一些旧代码，测试时发现某个功能不可用，直接抛出异常，异常是在运行时抛出的，编译、打包、部署都没有任何问题。我看到第一眼，就知道肯定是 Maven 依赖问题，要么是版本冲突（存在不同版本的 2 个相同依赖），要么是依赖版本不对（太高或者太低），但为了保险起见，我还是先检查了一下 Git 的提交记录，看看有没有对 pom.xml 配置文件做相关改动。检查后发现，除了一些业务逻辑的变动，以及无关 jackson 依赖的版本升级，没有其它对 pom.xml 文件的改动，由此可以断定，某个依赖的升级导致了此问题，问题原因找到了，接下来就是解决问题。解决办法 查看项目的 Maven 依赖树 由于依赖太多，使用可视化的插件查看太繁杂，所以选择直接使用 maven 的 dependency 构件来生成文本，然后再搜索查看：1mvn dependency:tree &gt; tree.txt在 tree.txt 文件中搜索 jackson，可以找到 jackson-databind 相关的依赖包，还有 jackson-annotations、jackson-core 这 2 个依赖包。jackson-databind 的版本为 2.9.3确定了使用的版本，接下来可以在 IDEA 里面搜索一下这个类，然后再找调用的方法，直接去查看源码，看看到底有没有这个方法。搜索 JavaType Java 类，注意包的路径，可能会有很多重名的类出现，我是用 Ctrl + Shift + T 的快捷键搜索，各位根据自己的快捷键设置进行搜索。然后进入类的源代码，搜索方法 isReferenceType，报错信息后面的大写的 Z，是 JNI 字段描述符，表示这个方法的返回值类型，Z 表示 Boolean 类型，我们搜索看看有没有这个方法。我们发现连同名的方法都没有，更不用看返回值类型了，但是注意还是要去父类还有接口里面去搜索一下，保证都没有才是最终的没有。经过查找，没发现这个方法（主要原因是父类 ResolvedType 的版本太低，父类所在的 jackson-core 的版本只有 2.3.3，所以找不到这个方法），到这里就要准备升级 jackson-core 或者降级 jackson-databind 依赖了。去除多余依赖 如果是检查到存在依赖冲突的情况，一般是高低版本之间的冲突（最多的情况是多级传递依赖引起的），然后 Maven 编译打包时会全部打进业务的包。1、导致运行时程序不知道选择哪一个，于是抛出 NoSuchMethodError 异常，此时根据需要，移除多余的依赖包即可；2、步骤 1 操作后，还是一种可能是虽然只存在一个版本，但是由于版本太新或者太旧，无法兼容所有的调用，导致多处需要调用这个依赖包的地方总会有某个地方出现 NoSuchMethodError 异常。此时就比较麻烦，如果能找到一个合适版本的依赖包，兼容所有的调用，当然是好的；或者升级调用处对应的接口版本；如果还是无法解决，就只能通过 Shade 构件解决问题了，此处就不赘述了。经过检查，我这里遇到的就是步骤 2 的情况，虽然只剩下一个依赖包，但是版本太低或者太高，导致调用时找不到 isReferenceType 方法，类其实是存在的，所以要采用升级或者降级的方式。升级降级依赖 如果是检查到只有一个依赖，并没有冲突的情况，就容易了，直接找到最稳定的版本或者适合使用的旧版本，提取依赖的坐标，配置到 pom.xml 文件中即可。经过检查，我这里遇到的就是这种情况，去 Maven 私服中搜索 jackson，找到合适的版本（自己根据需要选择，我这里选择 jackson-databind 的 2.9.7 版本，然后 jackson-core 也指定 2.9.7 版本，就可以了，然后又查资料也发现这个方法是 2.6.0 版本之后才开始加上的），配置到 pom.xml 文件中即可。私服搜索 配置到 pom.xml我这里使用了常量，在 pom.xml 文件的 properties 属性下面配置即可。踩坑总结1、其实 jackson 这个依赖我并没有使用，而是引用的一个第三方依赖内部使用的，但是这个第三方依赖并没有一同打进来，也没有说明需要什么版本的，所以导致我自己在实验，最终找到到底哪一个版本合适。2、为了统一，jackson-core 的版本要与 jackson-databind 的版本一致，jackson-databind 里面是已经自带了 jackson-annotations 的，由于 jackson-databind 里面的类继承了 jackson-core 里面的，所以才都要升级并且保持版本一致。3、搜索类方法时，注意留意父类和接口里面，不一定非要在当前类里面出现。更改版本后同样也去类里面搜索一下，看看有没有需要调用的方法出现，确定版本用对了再继续做测试。4、这种错误在编译、打包、部署阶段是检查不出来的，因为代码并没有实际调用到，属于运行时错误，只有跑起来程序，执行到需要使用该方法的时候，才会报错。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>NoSuchMethodError</tag>
        <tag>jackson</tag>
        <tag>Maven</tag>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub 个人站点绑定独立的域名]]></title>
    <url>%2F2018112701.html</url>
    <content type="text"><![CDATA[随着越来越多的人使用 GitHub，都在里面创建了自己的仓库，或者 clone 了别人的优秀项目，也有很多人想利用 GitHub 自带的 GitHub Pages 来搭建个人博客，此时就可以使用独立的域名 https://www.username.github.io 访问自己的博客，全部的资源都来自于 GitHub，并且是免费的，不需要其它任何配置或者购买，这里面包含域名、流量、带宽、存储空间、Htpps 认证等服务。但是，有的人可能购买了自己的独立域名，例如： https://www.abc.com ，并且想把域名直接绑定到 GitHub 免费的域名上面，这样以后访问博客的时候更容易辨识，本文就描述 GitHub Pages 绑定独立域名的操作过程，前提是 GitHub Pages 已经创建完成。我在 Godaddy 上面购买了域名：playpi.org，选择 Godaddy 主要是不想备案，国内的域名服务商都要求备案，我以前在阿里云上面买过一个，后来没按照要求备案就不能用了，我也放弃了。购买域名 当然，大家可以选择自己喜欢的域名服务商，例如腾讯云、阿里云等，但是这些域名服务商需要给域名备案，有点麻烦【当然不是所有的域名都需要备案】。所以我选择的域名服务商是 Godaddy，主页地址：https://sg.godaddy.com/zh ，在主页中点击左上角的 域名 ，开始搜索域名。我这里输入域名 playpi.org，可以看到被占用了，已经已经被我购买了，可以看到右侧显示出了可以购买的域名列表，并且带有报价。在左侧，可以添加筛选条件，过滤掉自己不想要的域名。如果找到了满意的域名，加入购物车购买就行了。选择域名服务器 有了域名，还没有用，因为还没有把域名用起来，所以接下来需要找域名服务器，把你的域名解析到 GitHub Pages 去。这样，才能保证访问你的域名，自动跳转到 GitHub Pages 去。我一开始选择的 Godaddy 自己的域名服务器，只需要在 我的产品 -&gt; 域名 -&gt;DNS，设置一些解析记录即可。Godaddy 的配置解析规则可以参考下图【可以先忽略解析规则，后面会讲到的】：后来由于 GitHub Pages 屏蔽百度爬虫的问题，我必须设置一条专门的解析规则去解析百度爬虫的请求，引入到我自己的 Web 服务器上面，但是 Godaddy 不支持线路的自定义，比较笼统，所以我就放弃了。转而选择了腾讯的 DNSPod，还是比较好用的，虽然前不久刚出过问题，大量的网络瘫痪，但是解析速度还是挺快的。先在 DNSPod 中添加域名，也就是在 Godaddy 中购买的域名【如果是直接在腾讯云中购买的，就不用配置了，默认就有】。添加完成后，可以看到提示我 NS 地址还未修改，也就是目前仍旧是 Godaddy 负责解析这个域名，所以要把域名服务器给切换过来。如果不知道是什么意思，可以点击提示链接查看帮助手册【其实就是去购买域名的服务商那里绑定 DNSPod 的域名服务器】提示我们修改 NS 地址 帮助手册 我在这就直接查看当前的域名的 NS 地址，选择域名，进入配置页面查看。去 Godaddy 中配置域名服务器，替换掉原本默认的。在 我的产品 -&gt; 域名 -&gt;DNS：我把它修改为我在 DNSPod 中查到的属于我的域名的域名服务器，一般都会有 2 个，保证可靠性。配置完成新的域名服务器【以前的解析记录都消失了】：配置完成，域名解析的工作就完全交给 DNSPod 了，我们可以退出 Godaddy 了【只是在这里买了一个域名】，接下来全程都要在 DNSPod 中配置其它信息。配置域名的解析规则 上一步骤我已经配置完成了域名的基本信息，接下来需要配置的就非常关键了，是域名的解析规则，它会指引着访问域名的请求怎么跳转。这里先提前说一下配置规则：主机记录 为 @表示直接访问域名，例如访问 playpi.org主机记录 为其它字符表示访问二级域名，例如访问 www.playpi.org 、blog.playpi.org记录类型 为 A 表示跳转到 ip 地址，后面的 记录值 就需要填 ip，例如 66.32.122.18记录类型 为 CNAME 表示跳转到域名，后面的 记录值 就需要填域名，例如 blog.playpi.org线路类型 是 DNSPod 自定义的逻辑分类，给访问的请求分类，例如百度爬虫、搜狗爬虫，这个选项对于我来说很有用，可以解决 GitHub Pages 屏蔽爬虫的问题 我把 Godaddy 中的解析记录直接抄过来就行，不同的是由于使用的是 DNSPod 免费版本，A 记录会少配置 2 个，基本不会有啥影响 【其实不配置 A 记录最好，直接配置 CNAME 就行了，会根据域名自动寻找 ip，以前我不懂】。另外还有一个就是需要针对百度爬虫专门配置一条 www 的 A 记录，针对百度的线路指向自己服务器的 ip【截图只是演示，其中 CNAME 记录应该配置域名，A 记录才是配置 ip】。如果使用的是第三方托管服务，直接添加 CNAME 记录，配置域名就行【例如 yoursite.gitcafe.io】。上面的配置里面的 A 记录明显是多余的，而且还要通过 ping 去寻找那几个 ip【我这里是 ping iplaypi.github.io 得到，大家换为自己的 GitHub 用户名即可，每个用户之间的 ip 应该有差别，不会完全一样】。所以建议大家不使用 A 记录的配置方式，直接使用 CNAME 配置。配置完成后使用 域名设置 里面的 自助诊断 功能，可以看到域名存在异常，主要是因为更改配置后的时间太少了，要耐心等待全球递归 DNS 服务器刷新【最多 72 小时】，不过一般 10 分钟就可以访问主页了。但是后来我发现，那个 GitHub 的域名【iplaypi.github.io】被墙了而 ip 没被墙，表现为每天总会有一段时间访问不了【DNSPod 也会给我发告警邮件，说宕机了，当然是他们的域名测试服务器连不上这个域名】，而且我用自己的浏览器也访问不了。而 blog 那个二级域名却可以正常访问，这就说明 GitHub 的那个域名不好使，而我自己给 blog 专门部署 Web 服务是正常的。因此，在主机记录为 @ 的解析规则里面还是配置 A 记录吧，把几个 ip 都配置上去【免费版本的 DNSPod 只能添加 2 条，可怜】。这样做还会引起 GitHub 的警告，因为这个 ip 地址可能会变化，所以 GitHub 建议配置域名。如果想知道域名对应的 ip 地址，除了使用 ping 之外，还有更快捷的方法：dig 命令。在 GitHub 中设置 CNAME关于域名的配置都完成了，最后还有一个重要的步骤，需要在 GitHub 的项目中添加一个文件，文件名称是 CNAME，文件内容就是域名【我这里使用的是二级域名，也可以，就是在直接访问域名的时候多了一次转换】。那这个文件的作用是什么呢，为什么要这么配置呢？其实，CNAME 是一个别名记录，它允许你将多个名字映射到同一台计算机，还决定着主页的链接最终展示的样子，直接是域名【https://playpi.org 】还是带二级域名【https://www.playpi.org 】。这里有 GitHub 的官方说明：https://help.github.com/en/articles/using-a-custom-domain-with-github-pages 。此外，在 GitHub 中还可以开启 https 认证，这样你的每一个文档链接都会有把小绿锁了，GitHub 使用的是 Lets Encrypt 的证书，有效期 3 个月，不过别担心过期问题，GitHub 会自动更新的。开启了 https 认证后，哪怕使用 http 的链接访问，也会自动跳转的。那如果有人想把我的域名访问指向自己的 GitHub，是不是他在自己的仓库里面新建一个 CNAME 文件，并且填上我的域名就行了呢？其实不行，GitHub 是禁止这样做的。即使有人真的在自己的仓库里面新建了 CNAME 文件并且填写了我的域名，GitHub 是不认可的并且会给出警告。当然，如果我自己在 GitHub 中没有使用这个域名，别人当然可以使用。现在突然想到一个问题，我把自己的域名和域名服务器都暴漏了，会不会有人在 DNSPod 中把我的域名解析到其它地方去了【看起来所有的 DNSPod 的域名服务器都是一样的 2 台机器】，然后我就访问不了自己网站了，或者说流量变小了。我觉得 DNSPod 应该会禁止这种行为。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>个人站点</tag>
        <tag>绑定域名</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10 输入法简繁体快捷键与 IDEA 冲突]]></title>
    <url>%2F2018112301.html</url>
    <content type="text"><![CDATA[用了 2 个月的 Windows 10 系统（教育版），又安装了 IDEA 代码集成工具，开发的时候，发现每一次只要我使用快捷键 Ctrl + Shift + F 格式化代码后（主要作用就是代码对齐），不起作用，而且写中文注释时发现输入法的中文就被切换为了繁体，再来一次就被切换为了简体。到这里，我知道 IDEA 的快捷键与输入法的快捷键冲突了。解决方案 1、如前文描述，在写代码的过程中发现这个问题，并且看出是快捷键冲突的问题，接下来就要解决它。作为一名工程师，IDEA 的快捷键是因为使用习惯设置的，是写代码效率的保证，不可能更改的，任何与它有冲突的快捷键都要让步，那肯定是要更改输入法的快捷键的；2、信心满满，打开 搜狗输入法 的 属性设置 界面，找到 高级 选项，选择，可以看到里面有 快捷键 的相关配置；配置所有的快捷键 3、看了半天，也就这么几个快捷键配置，里面根本没有 简体 / 繁体 切换这一个配置选择，去搜索了一下其它资料，发现 简体 / 繁体 切换这一个快捷键是 Windows 10 系统内置的，默认就是 Ctrl + Shift + F，默认是给微软输入法使用的，某些版本的 Windows 10 系统有 bug，无法更改，哪怕卸载微软输入法，安装其它输入法也无效；4、我看了我的 Windows 10 系统版本，已经是新版本了，不会有那个 bug 出现了，所以要从系统设置入手了，应该有地方设置才对，查看了语言里面的设置信息，没找到，只能又返回到搜狗输入法里面，这时突然看到里面有一个 系统功能快捷键 选项；5、就是这里了，点进去，把 简繁切换 关闭（如果需要保留的话，更改快捷键即可），解决问题。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>Win10</tag>
        <tag>输入法</tag>
        <tag>快捷键冲突</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 博客静态资源压缩优化]]></title>
    <url>%2F2018112101.html</url>
    <content type="text"><![CDATA[使用 hexo-cli 生成的静态网页 html 文件，使用文本编辑器打开，可以看到内容中有大量的回车换行等空白符。尽管是空白符，但是也占据着空间大小，而且那么多，导致 html 文件偏大，网页加载时不仅浪费流量，而且还影响速度。同时，最重要的是对于手机端来说，静态页面 html 文件太大了的确不友好。所以要做优化，用术语说是压缩，其实目的就是在生成 html 文件时，尽量去除内容中多余的空白符，减小 html 文件的大小。此外，顺便也把 css 文件、js 文件一起压缩了。当前现象 为了简单起见，只是列举 html 文件来看现象，目前查看生成的 8 个 html 静态页面（为了具有对比性，不包含当前页面），大小为 314 K。打开其中一个 html 文件查看内容，可以看到很多回车换行符。接下来就是要想办法消除这些空白符。压缩方式选择 通过查看 hexo 官网（附上插件库：hexo 插件库 ），搜索资料了解别人的例子，发现有两种方式： 一种是先全局（-g 参数）安装 gulp 模块，根据压缩需求再安装需要的模块，例如 gulp-htmlclean、gulp-htmlmin、gulp-imagemin、gulp-minify-css、gulp-uglify，每个模块都有自己的功能，另外需要单独配置一个 js 脚本（放在站点根目录下），指明使用的模块，文件所在目录或者通配符文件名，然后每次使用 hexo generate 之后再使用 gulp 就可以压缩文件了。这种方式灵活度高，可以自定义，而且 gulp 的功能与 hexo 解耦，如果有其它静态文件，也可以使用 gulp 进行压缩。但是缺点也显而易见，门槛太高了，根据我的折腾经验，如果出了问题肯定要捣鼓半天，对于我这种零基础的人来说不够友好，我不选择；另一种是类似于 hexo 的一个插件，像其它插件或者主题一样，直接安装一个模块，在配置文件中配置你想要的压缩内容，在 hexo generate 的时候就可以实现压缩，无需关心具体流程，也不用配置什么脚本，非常容易，我选择这个，目前我看到有两个类似的插件：hexo-neat、hexo-filter-cleanup，用法都差不多，我选择前者，其实这些插件也是依赖于其它插件，把多种插件的功能整合在一起而已。安装配置 hexo-neat 插件其实是使用 HTMLMinifier、clean-css、UglifyJS 插件实现。 安装（由于网络不稳定因素，可能不是一次就成功，可以多试几次）1npm install hexo-neat --save站点配置 编辑站点的配置文件 &#95;config.yml，开启对应的属性 12345678910111213141516171819202122# 文件压缩，设置一些需要跳过的文件 # hexo-neatneat_enable: true# 压缩 htmlneat_html: enable: true exclude:# 压缩 cssneat_css: enable: true exclude: - '**/*.min.css'# 压缩 jsneat_js: enable: true mangle: true output: compress: exclude: - '**/*.min.js' - '**/jquery.fancybox.pack.js' - '**/index.js' 查看效果 在执行 hexo generate 的命令行中就可以看到压缩率输出。8 个 html 文件被压缩后，大小只有 206 K，和之前的 314 K 比少了 108 K，虽然只是简单的数字，也可以看到压缩效果不错。继续打开先前打开的那个 html 文件，可以看到整个 html 文档被合并成为了一行文本内容，不影响浏览器对 html 文件的解析展示，回车换行的空白符内容肯定没有了。但是这样对于 html 文件的可读性变差了，最好还是使用一些回车换行符的，还好这些 html 文件我不会去看，能接受目前的效果。踩坑记录1、由于牵涉到压缩文件，所以 hexo 生成静态文件的速度会比以前慢一点，但是可以接受。2、不要跳过 .md 文件，也不要跳过 .swig 文件，因为是在 hexo generate 阶段进行压缩的，所以这些文件必须交给 hexo-neat 插件处理，才能保证生成的 html 文件纯净。3、参考博客：1、2、3、4。]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>建站</tag>
        <tag>Hexo</tag>
        <tag>代码压缩</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google 账号开启两步验证与应用专用密码]]></title>
    <url>%2F2018111901.html</url>
    <content type="text"><![CDATA[使用 Google 账号的都知道，带来了很多方便，不仅有强大的免费搜索服务，还有 Google 文档、云主机、云存储等各种服务，但是唯一的缺点是需要翻墙，让一些人望而却步，把很多人挡在了便利门外。本文是针对已经实现翻墙愿望，并在日常工作中会使用到 Google 账号的人，说不定可以给你带来一些冷知识，解决一些小问题。Google 账号的便利性 目前在日常工作与生活中，查找资料时，基本使用的都是 Google 搜索，并且使用非常好用的 Chrome 浏览器。其中我用的最多就是标签收藏，平时偶尔搜到什么有用的知识点或者需要反复查看的网页，来不及看完整理，就先把网页分类收藏了，以便日后查漏补缺。此时，利用 Chrome 浏览器的标签收藏功能，可以很方便地把一切网页收藏起来，并且可以很好地分类存放，清晰明了。可能有人说也有很多其它的工具可以做到这一点，不久收藏吗？但是我觉得还是利用 Chrome 浏览器自带的这个功能比较好，再配合 Google 账号，就可以达到同步更新的效果了，公司的电脑、家里的电脑，只要都登录了 Google 账号，所有收藏的标签都可以实时同步。而且，所有的浏览记录、搜索历史、记住的账号密码等等，都可以同步，跨机器使用也很方便。再配合 Chrome 浏览器的插件，对收藏的网页搜索起来非常方便。Google 账号开启两步验证 为了安全起见，最好给 Google 账号开启两步验证，可以选择绑定手机号、启用身份验证器、安全密钥等方式，为了方便，我选择了绑定手机号。开启两步验证后，在陌生的设备上登录 Google 账号（包括 Google 自家的各种应用，例如邮件、YouTube 等）需要验证码的二次验证，当然，如果把设备设置为可信任的设备，则不需要每次都重复输入验证码。开启的方式非常简单，登录 Google 账号，在” 登录与安全 “中有” 两步验证 “的开启选项，选择自己需要的方式，继续即可。启两步验证 1启两步验证 2如果使用” 身份验证器 “的方式，还需要在手机上安装一个” 身份验证器 “应用，校准时间后，每隔 30 秒更新验证码，登录账号时需要使用当前的验证码，并且在有效期内完成登录的操作，否则验证码过期，需要使用新的验证码，类似于手机收到的验证码只有 1 分钟一样。同时，如果使用 Google 邮箱账号注册了其它平台的账号，例如注册了 Twitter，注册了 Facebook，为了安全起见也可以使用” 身份验证器 “的方式，一种验证方式管理着多种账号的安全。开启两步验证后带来的问题 我遇到的问题之一就是自己手机的邮件客户端无法登录 Google 邮箱了，我使用的时第三方邮件客户端，总是提示我密码错误，其实密码没有错误，是因为 Google 账号开启两步验证后，邮箱的登录也需要对应方式的验证，但是第三方邮件应用并没有做这个验证，所以无法登录。本来是想着单独把 Google 邮箱的两步验证关闭，但是找了半天设置选项也没有找到，看来 Google 账号已经是一个大统一的账号，不允许单独设置涉及安全性的信息，可以理解。同理，使用其它应用客户端也会遇到相同的问题，当然，Google 官方解释说明也解释了有部分设备不需要关注这个问题，其它大部分设备或者应用还是要受到影响的。见：使用应用专用密码登录 此时，需要使用” 应用专用密码 “或者在手机上开发一个” 具有账号访问权限的应用 “用来代理整个 Google 的账号访问。问题的解决方法 应用专用密码方式的使用 1、在 Google 账号的登录和安全中，可以找到” 应用专用密码 “这个选项：2、点击进入后，可以看到选择应用与选择设备，由于我使用的是一种不知名的 Android 手机，所以官方选项中没有可以选择的，只好自定义一种，随便起一个名字标识即可。3、选择完成后，会生成一串 16 位的密码，这个密码就可以在其它设备上登录的时候使用，不需要使用原来的密码，也不需要使用 Google 验证码。4、在使用过程中还可以看到设备的情况。 具有账号访问权限的应用的使用 这种方式就是手机本身有一个后台应用，代理了 Google 账号的一切请求，把信息转发到本地应用（比如 Chrome 浏览器就是这样一个应用，只不过是官方开发的，只要登录了 Google 账号，邮件、YouTube、搜索、Play、相册、日历等等这些应用同步一起使用，不需要额外再登录，这也是我使用 Chrome 浏览器的原因。），所以后台应用如果知道了 Google 账号的用户名、密码，就可以代理所有 Google 应用的请求，无需关心 应用专用密码了。我发现锤子手机的 Smartisan OS 系统（v6.0.3，Android 版本 7.1.1）对邮件就做了这个后台应用 Smartisan Mail，所以在使用内置的邮件客户端时，即使开启了两步验证，也无需关心验证码的问题（第一次登录还是需要验证的）。下面截图则是一步一步设置：1、在邮件客户端设置中添加 Google 邮箱2、输入 Google 账号密码（也是邮箱密码）3、输入验证码（由于开启了两步验证，一定需要），此时切记勾选” 在此计算机上不再询问 “，才能保证邮件客户端正常收发 Goole 邮件，否则不行。4、允许，可以看到 Smartisan Mail 想要访问 Google 账号5、点开 Smartisan Mail，可以看到开发者信息，里面其实设置了代理转发6、此外，在登录成功后，在 Google 账号的登录和安全中，可以看到具有账号访问权限的应用：]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>Google 账号</tag>
        <tag>两步验证</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Vultr 搭建 Shadowsocks（VPS 搭建 SS）]]></title>
    <url>%2F2018111601.html</url>
    <content type="text"><![CDATA[本文讲述通过 Vultr 云主机搭建 Shadowsocks 的过程，非常不详细。当然，关于云主机很多 VPS 都可以选择，根据价格、配置、地区等可以自由选择。主机购买 使用 Vultr 的云主机，选择洛杉矶地区的或者日本的，我的推广链接：我的推广链接 ，官网：Vultr。 价格有 &#36;2.5 / 月（只有 IP6 地址）、&#36;3.5 / 月、&#36;5 / 月等等，更贵的也有，一般选择这三个中的一个就够用了，但是要注意便宜的经常售罄，而且最便宜的只支持 IP6，慎用。Shadowsocks 服务安装 云主机选择 CentOS 7 x64 版本，全程操作使用 Linux 命令（注意，如果选择其它系统命令会不一致，请自己查询，例如：Debian/Ubuntu 系统的安装命令更简洁，先 apt-get install python-pip，再 pip install shadowsocks 即可）。注意如果安装了防火墙（更安全），需要的端口一定要开启，否则启动 Shandowsocks 会失败。安装组件：123yum install m2crypto python-setuptoolseasy_install pippip install shadowsocks过程如图：配置服务器参数：1vi /etc/shadowsocks.json如下列出主要参数解释说明 参数名称 解释说明 server 服务器地址，填 ip 或域名 local_address 本地地址 local_port 本地端口，一般 1080，可任意 server_port 服务器对外开的端口 password 密码，每个端口可以设置不同的密码 port_passwordserver_port + password ，服务器端口加密码的组合timeout 超时重连 method 加密方法，默认：“aes-256-cfb”fast_open开启或关闭 TCP_FASTOPEN，填 true /false，需要服务端支持 配置多端口信息（多个帐号，多人也可用）：12345678910111213&#123; "server": "你的 IP 地址"（例如：192.168.0.1）, "local_address": "127.0.0.1"（默认值）, "local_port":1080（默认值）, "port_password"（开启的端口和密码，自己按需配置，确保端口打开并不被其它程序占用）: &#123; "1227": "pengfeivpn1227", "1226": "pengfeivpn1226", "1225": "pengfeivpn" &#125;, "timeout":300（超时时间，默认值）, "method":"aes-256-cfb"（加密方法，默认值）, "fast_open": false&#125;配置多端口信息（纯净版本，更改 ip、端口等信息直接复制使用）：12345678910111213&#123; "server": "x.x.x.x", "local_address": "127.0.0.1", "local_port":1080, "port_password": &#123; "1227": "vpn1227", "1226": "vpn1226", "1225": "vpn" &#125;, "timeout":300, "method":"aes-256-cfb", "fast_open": false&#125;配置一个端口信息（只有一个帐号，多人也可用）：12345678910&#123; "server":"你的 IP 地址"（例如：192.168.0.1）, "server_port":1225（唯一的端口）, "local_address":"127.0.0.1", "local_port":1080, "password":"pengfeivpn"（唯一的密码）, "timeout":300, "method":"aes-256-cfb", "fast_open":false&#125;配置一个端口信息（纯净版本，更改 ip、端口等信息直接复制使用）：12345678910&#123; "server":"x.x.x.x", "server_port":1225, "local_address":"127.0.0.1", "local_port":1080, "password":"vpn", "timeout":300, "method":"aes-256-cfb", "fast_open":false&#125;Shadowsocks 性能优化：另外还有很多参数可以优化性能，例如设置连接数、字节大小等，比较复杂，在此略过。防火墙安装：123456789101112# 安装防火墙 yum install firewalld# 启动防火墙 systemctl start firewalld# 查看目前已经开启的端口号 firewall-cmd --list-ports# 端口号是你自己设置的端口 firewall-cmd --permanent --zone=public --add-port=1225/tcpfirewall-cmd --permanent --zone=public --add-port=1226/tcpfirewall-cmd --permanent --zone=public --add-port=1227/tcp# 重载更新的端口信息 firewall-cmd --reload过程如图：启动 Shadowsocks：1234# 后台运行 ssserver -c /etc/shadowsocks.json -d start# 调试时使用下面命令，实时查看日志 ssserver -c /etc/shadowsocks.json过程如图：客户端使用 Windows 平台使用 下载 Windows 平台的客户端，下载地址：shadowsocks-windows GitHub，shadowsocks 官网 ，直接解压放入文件夹即可使用，不需要安装。 但是注意配置内容（端口、密码、加密协议等等），另外注意有些 Windows 系统缺失 Shadowsocks 必要的组件（.NET Framework），需要安装，官网也有说明。配置示例：实际上下载程序后，无需安装，直接解压即可，解压后只有一个 exe 文件，双击即可运行（最好放入指定文件夹中，便于程序管理和升级）。第一次启动，需要设置参数，如上图所示，至少配置一台机器，另外还可以设置开机启动，以后不用重新打开。此外，如果有更新版本的程序，会放在 ss_win_temp 文件夹下，直接解压后复制替换掉当前的 exe 文件即可；如果文件夹中有 gui-config.json、statistics-config.json 这 2 个文本文件，它们是程序的配置以及前面设置的翻墙配置，不能删掉；如果使用系统代理的 PAC 模式（推荐使用），会生成 pac.txt 文本文件，存放从 GFWList 获取的被墙的网址，必要时才会通过翻墙代理访问，其它正常的网址则直接访问，这样可以节约流量。如果有切换代理的需求，搭配浏览器的插件来完成，例如 Proxy SwitchyOmega 就可以。关于启动系统代理并使用 PAC 模式（根据条件过滤，不满足的直连），如果是入门级别使用，直接设置完就可以用了，不用再管其它设置，切记要定时更新 GFWList 列表，因为如果某些网站最近刚刚被屏蔽，不在以前的 HFWList 列表里面，就会导致无法连接，只有及时更新才能正常连接。但是还有一种极端情况，就是某些网站 GFWList 迟迟没有收录，怎么更新都不会起作用，别着急，此时可以使用用户自定义规则，模仿 GFWList 填写自己的过滤规则，即可实现灵活的切换，使用用户自定义规则后会在安装文件夹中生成 user-rule.txt 文本文件。其实，PAC 模式的原理就是根据公共的过滤规则（收集被屏蔽的网站列表），自动生成了一个脚本文件，把脚本文件绑定到浏览器的代理设置中，使浏览器访问网站前都会运行这个脚本，根据脚本的结果决定是直接访问还是通过本地代理访问，脚本在 Shadowsocks 的 PAC 设置中可以看到，浏览器的设置信息可以在代理设置中看到（浏览器在 Shadowsocks 开启系统代理的时候会自动设置代理，无需人工干预）。由此可以得知，通过本机访问网络，决定是直接连接还是通过 Shadowsocks 代理连接的是 PAC 脚本，并不是 Shadowsocks 本身，所以如果使用系统的 Ping 命令访问 www.google.com 仍然是不能访问的，因为直接 Ping 没有经过 PAC 脚本，还是直接连接了，不可能访问成功。除了浏览器之外，如果其它程序也想访问被屏蔽的网站（例如 Git、Maven 仓库），只能通过程序自己的代理设置进行配置，完成访问的目的。（如果放弃 PAC 模式，直接使用全局模式，则不需要配置任何信息，本机所有的网络请求会全部经过翻墙代理，当然这样做会导致流量消耗过大，并且国内的正常网站访问速度也会很慢）获取到的 PAC 脚本地址为：http://127.0.0.1:1080/pac?t=20181118030355597&amp;secret=qZKsW49fDFezR4jJQtRDhUVPRqnFu6JC3Nc+vtXDb0g=以上是查看 Chrome 浏览器和 IE 浏览器的代理设置信息，对于 Microsoft Edge（Windows 10 自带）浏览器来说，界面有点不一样，在设置 -&gt; 高级 -&gt; 代理设置里面。此外，如果在浏览器中有更灵活的需求应用， 例如在设置多个代理的情况下，针对公司内网是一套，针对指定的几个网站是一套，针对被屏蔽的网站是一套，剩余的直接连接。在这种情况下仅仅使用代理脚本就不能完成需求了，显得场景很单一，当然也可以把脚本写的复杂一点，但是成本太高，而且不方便维护更新。这个时候就需要浏览器的插件出场了，例如在 Chrome 下我选择了 SwitchyOmega 这个插件，可以设置多种情景模式，根据实际情况自由切换，非常方便。我设置了三种情景模式：hdpProxy（公司内网）、shadowSocks（翻墙代理）、auto switch（根据条件自动切换），前面两种情景模式直接设置完成即可，最后的 auto switch 需要配置得复杂一点，根据正则表达式或者通配符指定某些网站的访问方式必须使用 hdpProxy 代理，另外其它的根据规则列表 （https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt ，和 Shadowsocks 的 GFWList 列表类似）必须通过翻墙代理，剩余的才是直接连接。当然，此时就不需要把 Shadowsocks 设置为系统代理了，保持 Shadowsocks 后台运行就可以了。Android 平台使用Android 平台的安装使用方法就非常简单了，分为 安装、配置、启动 这 3 个步骤，没有其它多余的操作。安装 下载 Android 平台的客户端，一般我们都称之为 影梭 ，在应用商城是找不到的，因为通不过审核，所以只能去官网下载，下载地址：shadowsocks 官网 。切记，千万不要去第三方网站下载，因为下载的安装包可能带有其它的应用，导致给你的手机安装了一堆软件。当然，如果你连官网都不信，可以自己下载源代码，自己打包 apk 文件，也是可以的，懂一点点 Android 开发就行了，源代码全部是开源的，放在了 GitHub 上面：https://github.com/Jigsaw-Code/outline-client/ 。 下载完 apk 文件，安装也就是和安装普通的应用一样，需要注意的是有些 Android 手机会禁止外部来源的 app（不是从应用商店下载安装的）安装，所以需要同意一下，也就是 信任此应用 ，才能顺利完成安装。配置 需要配置的内容和 Windows 平台的一样，把那些必要的参数填进去就行了，其它内容不需要关心。例如我这里配置了 ip、端口、密码、加密方式等。启动 启动只要点击右上角的灰色圆形按钮，里面有一个小飞机，大概等待几秒钟，就会变绿，表示已经连接上 VPN 了，此时手机就可以连接被屏蔽的网站了。唯一的缺点就是，不支持设置类似于 PAC 规则的站点切换（ 路由 默认设置的是绕过中国大陆地址），因为只要一连上 VPN，手机上所有的国外连接都是走 VPN，会导致连某些正常的国外的网站也会慢一点，还浪费 VPS 的流量。当然，如果是在 WIFI 的环境下，通过 Android 系统的网络代理设置也可以设置一些类似于 PAC 的规则，就不细说了。启动后，还可以看到流量发送接收统计信息。在手机的设置里面也可以看到 VPN 的开启 踩坑记录 1、在云主机安装服务端后，又安装了防火墙，但是没有开启 Shadowsocks 需要的端口，导致启动 Shadowsocks 总是失败，但是报错信息又是 Python 和 Linux 的，看不懂，搜索资料也搜不到，后来重装，并且想清楚每一步骤是干什么的，会造成什么影响，通过排除法找到了根本原因。2、在 Windows 平台使用的时候，安装了客户端，也安装了 .NET Framework 组件，配置信息确认无误，但是就是上不了外网，同样的操作使用 Android 客户端却可以，所以有理由怀疑是自己的主机问题。后来，重启系统，检查网络，关闭杀毒软件，还是不行，后来，依靠搜索，找到了是杀毒软件 Avast 的问题，扫描 SSL 连接被开启了，大坑，关闭即可。3、参考： 梯子搭建4、本来以为 Shadowsocks 的系统代理中的 PAC 模式会在接收到网络请求的基础上进行过滤，即 Shadowsocks 能控制所有的网络请求进行过滤判断，然后该翻墙的翻墙，该直连的直连，后来发现不是的，浏览器插件 SwitchyOmega 设置代理规则后，PAC 脚本就不会生效了，全部使用 Shadowsocks 代理的网站都直接翻墙，不会有任何判断了，导致优酷视频消耗了大量的流量，而且速度还很慢。另外，为了保证国内的网站不是经过翻墙代理，能直接连接，就不能使用全局模式。5、使用插件 SwitchyOmega 的过程中，一开始是自己整理一些规则，而没有使用https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt 列表规则，导致配置信息很多，而且自己看着头都大，不好维护与查看，后来就发现了列表规则，解放了劳动力。6、解决了 Chrome 浏览器的收藏跨平台自动更新同步的问题，以前在三台电脑之间添加取消收藏，总是不能更新同步，需要手动开启系统代理设置全局模式（Chrome 浏览器的收藏同步功能被屏蔽了，我又不知道 url 是什么），等一会更新同步之后再关闭（防止其它场景也翻墙了）。目前使用规则列表，收藏可以自动更新同步了，不需要手动来回切换了，也不用担忘记同步的情况了。]]></content>
      <tags>
        <tag>Shadowsocks</tag>
        <tag>Vultr</tag>
        <tag>Avast</tag>
        <tag>VPS</tag>
        <tag>影梭</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Aria2 Web 管理面板使用]]></title>
    <url>%2F2018110902.html</url>
    <content type="text"><![CDATA[浏览器默认下载器下载百度云的文件速度大多不理想，非客户端下载限速，大文件下载中途失败，让人体验非常不友好。 哈哈哈哈]]></content>
  </entry>
  <entry>
    <title><![CDATA[使用 Aria2 加速百度网盘下载]]></title>
    <url>%2F2018110901.html</url>
    <content type="text"><![CDATA[在日常工作和生活当中，应该有不少人自愿或者被迫使用百度网盘，一是因为其它厂商基本都关停了网盘服务；二是在获取互联网资料的时候，基本都是先获取到百度网盘链接，然后自己再去下载；三是有时候想备份一些文件，也只能想起来有百度网盘可以使用。这样的话，慢慢地总是会碰到需要百度网盘的时候，我们暂且不考虑这家企业的口碑怎么样，百度网盘这个产品本身还是不错的：有免费的大量空间，使用人群多，分享获取资料方便。但是，产品让人诟病的地方也有几个，而且由此造成的用户体验非常差，大家骂声一片。本文就详细讲述百度网盘这个产品让人诟病的地方以及可以使用技术方式绕过它，从而提升自己的体验。当然，如果你的钱到位的话，直接充值会员吧，可以消除一切不好的使用体验，同时也免去了阅读本文的时间。使用中遇到的问题 本文是针对不充会员的免费用户群体的，在 Windows 平台安装，在 Chrome 浏览器中使用。下载速度太慢，慢到反人类 让人诟病的问题之一是下载速度太慢了，对于免费用户基本维持在几 KB/s 到十几 KB/s 之间，也就是说如果你想下载一部 1 G 大小的电影，按照 1000 M 计算，下载速度按照 10 KB/s 算（取这样的数值方便后续计算），下载完需要 1000 个 100 秒，也就是约等于 27.78 个小时（10 万秒），所以在下载列表中经常看到下载任务还需要大于一天才能完成，这怎么让人受得了，不骂才怪呢！但是只要充值会员，下载速度基本就暴增，可以完全利用宽带的带宽，例如 100 M 的宽带，下载速度可达 12.8 MB/s，哪怕只是 10 M 的宽带，下载速度也能到 1.28 MB/s。因此，百度网盘客户端对于免费用户限制速度限制得太严重了，不充值会员根本没法使用。而且，有时候勉强能使用的时候，经常会弹出会员试用 300 秒的提示，只要选择了，下载速度立马飞速提升，300 秒后又急速下降，经常下降到只有 3.14 KB/s，让人抓狂。网页版限制下载大文件，强迫安装百度网盘客户端 既然百度网盘客户端做了下载速度限制，那么大多数人会想到选择使用浏览器直接下载，同时又可以免去安装百度网盘客户端的麻烦，浏览器的下载速度通常在几百 KB/s，不会像百度网盘客户端那样特别地慢。但是，直接使用网页版的百度网盘下载文件，对文件大小有限制，太大的文件会被网页拦截，下载不了，而是弹出安装百度网盘客户端的提示，这样又回到了原点，因为如果用百度网盘客户端下载速度被限制了。解决问题 使用 aria2 突破线程数限制、下载速度限制 简介 Aria2 是一个多平台轻量级的下载工具，支持 Http、Ftp、BitTorrent、Web 资源等多种格式，使用命令行启动任务，更多具体信息查看官网说明：Aria2 介绍。这种工具可以最大程度利用你的网络带宽，实际上你可以自由配置，包括线程数、网络传输速度、RPC 端口、断点续传是否开启等。 安装 去官网下载安装包：Aria2 安装包 ，我的 Widows 系统 64 位，选择对应的安装包下载。 下载完成后，得到一个 zip 格式的文件，其实直接解压即可，不需要安装，解压后会得到一系列文件，为了方便管理，都放在 aria2 文件夹下面，再复制到程序对应的目录。其中，有一个 .exe 文件，就是运行任务时需要的文件。此外，为了方便起见，把 .exe 文件的路径配置到系统的环境变量中去，这样在任何目录都可以执行 aria2 命令了；如果不配置则只能在 aria2 目录中执行相关命令，否则会找不到程序。配置 1、如果单纯使用命令行启动下载任务，可以把参数信息直接跟在命令后面，临时生效，也就是参数只对当前下载任务有效。显然，这样做很麻烦，每次都是一长串的命令，而且当任务非常多的时候也无法管理，所以不建议使用这种方式。当然，如果只是测试折腾一下，或者也不经常使用，只是偶尔下载一个东西，还是用这种方式比较简介，不用管其它复杂的配置，不用管插件的安装。 单命令行启动任务示例，从电影天堂下载《一出好戏》这部电影。如果下载百度网盘的文件，需要使用 baiduexporter 插件生成 url，生成方式见后续步骤。1234567aria2c.exe -c -s32 -k32M -x16 -t1 -m0 --enable-rpc=true 下载 url 取值 -t1 表示的是每隔 1 秒重试一次 -m0 表示的是重试设置 此外，下载 url 中会包含 --header 的信息：User-Agent、Referer、Cookie、url 理论上 User-Agent、Referer 应该时固定的，Cookie、url 每次会生成不一样的 User-Agent: netdisk;5.3.4.5;PC;PC-Windows;5.1.2600;WindowsBaiduYunGuanJiaReferer: http://pan.baidu.com/disk/home2、如果是后台启动，通过其它管理插件来创建下载任务，则直接使用配置文件，文件名称为 aria2.conf，并在启动 aria2 时指定配置文件的位置。这样做的好处是使用一个配置文件就可以指定常用的参数配置，不用更改，每次下载文件前启动 aria2 即可。配置文件可选项如下，例如下载文件存放位置、是否开启 RPC、是否开启断点续传，具体更为详细的内容请参考文档：Aria2 配置信息文档 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394## '#' 开头为注释内容，选项都有相应的注释说明，根据需要修改 #### 被注释的选项填写的是默认值，建议在需要修改时再取消注释 #### 文件保存相关 ### 文件的保存路径 (可使用绝对路径或相对路径), 默认：当前启动位置 dir=E:\\aria2download\\# 启用磁盘缓存，0 为禁用缓存，需 1.16 以上版本，默认：16Mdisk-cache=32M# 文件预分配方式，能有效降低磁盘碎片，默认:prealloc# 预分配所需时间: none &lt; falloc &lt; trunc &lt; prealloc# NTFS 建议使用 fallocfile-allocation=none# 断点续传 continue=true## 下载连接相关 ### 最大同时下载任务数，运行时可修改，默认：5max-concurrent-downloads=32# 同一服务器连接数，添加时可指定，默认：1max-connection-per-server=5# 最小文件分片大小，添加时可指定，取值范围 1M -1024M, 默认：20M# 假定 size=10M, 文件为 20MiB 则使用两个来源下载；文件为 15MiB 则使用一个来源下载 min-split-size=16M# 单个任务最大线程数，添加时可指定，默认：5split=32# 整体下载速度限制，运行时可修改，默认：0#max-overall-download-limit=0# 单个任务下载速度限制，默认：0#max-download-limit=0# 整体上传速度限制，运行时可修改，默认：0max-overall-upload-limit=1M# 单个任务上传速度限制，默认：0#max-upload-limit=1000# 禁用 IPv6, 默认:falsedisable-ipv6=false## 进度保存相关 ### 从会话文件中读取下载任务 input-file=aria2.session# 在 Aria2 退出时保存 ` 错误 / 未完成 ` 的下载任务到会话文件 save-session=aria2.session# 定时保存会话，0 为退出时才保存，需 1.16.1 以上版本，默认：0#save-session-interval=60## RPC 相关设置 ### 启用 RPC, 默认:falseenable-rpc=true# 允许所有来源，默认:falserpc-allow-origin-all=true# 允许非外部访问，默认:falserpc-listen-all=true# 事件轮询方式，取值:[epoll, kqueue, port, poll, select], 不同系统默认值不同 #event-poll=select# RPC 监听端口，端口被占用时可以修改，默认：6800#rpc-listen-port=6800# 设置的 RPC 授权令牌，v1.18.4 新增功能，取代 --rpc-user 和 --rpc-passwd 选项 #rpc-secret=mivm.cn# 设置的 RPC 访问用户名，此选项新版已废弃，建议改用 --rpc-secret 选项 #rpc-user=&lt;USER&gt;# 设置的 RPC 访问密码，此选项新版已废弃，建议改用 --rpc-secret 选项 #rpc-passwd=&lt;PASSWD&gt;## BT/PT 下载相关 ### 当下载的是一个种子 (以.torrent 结尾) 时，自动开始 BT 任务，默认:truefollow-torrent=true# BT 监听端口，当端口被屏蔽时使用，默认：6881-6999listen-port=51413# 单个种子最大连接数，默认：55#bt-max-peers=55# 打开 DHT 功能，PT 需要禁用，默认:trueenable-dht=true# 打开 IPv6 DHT 功能，PT 需要禁用 #enable-dht6=false# DHT 网络监听端口，默认：6881-6999#dht-listen-port=6881-6999# 本地节点查找，PT 需要禁用，默认:false#bt-enable-lpd=true# 种子交换，PT 需要禁用，默认:trueenable-peer-exchange=true# 每个种子限速，对少种的 PT 很有用，默认：50K#bt-request-peer-speed-limit=50K# 客户端伪装，PT 需要 peer-id-prefix=-TR2770-user-agent=Transmission/2.77# 当种子的分享率达到这个数时，自动停止做种，0 为一直做种，默认：1.0seed-ratio=0.1# 强制保存会话，即使任务已经完成，默认:false# 较新的版本开启后会在任务完成后依然保留.aria2 文件 #force-save=false# BT 校验相关，默认:true#bt-hash-check-seed=true# 继续之前的 BT 任务时，无需再次校验，默认:falsebt-seed-unverified=true# 保存磁力链接元数据为种子文件 (.torrent 文件), 默认:false#bt-save-metadata=true 配置完成后在启动 aria2 时指定配置文件的位置即可，例如我把 aria.conf 与 aria2c.exe 放在同一个文件夹下，则启动时直接指定 1aria2c.exe --conf-path=aria2.conf 当然，这样做只是启动了 aria2，并没有开始创建下载任务，不像单个命令行那样简单，直接设置参数就起任务了。接下来还需要浏览器插件的配合，才能保证下载任务的创建与监控，虽然配置步骤麻烦一点，但是使用起来更为方便。为了避免启动时还要输入命令行，在 Windows 平台下可以写一个 bat 脚本，每次双击脚本即可，以下脚本内容供参考：12@echo off &amp; title Aria2aria2c.exe --conf-path=aria2.conf使用 1、使用命令行启动单个任务无需多做介绍，直接敲下命令行，等待文件下载就行了。如果需要连续下载多个文件，则唯一的做法就是多敲下几个命令，多等待而已。因此，这种方式不适合任务数量多的情况，那这种情况下显然是需要批量下载的，并且可以对下载任务进行管理，那就要看下面的一项了：后台起 aria2 服务。 生成下载 url 的过程需要借助 baiduexporter、YAAW for Chrome 插件，直接从 Chrome 浏览器的插件商店搜索安装即可，如果无法翻墙，也可以从离线镜像库下载离线文件进行安装，离线库可以参考本站点的 关于页面 给出的工具链接。 接下来描述使用方式，登录百度网盘账号，把需要下载的文件保存在自己的网盘中，选择需要下载的文件，然后可以看到本来的下载按钮旁边又多了导出下载按钮，包含几个选项：ARIA2 RPC、文本导出、设置。选择文本导出就会弹出当前下载文件的下载 url，复制粘贴到命令后即可直接下载该资源。导出的内容格式如下，当然实际使用的时候里面的参数也是可以更改的，但是下载 url 一定不不能变的。1https://pcs.baidu.com/rest/2.0/pcs/file?method=download&amp;app_id=250528&amp;path=%2F% E9%80%86% E5%90%91% E8% B5%84% E6%96%99%2FIDA%20Pro% E6%9D%83% E5% A8%81% E6%8C%87% E5%8D%97.pdf2、根据前面的描述，后台起了 aria2 服务，但是还没真正用起来，想要用起来，必须配合两个插件：baiduexporter、YAAW for Chrome。这 2 个插件中前者的作用是获取百度网盘的文件 url，这个 url 当然不是分享文件产生的 url，而是下载文件产生的 url；后者插件的作用是配合前者自动创建下载任务，实际下载利用的是已经启动的 aria2 后台，并时时监控任务状态，提供任务管理界面。插件的安装不再赘述，接下来直接描述使用流程，要确保以上两个安装的插件都已经启用。根据上一步骤已经知道导出下载这个按钮，里面包含着一个 ARIA2 RPC 选项，这个选项就是直接使用 后台 aria2 服务创建下载任务，然后 YAAW for Chrome 插件监控着所有下载任务。还有一个前提，就是启动 aria2 服务时要开启 RPC 模式。12# 启用 RPC, 默认:falseenable-rpc=true这样做了之后，aria2 后台服务会开启一个端口，一般默认 6800（如果 aria2 更改了端口，YAAW for Chrome 也要做相应的配置），这个端口用来给 YAAW for Chrome 汇报下载任务的情况，并提供管理下载任务的接口，这样的话，直接通过 YAAW for Chrome 就可以通过可视化的方式创建、暂停、查看任务。后台启动 aria2，开启 RPC 模式。打开 YAAW for Chrome 插件查看端口配置信息。通过 baiduexporter 插件，直接选择 PRC 下载，再去 YAAW 界面刷新查看下载任务。可以看到，aria2 参数还没优化（线程数、分块大小设置），下载速度已经有将近 400 Kb/s 了。使用油猴插件绕过浏览器下载大文件的限制 现象 还是刚才那个文件，文件大小只有 149 M，不想通过百度网盘客户端下载，只想通过网页版下载，那就直接点击下载按钮，发现被限制了，必须让你安装百度网盘客户端。本来还在想通过网页版直接下载，速度也不会很慢，但是被限制了，这个时候我们的万能插件要出场了：Tampermonkey，又称油猴、暴力猴。解决方式 使用万能的插件，屏蔽百度网盘网页版原来的网页内容，从而导致百度网盘的限制失效，这个插件就是 Tampermonkey：官网 、Chrome 浏览器插件商店。 这个插件的作用其实就是帮你管理各种自定义脚本，并运用在网页解析渲染中，从而实现对网页内容的改变，例如：去除网页的广告、去除百度搜索内容的广告条目、更改新浪微博展示界面。其中，也包括让百度网盘的下载文件大小限制失效，从而可以自由下载。1、好，现在需要在插件的基础上安装一个脚本：百度网盘直接下载助手。要安装这个脚本，则首先需要找到它，选择获取新脚本，会引导我们进入脚本仓库。2、各种脚本仓库，我们选择 GreasyFork。3、在搜索框中搜索：百度网盘直接下载助手，选择其中一个。4、安装选择的脚本。5、可以看到脚本内容，点击安装。6、安装完成后，选择管理面板可以查看已经安装的脚本以及是否启用，也可以删除或者二次编辑。7、回到百度网盘，选择文件，可以看到多了一个下载助手选项，选择 API 下载，下载，即可使用浏览器直接下载，不会因为文件太大有网页的限制。8、当然，如果自己会写脚本，或者从别处直接复制的源脚本代码，在插件中选择添加新脚本，自己编辑即可。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>Aria2</tag>
        <tag>百度网盘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WPS 关闭广告推送与自动升级]]></title>
    <url>%2F2018110301.html</url>
    <content type="text"><![CDATA[在工作和生活中，很多人使用金山的 WPS 套件，类似于微软的 Office 套件，而且是免费的。但是很多人会遇到 广告推送 或者 WPS 热点推送 ，每隔几天就会出现，有时候可以点击七天内不再出现，便可以清静几天，而且碍于是免费版本，不想购买会员，于是也就忍了。其实，WPS 自身是有设置可以关闭 广告推送 的，当然也可以关闭 WPS 热点推送 。WPS 的弹框 以下现象描述与截图均出自版本：WPS 2019，v11.1.0.8013 - Release 正式版。操作系统为：Windows 2007 专业版。在使用 WPS 的过程中，经常遇到广告推送与 WPS 热点推送，觉得很受打扰，但是碍于使用的是免费软件，又只能忍受。我一直在想以前是有设置可以关闭的，后来升级了就找不到是在哪里设置的了，后来又查阅了资料，发现果然是有地方可以设置的，只不过隐藏的太深了，不好寻找而已。接下来就一步一步说明具体设置步骤。设置关闭 如果你在互联网上搜索 WPS 广告推送相关话题，可以看到大量的帖子（或者说是方法教程）已经整理出了各种方案，可以帮你解决这个问题，例如：直接更改 WPS 安装目录中的某些文件、利用杀毒软件屏蔽广告推送、直接设置 WPS 等等。显然，前 2 种方案是在走弯路，而最后一种方案才是最简单直接的。1、打开 WPS 主页，在右上角找到 设置 按钮；2、点击 设置 ，选择 配置和修复工具 ；3、在弹出的对话框中选择 高级 ；4、选择对话框的 其它选项 标签页，取消截图中的 3 项勾选，即同时关闭 升级完成后推荐精选软件 、 订阅 WPS 热点 、 接受广告推送 ；此外，进入步骤 3 也可以直接通过系统的安装程序列表（开始 –&gt; 所有程序 –&gt;WPS Office–&gt;WPS Office 工具 –&gt; 配置工具），步骤如下图：按照以上步骤设置， WPS 就不再会弹出广告推送和 WPS 热点推送了，亲测有效。注意事项 1、说实话，我是没想到这个设置会隐藏的这么深，但至少暴露出来了；2、要注意版本区别，可能每个版本的设置步骤有所不同，而且也不排除以后更新的版本会取消这些设置选项，或者隐藏的更深。当然，如果 WPS 找到了其它盈利方式，也可能会取消这些广告推送；3、WPS 每次更新后，上述设置会还原，也就是又回到默认开启的状态，此时需要重新设置一次。当然，为了以后不会莫名其妙又弹出广告推送，可以直接关闭自动升级（和前面关闭广告的步骤一致，但是选择的是 升级设置 标签页），以后想升级的时候再手动升级。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>WPS</tag>
        <tag>关闭广告推送</tag>
        <tag>关闭自动升级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微博 URL 短网址生成算法 - Java 版本]]></title>
    <url>%2F2018101501.html</url>
    <content type="text"><![CDATA[微博短链接是微博官方提供的网址压缩功能产生的一种只包含少量字符的短网址，例如：http://finance.sina.com.cn ，压缩后为：http://t.cn/RnM1Uti 。这样的话，发微博时链接占用更少的字符长度。如果发微博时，内容中带了链接，例如视频地址、淘宝店地址，会被自动压缩为短链接。微博短链接可以直接在浏览器中访问，会被微博的网址解析服务器转换为原来的正常链接再访问。本文描述微博 URL 短网址生成算法，编程语言是使用 Java。 待整理。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>微博URL短网址</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android 逆向系列 2-- 破解第一个 Android 程序]]></title>
    <url>%2F2018101101.html</url>
    <content type="text"><![CDATA[本文简单介绍一下对一个简单的 Android 程序的逆向破解，算是对 Android 逆向的入门了解。 待整理。]]></content>
      <categories>
        <category>Android 逆向系列</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>逆向工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android 逆向系列 1-- 编写第一个 Android 程序]]></title>
    <url>%2F2018101001.html</url>
    <content type="text"><![CDATA[本文简单介绍一下 Android 开发的入门程序，编写一个简单的 Android 程序。 待整理。]]></content>
      <categories>
        <category>Android 逆向系列</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>逆向工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android 逆向系列 0-- 初识 Android 以及逆向工程]]></title>
    <url>%2F2018100901.html</url>
    <content type="text"><![CDATA[本文简单介绍一下 Android 开发以及关于 Android 的逆向工程，算是入门了解。 待整理。]]></content>
      <categories>
        <category>Android 逆向系列</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>逆向工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Kryo 异常]]></title>
    <url>%2F2018100801.html</url>
    <content type="text"><![CDATA[本文讲述使用 es-hadoop （版本 v5.6.8）组件，运行 Spark 任务遇到的异常：123Caused by: java.io.EOFExceptionat org.apache.spark.serializer.KryoDeserializationStream.readObject (KryoSerializer.scala:232)at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject (TorrentBroadcast.scala:217)以及通过 Maven 依赖查找分析的解决方法。遇到问题 由于最近的 elasticsearch 集群升级版本，到了 v5.6.8 版本，所用的功能为了兼容处理高版本的 elasticsearch 集群，需要升级 es-hadoop 相关依赖包到版本 v5.6.8，结果就遇到了问题：代码逻辑就是通过 es-spark 直接读取 elasticsearch 里面的数据，生成 RDD，然后简单处理，直接写入 HDFS 里面。本机在测试平台测试一切正常，没有任何问题，但是在线上运行就会抛出异常。解决方法 先分析一下这个问题产生的原因，在代码层面没有任何变动，只是更改了依赖的版本，所以问题在于更改版本之后是不是导致了传递依赖包的缺失，或者版本冲突，所以总体而言，肯定是 Maven 依赖包的问题，这个思路没问题。提前说明下面截图中出现的 Maven 中的常量：12&lt;elasticsearch-hadoop.version&gt;5.6.8&lt;/elasticsearch-hadoop.version&gt;&lt;spark-core_2.10.version&gt;1.6.2&lt;/spark-core_2.10.version&gt;1、local 模式 通过在本机连接测试平台，运行起来没有问题，但是部署到正式环境，运行不起来，直接抛出上图所示的异常信息。首先去依赖树里面查看与 kryo 相关的依赖信息（使用 mvn dependency:tree 命令）：发现两个依赖包（es-hadoop v5.6.8，spark-core_2.10 v1.6.2）里面都有与之相关的传递依赖，而且版本（奇怪的是 groupId 也稍有不同，这导致了我后续判断失误）不一致，这必然导致依赖包的版本冲突，通过 exclusions 方式去除其中一个依赖（其实不是随意去除一个，要经过分析去除错误的那个，保留正确的那个），local 模式可以完美运行。此图是 pom.xml 文件里面的移除信息，是我根据依赖树整理的，可以更加清楚地看到传递依赖的影响。2、yarn-client 模式 通过步骤 1 解决了 local 模式运行的问题，但是当使用 yarn-client 模式向 yarn 集群提交 Spark 任务时，如果移除的是 spark-core_2.10 里面的 kryo 依赖，异常信息仍然存在，无法正常运行。此时，我想到了前面所说的 2 个 kryo 依赖包的 groupId 有一点不一样，所以这 2 个依赖包虽然是同一种依赖包，但是可能由于版本不同的原因，导致名称有些不同。我认为使用的 es-hadoop 依赖的版本比较高，可能没有兼容低版本的 spark-core_2.10，所以需要保留 spark-core_2.10 里面的 kryo 依赖，而是把 es-hadoop 里面的 kryo 依赖移除。果然，再次完美运行。总结说明 这次通过 Maven 依赖找到了问题，但是版本仅仅限定在我使用的版本，其它的版本之间会有什么冲突我无法得知，但是这种处理问题的思路是正确的，避免走冤枉路，浪费不必要的时间。另外，提醒一下大家，更新 pom.xm 文件（包括新增依赖和更新依赖版本）一定要谨慎而行，并且对所要引入的依赖有一个全面的了解，知道要去除什么、保留什么，否则会浪费一些不必要的时间去查找依赖引发的一系列问题。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Kryo</tag>
        <tag>序列化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[依赖包缺失导致 Spark 任务无法启动]]></title>
    <url>%2F2018100701.html</url>
    <content type="text"><![CDATA[本文讲述使用 Spark 的过程中遇到的错误：1class "javax.servlet.FilterRegistration"'s signer information does not match signer information of other classes in the same package最终通过查找分析 Maven 依赖解决问题。遇到问题 由于最近的 elasticsearch 集群升级版本，到了 v5.6.8 版本，所用的功能为了兼容处理高版本的 elasticsearch 集群，需要升级相关依赖包，结果就遇到了问题。使用 es-hadoop 包（v5.6.8）处理 elasticsearch （v5.6.8）里面的数据，具体点就是通过 es-spark 直接读取 elasticsearch 里面的数据，生成 RDD，然后简单处理，直接写入 HDFS 里面。编译、打包的过程正常，运行代码的时候，抛出异常：1class "javax.servlet.FilterRegistration"'s signer information does not match signer information of other classes in the same package一看到这种错误，就知道肯定是 Maven 依赖出现了问题，要么是版本冲突，要么是包缺失，但是从这个错误信息里面来看，无法区分具体是哪一种，因为没有报 ClassNotFound 之类的错误。解决方法 现象已经看到了，问题也找到了，那么第一步就是直接搜索 Maven 项目的依赖，看看有没有 FilterRegistration 这个类，我的 IDEA 直接使用 Ctrl + Shift + T 快捷键，搜索 FilterRegistration，发现有这个类，但是包名对不上，注意包名是：javax.servlet。现在就可以断定，是包缺失，通过搜索引擎查找文档，需要引入 javax.servlet-api 相关的包， pom.xml 文件的具体依赖信息是：12345&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;4.0.1&lt;/version&gt;&lt;/dependency&gt;当然，版本信息根据实际的场景需要进行选择，我这里选择 4.0.1 版本。需要注意的是，有另外一个包，它的 artifactId 是 servlet-api，可能你会因为没看清而配置了这个依赖包，导致还是包缺失，所以一定要看清楚。我这里遇到的问题比较简单，只是包缺失而已，如果遇到的是包版本冲突，需要移除不需要的版本，只保留一个依赖包即可，此时可以借助 Maven 的 dependency 构建来进行分析查找：1mvn dependency:tree这个命令会输出项目的所有依赖树，非常清晰，如果内容太多，可以使用：1mvn dependency:tree &gt; ./tree.txt重定向到文本文件中，再进行搜索查找。总结 1、还遇到一种情况，在正式环境运行正常【没有单独配置这个依赖，使用的是别的依赖包里面的同名类，org.eclipse.jetty.orbit:javax.servlet】，但是在本机跑，创建 SparkContext 的时候就会报错，无法创建成功。其实还是因为包缺失，确保要使用 javax.servlet-api 这个依赖，其它的都不好使。2、在本机连接测试环境的 yarn，创建 SparkContext 的时候无法指定用户名，默认总是当前系统的用户名，导致创建 SparkContext 失败，伪装用户无效，只有打 jar 包执行前使用命令切换用户名：export HADOOP_USER_NAME=xx，而在代码中设置 System.setProperty (“user.name”, “xx”)、System.setProperty (“HADOOP_USER_NAME”, “xx”) 是无效的（这个问题会有一篇文章专门分析，需要查看源代码）；3、针对 2 的情况，简单通过 local 模式解决，暂时不使用 yarn-client 模式；4、针对 2 的情况，还有一种简单的方法，那就是直接设置 IDEA 的环境变量参数（不是设置操作系统的环境变量，我试了无效），如下图（和设置运行参数类似）； 设置 IEDA 的环境变量：5、此外，还有一种情况，当需要操作 HDFS 的时候，发现无论怎么设置环境变量都不可以（配置文件配置、代码设置），总是读取的系统默认用户，就和 2 中讲的一致，其实如果只是单纯地操作 HDFS，还可以在创建文件流的时候指定用户名（不过这种方法要先从 conf 中获取 uri）；12String uri = conf.get ("fs.defaultFS");FileSystem fs = FileSystem.get (new URI (uri), CONF, "zeus");]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Maven</tag>
        <tag>依赖问题</tag>
        <tag>FilterRegistration</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[地锅鸡做法总结（皖北、苏北地区）]]></title>
    <url>%2F2018100601.html</url>
    <content type="text"><![CDATA[地锅鸡是流行于皖北、苏北地区的一道传统名菜，并经过改良产生了地锅鱼、地锅豆腐、地锅牛肉、地锅三鲜等一系列菜品，但是配饼始终不变，一般是面饼或者玉米饼。本文用于记录地锅鸡做法总结。地锅鸡简介 地锅鸡是一道起源于江苏省的北部、山东省的南部的传统名菜，在安徽北部地区也非常流行，主要食材就是鸡肉、面饼、辣椒，做成菜品后既有主食又包含配菜，口味醇香，饼借菜味，菜借饼香，吃起来回味无穷。另外，经过不断的改进创新，还产生了地锅鱼、地锅豆腐、地锅牛肉、地锅三鲜等一系列菜品，它们的核心都在于使用地锅制作，并配以面饼、玉米饼。材料准备 2 人份的材料： 面粉 150 克 鸡肉 300 克 鸡蛋 1 个（和面使用，也可以不用）花椒 10-15 粒 姜片 4 片 大葱 4 段 干辣椒 5 个 桂皮 1 小块 八角 2 个 大蒜 5 瓣，不用切 青椒、红椒各半个，滚刀切好 酵母菌 各种调味料 主要步骤 1、使用面粉 150 克和面，使用温水，加入 0.5 克酵母菌，也可以加一个鸡蛋一起，和完的面很软但是不粘手，使用保鲜膜包住，注意先撒一点面粉再包（或者直接放碗里用保鲜膜盖住密封，也需要先在碗里撒一层面粉），这样是避免最后粘住。大概需要发 20-30 分钟，等待的过程可以去做其它准备工作了。2、取出配料，花椒、姜片、大葱、干辣椒、桂皮、八角、大蒜。锅烧热，放油，多放一点油，先放花椒、桂皮、八角，几秒后再放入葱段、姜片、大蒜、干辣椒，大概 10 秒煸出香味，捞出大蒜备用，其它配料不用捞出。3、鸡肉洗干净，放入锅内中火炒 5-10 分钟，放入老抽、生抽、白糖、料酒、豆瓣酱，混合后加入开水，稍微没过鸡肉一点，转为大火，烧开。烧开后中小火焖 15-20 分钟，此时鸡肉已经熟了，要保证还有一些水汤在锅里，因为等一下还需要贴饼、调味、继续焖、收汁等步骤。4、在步骤 3 的过程中，面已经发好，均匀分成条状，具体做法是先拉伸，变成长条，然后揪断就行了，大概 20 个左右（如果锅小了一次贴不完，就分 2 次，贴 1 次先吃着，吃完再加一点汤继续贴下一锅），放入清水中，主要不要再动了，就让它们浸泡在水中。5、步骤 3 结束后，改为小火，加盐调味，并准备贴饼（如果感觉鸡肉的颜色不够，可以再加一点酱油上色）。步骤 4 的面团在水中浸泡了大概 10 分钟，一个一个取出，是湿漉漉的，用手扯成长条饼状，一半贴在锅沿，用力压一下确保贴紧，一半放入汤中。这样，上半部分会焦脆，下半部分吸收了汤汁很美味。饼贴满后，小火继续焖 10-15 分钟，此时注意如果锅受热不均匀，需要每隔几分钟旋转一下锅。6、在步骤 5 中，几分钟后，饼快熟了，就可以加入步骤 2 中捞出来的大蒜，和滚刀切的青椒、红椒，加点香油，稍微搅拌一下，此时汤汁已经基本没了。饼完全熟透了，开锅，大火稍微翻炒几下，就可以吃了，直接在锅里吃。 注意事项 1、锅、灶的选择，在农村地区、乡镇地区、城市周边的农家乐，才会有地锅这种设备，所以在家里自己做是很难找到地锅的，只能退而求其次使用普通的炒锅，也是可以的，注意尺寸要大一点的（做 2 人份的地锅鸡就选 3-4 人份的炒锅）；另外最好保证灶的火力能大一点，有用；2、面发的时间，和面时可以放鸡蛋也可以不放，最好使用温水，发面的时间不要太久，一般 30 分钟就行了，甚至可以不发，直接使用死面；3、贴饼的时候速度一定要快，不然刚刚贴了半圈就已经熟了，丧失了饼的香味；另外饼要贴紧一点，粘在锅上，如果锅受热不均匀，注意每隔几分钟旋转一下锅，保证饼的上半部分能焦脆，这也是需要灶的火力大一点的原因。 上图 在安徽合肥吃到的 在 2018 年中秋期间，回老家经过合肥，于是在以前的同学的带领下吃了一次地锅鸡，非常满足。这个店的地点在安徽大学（馨苑校区）的西门附近，那条路叫九龙路，一条街都是吃的，又名九龙美食街。自己做的 自己在 2018 年国庆期间做了一次，由于没有地锅可以使用，只好选用了普通的炒菜锅，做起来味道还是不错的，只不过饼没有达到香脆的水平，稍有遗憾。此外，家用煤气灶的火力不行，需要更大火的时候不够，导致温度不够高，间接导致了鸡肉的香味和饼的香味没有充分融合，吃的时候感受不到纯正的地锅鸡的香味。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>地锅鸡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10 取消 Skype for Business 自动登录]]></title>
    <url>%2F2018090701.html</url>
    <content type="text"><![CDATA[最近在使用 Win10 系统，遇到一个问题，每次开机，Skype for Business 都会自动弹出来，提示登录，每次我都会关掉它。遇到多次之后，我想这个应用我不需要，直接卸载掉算了，但是却找不到这个应用的信息，最后只能通过关闭 开机自动启动 的方式来解决问题，本文记录解决问题的过程。问题出现 最近在使用 Win10 的时候，每次开机后，Skype for Business（这个应用不同于 Skype，虽然功能一样）总会弹出来，提示我登录，我每次都会毫不犹豫地关掉它。Skype for Business 登录界面 正常的 Skype 应用登录界面 但是出现多次之后，很麻烦，当我想卸载这个应用的时候，发现从应用列表里面找不到，也就无从卸载。后来就想能不能关闭开机启动，找了一些文档发现可以，那就这么办了（而且还发现 Skype for Business 根本卸载不了）。问题解决 1、Skype for Business 是属于 Office 套件中的一个软件，所以在安装整个 Office 的同时也会自动安装上 Skype for Business。由于是一次安装整个 Office 套件，所以无法单独删除其中的一个软件（Skype for Business）。如果不需要开机自动启动 Skype for Business（也就不会提示我登录了），可以在 Skype for Business 的 设置 菜单中的 个人 选项里将 当我登录到 Windows 时自动启动应用 这个设置取消。设置（在登录界面的右上角，有一个齿轮按钮）取消当我登录到 Windows 时自动启动应用 2、不知道哪一天 Windows 升级到了新的系统后，Skype for Business 不见了（怎么找也找不到），随之而来的是 Skype，尽管它也属于 Office 中的一个应用（还有很多其它一系列应用），但是这个应用可以单独安装卸载，不再与 Office 绑为一个整体。 打开我的 Office查看应用列表，也可以直接安装显示的应用 问题总结 1、我是一开始关闭了 Skype for Business 的登录界面，然后再想打开它，就找不到了，不知道在哪（理论上应该隐藏在某个应用列表里面，目前我还没找到，可能是 Windows 系统升级导致的），但是现在却自动有了 Skype 这个应用（可能是 Windows 系统升级替换了以前的 business 版本），其实这 2 个应用应该差不多。2、现在 Windows 系统升级到最新版本（最新升级时间是 2019-02-17）后，Skype for Business 已经不存在了，替换它的是 Skype，而这个应用是可以单独卸载的。3、参考： 官方回复 、 设置方式、Skype for Business 应用介绍 。]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>Skype for Business</tag>
        <tag>自动登录</tag>
        <tag>Win10</tag>
        <tag>Skype</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[踩坑特殊字符之硬空格]]></title>
    <url>%2F2018090601.html</url>
    <content type="text"><![CDATA[最近处理数据的过程中，发现一个奇怪的问题，处理数据逻辑如下：有一个短字符串，我需要从一个长字符串寻找这个短字符串是否出现。这个逻辑很简单，使用任何一种编程语言，基本上都会有 包含 这种方法，直接就可以判断了，但是我遇到的情况明明就是包含了，子串就是存在，但是判断结果却不包含。另外我又直接把 2 个字符串单独取出来，肉眼去看，也是包含的。愁眉苦展之际，突然灵光闪现：会不会字符串中包含一些奇怪的特殊字符，并且肉眼难以发现。于是立马去验证一下，果然是这样，解决了我的问题，本文记录解决问题的过程。问题出现 在使用 Java 编程语言处理数据的时候，有一个字符串包含判断的逻辑，明明是包含关系，判断的结果却是不包含，代码示例如下：1234@Testpublic void containsTest() &#123; System.out.println ("每当 #每月 28 日京东企业会员日 #来临，就会有优惠".contains ("# 每月 28 日京东企业会员日 #".trim ()));&#125;运行结果：尽管加上了 trim () 方法，但是返回结果仍然是 false，足以说明子串中头尾有某个符号很特殊，并没有被清除掉，所以我觉得很蹊跷。虽然在代码和截图中，那个大大的空格（其实不是空格字符）看起来很显眼，但是在实际运用中是在文本中存放的，而且不止这一个字符串，有很多，所以在检查时使用搜索替换功能（把空格替换为空白），也是没把这个特殊符号清除掉。同时，也没有料想到文本中会出现这样特殊的符号，所以只是简单地使用 trim () 方法来清除头尾空白符（还以为生效了，其实遇到这种特殊的符号就没作用了）。把内容复制到 Notepad++ 文本编辑器中，并且设置文本编辑器的视图显示所有的字符（会用带有颜色的特殊图标来表示文本中的特殊字符，例如肉眼看不到的字符：空格、换行等），然后可以看到空格（橙色点点）、Tab 符号（橙色箭头）、回车换行（黑块）都会显示出来，但是唯独这个特殊字符没有显示出来，仍然是空白一片。使用文本编辑器打开 问题解决 已经知道这是个特殊字符了，下一步只要搞清楚这是个什么字符就行了，问题就会迎刃而解。先把特殊字符复制出来，找一个转码器，把字符转为十六进制编码，看看它的编码是什么，在线编码转换工具参考：http://ctf.ssleye.com/jinzhi.html 。在 文本 这个文本框中输入特殊字符（–&gt; &lt;–），然后在 十六进制 文本框中可以看到编码是 a0。可以看到十六进制的结果是 a0好，接下来去 Unicode 字符列表（维基百科里面的：https://zh.wikipedia.org/wiki/Unicode% E5% AD%97% E7% AC% A6% E5%88%97% E8% A1% A8 ）查看这到底是个什么特殊字符。直接搜索 00A0，就可以找到，发现这是 不换行空格 ，在 拉丁字符 - 1 辅助 里面。不换行空格 更进一步，我去维基百科查看这一特殊符号的介绍，发现这个符号还是挺有用的，附上链接：https://zh.wikipedia.org/wiki/% E4% B8%8D% E6%8D% A2% E8% A1%8C% E7% A9% BA% E6% A0% BC 。不换行空格介绍，一般用在网页排版中 至此，问题原因找到了，我竟然被一个特殊符号坑了（以前也被输入法的全角、半角问题坑过），那解决办法就很简单了，针对这种符号做替换清除就行了。我仍在思考，这个符号是怎么被输进文本文件给我的，因为正常人通过输入法不可能打出这个符号。后来询问相关人，得知他们是从网页上直接复制的内容粘贴到文本文件中，这就可以解释了，因为这个符号就是针对网页的自动压缩空白符问题，量身定做的，怪不得。问题总结 1、这个特殊字符（–&gt; &lt;–，编码 a0）有些编辑器不一定支持，会产生丢失，例如我把这条字符串内容放在了 Tower 上面的任务回复中，想记录一下，结果再复制下来就变成了空格字符，说明丢失了（并且被转为了空格字符）。但是没关系，我们记住它的 Unicode 编码就行了，找一个工具（例如：http://ctf.ssleye.com/jinzhi.html ）就可以转换了，Unicode 编码是：U+00A0，把 a0 复制到 十六进制 的文本框中，则 文本 这个文本框中出现的就是它的字符，当然，直接肉眼看不出来，要选中变为蓝色才会发现有一个字符。编码恢复到字符2、以后处理字符串问题的时候，一定要区分场景，真的是自己知识面之外的情况都可能出现，而且是正常的，自己千万不要怀疑人生，而是要抽丝剥茧，一步一步找问题。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>硬空格</tag>
        <tag>不换行空格</tag>
        <tag>hard-space</tag>
        <tag>fixed-space</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[归来]]></title>
    <url>%2F2018090501.html</url>
    <content type="text"><![CDATA[好，整理完成后，重新出发。二级标题，自动创建锚点和目录 三级标题 - 开始 1$ hexo new "My New Post"More info: Writing 三级标题 - 中间 1$ hexo serverMore info: Server 三级标题 - 结束 1$ hexo generateMore info: Generating 三级标题 - 备注 1$ hexo deployMore info: Deployment 踩坑记录 注意使用 hexo 对 Markdown 文件进行解析时，有一些转义字符是会失败的（使用反斜杠 \ 进行转义的，例如美元符号 &#36;，成对出现有特殊含义，所以需要转义，在 Markdown 中可以使用 \$ 进行转义，但是 hexo 解析完成 html 文件是失败的），所以最好使用编码解决，例如美元符号使用 &amp;#36; 替代。]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>建站</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[武功山两天徒步登山总结]]></title>
    <url>%2F2018071601.html</url>
    <content type="text"><![CDATA[公司部门组织团建，在 2018-07-13 这天开始，一行二十多人傍晚从公司出发，踏上了去往江西武功山的旅程。本文就详细讲述 2018-07-13 晚上从广州出发，2018-07-16 凌晨到达广州的整个旅程。 图文并茂，等待整理。]]></content>
      <categories>
        <category>游记</category>
      </categories>
      <tags>
        <tag>武功山</tag>
        <tag>徒步</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[辣椒炒肉做法总结]]></title>
    <url>%2F2018063001.html</url>
    <content type="text"><![CDATA[辣椒炒肉，农家小炒肉，青椒肉片，很多类似的菜品，有的是川菜，有的是湘菜，但是它们有一个共同点，都是食材简单、可口下饭，本文就讲述辣椒炒肉的步骤以及需要注意的地方，本文讲述的做法是湘菜的做法。食材准备 辣椒炒肉的食材比较简单：辣椒 4 个（最好是螺丝椒，次之也可以用普通的青椒）二刀肉 300 克（或者是普通的后腿肉也行，实在没有用前腿肉也勉强可以，确保肥瘦比例是 2：3 就行了，一定要有肥肉）老抽、生抽、料酒（用来腌制瘦肉使用，最后也要使用生抽提鲜）食用盐 大蒜 5 瓣 炒制步骤 1、螺丝椒 4 个，脆嫩皮薄，买的时候一定要挑选颜色比较鲜亮的（颜色发暗的不能要，不新鲜了），同时捏起来比较脆硬（软的不能要），说明新鲜。此外尽量挑大个的，比较容易去籽、容易滚刀切。买回来后去除头部，去除籽粒（一定要去除干净，否则炒制的时候辣椒籽容易变黑变苦，影响颜色与口感，当然稍微有个别的辣椒籽可以忽略），滚刀切成小块，沥干水分（尽量让它保持干燥）； 认准螺丝椒 螺丝椒去籽洗干净 滚刀切螺丝椒 2、将二刀肉去皮，肥瘦分离（这个步骤自己处理比较麻烦，最好让卖肉的大叔大姐帮忙处理了），肥肉切片，稍微切大一点，方便后续炼油，沥干水分，不做任何处理，直接放入碗中备用。瘦肉也是切片，放入碗中，接着放入生抽、老抽、食用盐、料酒，用手搅拌 1 分钟，再腌制 10 分钟； 肥肉沥干水分装碗备用 腌制瘦肉 3、大蒜切片，每一瓣切 3-5 片，尽量厚一点，备用； 忽略掉旁边的葱花和蒜粒，那是炒其它菜使用的 4、干锅煸炒辣椒（这个过程大概 3 分钟），先将炒锅烧干烧热，不放油、不放水，直接将步骤 1 中切好的辣椒片扔进去（这也是辣椒切好后必须沥干水分的原因），就是干煸，这是决定这道菜口感的第一步。控制中小火（大火容易糊），让辣椒块水分逐渐挥发，表皮起皱，即俗称起虎皮，这时候会有少量烟冒出，并伴随着独特的呛香辣味（像我这样家里没有抽油烟机的，只能在旁边准备好湿毛巾，不时去捂一下口鼻）。此时可以稍微放一点食用盐，这样辣椒才能入味，看到辣椒表皮起皱了，就可以了，切记不能熟透（熟透严重影响口感，因为最后还要回锅调味）。此时大概七八分熟，俗称断生，盛出放入碗中备用；5、炒制肥肉（这个过程大概 6 分钟），大火将炒锅烧热，放入少量花生油（少量就可以，只是为了防止肥肉下锅时粘锅，不是为了炒菜），下肥肉，先煸炒 1 分钟，待煸炒出少量猪油，火力转中小火，开始炼油。这里耗费时间长一点，大概 5 分钟，肥肉慢慢消失，本来的肥肉块越来越小，越来越干，炼出了很多猪油，很香； 肥肉炼油，很香 6、炒制瘦肉（这个过程大概 1 分钟），等步骤 5 中的肥肉炼成了肉干（这里自己把握，也可以炼油时间短一点，保留多一点肥肉，吃起来更香），火力转大火，将步骤 2 中腌制的瘦肉倒进来翻炒，大概 1 分钟，炒到瘦肉变色，基本熟了；7、调味翻炒，辣椒回锅（这个过程大概 2 分钟，根据放调味料和辣椒的速度而定），等瘦肉基本熟了，放入大蒜片（不是一开始和瘦肉一起放，否则大蒜片都炒烂了），此时按照湖南的做法，还要放点豆豉进来（注意这是几秒内就要完成的动作，如果大蒜片和豆豉没放在边上，记得先关小火，等放完了再开大火）。接着把步骤 4 中的干煸后的辣椒倒进来，先翻炒几下，然后不停地用锅铲戳，戳 1 分钟左右，倒点生抽提鲜，放点食用盐，再翻炒 10 秒，关火。 辣椒炒肉成品，不过辣椒有点炒过头 注意事项 1、所谓 二刀肉 ，是指屠户旋掉猪尾巴那圈肉以后，靠近后腿的那块肉，因为它是第二刀，顾名思义，就称为二刀肉。那地方的肉有肥有瘦，肥瘦搭配，一刀肉肥的多，二刀肉是 肥四瘦六 ，比较适合做这道菜，同时也适合做 回锅肉 这道菜；2、腌制瘦肉，煸炒辣椒块，都已经放过少量的盐了，所以最后千万不要又放了很多盐，根据个人口味添加；3、购买辣椒时，一定要选择新鲜的，否则口感不好；4、步骤 4 中的干煸辣椒千万不要把辣椒炒熟了，否则最后的成品辣椒不够脆嫩，而且辣味会全部丢失；5、这道菜看起来注定非常油腻，因为肥肉炼出了大量的猪油，但是吃起来不会腻，而且菜底的这个油非常香，甚至可以用来拌饭吃；6、为了让成品的颜色好看一点，腌制瘦肉时需要老抽，如果在炒制瘦肉的最后发现颜色不够，可以再补加一点老抽。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>辣椒炒肉</tag>
        <tag>川菜</tag>
        <tag>湘菜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[红烧肉做法总结]]></title>
    <url>%2F2018060301.html</url>
    <content type="text"><![CDATA[红烧肉，是一道做法非常简单的传统菜品，而且有多种版本，也有多种口味，同时基于红烧肉再补充其它配菜，又创新出了很多菜品。本文就讲述红烧肉的做法总结，口味偏甜，江南一带的做法。食材准备 以下的食材分量大约 2 人份（实在吃多了也会腻的，要和其它菜配在一起吃）：1、五花肉 500 克；2、冰糖 30 克；3、大葱 3 段、姜片 3 片、香叶 2 片、八角 2 个、桂皮 2 小块、大蒜 2 瓣（可以不用）；4、老抽（糖色不够的时候加老抽补充颜色）、料酒、食用盐（看口味，有时候不需要加盐了）；制作步骤 1、配料准备，装盘备用，实际上只需要少量即可，比我想象中的少很多； 几种大料准备 2、五花肉切块，本来应该切大块的，我的炒锅小，同时五花肉的量也少，不方便做，就切小块了（如果买的五花肉质量不好，肥肉肥油比较多的话，不适合直接下锅，最好焯水一下，把肥油过滤掉一些，如果肉的质量好，直接沥干水分就可以）； 原始的五花肉 3、炒糖色，下五花肉，炒糖色其实就是放少量油，把冰糖融化，然后在高温下冰糖渐渐变色，类似棕色或者深红色，此时放进去的五花肉小块就有颜色了（如果炒糖色成功了，后面就不用放老抽了，但是要注意不能炒太久，否则糖会变苦），此步骤同时也可以把五花肉小块定型，防止煮的过久烂掉； 给五花肉小块上色 4、加水（水量很重要，漫过所有的肉块再多一点，否则最后无法完成收汁操作），放大料，开大火煮开，然后转为小火，煮 50 分钟（中途可以晃一下锅，最好不要打开锅盖，还要注意一下水够不够）； 加水，放大料 煮了 30 分钟 5、50 分钟后，开锅，拣出大料，扔掉，开大火，收汁（一般 10 分钟就够，如果水量少了可能 3 分钟水就干了，看情况根据需要及时放老抽、食用盐等调料），我放的糖不多，于是加了一点食用盐，收汁完成盛出（可以看到我这份颜色还是不够啊），成品看起来虽然有点油腻，但是吃起来绝对肥而不腻，入口即化。 拣出大料 收汁完成，成品 附加一份以前做的另外一份红烧肉，当时忘记炒糖色，只好使用老抽上色，并且加了很多盐，味道也是非常棒（由于变成了咸口味，感觉味道和卤肉差不多）注意事项1、炒糖色这一步骤是为了替代生抽给红烧肉上色，那种棕色或者深红色是糖遇热产生的颜色，很好看（一开始炒完颜色很好看，然后加水煮的时候可能看不出来了，没关系，等最后收汁的时候颜色会回来的），此外，炒糖色这一步骤也可以不进行，直接在煮的时候放糖（增甜增鲜），然后最后加老抽上色即可；2、大火煮开之后一定要转小火，慢慢煮（小火才能把肉煮的又透又软，达到入口即化的地步），就和普通的煲汤的火力一样，煮够 50 分钟；3、其实只有在五花肉的质量非常好的情况下，才能免除焯水，一般的五花肉肥油太多，不提前处理一下做出来的红烧肉非常油腻。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>红烧肉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[玉米胡萝卜排骨汤做法总结]]></title>
    <url>%2F2018053001.html</url>
    <content type="text"><![CDATA[排骨汤，是一道做法非常简单的汤，需要的只是新鲜的食材与足够的耐心而已。除了排骨，还可以增加玉米、胡萝卜这两种配菜，以增加排骨汤的甘甜与鲜美。本文就讲述玉米胡萝卜排骨汤的做法总结。食材准备 用最少的食材做排骨汤，才能做成真正的排骨汤，不需要各种配料，也不需要各种调味料，最终炖出来的排骨汤才能有排骨的鲜美。以下的食材分量大约 3 人份：排骨 300 克（排骨越好汤越好，我用过各种价格的排骨，12 元 / 斤 - 40 元 / 斤）甜玉米 1 根（稍微大一点嫩一点最好，买玉米的时候可以品尝一粒生的）姜片 2-3 片（不能太多，或者也可以不放）胡萝卜一根 食用盐适量 制作步骤 不计算食材的准备，从开火到关火总耗时预计 70 分钟；1、清洗排骨，稍微用水冲洗一下，然后在冷水中浸泡 10 分钟 - 20 分钟，目的是去除杂质与血水，但是油脂仍然保留，一般购买排骨的时候会让卖菜的帮忙剁好，否则自己用菜刀处理很麻烦；2、清洗玉米，切片，可以切薄一点，大概每片的厚度是 3 粒玉米的距离，玉米很难切，所以菜刀一定要使用锋利点的，否则会损坏玉米粒的，如果购买的玉米很新鲜，玉米棒里面也是很甜的，有助于增加排骨汤的甘甜；3、清洗胡萝卜，去皮，切片，注意最好去皮，否则最终部分胡萝卜皮会混在汤里，影响汤的品质；4、姜片，准备 2-3 片即可；5、汤锅准备好，加水，加姜片，冷水就下排骨，开大火煮，水开后立马转小火，把水表面的浮沫撇去，小火继续煮 30 分钟；6、步骤 5 的 30 分钟后，加玉米片，继续小火煮 20 分钟；7、步骤 6 的 20 分钟后，加胡萝卜片，继续小火煮 10 分钟；8、加盐调味，关火；一锅排骨汤 一碗排骨汤 以前做的另外一锅排骨汤，当时买的排骨 38 元 / 斤 注意事项1、排骨不能洗的太彻底，比如洗排骨的时候用力搓，不仅洗掉了血水，还把油脂洗没了，这样会导致汤里面丧失了香味；2、如果选择了排骨焯水（或者是选择了先在水里煮一下），切记不要焯太久（或者煮太久），10-30 秒即可，目的只是去除血水，否则也会流失油脂；3、为了省事，我一般不会对排骨怎么清洗，稍微用水冲一下，确保没有杂质与血水即可，然后直接煮，但是切记煮开后立即用勺子撇掉浮沫（油脂与碎渣），否则最终的汤会浑浊，而且没有香味；4、玉米一定要选择甜玉米，嫩的最好；5、时间一定要控制好，总计 70 分钟，也要注意火力的控制，除了一开始是大火，后面的 60 分钟全部是小火；6、如果只想喝汤，排骨可以用来做红烧排骨、糖醋排骨、酱排骨等等。]]></content>
      <categories>
        <category>菜谱</category>
      </categories>
      <tags>
        <tag>排骨汤</tag>
        <tag>玉米排骨汤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[注册 Facebook Twitter Tumblr 遇到的问题]]></title>
    <url>%2F2018020101.html</url>
    <content type="text"><![CDATA[本文讲述注册使用 Facebook、Twitter、Tumblr 等社交账号的过程、遇到的问题、解决的办法，给自己留一个备份，同时也可能给大家带去一丝方便。FacebookTwitter注册 注册 Twitter 帐号，首先需要一个邮箱帐号，或者手机号，进入注册首页，进行信息填写 注册页 ，填写完成后，接下来也就是常规流程，发送短信验证码、语音验证码、邮箱激活链接等，基本没什么问题。 绑定手机号问题 由于我选择的是 Google 邮箱注册，注册完成之后正常登录，但是进入不到主页面，就被绑定手机号页面拦截了，一直提示需要添加一个手机号，要不然就在当前页面，什么也做不了，除非退出。但是呢，诡异的是我使用自己的手机号进行绑定时，提示错误：由于技术问题，无法完成当前的请求，请重试（Due to a technical issue, we couldn’t complete this request. Please try again.），我怀疑是因为中国的手机号无法进行绑定。上网搜索了一下资料，果然是这个原因，大家都建议一开始直接使用手机号注册，不要使用邮箱注册，就不会有这个问题了。接下来没有办法了，只能尝试寻找可行的办法，毕竟邮箱已经注册过了，不想浪费。绑定手机号解决方案尝试 官方说当前帐号疑似是机器人（不是一个真实的人类），所以被冻结了，必须添加一个可用的手机号，用来接收验证码，才能证明当前帐号是人为注册的，才能进行接下来的操作。1、利用 Chrome 浏览器的开发者工具更改下拉列表的值，把日本的编号 81 改为 86，应用在页面上，实际操作发现不行，Twitter 验证的时候还会重新刷新下拉列表。在 Chrome 浏览器的对应页面，按下键盘的 F12 按键，就可以打开调试工具（或者点击鼠标的右键，选择检查），在 “Elements” 选项中可以看到源代码，更改表单里面的下拉列表的值，即可。2、去帮助中心，找客服，发送申诉邮件，说明你是一个真实的人，现在注册帐号被冻结了，Twitter 帮助中心 。在帮助中心选择 “Contact us”，进一步选择 “View all support topics”。 进入选择页面后，进一步选择 “Suspended or locked account”，对冻结或者锁定的帐号进行申诉处理。最终进入的页面就是这样的：申诉信息填写 。 这里面最主要的内容就是问题描述，请描述清楚你的问题，另外设备的选择按照自己的实际情况填写，全名和手机号也按照实际情况填写。此外，注意填写信息前需要登录帐号，否则页面是锁定状态，无法填写任何信息，而且登录后，大部分信息都是自动填充完成的，无需填写，只需要填写重要的几项内容。例如我填写的问题描述：1Account suspended.Could not unsuspend it through phone number.Pls help to unsuspend the account.Thanks.提交后会收到一封由 Twitter 官方技术支持（Twitter Support &#x73;&#x75;&#x70;&#112;&#111;&#114;&#x74;&#x40;&#x74;&#x77;&#105;&#116;&#116;&#x65;&#114;&#x2e;&#x63;&#111;&#109;）发送的邮件，告诉你应该怎么做，邮件内容如下。但是看内容也看不出来什么，只是说疑似机器人帐号，需要绑定手机号码，列出一系列步骤。其实我也是想做这一步，但是奈何中国的手机号码不支持。仔细看最后一句话，如果还有问题，可以直接回复此邮件并说明问题详细。接下来我又回复了一封邮件，说明遇到的问题，内容大概如下，解释说明自己是一个真实的人，但是由于手机号码是中国的，无法接收到验证码，请求解决：1234Hello, I try in this way,But i am in China,i can not receive messages. I am a human indeed,and my phone number is +86 1********06. best wishes.接下来就是等待官方的回复了（希望晚上睡一觉后明天有好消息）。在等待了一夜后，又过了半天时间（总共大概 17 个小时），收到了 Twitter 官方的回复，说我的帐号已经解冻，并解释了原因。这次回复等待了这么长时间，不像上次申诉回复那么快，说明很大可能是人工审核的，然后解冻了你的帐号，再回复这封通知邮件给我。不管怎样，帐号可以使用了。接下来为了保证不被封号，最好重新设置一下昵称，并且填写一些必要的信息：用户名（id）、头像、生日、国家、描述等，也可以关注一些其他推主。更改用户名在 “Settings and Privacy” 里面，由于用户名是唯一的（和 GitHub 的策略一样），所以常用的都被别人注册过了，自己要注意寻找，否则更改不了，显示被占用。更改昵称、头像、背景墙、描述等在 “Profile” 里面。流程总结：1、只针对使用 Google 邮箱注册的情况，注册后帐号被冻结，什么也做不了，绑定手机号又说不支持，只能通过申诉来解决；2、申诉的目的是为了解冻帐号，但是官方是自动回复，让绑定手机号，又回到了原地；3、在步骤 2 的基础上可以直接回复邮件（邮件中有提示），说明遇到的问题，等待将近一天就行了；（如果没有步骤 2，直接给官方技术支持发邮件，不知道行不行）4、步骤 3 官方回复的邮件中，提示说不要回复此邮件。（回复了应该也没人理）Tumblr]]></content>
      <categories>
        <category>知识改变生活</category>
      </categories>
      <tags>
        <tag>Facebook</tag>
        <tag>Twitter</tag>
        <tag>Tumblr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017102901.html</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.Quick StartCreate a new post1$ hexo new "My New Post"More info: WritingRun server1$ hexo serverMore info: ServerGenerate static files1$ hexo generateMore info: GeneratingDeploy to remote sites1$ hexo deployMore info: Deployment]]></content>
      <categories>
        <category>基础技术知识</category>
      </categories>
      <tags>
        <tag>建站</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 入门系列 0-- 初识 Hadoop]]></title>
    <url>%2F2017040101.html</url>
    <content type="text"><![CDATA[待开始整理 开始 今天是愚人节。]]></content>
      <categories>
        <category>Hadoop 从零基础到入门系列</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
</search>
